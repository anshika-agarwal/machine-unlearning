{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 12500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008,
      "grad_norm": 33.29362106323242,
      "learning_rate": 0.00019997333333333334,
      "loss": -11.7693,
      "step": 10
    },
    {
      "epoch": 0.0016,
      "grad_norm": 88.6811294555664,
      "learning_rate": 0.0001999466666666667,
      "loss": -19.5457,
      "step": 20
    },
    {
      "epoch": 0.0024,
      "grad_norm": 80.93682861328125,
      "learning_rate": 0.00019992000000000002,
      "loss": -32.6267,
      "step": 30
    },
    {
      "epoch": 0.0032,
      "grad_norm": 62.05029296875,
      "learning_rate": 0.00019989333333333333,
      "loss": -44.2532,
      "step": 40
    },
    {
      "epoch": 0.004,
      "grad_norm": 95.35437774658203,
      "learning_rate": 0.00019986666666666668,
      "loss": -54.6137,
      "step": 50
    },
    {
      "epoch": 0.0048,
      "grad_norm": 285.8903503417969,
      "learning_rate": 0.00019984,
      "loss": -67.3955,
      "step": 60
    },
    {
      "epoch": 0.0056,
      "grad_norm": 66.17249298095703,
      "learning_rate": 0.00019981333333333334,
      "loss": -77.2874,
      "step": 70
    },
    {
      "epoch": 0.0064,
      "grad_norm": 47.76024627685547,
      "learning_rate": 0.00019978666666666667,
      "loss": -83.9726,
      "step": 80
    },
    {
      "epoch": 0.0072,
      "grad_norm": 70.7253189086914,
      "learning_rate": 0.00019976000000000003,
      "loss": -89.4578,
      "step": 90
    },
    {
      "epoch": 0.008,
      "grad_norm": 62.08152770996094,
      "learning_rate": 0.00019973333333333335,
      "loss": -93.1687,
      "step": 100
    },
    {
      "epoch": 0.0088,
      "grad_norm": 36.339698791503906,
      "learning_rate": 0.00019970666666666668,
      "loss": -97.4488,
      "step": 110
    },
    {
      "epoch": 0.0096,
      "grad_norm": 47.71453857421875,
      "learning_rate": 0.00019968,
      "loss": -98.5919,
      "step": 120
    },
    {
      "epoch": 0.0104,
      "grad_norm": 56.312496185302734,
      "learning_rate": 0.00019965333333333334,
      "loss": -100.9339,
      "step": 130
    },
    {
      "epoch": 0.0112,
      "grad_norm": 55.961978912353516,
      "learning_rate": 0.00019962666666666667,
      "loss": -102.526,
      "step": 140
    },
    {
      "epoch": 0.012,
      "grad_norm": 50.273353576660156,
      "learning_rate": 0.0001996,
      "loss": -102.9351,
      "step": 150
    },
    {
      "epoch": 0.0128,
      "grad_norm": 63.39498519897461,
      "learning_rate": 0.00019957333333333336,
      "loss": -104.2318,
      "step": 160
    },
    {
      "epoch": 0.0136,
      "grad_norm": 80.68345642089844,
      "learning_rate": 0.00019954666666666669,
      "loss": -105.9233,
      "step": 170
    },
    {
      "epoch": 0.0144,
      "grad_norm": 101.63463592529297,
      "learning_rate": 0.00019952000000000001,
      "loss": -105.9457,
      "step": 180
    },
    {
      "epoch": 0.0152,
      "grad_norm": 99.14067840576172,
      "learning_rate": 0.00019949333333333334,
      "loss": -105.4273,
      "step": 190
    },
    {
      "epoch": 0.016,
      "grad_norm": 98.58280944824219,
      "learning_rate": 0.00019946666666666667,
      "loss": -107.6783,
      "step": 200
    },
    {
      "epoch": 0.0168,
      "grad_norm": 71.8197021484375,
      "learning_rate": 0.00019944,
      "loss": -108.0865,
      "step": 210
    },
    {
      "epoch": 0.0176,
      "grad_norm": 89.12177276611328,
      "learning_rate": 0.00019941333333333333,
      "loss": -108.4013,
      "step": 220
    },
    {
      "epoch": 0.0184,
      "grad_norm": 96.14568328857422,
      "learning_rate": 0.0001993866666666667,
      "loss": -108.9349,
      "step": 230
    },
    {
      "epoch": 0.0192,
      "grad_norm": 58.35210418701172,
      "learning_rate": 0.00019936000000000002,
      "loss": -110.3853,
      "step": 240
    },
    {
      "epoch": 0.02,
      "grad_norm": 63.979820251464844,
      "learning_rate": 0.00019933333333333334,
      "loss": -109.618,
      "step": 250
    },
    {
      "epoch": 0.0208,
      "grad_norm": 89.14130401611328,
      "learning_rate": 0.00019930666666666667,
      "loss": -110.2455,
      "step": 260
    },
    {
      "epoch": 0.0216,
      "grad_norm": 87.4437026977539,
      "learning_rate": 0.00019928,
      "loss": -111.7455,
      "step": 270
    },
    {
      "epoch": 0.0224,
      "grad_norm": 106.41680908203125,
      "learning_rate": 0.00019925333333333333,
      "loss": -110.5917,
      "step": 280
    },
    {
      "epoch": 0.0232,
      "grad_norm": 72.2378921508789,
      "learning_rate": 0.00019922666666666666,
      "loss": -110.4584,
      "step": 290
    },
    {
      "epoch": 0.024,
      "grad_norm": 90.60639953613281,
      "learning_rate": 0.00019920000000000002,
      "loss": -111.878,
      "step": 300
    },
    {
      "epoch": 0.0248,
      "grad_norm": 63.72541046142578,
      "learning_rate": 0.00019917333333333335,
      "loss": -110.814,
      "step": 310
    },
    {
      "epoch": 0.0256,
      "grad_norm": 112.77288818359375,
      "learning_rate": 0.00019914666666666668,
      "loss": -111.3397,
      "step": 320
    },
    {
      "epoch": 0.0264,
      "grad_norm": 91.1357650756836,
      "learning_rate": 0.00019912,
      "loss": -111.671,
      "step": 330
    },
    {
      "epoch": 0.0272,
      "grad_norm": 63.38945007324219,
      "learning_rate": 0.00019909333333333336,
      "loss": -112.2091,
      "step": 340
    },
    {
      "epoch": 0.028,
      "grad_norm": 59.81158447265625,
      "learning_rate": 0.00019906666666666666,
      "loss": -111.018,
      "step": 350
    },
    {
      "epoch": 0.0288,
      "grad_norm": 75.66832733154297,
      "learning_rate": 0.00019904,
      "loss": -112.6134,
      "step": 360
    },
    {
      "epoch": 0.0296,
      "grad_norm": 130.0281219482422,
      "learning_rate": 0.00019901333333333335,
      "loss": -112.6587,
      "step": 370
    },
    {
      "epoch": 0.0304,
      "grad_norm": 94.29589080810547,
      "learning_rate": 0.00019898666666666668,
      "loss": -111.5289,
      "step": 380
    },
    {
      "epoch": 0.0312,
      "grad_norm": 651.4603881835938,
      "learning_rate": 0.00019896,
      "loss": -112.362,
      "step": 390
    },
    {
      "epoch": 0.032,
      "grad_norm": 70.79876708984375,
      "learning_rate": 0.00019893333333333336,
      "loss": -112.3065,
      "step": 400
    },
    {
      "epoch": 0.0328,
      "grad_norm": 109.63276672363281,
      "learning_rate": 0.0001989066666666667,
      "loss": -112.4015,
      "step": 410
    },
    {
      "epoch": 0.0336,
      "grad_norm": 84.60466766357422,
      "learning_rate": 0.00019888,
      "loss": -112.1274,
      "step": 420
    },
    {
      "epoch": 0.0344,
      "grad_norm": 105.31868743896484,
      "learning_rate": 0.00019885333333333335,
      "loss": -113.687,
      "step": 430
    },
    {
      "epoch": 0.0352,
      "grad_norm": 59.35783767700195,
      "learning_rate": 0.00019882666666666668,
      "loss": -112.9827,
      "step": 440
    },
    {
      "epoch": 0.036,
      "grad_norm": 78.44389343261719,
      "learning_rate": 0.0001988,
      "loss": -113.8208,
      "step": 450
    },
    {
      "epoch": 0.0368,
      "grad_norm": 97.66646575927734,
      "learning_rate": 0.00019877333333333334,
      "loss": -114.2702,
      "step": 460
    },
    {
      "epoch": 0.0376,
      "grad_norm": 152.36741638183594,
      "learning_rate": 0.0001987466666666667,
      "loss": -111.9326,
      "step": 470
    },
    {
      "epoch": 0.0384,
      "grad_norm": 124.24365234375,
      "learning_rate": 0.00019872000000000002,
      "loss": -113.9961,
      "step": 480
    },
    {
      "epoch": 0.0392,
      "grad_norm": 90.356689453125,
      "learning_rate": 0.00019869333333333335,
      "loss": -114.2804,
      "step": 490
    },
    {
      "epoch": 0.04,
      "grad_norm": 77.0312271118164,
      "learning_rate": 0.00019866666666666668,
      "loss": -113.9764,
      "step": 500
    },
    {
      "epoch": 0.0408,
      "grad_norm": 69.40702056884766,
      "learning_rate": 0.00019864,
      "loss": -113.8124,
      "step": 510
    },
    {
      "epoch": 0.0416,
      "grad_norm": 87.39144134521484,
      "learning_rate": 0.00019861333333333334,
      "loss": -114.4427,
      "step": 520
    },
    {
      "epoch": 0.0424,
      "grad_norm": 62.119869232177734,
      "learning_rate": 0.00019858666666666667,
      "loss": -114.0859,
      "step": 530
    },
    {
      "epoch": 0.0432,
      "grad_norm": 77.51637268066406,
      "learning_rate": 0.00019856000000000002,
      "loss": -113.9016,
      "step": 540
    },
    {
      "epoch": 0.044,
      "grad_norm": 71.01506805419922,
      "learning_rate": 0.00019853333333333335,
      "loss": -115.0679,
      "step": 550
    },
    {
      "epoch": 0.0448,
      "grad_norm": 92.6697998046875,
      "learning_rate": 0.00019850666666666668,
      "loss": -113.6978,
      "step": 560
    },
    {
      "epoch": 0.0456,
      "grad_norm": 92.64653015136719,
      "learning_rate": 0.00019848,
      "loss": -115.451,
      "step": 570
    },
    {
      "epoch": 0.0464,
      "grad_norm": 104.21144104003906,
      "learning_rate": 0.00019845333333333334,
      "loss": -113.819,
      "step": 580
    },
    {
      "epoch": 0.0472,
      "grad_norm": 160.2574462890625,
      "learning_rate": 0.00019842666666666667,
      "loss": -114.2994,
      "step": 590
    },
    {
      "epoch": 0.048,
      "grad_norm": 77.76377868652344,
      "learning_rate": 0.0001984,
      "loss": -114.3774,
      "step": 600
    },
    {
      "epoch": 0.0488,
      "grad_norm": 80.45144653320312,
      "learning_rate": 0.00019837333333333335,
      "loss": -115.2262,
      "step": 610
    },
    {
      "epoch": 0.0496,
      "grad_norm": 127.11882019042969,
      "learning_rate": 0.00019834666666666668,
      "loss": -115.102,
      "step": 620
    },
    {
      "epoch": 0.0504,
      "grad_norm": 82.68360900878906,
      "learning_rate": 0.00019832,
      "loss": -114.2812,
      "step": 630
    },
    {
      "epoch": 0.0512,
      "grad_norm": 90.7738037109375,
      "learning_rate": 0.00019829333333333334,
      "loss": -114.5657,
      "step": 640
    },
    {
      "epoch": 0.052,
      "grad_norm": 100.32475280761719,
      "learning_rate": 0.00019826666666666667,
      "loss": -115.4703,
      "step": 650
    },
    {
      "epoch": 0.0528,
      "grad_norm": 94.75779724121094,
      "learning_rate": 0.00019824,
      "loss": -114.7218,
      "step": 660
    },
    {
      "epoch": 0.0536,
      "grad_norm": 85.77596282958984,
      "learning_rate": 0.00019821333333333333,
      "loss": -114.2901,
      "step": 670
    },
    {
      "epoch": 0.0544,
      "grad_norm": 81.40138244628906,
      "learning_rate": 0.00019818666666666668,
      "loss": -115.8699,
      "step": 680
    },
    {
      "epoch": 0.0552,
      "grad_norm": 126.37874603271484,
      "learning_rate": 0.00019816000000000001,
      "loss": -115.755,
      "step": 690
    },
    {
      "epoch": 0.056,
      "grad_norm": 146.80467224121094,
      "learning_rate": 0.00019813333333333334,
      "loss": -114.5479,
      "step": 700
    },
    {
      "epoch": 0.0568,
      "grad_norm": 90.47672271728516,
      "learning_rate": 0.00019810666666666667,
      "loss": -114.4874,
      "step": 710
    },
    {
      "epoch": 0.0576,
      "grad_norm": 178.48268127441406,
      "learning_rate": 0.00019808,
      "loss": -114.075,
      "step": 720
    },
    {
      "epoch": 0.0584,
      "grad_norm": 104.98505401611328,
      "learning_rate": 0.00019805333333333333,
      "loss": -115.8809,
      "step": 730
    },
    {
      "epoch": 0.0592,
      "grad_norm": 131.71875,
      "learning_rate": 0.00019802666666666666,
      "loss": -113.7449,
      "step": 740
    },
    {
      "epoch": 0.06,
      "grad_norm": 96.82987976074219,
      "learning_rate": 0.00019800000000000002,
      "loss": -114.128,
      "step": 750
    },
    {
      "epoch": 0.0608,
      "grad_norm": 136.55215454101562,
      "learning_rate": 0.00019797333333333334,
      "loss": -114.9512,
      "step": 760
    },
    {
      "epoch": 0.0616,
      "grad_norm": 78.0204086303711,
      "learning_rate": 0.00019794666666666667,
      "loss": -114.3488,
      "step": 770
    },
    {
      "epoch": 0.0624,
      "grad_norm": 79.91584014892578,
      "learning_rate": 0.00019792000000000003,
      "loss": -115.168,
      "step": 780
    },
    {
      "epoch": 0.0632,
      "grad_norm": 133.26724243164062,
      "learning_rate": 0.00019789333333333336,
      "loss": -115.1987,
      "step": 790
    },
    {
      "epoch": 0.064,
      "grad_norm": 118.00567626953125,
      "learning_rate": 0.00019786666666666666,
      "loss": -114.4543,
      "step": 800
    },
    {
      "epoch": 0.0648,
      "grad_norm": 162.2584991455078,
      "learning_rate": 0.00019784,
      "loss": -115.0464,
      "step": 810
    },
    {
      "epoch": 0.0656,
      "grad_norm": 154.41412353515625,
      "learning_rate": 0.00019781333333333335,
      "loss": -114.2008,
      "step": 820
    },
    {
      "epoch": 0.0664,
      "grad_norm": 93.78880310058594,
      "learning_rate": 0.00019778666666666667,
      "loss": -114.3747,
      "step": 830
    },
    {
      "epoch": 0.0672,
      "grad_norm": 185.42837524414062,
      "learning_rate": 0.00019776,
      "loss": -113.1197,
      "step": 840
    },
    {
      "epoch": 0.068,
      "grad_norm": 109.28009033203125,
      "learning_rate": 0.00019773333333333336,
      "loss": -114.077,
      "step": 850
    },
    {
      "epoch": 0.0688,
      "grad_norm": 81.7590560913086,
      "learning_rate": 0.0001977066666666667,
      "loss": -114.7538,
      "step": 860
    },
    {
      "epoch": 0.0696,
      "grad_norm": 103.29158782958984,
      "learning_rate": 0.00019768,
      "loss": -116.5384,
      "step": 870
    },
    {
      "epoch": 0.0704,
      "grad_norm": 95.2335205078125,
      "learning_rate": 0.00019765333333333335,
      "loss": -115.241,
      "step": 880
    },
    {
      "epoch": 0.0712,
      "grad_norm": 107.10286712646484,
      "learning_rate": 0.00019762666666666668,
      "loss": -114.4508,
      "step": 890
    },
    {
      "epoch": 0.072,
      "grad_norm": 95.78987121582031,
      "learning_rate": 0.0001976,
      "loss": -115.3888,
      "step": 900
    },
    {
      "epoch": 0.0728,
      "grad_norm": 101.52413940429688,
      "learning_rate": 0.00019757333333333333,
      "loss": -114.9042,
      "step": 910
    },
    {
      "epoch": 0.0736,
      "grad_norm": 93.3606948852539,
      "learning_rate": 0.0001975466666666667,
      "loss": -115.4693,
      "step": 920
    },
    {
      "epoch": 0.0744,
      "grad_norm": 77.70208740234375,
      "learning_rate": 0.00019752000000000002,
      "loss": -115.6664,
      "step": 930
    },
    {
      "epoch": 0.0752,
      "grad_norm": 129.92074584960938,
      "learning_rate": 0.00019749333333333335,
      "loss": -114.164,
      "step": 940
    },
    {
      "epoch": 0.076,
      "grad_norm": 129.43418884277344,
      "learning_rate": 0.00019746666666666668,
      "loss": -115.4093,
      "step": 950
    },
    {
      "epoch": 0.0768,
      "grad_norm": 161.49240112304688,
      "learning_rate": 0.00019744,
      "loss": -114.6987,
      "step": 960
    },
    {
      "epoch": 0.0776,
      "grad_norm": 95.78922271728516,
      "learning_rate": 0.00019741333333333334,
      "loss": -115.1326,
      "step": 970
    },
    {
      "epoch": 0.0784,
      "grad_norm": 137.501708984375,
      "learning_rate": 0.00019738666666666667,
      "loss": -117.0616,
      "step": 980
    },
    {
      "epoch": 0.0792,
      "grad_norm": 90.75619506835938,
      "learning_rate": 0.00019736000000000002,
      "loss": -114.6751,
      "step": 990
    },
    {
      "epoch": 0.08,
      "grad_norm": 99.48433685302734,
      "learning_rate": 0.00019733333333333335,
      "loss": -114.6571,
      "step": 1000
    },
    {
      "epoch": 0.0808,
      "grad_norm": 103.70498657226562,
      "learning_rate": 0.00019730666666666668,
      "loss": -114.7998,
      "step": 1010
    },
    {
      "epoch": 0.0816,
      "grad_norm": 115.91633605957031,
      "learning_rate": 0.00019728,
      "loss": -114.9305,
      "step": 1020
    },
    {
      "epoch": 0.0824,
      "grad_norm": 97.43905639648438,
      "learning_rate": 0.00019725333333333334,
      "loss": -114.9948,
      "step": 1030
    },
    {
      "epoch": 0.0832,
      "grad_norm": 98.72494506835938,
      "learning_rate": 0.00019722666666666667,
      "loss": -114.7399,
      "step": 1040
    },
    {
      "epoch": 0.084,
      "grad_norm": 100.03331756591797,
      "learning_rate": 0.0001972,
      "loss": -115.4478,
      "step": 1050
    },
    {
      "epoch": 0.0848,
      "grad_norm": 64.29741668701172,
      "learning_rate": 0.00019717333333333335,
      "loss": -114.6254,
      "step": 1060
    },
    {
      "epoch": 0.0856,
      "grad_norm": 100.44302368164062,
      "learning_rate": 0.00019714666666666668,
      "loss": -115.1388,
      "step": 1070
    },
    {
      "epoch": 0.0864,
      "grad_norm": 97.56311798095703,
      "learning_rate": 0.00019712,
      "loss": -116.1743,
      "step": 1080
    },
    {
      "epoch": 0.0872,
      "grad_norm": 112.4156265258789,
      "learning_rate": 0.00019709333333333334,
      "loss": -114.7417,
      "step": 1090
    },
    {
      "epoch": 0.088,
      "grad_norm": 78.50469970703125,
      "learning_rate": 0.00019706666666666667,
      "loss": -115.5503,
      "step": 1100
    },
    {
      "epoch": 0.0888,
      "grad_norm": 88.62971496582031,
      "learning_rate": 0.00019704,
      "loss": -116.8199,
      "step": 1110
    },
    {
      "epoch": 0.0896,
      "grad_norm": 82.18741607666016,
      "learning_rate": 0.00019701333333333333,
      "loss": -115.5558,
      "step": 1120
    },
    {
      "epoch": 0.0904,
      "grad_norm": 68.86174011230469,
      "learning_rate": 0.00019698666666666668,
      "loss": -116.2156,
      "step": 1130
    },
    {
      "epoch": 0.0912,
      "grad_norm": 78.0696029663086,
      "learning_rate": 0.00019696,
      "loss": -114.6772,
      "step": 1140
    },
    {
      "epoch": 0.092,
      "grad_norm": 124.79463958740234,
      "learning_rate": 0.00019693333333333334,
      "loss": -115.2518,
      "step": 1150
    },
    {
      "epoch": 0.0928,
      "grad_norm": 99.35363006591797,
      "learning_rate": 0.0001969066666666667,
      "loss": -117.0764,
      "step": 1160
    },
    {
      "epoch": 0.0936,
      "grad_norm": 109.3826675415039,
      "learning_rate": 0.00019688000000000003,
      "loss": -115.577,
      "step": 1170
    },
    {
      "epoch": 0.0944,
      "grad_norm": 134.3365936279297,
      "learning_rate": 0.00019685333333333333,
      "loss": -115.8222,
      "step": 1180
    },
    {
      "epoch": 0.0952,
      "grad_norm": 120.15205383300781,
      "learning_rate": 0.00019682666666666666,
      "loss": -116.0396,
      "step": 1190
    },
    {
      "epoch": 0.096,
      "grad_norm": 141.12403869628906,
      "learning_rate": 0.0001968,
      "loss": -116.0833,
      "step": 1200
    },
    {
      "epoch": 0.0968,
      "grad_norm": 115.08223724365234,
      "learning_rate": 0.00019677333333333334,
      "loss": -114.9823,
      "step": 1210
    },
    {
      "epoch": 0.0976,
      "grad_norm": 116.84838104248047,
      "learning_rate": 0.00019674666666666667,
      "loss": -114.8518,
      "step": 1220
    },
    {
      "epoch": 0.0984,
      "grad_norm": 108.42768859863281,
      "learning_rate": 0.00019672000000000003,
      "loss": -115.5387,
      "step": 1230
    },
    {
      "epoch": 0.0992,
      "grad_norm": 105.79828643798828,
      "learning_rate": 0.00019669333333333336,
      "loss": -116.0635,
      "step": 1240
    },
    {
      "epoch": 0.1,
      "grad_norm": 110.59992980957031,
      "learning_rate": 0.00019666666666666666,
      "loss": -115.2531,
      "step": 1250
    },
    {
      "epoch": 0.1008,
      "grad_norm": 154.4980926513672,
      "learning_rate": 0.00019664000000000001,
      "loss": -116.1368,
      "step": 1260
    },
    {
      "epoch": 0.1016,
      "grad_norm": 151.39822387695312,
      "learning_rate": 0.00019661333333333334,
      "loss": -115.4222,
      "step": 1270
    },
    {
      "epoch": 0.1024,
      "grad_norm": 121.9175033569336,
      "learning_rate": 0.00019658666666666667,
      "loss": -115.5649,
      "step": 1280
    },
    {
      "epoch": 0.1032,
      "grad_norm": 98.10501098632812,
      "learning_rate": 0.00019656,
      "loss": -116.006,
      "step": 1290
    },
    {
      "epoch": 0.104,
      "grad_norm": 111.61466217041016,
      "learning_rate": 0.00019653333333333336,
      "loss": -116.8658,
      "step": 1300
    },
    {
      "epoch": 0.1048,
      "grad_norm": 108.80281829833984,
      "learning_rate": 0.0001965066666666667,
      "loss": -116.3845,
      "step": 1310
    },
    {
      "epoch": 0.1056,
      "grad_norm": 99.57032775878906,
      "learning_rate": 0.00019648000000000002,
      "loss": -114.9959,
      "step": 1320
    },
    {
      "epoch": 0.1064,
      "grad_norm": 87.7214584350586,
      "learning_rate": 0.00019645333333333335,
      "loss": -116.0236,
      "step": 1330
    },
    {
      "epoch": 0.1072,
      "grad_norm": 98.60572052001953,
      "learning_rate": 0.00019642666666666667,
      "loss": -116.9043,
      "step": 1340
    },
    {
      "epoch": 0.108,
      "grad_norm": 70.14616394042969,
      "learning_rate": 0.0001964,
      "loss": -116.6236,
      "step": 1350
    },
    {
      "epoch": 0.1088,
      "grad_norm": 92.24253845214844,
      "learning_rate": 0.00019637333333333333,
      "loss": -116.6087,
      "step": 1360
    },
    {
      "epoch": 0.1096,
      "grad_norm": 71.57415771484375,
      "learning_rate": 0.0001963466666666667,
      "loss": -115.9293,
      "step": 1370
    },
    {
      "epoch": 0.1104,
      "grad_norm": 122.57061767578125,
      "learning_rate": 0.00019632000000000002,
      "loss": -116.9844,
      "step": 1380
    },
    {
      "epoch": 0.1112,
      "grad_norm": 81.31510925292969,
      "learning_rate": 0.00019629333333333335,
      "loss": -116.5014,
      "step": 1390
    },
    {
      "epoch": 0.112,
      "grad_norm": 71.13145446777344,
      "learning_rate": 0.00019626666666666668,
      "loss": -114.9878,
      "step": 1400
    },
    {
      "epoch": 0.1128,
      "grad_norm": 96.53685760498047,
      "learning_rate": 0.00019624,
      "loss": -116.9482,
      "step": 1410
    },
    {
      "epoch": 0.1136,
      "grad_norm": 88.7847900390625,
      "learning_rate": 0.00019621333333333333,
      "loss": -115.7338,
      "step": 1420
    },
    {
      "epoch": 0.1144,
      "grad_norm": 88.48904418945312,
      "learning_rate": 0.00019618666666666666,
      "loss": -116.507,
      "step": 1430
    },
    {
      "epoch": 0.1152,
      "grad_norm": 108.9176254272461,
      "learning_rate": 0.00019616000000000002,
      "loss": -116.542,
      "step": 1440
    },
    {
      "epoch": 0.116,
      "grad_norm": 94.55103302001953,
      "learning_rate": 0.00019613333333333335,
      "loss": -116.511,
      "step": 1450
    },
    {
      "epoch": 0.1168,
      "grad_norm": 117.89035034179688,
      "learning_rate": 0.00019610666666666668,
      "loss": -115.9994,
      "step": 1460
    },
    {
      "epoch": 0.1176,
      "grad_norm": 86.69703674316406,
      "learning_rate": 0.00019608,
      "loss": -116.4851,
      "step": 1470
    },
    {
      "epoch": 0.1184,
      "grad_norm": 87.14048767089844,
      "learning_rate": 0.00019605333333333334,
      "loss": -116.1789,
      "step": 1480
    },
    {
      "epoch": 0.1192,
      "grad_norm": 90.2332992553711,
      "learning_rate": 0.00019602666666666666,
      "loss": -115.4658,
      "step": 1490
    },
    {
      "epoch": 0.12,
      "grad_norm": 117.67054748535156,
      "learning_rate": 0.000196,
      "loss": -116.1913,
      "step": 1500
    },
    {
      "epoch": 0.1208,
      "grad_norm": 163.81149291992188,
      "learning_rate": 0.00019597333333333335,
      "loss": -117.0726,
      "step": 1510
    },
    {
      "epoch": 0.1216,
      "grad_norm": 134.6744842529297,
      "learning_rate": 0.00019594666666666668,
      "loss": -116.0074,
      "step": 1520
    },
    {
      "epoch": 0.1224,
      "grad_norm": 119.5706558227539,
      "learning_rate": 0.00019592,
      "loss": -115.8861,
      "step": 1530
    },
    {
      "epoch": 0.1232,
      "grad_norm": 142.79071044921875,
      "learning_rate": 0.00019589333333333336,
      "loss": -116.7827,
      "step": 1540
    },
    {
      "epoch": 0.124,
      "grad_norm": 105.36556243896484,
      "learning_rate": 0.00019586666666666667,
      "loss": -117.1157,
      "step": 1550
    },
    {
      "epoch": 0.1248,
      "grad_norm": 75.69544219970703,
      "learning_rate": 0.00019584,
      "loss": -114.6769,
      "step": 1560
    },
    {
      "epoch": 0.1256,
      "grad_norm": 95.05902862548828,
      "learning_rate": 0.00019581333333333332,
      "loss": -117.5647,
      "step": 1570
    },
    {
      "epoch": 0.1264,
      "grad_norm": 129.49606323242188,
      "learning_rate": 0.00019578666666666668,
      "loss": -116.6159,
      "step": 1580
    },
    {
      "epoch": 0.1272,
      "grad_norm": 111.02128601074219,
      "learning_rate": 0.00019576,
      "loss": -117.2219,
      "step": 1590
    },
    {
      "epoch": 0.128,
      "grad_norm": 122.70800018310547,
      "learning_rate": 0.00019573333333333334,
      "loss": -115.871,
      "step": 1600
    },
    {
      "epoch": 0.1288,
      "grad_norm": 85.11271667480469,
      "learning_rate": 0.0001957066666666667,
      "loss": -116.4287,
      "step": 1610
    },
    {
      "epoch": 0.1296,
      "grad_norm": 198.78150939941406,
      "learning_rate": 0.00019568000000000002,
      "loss": -116.5709,
      "step": 1620
    },
    {
      "epoch": 0.1304,
      "grad_norm": 117.51506042480469,
      "learning_rate": 0.00019565333333333333,
      "loss": -116.3797,
      "step": 1630
    },
    {
      "epoch": 0.1312,
      "grad_norm": 124.05500793457031,
      "learning_rate": 0.00019562666666666668,
      "loss": -115.9587,
      "step": 1640
    },
    {
      "epoch": 0.132,
      "grad_norm": 80.87190246582031,
      "learning_rate": 0.0001956,
      "loss": -115.9186,
      "step": 1650
    },
    {
      "epoch": 0.1328,
      "grad_norm": 84.8852310180664,
      "learning_rate": 0.00019557333333333334,
      "loss": -116.6733,
      "step": 1660
    },
    {
      "epoch": 0.1336,
      "grad_norm": 146.5045166015625,
      "learning_rate": 0.00019554666666666667,
      "loss": -114.6718,
      "step": 1670
    },
    {
      "epoch": 0.1344,
      "grad_norm": 68.75531005859375,
      "learning_rate": 0.00019552000000000003,
      "loss": -115.5139,
      "step": 1680
    },
    {
      "epoch": 0.1352,
      "grad_norm": 104.24674987792969,
      "learning_rate": 0.00019549333333333335,
      "loss": -115.1525,
      "step": 1690
    },
    {
      "epoch": 0.136,
      "grad_norm": 91.59434509277344,
      "learning_rate": 0.00019546666666666668,
      "loss": -116.5295,
      "step": 1700
    },
    {
      "epoch": 0.1368,
      "grad_norm": 155.1585693359375,
      "learning_rate": 0.00019544,
      "loss": -115.6815,
      "step": 1710
    },
    {
      "epoch": 0.1376,
      "grad_norm": 100.99142456054688,
      "learning_rate": 0.00019541333333333334,
      "loss": -116.4891,
      "step": 1720
    },
    {
      "epoch": 0.1384,
      "grad_norm": 120.42520141601562,
      "learning_rate": 0.00019538666666666667,
      "loss": -115.6239,
      "step": 1730
    },
    {
      "epoch": 0.1392,
      "grad_norm": 111.8515853881836,
      "learning_rate": 0.00019536,
      "loss": -116.2184,
      "step": 1740
    },
    {
      "epoch": 0.14,
      "grad_norm": 152.400390625,
      "learning_rate": 0.00019533333333333336,
      "loss": -116.7681,
      "step": 1750
    },
    {
      "epoch": 0.1408,
      "grad_norm": 92.07115936279297,
      "learning_rate": 0.00019530666666666669,
      "loss": -116.0309,
      "step": 1760
    },
    {
      "epoch": 0.1416,
      "grad_norm": 69.31975555419922,
      "learning_rate": 0.00019528000000000001,
      "loss": -115.8824,
      "step": 1770
    },
    {
      "epoch": 0.1424,
      "grad_norm": 81.69468688964844,
      "learning_rate": 0.00019525333333333334,
      "loss": -116.9399,
      "step": 1780
    },
    {
      "epoch": 0.1432,
      "grad_norm": 81.63911437988281,
      "learning_rate": 0.00019522666666666667,
      "loss": -117.6871,
      "step": 1790
    },
    {
      "epoch": 0.144,
      "grad_norm": 216.7011260986328,
      "learning_rate": 0.0001952,
      "loss": -116.508,
      "step": 1800
    },
    {
      "epoch": 0.1448,
      "grad_norm": 92.41412353515625,
      "learning_rate": 0.00019517333333333333,
      "loss": -116.3433,
      "step": 1810
    },
    {
      "epoch": 0.1456,
      "grad_norm": 100.36270904541016,
      "learning_rate": 0.0001951466666666667,
      "loss": -117.1633,
      "step": 1820
    },
    {
      "epoch": 0.1464,
      "grad_norm": 90.75164794921875,
      "learning_rate": 0.00019512000000000002,
      "loss": -116.9237,
      "step": 1830
    },
    {
      "epoch": 0.1472,
      "grad_norm": 104.2852783203125,
      "learning_rate": 0.00019509333333333334,
      "loss": -116.464,
      "step": 1840
    },
    {
      "epoch": 0.148,
      "grad_norm": 83.07512664794922,
      "learning_rate": 0.00019506666666666667,
      "loss": -115.2329,
      "step": 1850
    },
    {
      "epoch": 0.1488,
      "grad_norm": 128.6038818359375,
      "learning_rate": 0.00019504,
      "loss": -116.7028,
      "step": 1860
    },
    {
      "epoch": 0.1496,
      "grad_norm": 110.7585678100586,
      "learning_rate": 0.00019501333333333333,
      "loss": -117.946,
      "step": 1870
    },
    {
      "epoch": 0.1504,
      "grad_norm": 136.41221618652344,
      "learning_rate": 0.00019498666666666666,
      "loss": -115.3426,
      "step": 1880
    },
    {
      "epoch": 0.1512,
      "grad_norm": 79.72608184814453,
      "learning_rate": 0.00019496000000000002,
      "loss": -117.0876,
      "step": 1890
    },
    {
      "epoch": 0.152,
      "grad_norm": 122.6067123413086,
      "learning_rate": 0.00019493333333333335,
      "loss": -116.174,
      "step": 1900
    },
    {
      "epoch": 0.1528,
      "grad_norm": 111.66474151611328,
      "learning_rate": 0.00019490666666666668,
      "loss": -116.375,
      "step": 1910
    },
    {
      "epoch": 0.1536,
      "grad_norm": 123.6317367553711,
      "learning_rate": 0.00019488000000000003,
      "loss": -116.6466,
      "step": 1920
    },
    {
      "epoch": 0.1544,
      "grad_norm": 87.61900329589844,
      "learning_rate": 0.00019485333333333333,
      "loss": -117.6579,
      "step": 1930
    },
    {
      "epoch": 0.1552,
      "grad_norm": 82.45188903808594,
      "learning_rate": 0.00019482666666666666,
      "loss": -116.1819,
      "step": 1940
    },
    {
      "epoch": 0.156,
      "grad_norm": 103.1208267211914,
      "learning_rate": 0.0001948,
      "loss": -117.2946,
      "step": 1950
    },
    {
      "epoch": 0.1568,
      "grad_norm": 112.88037109375,
      "learning_rate": 0.00019477333333333335,
      "loss": -117.2511,
      "step": 1960
    },
    {
      "epoch": 0.1576,
      "grad_norm": 77.01392364501953,
      "learning_rate": 0.00019474666666666668,
      "loss": -116.9234,
      "step": 1970
    },
    {
      "epoch": 0.1584,
      "grad_norm": 74.04377746582031,
      "learning_rate": 0.00019472,
      "loss": -115.76,
      "step": 1980
    },
    {
      "epoch": 0.1592,
      "grad_norm": 73.6789321899414,
      "learning_rate": 0.00019469333333333336,
      "loss": -116.1822,
      "step": 1990
    },
    {
      "epoch": 0.16,
      "grad_norm": 94.78670501708984,
      "learning_rate": 0.0001946666666666667,
      "loss": -118.0996,
      "step": 2000
    },
    {
      "epoch": 0.1608,
      "grad_norm": 102.3202133178711,
      "learning_rate": 0.00019464,
      "loss": -116.683,
      "step": 2010
    },
    {
      "epoch": 0.1616,
      "grad_norm": 95.40208435058594,
      "learning_rate": 0.00019461333333333335,
      "loss": -115.7574,
      "step": 2020
    },
    {
      "epoch": 0.1624,
      "grad_norm": 100.09839630126953,
      "learning_rate": 0.00019458666666666668,
      "loss": -117.3755,
      "step": 2030
    },
    {
      "epoch": 0.1632,
      "grad_norm": 117.47175598144531,
      "learning_rate": 0.00019456,
      "loss": -116.6912,
      "step": 2040
    },
    {
      "epoch": 0.164,
      "grad_norm": 94.3270034790039,
      "learning_rate": 0.00019453333333333334,
      "loss": -116.4651,
      "step": 2050
    },
    {
      "epoch": 0.1648,
      "grad_norm": 103.73236846923828,
      "learning_rate": 0.0001945066666666667,
      "loss": -117.0672,
      "step": 2060
    },
    {
      "epoch": 0.1656,
      "grad_norm": 115.82574462890625,
      "learning_rate": 0.00019448000000000002,
      "loss": -117.8261,
      "step": 2070
    },
    {
      "epoch": 0.1664,
      "grad_norm": 75.38306427001953,
      "learning_rate": 0.00019445333333333332,
      "loss": -116.4981,
      "step": 2080
    },
    {
      "epoch": 0.1672,
      "grad_norm": 80.08241271972656,
      "learning_rate": 0.00019442666666666668,
      "loss": -116.2032,
      "step": 2090
    },
    {
      "epoch": 0.168,
      "grad_norm": 133.91014099121094,
      "learning_rate": 0.0001944,
      "loss": -117.7254,
      "step": 2100
    },
    {
      "epoch": 0.1688,
      "grad_norm": 95.9876480102539,
      "learning_rate": 0.00019437333333333334,
      "loss": -115.9967,
      "step": 2110
    },
    {
      "epoch": 0.1696,
      "grad_norm": 100.36714935302734,
      "learning_rate": 0.00019434666666666667,
      "loss": -117.9294,
      "step": 2120
    },
    {
      "epoch": 0.1704,
      "grad_norm": 106.18048858642578,
      "learning_rate": 0.00019432000000000002,
      "loss": -116.9587,
      "step": 2130
    },
    {
      "epoch": 0.1712,
      "grad_norm": 121.1848373413086,
      "learning_rate": 0.00019429333333333335,
      "loss": -115.8333,
      "step": 2140
    },
    {
      "epoch": 0.172,
      "grad_norm": 108.23075103759766,
      "learning_rate": 0.00019426666666666668,
      "loss": -118.0052,
      "step": 2150
    },
    {
      "epoch": 0.1728,
      "grad_norm": 95.86546325683594,
      "learning_rate": 0.00019424,
      "loss": -116.8358,
      "step": 2160
    },
    {
      "epoch": 0.1736,
      "grad_norm": 114.08641815185547,
      "learning_rate": 0.00019421333333333334,
      "loss": -116.7631,
      "step": 2170
    },
    {
      "epoch": 0.1744,
      "grad_norm": 66.86699676513672,
      "learning_rate": 0.00019418666666666667,
      "loss": -116.4661,
      "step": 2180
    },
    {
      "epoch": 0.1752,
      "grad_norm": 99.85564422607422,
      "learning_rate": 0.00019416,
      "loss": -116.3232,
      "step": 2190
    },
    {
      "epoch": 0.176,
      "grad_norm": 87.9434585571289,
      "learning_rate": 0.00019413333333333335,
      "loss": -117.6634,
      "step": 2200
    },
    {
      "epoch": 0.1768,
      "grad_norm": 110.09405517578125,
      "learning_rate": 0.00019410666666666668,
      "loss": -116.4121,
      "step": 2210
    },
    {
      "epoch": 0.1776,
      "grad_norm": 81.56073760986328,
      "learning_rate": 0.00019408,
      "loss": -116.647,
      "step": 2220
    },
    {
      "epoch": 0.1784,
      "grad_norm": 110.5086441040039,
      "learning_rate": 0.00019405333333333334,
      "loss": -116.3069,
      "step": 2230
    },
    {
      "epoch": 0.1792,
      "grad_norm": 96.45773315429688,
      "learning_rate": 0.00019402666666666667,
      "loss": -116.8556,
      "step": 2240
    },
    {
      "epoch": 0.18,
      "grad_norm": 115.26629638671875,
      "learning_rate": 0.000194,
      "loss": -115.8583,
      "step": 2250
    },
    {
      "epoch": 0.1808,
      "grad_norm": 111.51345825195312,
      "learning_rate": 0.00019397333333333333,
      "loss": -115.7597,
      "step": 2260
    },
    {
      "epoch": 0.1816,
      "grad_norm": 92.91079711914062,
      "learning_rate": 0.00019394666666666668,
      "loss": -116.8208,
      "step": 2270
    },
    {
      "epoch": 0.1824,
      "grad_norm": 81.81238555908203,
      "learning_rate": 0.00019392000000000001,
      "loss": -116.7149,
      "step": 2280
    },
    {
      "epoch": 0.1832,
      "grad_norm": 92.9188232421875,
      "learning_rate": 0.00019389333333333334,
      "loss": -116.3398,
      "step": 2290
    },
    {
      "epoch": 0.184,
      "grad_norm": 122.13356018066406,
      "learning_rate": 0.0001938666666666667,
      "loss": -115.5792,
      "step": 2300
    },
    {
      "epoch": 0.1848,
      "grad_norm": 113.79287719726562,
      "learning_rate": 0.00019384,
      "loss": -116.7411,
      "step": 2310
    },
    {
      "epoch": 0.1856,
      "grad_norm": 85.51744842529297,
      "learning_rate": 0.00019381333333333333,
      "loss": -116.8315,
      "step": 2320
    },
    {
      "epoch": 0.1864,
      "grad_norm": 110.90833282470703,
      "learning_rate": 0.00019378666666666666,
      "loss": -117.0648,
      "step": 2330
    },
    {
      "epoch": 0.1872,
      "grad_norm": 88.8200912475586,
      "learning_rate": 0.00019376000000000002,
      "loss": -116.8185,
      "step": 2340
    },
    {
      "epoch": 0.188,
      "grad_norm": 96.23922729492188,
      "learning_rate": 0.00019373333333333334,
      "loss": -116.912,
      "step": 2350
    },
    {
      "epoch": 0.1888,
      "grad_norm": 72.27816772460938,
      "learning_rate": 0.00019370666666666667,
      "loss": -117.1941,
      "step": 2360
    },
    {
      "epoch": 0.1896,
      "grad_norm": 95.32795715332031,
      "learning_rate": 0.00019368000000000003,
      "loss": -116.3282,
      "step": 2370
    },
    {
      "epoch": 0.1904,
      "grad_norm": 57.43355941772461,
      "learning_rate": 0.00019365333333333336,
      "loss": -117.7411,
      "step": 2380
    },
    {
      "epoch": 0.1912,
      "grad_norm": 112.93827056884766,
      "learning_rate": 0.00019362666666666666,
      "loss": -115.314,
      "step": 2390
    },
    {
      "epoch": 0.192,
      "grad_norm": 112.46293640136719,
      "learning_rate": 0.00019360000000000002,
      "loss": -117.9412,
      "step": 2400
    },
    {
      "epoch": 0.1928,
      "grad_norm": 117.99288177490234,
      "learning_rate": 0.00019357333333333335,
      "loss": -116.1822,
      "step": 2410
    },
    {
      "epoch": 0.1936,
      "grad_norm": 140.0335693359375,
      "learning_rate": 0.00019354666666666667,
      "loss": -116.8385,
      "step": 2420
    },
    {
      "epoch": 0.1944,
      "grad_norm": 96.07984161376953,
      "learning_rate": 0.00019352,
      "loss": -116.1797,
      "step": 2430
    },
    {
      "epoch": 0.1952,
      "grad_norm": 70.34224700927734,
      "learning_rate": 0.00019349333333333336,
      "loss": -117.2126,
      "step": 2440
    },
    {
      "epoch": 0.196,
      "grad_norm": 41.83427429199219,
      "learning_rate": 0.0001934666666666667,
      "loss": -116.769,
      "step": 2450
    },
    {
      "epoch": 0.1968,
      "grad_norm": 134.31002807617188,
      "learning_rate": 0.00019344,
      "loss": -117.0863,
      "step": 2460
    },
    {
      "epoch": 0.1976,
      "grad_norm": 73.14415740966797,
      "learning_rate": 0.00019341333333333335,
      "loss": -118.0746,
      "step": 2470
    },
    {
      "epoch": 0.1984,
      "grad_norm": 70.27688598632812,
      "learning_rate": 0.00019338666666666668,
      "loss": -116.6753,
      "step": 2480
    },
    {
      "epoch": 0.1992,
      "grad_norm": 84.46620178222656,
      "learning_rate": 0.00019336,
      "loss": -117.0398,
      "step": 2490
    },
    {
      "epoch": 0.2,
      "grad_norm": 90.57164764404297,
      "learning_rate": 0.00019333333333333333,
      "loss": -116.4089,
      "step": 2500
    },
    {
      "epoch": 0.2008,
      "grad_norm": 95.27186584472656,
      "learning_rate": 0.0001933066666666667,
      "loss": -116.9419,
      "step": 2510
    },
    {
      "epoch": 0.2016,
      "grad_norm": 105.79436492919922,
      "learning_rate": 0.00019328000000000002,
      "loss": -114.8314,
      "step": 2520
    },
    {
      "epoch": 0.2024,
      "grad_norm": 76.89646911621094,
      "learning_rate": 0.00019325333333333335,
      "loss": -116.8276,
      "step": 2530
    },
    {
      "epoch": 0.2032,
      "grad_norm": 101.41246032714844,
      "learning_rate": 0.00019322666666666668,
      "loss": -117.0191,
      "step": 2540
    },
    {
      "epoch": 0.204,
      "grad_norm": 66.12213897705078,
      "learning_rate": 0.0001932,
      "loss": -117.2247,
      "step": 2550
    },
    {
      "epoch": 0.2048,
      "grad_norm": 98.54371643066406,
      "learning_rate": 0.00019317333333333334,
      "loss": -117.6077,
      "step": 2560
    },
    {
      "epoch": 0.2056,
      "grad_norm": 107.4691390991211,
      "learning_rate": 0.00019314666666666667,
      "loss": -115.3108,
      "step": 2570
    },
    {
      "epoch": 0.2064,
      "grad_norm": 114.70890808105469,
      "learning_rate": 0.00019312000000000002,
      "loss": -116.9863,
      "step": 2580
    },
    {
      "epoch": 0.2072,
      "grad_norm": 135.0662078857422,
      "learning_rate": 0.00019309333333333335,
      "loss": -115.9663,
      "step": 2590
    },
    {
      "epoch": 0.208,
      "grad_norm": 98.53016662597656,
      "learning_rate": 0.00019306666666666668,
      "loss": -117.3134,
      "step": 2600
    },
    {
      "epoch": 0.2088,
      "grad_norm": 78.45132446289062,
      "learning_rate": 0.00019304,
      "loss": -117.065,
      "step": 2610
    },
    {
      "epoch": 0.2096,
      "grad_norm": 92.63209533691406,
      "learning_rate": 0.00019301333333333334,
      "loss": -117.3034,
      "step": 2620
    },
    {
      "epoch": 0.2104,
      "grad_norm": 79.07618713378906,
      "learning_rate": 0.00019298666666666667,
      "loss": -116.2444,
      "step": 2630
    },
    {
      "epoch": 0.2112,
      "grad_norm": 131.80841064453125,
      "learning_rate": 0.00019296,
      "loss": -116.0583,
      "step": 2640
    },
    {
      "epoch": 0.212,
      "grad_norm": 83.16053009033203,
      "learning_rate": 0.00019293333333333335,
      "loss": -116.8296,
      "step": 2650
    },
    {
      "epoch": 0.2128,
      "grad_norm": 90.58116149902344,
      "learning_rate": 0.00019290666666666668,
      "loss": -116.5885,
      "step": 2660
    },
    {
      "epoch": 0.2136,
      "grad_norm": 101.73612976074219,
      "learning_rate": 0.00019288,
      "loss": -117.2519,
      "step": 2670
    },
    {
      "epoch": 0.2144,
      "grad_norm": 63.02219772338867,
      "learning_rate": 0.00019285333333333334,
      "loss": -117.4051,
      "step": 2680
    },
    {
      "epoch": 0.2152,
      "grad_norm": 79.68089294433594,
      "learning_rate": 0.00019282666666666667,
      "loss": -116.7162,
      "step": 2690
    },
    {
      "epoch": 0.216,
      "grad_norm": 114.16490936279297,
      "learning_rate": 0.0001928,
      "loss": -116.7875,
      "step": 2700
    },
    {
      "epoch": 0.2168,
      "grad_norm": 67.18409729003906,
      "learning_rate": 0.00019277333333333333,
      "loss": -115.961,
      "step": 2710
    },
    {
      "epoch": 0.2176,
      "grad_norm": 99.48344421386719,
      "learning_rate": 0.00019274666666666668,
      "loss": -118.0206,
      "step": 2720
    },
    {
      "epoch": 0.2184,
      "grad_norm": 82.21388244628906,
      "learning_rate": 0.00019272,
      "loss": -116.6992,
      "step": 2730
    },
    {
      "epoch": 0.2192,
      "grad_norm": 90.50921630859375,
      "learning_rate": 0.00019269333333333334,
      "loss": -115.4256,
      "step": 2740
    },
    {
      "epoch": 0.22,
      "grad_norm": 91.21497344970703,
      "learning_rate": 0.0001926666666666667,
      "loss": -117.903,
      "step": 2750
    },
    {
      "epoch": 0.2208,
      "grad_norm": 82.18656921386719,
      "learning_rate": 0.00019264,
      "loss": -116.5947,
      "step": 2760
    },
    {
      "epoch": 0.2216,
      "grad_norm": 78.10005187988281,
      "learning_rate": 0.00019261333333333333,
      "loss": -117.01,
      "step": 2770
    },
    {
      "epoch": 0.2224,
      "grad_norm": 124.70529174804688,
      "learning_rate": 0.00019258666666666668,
      "loss": -117.577,
      "step": 2780
    },
    {
      "epoch": 0.2232,
      "grad_norm": 62.79027557373047,
      "learning_rate": 0.00019256,
      "loss": -117.6779,
      "step": 2790
    },
    {
      "epoch": 0.224,
      "grad_norm": 84.61380767822266,
      "learning_rate": 0.00019253333333333334,
      "loss": -117.1526,
      "step": 2800
    },
    {
      "epoch": 0.2248,
      "grad_norm": 135.67636108398438,
      "learning_rate": 0.00019250666666666667,
      "loss": -117.923,
      "step": 2810
    },
    {
      "epoch": 0.2256,
      "grad_norm": 97.17060089111328,
      "learning_rate": 0.00019248000000000003,
      "loss": -117.3728,
      "step": 2820
    },
    {
      "epoch": 0.2264,
      "grad_norm": 73.30915069580078,
      "learning_rate": 0.00019245333333333336,
      "loss": -117.3215,
      "step": 2830
    },
    {
      "epoch": 0.2272,
      "grad_norm": 64.7952880859375,
      "learning_rate": 0.00019242666666666666,
      "loss": -116.3177,
      "step": 2840
    },
    {
      "epoch": 0.228,
      "grad_norm": 103.53379821777344,
      "learning_rate": 0.00019240000000000001,
      "loss": -116.813,
      "step": 2850
    },
    {
      "epoch": 0.2288,
      "grad_norm": 86.75952911376953,
      "learning_rate": 0.00019237333333333334,
      "loss": -117.1092,
      "step": 2860
    },
    {
      "epoch": 0.2296,
      "grad_norm": 83.12065887451172,
      "learning_rate": 0.00019234666666666667,
      "loss": -115.701,
      "step": 2870
    },
    {
      "epoch": 0.2304,
      "grad_norm": 104.2449722290039,
      "learning_rate": 0.00019232,
      "loss": -117.5413,
      "step": 2880
    },
    {
      "epoch": 0.2312,
      "grad_norm": 136.86949157714844,
      "learning_rate": 0.00019229333333333336,
      "loss": -117.9015,
      "step": 2890
    },
    {
      "epoch": 0.232,
      "grad_norm": 84.23482513427734,
      "learning_rate": 0.0001922666666666667,
      "loss": -116.8577,
      "step": 2900
    },
    {
      "epoch": 0.2328,
      "grad_norm": 69.14227294921875,
      "learning_rate": 0.00019224000000000002,
      "loss": -116.9006,
      "step": 2910
    },
    {
      "epoch": 0.2336,
      "grad_norm": 118.1960220336914,
      "learning_rate": 0.00019221333333333335,
      "loss": -117.615,
      "step": 2920
    },
    {
      "epoch": 0.2344,
      "grad_norm": 93.56986236572266,
      "learning_rate": 0.00019218666666666667,
      "loss": -117.0039,
      "step": 2930
    },
    {
      "epoch": 0.2352,
      "grad_norm": 114.42630767822266,
      "learning_rate": 0.00019216,
      "loss": -116.9503,
      "step": 2940
    },
    {
      "epoch": 0.236,
      "grad_norm": 95.061279296875,
      "learning_rate": 0.00019213333333333333,
      "loss": -117.6494,
      "step": 2950
    },
    {
      "epoch": 0.2368,
      "grad_norm": 88.93921661376953,
      "learning_rate": 0.0001921066666666667,
      "loss": -117.2059,
      "step": 2960
    },
    {
      "epoch": 0.2376,
      "grad_norm": 72.67536163330078,
      "learning_rate": 0.00019208000000000002,
      "loss": -117.2899,
      "step": 2970
    },
    {
      "epoch": 0.2384,
      "grad_norm": 77.17446899414062,
      "learning_rate": 0.00019205333333333335,
      "loss": -117.7868,
      "step": 2980
    },
    {
      "epoch": 0.2392,
      "grad_norm": 82.28032684326172,
      "learning_rate": 0.00019202666666666668,
      "loss": -116.3687,
      "step": 2990
    },
    {
      "epoch": 0.24,
      "grad_norm": 114.46782684326172,
      "learning_rate": 0.000192,
      "loss": -117.6017,
      "step": 3000
    },
    {
      "epoch": 0.2408,
      "grad_norm": 77.67455291748047,
      "learning_rate": 0.00019197333333333333,
      "loss": -118.2281,
      "step": 3010
    },
    {
      "epoch": 0.2416,
      "grad_norm": 93.7262954711914,
      "learning_rate": 0.00019194666666666666,
      "loss": -116.5188,
      "step": 3020
    },
    {
      "epoch": 0.2424,
      "grad_norm": 133.79672241210938,
      "learning_rate": 0.00019192000000000002,
      "loss": -118.094,
      "step": 3030
    },
    {
      "epoch": 0.2432,
      "grad_norm": 98.30145263671875,
      "learning_rate": 0.00019189333333333335,
      "loss": -116.5864,
      "step": 3040
    },
    {
      "epoch": 0.244,
      "grad_norm": 90.44749450683594,
      "learning_rate": 0.00019186666666666668,
      "loss": -117.3334,
      "step": 3050
    },
    {
      "epoch": 0.2448,
      "grad_norm": 66.46038055419922,
      "learning_rate": 0.00019184,
      "loss": -116.9456,
      "step": 3060
    },
    {
      "epoch": 0.2456,
      "grad_norm": 83.99249267578125,
      "learning_rate": 0.00019181333333333334,
      "loss": -116.6775,
      "step": 3070
    },
    {
      "epoch": 0.2464,
      "grad_norm": 64.46127319335938,
      "learning_rate": 0.00019178666666666666,
      "loss": -117.0517,
      "step": 3080
    },
    {
      "epoch": 0.2472,
      "grad_norm": 80.6865463256836,
      "learning_rate": 0.00019176,
      "loss": -117.7743,
      "step": 3090
    },
    {
      "epoch": 0.248,
      "grad_norm": 64.85520935058594,
      "learning_rate": 0.00019173333333333335,
      "loss": -117.103,
      "step": 3100
    },
    {
      "epoch": 0.2488,
      "grad_norm": 78.61631774902344,
      "learning_rate": 0.00019170666666666668,
      "loss": -116.466,
      "step": 3110
    },
    {
      "epoch": 0.2496,
      "grad_norm": 59.374820709228516,
      "learning_rate": 0.00019168,
      "loss": -116.4758,
      "step": 3120
    },
    {
      "epoch": 0.2504,
      "grad_norm": 97.58914184570312,
      "learning_rate": 0.00019165333333333336,
      "loss": -117.4509,
      "step": 3130
    },
    {
      "epoch": 0.2512,
      "grad_norm": 135.64537048339844,
      "learning_rate": 0.00019162666666666667,
      "loss": -116.115,
      "step": 3140
    },
    {
      "epoch": 0.252,
      "grad_norm": 63.67714309692383,
      "learning_rate": 0.0001916,
      "loss": -116.8232,
      "step": 3150
    },
    {
      "epoch": 0.2528,
      "grad_norm": 49.535396575927734,
      "learning_rate": 0.00019157333333333335,
      "loss": -116.654,
      "step": 3160
    },
    {
      "epoch": 0.2536,
      "grad_norm": 96.02322387695312,
      "learning_rate": 0.00019154666666666668,
      "loss": -117.17,
      "step": 3170
    },
    {
      "epoch": 0.2544,
      "grad_norm": 89.24690246582031,
      "learning_rate": 0.00019152,
      "loss": -117.594,
      "step": 3180
    },
    {
      "epoch": 0.2552,
      "grad_norm": 84.67578125,
      "learning_rate": 0.00019149333333333334,
      "loss": -116.8177,
      "step": 3190
    },
    {
      "epoch": 0.256,
      "grad_norm": 66.39517974853516,
      "learning_rate": 0.0001914666666666667,
      "loss": -117.5244,
      "step": 3200
    },
    {
      "epoch": 0.2568,
      "grad_norm": 94.3587417602539,
      "learning_rate": 0.00019144000000000002,
      "loss": -116.7972,
      "step": 3210
    },
    {
      "epoch": 0.2576,
      "grad_norm": 104.29000091552734,
      "learning_rate": 0.00019141333333333333,
      "loss": -117.5728,
      "step": 3220
    },
    {
      "epoch": 0.2584,
      "grad_norm": 68.11003112792969,
      "learning_rate": 0.00019138666666666668,
      "loss": -116.6683,
      "step": 3230
    },
    {
      "epoch": 0.2592,
      "grad_norm": 105.76512908935547,
      "learning_rate": 0.00019136,
      "loss": -116.5012,
      "step": 3240
    },
    {
      "epoch": 0.26,
      "grad_norm": 104.28616333007812,
      "learning_rate": 0.00019133333333333334,
      "loss": -117.4536,
      "step": 3250
    },
    {
      "epoch": 0.2608,
      "grad_norm": 98.94849395751953,
      "learning_rate": 0.00019130666666666667,
      "loss": -117.4869,
      "step": 3260
    },
    {
      "epoch": 0.2616,
      "grad_norm": 107.40431213378906,
      "learning_rate": 0.00019128000000000003,
      "loss": -116.3212,
      "step": 3270
    },
    {
      "epoch": 0.2624,
      "grad_norm": 91.6134033203125,
      "learning_rate": 0.00019125333333333335,
      "loss": -118.3244,
      "step": 3280
    },
    {
      "epoch": 0.2632,
      "grad_norm": 56.32924270629883,
      "learning_rate": 0.00019122666666666666,
      "loss": -117.6362,
      "step": 3290
    },
    {
      "epoch": 0.264,
      "grad_norm": 74.73661041259766,
      "learning_rate": 0.0001912,
      "loss": -117.5714,
      "step": 3300
    },
    {
      "epoch": 0.2648,
      "grad_norm": 89.59890747070312,
      "learning_rate": 0.00019117333333333334,
      "loss": -118.0859,
      "step": 3310
    },
    {
      "epoch": 0.2656,
      "grad_norm": 58.61537551879883,
      "learning_rate": 0.00019114666666666667,
      "loss": -117.8918,
      "step": 3320
    },
    {
      "epoch": 0.2664,
      "grad_norm": 63.01579284667969,
      "learning_rate": 0.00019112,
      "loss": -117.6317,
      "step": 3330
    },
    {
      "epoch": 0.2672,
      "grad_norm": 54.17140579223633,
      "learning_rate": 0.00019109333333333336,
      "loss": -117.0268,
      "step": 3340
    },
    {
      "epoch": 0.268,
      "grad_norm": 86.9528579711914,
      "learning_rate": 0.00019106666666666668,
      "loss": -117.4514,
      "step": 3350
    },
    {
      "epoch": 0.2688,
      "grad_norm": 91.69364166259766,
      "learning_rate": 0.00019104000000000001,
      "loss": -117.0095,
      "step": 3360
    },
    {
      "epoch": 0.2696,
      "grad_norm": 141.9312286376953,
      "learning_rate": 0.00019101333333333334,
      "loss": -117.9608,
      "step": 3370
    },
    {
      "epoch": 0.2704,
      "grad_norm": 96.09416961669922,
      "learning_rate": 0.00019098666666666667,
      "loss": -117.7899,
      "step": 3380
    },
    {
      "epoch": 0.2712,
      "grad_norm": 59.9031867980957,
      "learning_rate": 0.00019096,
      "loss": -117.7108,
      "step": 3390
    },
    {
      "epoch": 0.272,
      "grad_norm": 68.49246978759766,
      "learning_rate": 0.00019093333333333333,
      "loss": -116.4381,
      "step": 3400
    },
    {
      "epoch": 0.2728,
      "grad_norm": 82.70870971679688,
      "learning_rate": 0.0001909066666666667,
      "loss": -115.9575,
      "step": 3410
    },
    {
      "epoch": 0.2736,
      "grad_norm": 84.50139617919922,
      "learning_rate": 0.00019088000000000002,
      "loss": -118.2018,
      "step": 3420
    },
    {
      "epoch": 0.2744,
      "grad_norm": 108.35196685791016,
      "learning_rate": 0.00019085333333333334,
      "loss": -117.5989,
      "step": 3430
    },
    {
      "epoch": 0.2752,
      "grad_norm": 89.82722473144531,
      "learning_rate": 0.00019082666666666667,
      "loss": -117.3292,
      "step": 3440
    },
    {
      "epoch": 0.276,
      "grad_norm": 108.0348129272461,
      "learning_rate": 0.0001908,
      "loss": -116.8116,
      "step": 3450
    },
    {
      "epoch": 0.2768,
      "grad_norm": 87.86354064941406,
      "learning_rate": 0.00019077333333333333,
      "loss": -117.4632,
      "step": 3460
    },
    {
      "epoch": 0.2776,
      "grad_norm": 82.20124053955078,
      "learning_rate": 0.00019074666666666666,
      "loss": -117.7412,
      "step": 3470
    },
    {
      "epoch": 0.2784,
      "grad_norm": 95.33695220947266,
      "learning_rate": 0.00019072000000000002,
      "loss": -116.692,
      "step": 3480
    },
    {
      "epoch": 0.2792,
      "grad_norm": 80.60599517822266,
      "learning_rate": 0.00019069333333333335,
      "loss": -117.069,
      "step": 3490
    },
    {
      "epoch": 0.28,
      "grad_norm": 101.62773132324219,
      "learning_rate": 0.00019066666666666668,
      "loss": -116.7947,
      "step": 3500
    },
    {
      "epoch": 0.2808,
      "grad_norm": 108.25862121582031,
      "learning_rate": 0.00019064000000000003,
      "loss": -116.9195,
      "step": 3510
    },
    {
      "epoch": 0.2816,
      "grad_norm": 91.21430206298828,
      "learning_rate": 0.00019061333333333333,
      "loss": -116.1421,
      "step": 3520
    },
    {
      "epoch": 0.2824,
      "grad_norm": 91.51296997070312,
      "learning_rate": 0.00019058666666666666,
      "loss": -116.8669,
      "step": 3530
    },
    {
      "epoch": 0.2832,
      "grad_norm": 90.5048828125,
      "learning_rate": 0.00019056000000000002,
      "loss": -118.335,
      "step": 3540
    },
    {
      "epoch": 0.284,
      "grad_norm": 74.15998077392578,
      "learning_rate": 0.00019053333333333335,
      "loss": -117.7986,
      "step": 3550
    },
    {
      "epoch": 0.2848,
      "grad_norm": 65.29568481445312,
      "learning_rate": 0.00019050666666666668,
      "loss": -117.5826,
      "step": 3560
    },
    {
      "epoch": 0.2856,
      "grad_norm": 86.4378433227539,
      "learning_rate": 0.00019048,
      "loss": -117.0385,
      "step": 3570
    },
    {
      "epoch": 0.2864,
      "grad_norm": 97.05860900878906,
      "learning_rate": 0.00019045333333333336,
      "loss": -118.2543,
      "step": 3580
    },
    {
      "epoch": 0.2872,
      "grad_norm": 80.80037689208984,
      "learning_rate": 0.0001904266666666667,
      "loss": -118.2416,
      "step": 3590
    },
    {
      "epoch": 0.288,
      "grad_norm": 62.77082443237305,
      "learning_rate": 0.0001904,
      "loss": -116.7474,
      "step": 3600
    },
    {
      "epoch": 0.2888,
      "grad_norm": 77.68909454345703,
      "learning_rate": 0.00019037333333333335,
      "loss": -118.114,
      "step": 3610
    },
    {
      "epoch": 0.2896,
      "grad_norm": 76.13875579833984,
      "learning_rate": 0.00019034666666666668,
      "loss": -117.0027,
      "step": 3620
    },
    {
      "epoch": 0.2904,
      "grad_norm": 81.9204330444336,
      "learning_rate": 0.00019032,
      "loss": -116.2253,
      "step": 3630
    },
    {
      "epoch": 0.2912,
      "grad_norm": 74.04994201660156,
      "learning_rate": 0.00019029333333333334,
      "loss": -117.8914,
      "step": 3640
    },
    {
      "epoch": 0.292,
      "grad_norm": 60.830421447753906,
      "learning_rate": 0.0001902666666666667,
      "loss": -116.3869,
      "step": 3650
    },
    {
      "epoch": 0.2928,
      "grad_norm": 89.9209213256836,
      "learning_rate": 0.00019024000000000002,
      "loss": -118.2393,
      "step": 3660
    },
    {
      "epoch": 0.2936,
      "grad_norm": 78.48104858398438,
      "learning_rate": 0.00019021333333333332,
      "loss": -117.9432,
      "step": 3670
    },
    {
      "epoch": 0.2944,
      "grad_norm": 91.11188507080078,
      "learning_rate": 0.00019018666666666668,
      "loss": -118.4355,
      "step": 3680
    },
    {
      "epoch": 0.2952,
      "grad_norm": 101.74385833740234,
      "learning_rate": 0.00019016,
      "loss": -116.879,
      "step": 3690
    },
    {
      "epoch": 0.296,
      "grad_norm": 59.35485076904297,
      "learning_rate": 0.00019013333333333334,
      "loss": -117.561,
      "step": 3700
    },
    {
      "epoch": 0.2968,
      "grad_norm": 102.1507797241211,
      "learning_rate": 0.00019010666666666667,
      "loss": -117.1112,
      "step": 3710
    },
    {
      "epoch": 0.2976,
      "grad_norm": 91.93021392822266,
      "learning_rate": 0.00019008000000000002,
      "loss": -117.3757,
      "step": 3720
    },
    {
      "epoch": 0.2984,
      "grad_norm": 94.06929016113281,
      "learning_rate": 0.00019005333333333335,
      "loss": -117.2906,
      "step": 3730
    },
    {
      "epoch": 0.2992,
      "grad_norm": 90.54251861572266,
      "learning_rate": 0.00019002666666666668,
      "loss": -117.5997,
      "step": 3740
    },
    {
      "epoch": 0.3,
      "grad_norm": 80.07730865478516,
      "learning_rate": 0.00019,
      "loss": -117.117,
      "step": 3750
    },
    {
      "epoch": 0.3008,
      "grad_norm": 90.19027709960938,
      "learning_rate": 0.00018997333333333334,
      "loss": -117.6849,
      "step": 3760
    },
    {
      "epoch": 0.3016,
      "grad_norm": 100.54865264892578,
      "learning_rate": 0.00018994666666666667,
      "loss": -116.2589,
      "step": 3770
    },
    {
      "epoch": 0.3024,
      "grad_norm": 76.18262481689453,
      "learning_rate": 0.00018992,
      "loss": -115.5237,
      "step": 3780
    },
    {
      "epoch": 0.3032,
      "grad_norm": 50.74720001220703,
      "learning_rate": 0.00018989333333333335,
      "loss": -117.5807,
      "step": 3790
    },
    {
      "epoch": 0.304,
      "grad_norm": 65.12234497070312,
      "learning_rate": 0.00018986666666666668,
      "loss": -118.6389,
      "step": 3800
    },
    {
      "epoch": 0.3048,
      "grad_norm": 70.130615234375,
      "learning_rate": 0.00018984,
      "loss": -117.7885,
      "step": 3810
    },
    {
      "epoch": 0.3056,
      "grad_norm": 91.68362426757812,
      "learning_rate": 0.00018981333333333334,
      "loss": -118.3332,
      "step": 3820
    },
    {
      "epoch": 0.3064,
      "grad_norm": 72.5172348022461,
      "learning_rate": 0.00018978666666666667,
      "loss": -118.2262,
      "step": 3830
    },
    {
      "epoch": 0.3072,
      "grad_norm": 90.45062255859375,
      "learning_rate": 0.00018976,
      "loss": -117.2808,
      "step": 3840
    },
    {
      "epoch": 0.308,
      "grad_norm": 78.4048080444336,
      "learning_rate": 0.00018973333333333333,
      "loss": -118.6012,
      "step": 3850
    },
    {
      "epoch": 0.3088,
      "grad_norm": 84.51206970214844,
      "learning_rate": 0.00018970666666666668,
      "loss": -117.4165,
      "step": 3860
    },
    {
      "epoch": 0.3096,
      "grad_norm": 75.72750091552734,
      "learning_rate": 0.00018968,
      "loss": -115.9967,
      "step": 3870
    },
    {
      "epoch": 0.3104,
      "grad_norm": 101.60262298583984,
      "learning_rate": 0.00018965333333333334,
      "loss": -117.82,
      "step": 3880
    },
    {
      "epoch": 0.3112,
      "grad_norm": 48.47683334350586,
      "learning_rate": 0.0001896266666666667,
      "loss": -116.8025,
      "step": 3890
    },
    {
      "epoch": 0.312,
      "grad_norm": 59.449317932128906,
      "learning_rate": 0.0001896,
      "loss": -117.2565,
      "step": 3900
    },
    {
      "epoch": 0.3128,
      "grad_norm": 57.14265060424805,
      "learning_rate": 0.00018957333333333333,
      "loss": -117.9141,
      "step": 3910
    },
    {
      "epoch": 0.3136,
      "grad_norm": 54.19742965698242,
      "learning_rate": 0.00018954666666666666,
      "loss": -115.9009,
      "step": 3920
    },
    {
      "epoch": 0.3144,
      "grad_norm": 101.91368103027344,
      "learning_rate": 0.00018952000000000002,
      "loss": -117.9766,
      "step": 3930
    },
    {
      "epoch": 0.3152,
      "grad_norm": 87.10079956054688,
      "learning_rate": 0.00018949333333333334,
      "loss": -118.4094,
      "step": 3940
    },
    {
      "epoch": 0.316,
      "grad_norm": 69.60458374023438,
      "learning_rate": 0.00018946666666666667,
      "loss": -116.8932,
      "step": 3950
    },
    {
      "epoch": 0.3168,
      "grad_norm": 77.1846923828125,
      "learning_rate": 0.00018944000000000003,
      "loss": -116.9866,
      "step": 3960
    },
    {
      "epoch": 0.3176,
      "grad_norm": 55.113372802734375,
      "learning_rate": 0.00018941333333333333,
      "loss": -117.105,
      "step": 3970
    },
    {
      "epoch": 0.3184,
      "grad_norm": 75.24275207519531,
      "learning_rate": 0.00018938666666666666,
      "loss": -117.9927,
      "step": 3980
    },
    {
      "epoch": 0.3192,
      "grad_norm": 79.59107208251953,
      "learning_rate": 0.00018936000000000002,
      "loss": -116.6788,
      "step": 3990
    },
    {
      "epoch": 0.32,
      "grad_norm": 77.52447509765625,
      "learning_rate": 0.00018933333333333335,
      "loss": -116.7331,
      "step": 4000
    },
    {
      "epoch": 0.3208,
      "grad_norm": 66.54534912109375,
      "learning_rate": 0.00018930666666666667,
      "loss": -117.8879,
      "step": 4010
    },
    {
      "epoch": 0.3216,
      "grad_norm": 61.32707214355469,
      "learning_rate": 0.00018928,
      "loss": -117.8043,
      "step": 4020
    },
    {
      "epoch": 0.3224,
      "grad_norm": 58.39420700073242,
      "learning_rate": 0.00018925333333333336,
      "loss": -117.9269,
      "step": 4030
    },
    {
      "epoch": 0.3232,
      "grad_norm": 72.73858642578125,
      "learning_rate": 0.0001892266666666667,
      "loss": -117.2198,
      "step": 4040
    },
    {
      "epoch": 0.324,
      "grad_norm": 64.34188079833984,
      "learning_rate": 0.0001892,
      "loss": -118.7229,
      "step": 4050
    },
    {
      "epoch": 0.3248,
      "grad_norm": 73.39701843261719,
      "learning_rate": 0.00018917333333333335,
      "loss": -117.2468,
      "step": 4060
    },
    {
      "epoch": 0.3256,
      "grad_norm": 58.50059509277344,
      "learning_rate": 0.00018914666666666668,
      "loss": -118.2619,
      "step": 4070
    },
    {
      "epoch": 0.3264,
      "grad_norm": 68.39834594726562,
      "learning_rate": 0.00018912,
      "loss": -116.2637,
      "step": 4080
    },
    {
      "epoch": 0.3272,
      "grad_norm": 67.35710144042969,
      "learning_rate": 0.00018909333333333333,
      "loss": -118.8408,
      "step": 4090
    },
    {
      "epoch": 0.328,
      "grad_norm": 50.09440231323242,
      "learning_rate": 0.0001890666666666667,
      "loss": -117.9853,
      "step": 4100
    },
    {
      "epoch": 0.3288,
      "grad_norm": 91.46844482421875,
      "learning_rate": 0.00018904000000000002,
      "loss": -117.205,
      "step": 4110
    },
    {
      "epoch": 0.3296,
      "grad_norm": 60.12696838378906,
      "learning_rate": 0.00018901333333333335,
      "loss": -119.073,
      "step": 4120
    },
    {
      "epoch": 0.3304,
      "grad_norm": 74.77214813232422,
      "learning_rate": 0.00018898666666666668,
      "loss": -116.5203,
      "step": 4130
    },
    {
      "epoch": 0.3312,
      "grad_norm": 114.73848724365234,
      "learning_rate": 0.00018896,
      "loss": -116.171,
      "step": 4140
    },
    {
      "epoch": 0.332,
      "grad_norm": 55.35664749145508,
      "learning_rate": 0.00018893333333333334,
      "loss": -116.291,
      "step": 4150
    },
    {
      "epoch": 0.3328,
      "grad_norm": 55.78053665161133,
      "learning_rate": 0.00018890666666666667,
      "loss": -117.0785,
      "step": 4160
    },
    {
      "epoch": 0.3336,
      "grad_norm": 53.554447174072266,
      "learning_rate": 0.00018888000000000002,
      "loss": -117.9424,
      "step": 4170
    },
    {
      "epoch": 0.3344,
      "grad_norm": 66.32056427001953,
      "learning_rate": 0.00018885333333333335,
      "loss": -116.3762,
      "step": 4180
    },
    {
      "epoch": 0.3352,
      "grad_norm": 69.76102447509766,
      "learning_rate": 0.00018882666666666668,
      "loss": -116.6085,
      "step": 4190
    },
    {
      "epoch": 0.336,
      "grad_norm": 75.03665161132812,
      "learning_rate": 0.0001888,
      "loss": -117.708,
      "step": 4200
    },
    {
      "epoch": 0.3368,
      "grad_norm": 79.42057800292969,
      "learning_rate": 0.00018877333333333334,
      "loss": -117.3296,
      "step": 4210
    },
    {
      "epoch": 0.3376,
      "grad_norm": 76.7390365600586,
      "learning_rate": 0.00018874666666666667,
      "loss": -116.4856,
      "step": 4220
    },
    {
      "epoch": 0.3384,
      "grad_norm": 66.21896362304688,
      "learning_rate": 0.00018872,
      "loss": -118.2399,
      "step": 4230
    },
    {
      "epoch": 0.3392,
      "grad_norm": 93.60064697265625,
      "learning_rate": 0.00018869333333333335,
      "loss": -116.7912,
      "step": 4240
    },
    {
      "epoch": 0.34,
      "grad_norm": 95.3961181640625,
      "learning_rate": 0.00018866666666666668,
      "loss": -117.4466,
      "step": 4250
    },
    {
      "epoch": 0.3408,
      "grad_norm": 61.86552810668945,
      "learning_rate": 0.00018864,
      "loss": -116.6594,
      "step": 4260
    },
    {
      "epoch": 0.3416,
      "grad_norm": 61.491058349609375,
      "learning_rate": 0.00018861333333333337,
      "loss": -116.4522,
      "step": 4270
    },
    {
      "epoch": 0.3424,
      "grad_norm": 66.52474975585938,
      "learning_rate": 0.00018858666666666667,
      "loss": -117.536,
      "step": 4280
    },
    {
      "epoch": 0.3432,
      "grad_norm": 82.46476745605469,
      "learning_rate": 0.00018856,
      "loss": -117.8784,
      "step": 4290
    },
    {
      "epoch": 0.344,
      "grad_norm": 91.26985931396484,
      "learning_rate": 0.00018853333333333333,
      "loss": -117.48,
      "step": 4300
    },
    {
      "epoch": 0.3448,
      "grad_norm": 73.15975952148438,
      "learning_rate": 0.00018850666666666668,
      "loss": -117.187,
      "step": 4310
    },
    {
      "epoch": 0.3456,
      "grad_norm": 58.43321990966797,
      "learning_rate": 0.00018848,
      "loss": -117.3293,
      "step": 4320
    },
    {
      "epoch": 0.3464,
      "grad_norm": 73.56160736083984,
      "learning_rate": 0.00018845333333333334,
      "loss": -116.6048,
      "step": 4330
    },
    {
      "epoch": 0.3472,
      "grad_norm": 51.17985916137695,
      "learning_rate": 0.0001884266666666667,
      "loss": -118.1112,
      "step": 4340
    },
    {
      "epoch": 0.348,
      "grad_norm": 70.79485321044922,
      "learning_rate": 0.0001884,
      "loss": -117.7149,
      "step": 4350
    },
    {
      "epoch": 0.3488,
      "grad_norm": 70.31846618652344,
      "learning_rate": 0.00018837333333333333,
      "loss": -117.7418,
      "step": 4360
    },
    {
      "epoch": 0.3496,
      "grad_norm": 61.23720932006836,
      "learning_rate": 0.00018834666666666668,
      "loss": -117.6812,
      "step": 4370
    },
    {
      "epoch": 0.3504,
      "grad_norm": 64.12623596191406,
      "learning_rate": 0.00018832,
      "loss": -118.773,
      "step": 4380
    },
    {
      "epoch": 0.3512,
      "grad_norm": 73.33262634277344,
      "learning_rate": 0.00018829333333333334,
      "loss": -118.2909,
      "step": 4390
    },
    {
      "epoch": 0.352,
      "grad_norm": 62.65521240234375,
      "learning_rate": 0.00018826666666666667,
      "loss": -119.0553,
      "step": 4400
    },
    {
      "epoch": 0.3528,
      "grad_norm": 87.64215087890625,
      "learning_rate": 0.00018824000000000003,
      "loss": -118.2292,
      "step": 4410
    },
    {
      "epoch": 0.3536,
      "grad_norm": 82.80455017089844,
      "learning_rate": 0.00018821333333333336,
      "loss": -116.9786,
      "step": 4420
    },
    {
      "epoch": 0.3544,
      "grad_norm": 57.36896896362305,
      "learning_rate": 0.00018818666666666666,
      "loss": -117.8281,
      "step": 4430
    },
    {
      "epoch": 0.3552,
      "grad_norm": 62.05329895019531,
      "learning_rate": 0.00018816000000000001,
      "loss": -116.1513,
      "step": 4440
    },
    {
      "epoch": 0.356,
      "grad_norm": 88.54759979248047,
      "learning_rate": 0.00018813333333333334,
      "loss": -118.5358,
      "step": 4450
    },
    {
      "epoch": 0.3568,
      "grad_norm": 63.52227020263672,
      "learning_rate": 0.00018810666666666667,
      "loss": -118.5384,
      "step": 4460
    },
    {
      "epoch": 0.3576,
      "grad_norm": 52.25962829589844,
      "learning_rate": 0.00018808,
      "loss": -117.7742,
      "step": 4470
    },
    {
      "epoch": 0.3584,
      "grad_norm": 96.7466049194336,
      "learning_rate": 0.00018805333333333336,
      "loss": -118.7534,
      "step": 4480
    },
    {
      "epoch": 0.3592,
      "grad_norm": 79.41165924072266,
      "learning_rate": 0.0001880266666666667,
      "loss": -116.3055,
      "step": 4490
    },
    {
      "epoch": 0.36,
      "grad_norm": 102.44923400878906,
      "learning_rate": 0.000188,
      "loss": -117.312,
      "step": 4500
    },
    {
      "epoch": 0.3608,
      "grad_norm": 68.21475982666016,
      "learning_rate": 0.00018797333333333335,
      "loss": -116.1211,
      "step": 4510
    },
    {
      "epoch": 0.3616,
      "grad_norm": 53.773834228515625,
      "learning_rate": 0.00018794666666666667,
      "loss": -117.0457,
      "step": 4520
    },
    {
      "epoch": 0.3624,
      "grad_norm": 69.13665771484375,
      "learning_rate": 0.00018792,
      "loss": -117.9965,
      "step": 4530
    },
    {
      "epoch": 0.3632,
      "grad_norm": 60.934356689453125,
      "learning_rate": 0.00018789333333333333,
      "loss": -118.7036,
      "step": 4540
    },
    {
      "epoch": 0.364,
      "grad_norm": 59.8915901184082,
      "learning_rate": 0.0001878666666666667,
      "loss": -117.4698,
      "step": 4550
    },
    {
      "epoch": 0.3648,
      "grad_norm": 65.64076232910156,
      "learning_rate": 0.00018784000000000002,
      "loss": -118.0582,
      "step": 4560
    },
    {
      "epoch": 0.3656,
      "grad_norm": 101.34564971923828,
      "learning_rate": 0.00018781333333333335,
      "loss": -117.8591,
      "step": 4570
    },
    {
      "epoch": 0.3664,
      "grad_norm": 87.6083984375,
      "learning_rate": 0.00018778666666666668,
      "loss": -118.6754,
      "step": 4580
    },
    {
      "epoch": 0.3672,
      "grad_norm": 56.80071258544922,
      "learning_rate": 0.00018776,
      "loss": -116.879,
      "step": 4590
    },
    {
      "epoch": 0.368,
      "grad_norm": 63.74310302734375,
      "learning_rate": 0.00018773333333333333,
      "loss": -116.4777,
      "step": 4600
    },
    {
      "epoch": 0.3688,
      "grad_norm": 63.60380172729492,
      "learning_rate": 0.00018770666666666666,
      "loss": -117.5199,
      "step": 4610
    },
    {
      "epoch": 0.3696,
      "grad_norm": 59.35285949707031,
      "learning_rate": 0.00018768000000000002,
      "loss": -118.585,
      "step": 4620
    },
    {
      "epoch": 0.3704,
      "grad_norm": 75.16741943359375,
      "learning_rate": 0.00018765333333333335,
      "loss": -117.9438,
      "step": 4630
    },
    {
      "epoch": 0.3712,
      "grad_norm": 63.400657653808594,
      "learning_rate": 0.00018762666666666668,
      "loss": -117.0587,
      "step": 4640
    },
    {
      "epoch": 0.372,
      "grad_norm": 51.25811767578125,
      "learning_rate": 0.0001876,
      "loss": -117.5519,
      "step": 4650
    },
    {
      "epoch": 0.3728,
      "grad_norm": 94.19110107421875,
      "learning_rate": 0.00018757333333333334,
      "loss": -117.2351,
      "step": 4660
    },
    {
      "epoch": 0.3736,
      "grad_norm": 75.27001953125,
      "learning_rate": 0.00018754666666666666,
      "loss": -117.7651,
      "step": 4670
    },
    {
      "epoch": 0.3744,
      "grad_norm": 67.87802124023438,
      "learning_rate": 0.00018752,
      "loss": -118.1247,
      "step": 4680
    },
    {
      "epoch": 0.3752,
      "grad_norm": 60.08438491821289,
      "learning_rate": 0.00018749333333333335,
      "loss": -118.38,
      "step": 4690
    },
    {
      "epoch": 0.376,
      "grad_norm": 66.25332641601562,
      "learning_rate": 0.00018746666666666668,
      "loss": -117.9075,
      "step": 4700
    },
    {
      "epoch": 0.3768,
      "grad_norm": 67.30878448486328,
      "learning_rate": 0.00018744,
      "loss": -118.4781,
      "step": 4710
    },
    {
      "epoch": 0.3776,
      "grad_norm": 65.8761215209961,
      "learning_rate": 0.00018741333333333336,
      "loss": -117.2142,
      "step": 4720
    },
    {
      "epoch": 0.3784,
      "grad_norm": 58.27385711669922,
      "learning_rate": 0.00018738666666666667,
      "loss": -118.2746,
      "step": 4730
    },
    {
      "epoch": 0.3792,
      "grad_norm": 53.382606506347656,
      "learning_rate": 0.00018736,
      "loss": -117.6923,
      "step": 4740
    },
    {
      "epoch": 0.38,
      "grad_norm": 60.920108795166016,
      "learning_rate": 0.00018733333333333335,
      "loss": -117.7123,
      "step": 4750
    },
    {
      "epoch": 0.3808,
      "grad_norm": 65.83172607421875,
      "learning_rate": 0.00018730666666666668,
      "loss": -118.494,
      "step": 4760
    },
    {
      "epoch": 0.3816,
      "grad_norm": 70.22647094726562,
      "learning_rate": 0.00018728,
      "loss": -117.3532,
      "step": 4770
    },
    {
      "epoch": 0.3824,
      "grad_norm": 58.18931579589844,
      "learning_rate": 0.00018725333333333334,
      "loss": -117.0561,
      "step": 4780
    },
    {
      "epoch": 0.3832,
      "grad_norm": 60.22052001953125,
      "learning_rate": 0.0001872266666666667,
      "loss": -117.3075,
      "step": 4790
    },
    {
      "epoch": 0.384,
      "grad_norm": 72.55534362792969,
      "learning_rate": 0.00018720000000000002,
      "loss": -118.9561,
      "step": 4800
    },
    {
      "epoch": 0.3848,
      "grad_norm": 71.63127136230469,
      "learning_rate": 0.00018717333333333333,
      "loss": -118.3494,
      "step": 4810
    },
    {
      "epoch": 0.3856,
      "grad_norm": 71.6113052368164,
      "learning_rate": 0.00018714666666666668,
      "loss": -118.5831,
      "step": 4820
    },
    {
      "epoch": 0.3864,
      "grad_norm": 71.90507507324219,
      "learning_rate": 0.00018712,
      "loss": -117.8411,
      "step": 4830
    },
    {
      "epoch": 0.3872,
      "grad_norm": 73.09596252441406,
      "learning_rate": 0.00018709333333333334,
      "loss": -118.1457,
      "step": 4840
    },
    {
      "epoch": 0.388,
      "grad_norm": 58.62297058105469,
      "learning_rate": 0.00018706666666666667,
      "loss": -117.177,
      "step": 4850
    },
    {
      "epoch": 0.3888,
      "grad_norm": 59.84435272216797,
      "learning_rate": 0.00018704000000000003,
      "loss": -117.9997,
      "step": 4860
    },
    {
      "epoch": 0.3896,
      "grad_norm": 51.79825973510742,
      "learning_rate": 0.00018701333333333335,
      "loss": -118.0983,
      "step": 4870
    },
    {
      "epoch": 0.3904,
      "grad_norm": 68.60951232910156,
      "learning_rate": 0.00018698666666666666,
      "loss": -117.2107,
      "step": 4880
    },
    {
      "epoch": 0.3912,
      "grad_norm": 58.92770767211914,
      "learning_rate": 0.00018696,
      "loss": -117.2655,
      "step": 4890
    },
    {
      "epoch": 0.392,
      "grad_norm": 69.91917419433594,
      "learning_rate": 0.00018693333333333334,
      "loss": -117.1192,
      "step": 4900
    },
    {
      "epoch": 0.3928,
      "grad_norm": 85.4964828491211,
      "learning_rate": 0.00018690666666666667,
      "loss": -118.1314,
      "step": 4910
    },
    {
      "epoch": 0.3936,
      "grad_norm": 60.05213165283203,
      "learning_rate": 0.00018688,
      "loss": -117.9704,
      "step": 4920
    },
    {
      "epoch": 0.3944,
      "grad_norm": 71.92659759521484,
      "learning_rate": 0.00018685333333333336,
      "loss": -118.7735,
      "step": 4930
    },
    {
      "epoch": 0.3952,
      "grad_norm": 66.6703109741211,
      "learning_rate": 0.00018682666666666668,
      "loss": -118.9531,
      "step": 4940
    },
    {
      "epoch": 0.396,
      "grad_norm": 85.85305786132812,
      "learning_rate": 0.00018680000000000001,
      "loss": -119.1412,
      "step": 4950
    },
    {
      "epoch": 0.3968,
      "grad_norm": 76.76334381103516,
      "learning_rate": 0.00018677333333333334,
      "loss": -117.604,
      "step": 4960
    },
    {
      "epoch": 0.3976,
      "grad_norm": 73.1579818725586,
      "learning_rate": 0.00018674666666666667,
      "loss": -118.1296,
      "step": 4970
    },
    {
      "epoch": 0.3984,
      "grad_norm": 56.71463394165039,
      "learning_rate": 0.00018672,
      "loss": -117.4059,
      "step": 4980
    },
    {
      "epoch": 0.3992,
      "grad_norm": 72.11493682861328,
      "learning_rate": 0.00018669333333333333,
      "loss": -118.1254,
      "step": 4990
    },
    {
      "epoch": 0.4,
      "grad_norm": 59.09806823730469,
      "learning_rate": 0.0001866666666666667,
      "loss": -117.6472,
      "step": 5000
    },
    {
      "epoch": 0.4008,
      "grad_norm": 81.44220733642578,
      "learning_rate": 0.00018664000000000002,
      "loss": -118.5603,
      "step": 5010
    },
    {
      "epoch": 0.4016,
      "grad_norm": 50.14027404785156,
      "learning_rate": 0.00018661333333333334,
      "loss": -117.9923,
      "step": 5020
    },
    {
      "epoch": 0.4024,
      "grad_norm": 52.13078689575195,
      "learning_rate": 0.00018658666666666667,
      "loss": -117.026,
      "step": 5030
    },
    {
      "epoch": 0.4032,
      "grad_norm": 86.62969207763672,
      "learning_rate": 0.00018656,
      "loss": -118.3571,
      "step": 5040
    },
    {
      "epoch": 0.404,
      "grad_norm": 54.009525299072266,
      "learning_rate": 0.00018653333333333333,
      "loss": -118.1159,
      "step": 5050
    },
    {
      "epoch": 0.4048,
      "grad_norm": 51.501773834228516,
      "learning_rate": 0.00018650666666666666,
      "loss": -118.2261,
      "step": 5060
    },
    {
      "epoch": 0.4056,
      "grad_norm": 78.20857238769531,
      "learning_rate": 0.00018648000000000002,
      "loss": -117.3095,
      "step": 5070
    },
    {
      "epoch": 0.4064,
      "grad_norm": 70.96256256103516,
      "learning_rate": 0.00018645333333333335,
      "loss": -117.8302,
      "step": 5080
    },
    {
      "epoch": 0.4072,
      "grad_norm": 81.35531616210938,
      "learning_rate": 0.00018642666666666668,
      "loss": -117.8703,
      "step": 5090
    },
    {
      "epoch": 0.408,
      "grad_norm": 59.57603454589844,
      "learning_rate": 0.00018640000000000003,
      "loss": -118.7605,
      "step": 5100
    },
    {
      "epoch": 0.4088,
      "grad_norm": 52.101078033447266,
      "learning_rate": 0.00018637333333333333,
      "loss": -118.234,
      "step": 5110
    },
    {
      "epoch": 0.4096,
      "grad_norm": 79.62602996826172,
      "learning_rate": 0.00018634666666666666,
      "loss": -118.0338,
      "step": 5120
    },
    {
      "epoch": 0.4104,
      "grad_norm": 57.989437103271484,
      "learning_rate": 0.00018632000000000002,
      "loss": -118.0222,
      "step": 5130
    },
    {
      "epoch": 0.4112,
      "grad_norm": 97.660888671875,
      "learning_rate": 0.00018629333333333335,
      "loss": -118.4978,
      "step": 5140
    },
    {
      "epoch": 0.412,
      "grad_norm": 64.05035400390625,
      "learning_rate": 0.00018626666666666668,
      "loss": -118.6407,
      "step": 5150
    },
    {
      "epoch": 0.4128,
      "grad_norm": 67.84925079345703,
      "learning_rate": 0.00018624,
      "loss": -117.9959,
      "step": 5160
    },
    {
      "epoch": 0.4136,
      "grad_norm": 61.76091003417969,
      "learning_rate": 0.00018621333333333336,
      "loss": -117.6654,
      "step": 5170
    },
    {
      "epoch": 0.4144,
      "grad_norm": 71.74740600585938,
      "learning_rate": 0.00018618666666666666,
      "loss": -118.2965,
      "step": 5180
    },
    {
      "epoch": 0.4152,
      "grad_norm": 58.21009063720703,
      "learning_rate": 0.00018616,
      "loss": -117.7312,
      "step": 5190
    },
    {
      "epoch": 0.416,
      "grad_norm": 53.4370231628418,
      "learning_rate": 0.00018613333333333335,
      "loss": -119.2453,
      "step": 5200
    },
    {
      "epoch": 0.4168,
      "grad_norm": 56.698829650878906,
      "learning_rate": 0.00018610666666666668,
      "loss": -116.519,
      "step": 5210
    },
    {
      "epoch": 0.4176,
      "grad_norm": 55.16939163208008,
      "learning_rate": 0.00018608,
      "loss": -118.0519,
      "step": 5220
    },
    {
      "epoch": 0.4184,
      "grad_norm": 55.251041412353516,
      "learning_rate": 0.00018605333333333334,
      "loss": -117.6699,
      "step": 5230
    },
    {
      "epoch": 0.4192,
      "grad_norm": 57.12929153442383,
      "learning_rate": 0.0001860266666666667,
      "loss": -117.4114,
      "step": 5240
    },
    {
      "epoch": 0.42,
      "grad_norm": 61.62242889404297,
      "learning_rate": 0.00018600000000000002,
      "loss": -117.9552,
      "step": 5250
    },
    {
      "epoch": 0.4208,
      "grad_norm": 68.20054626464844,
      "learning_rate": 0.00018597333333333332,
      "loss": -116.5247,
      "step": 5260
    },
    {
      "epoch": 0.4216,
      "grad_norm": 61.62418746948242,
      "learning_rate": 0.00018594666666666668,
      "loss": -117.8296,
      "step": 5270
    },
    {
      "epoch": 0.4224,
      "grad_norm": 94.54126739501953,
      "learning_rate": 0.00018592,
      "loss": -117.4134,
      "step": 5280
    },
    {
      "epoch": 0.4232,
      "grad_norm": 50.19167709350586,
      "learning_rate": 0.00018589333333333334,
      "loss": -117.1484,
      "step": 5290
    },
    {
      "epoch": 0.424,
      "grad_norm": 51.50139236450195,
      "learning_rate": 0.00018586666666666667,
      "loss": -117.636,
      "step": 5300
    },
    {
      "epoch": 0.4248,
      "grad_norm": 48.563392639160156,
      "learning_rate": 0.00018584000000000002,
      "loss": -117.1007,
      "step": 5310
    },
    {
      "epoch": 0.4256,
      "grad_norm": 61.08380126953125,
      "learning_rate": 0.00018581333333333335,
      "loss": -118.976,
      "step": 5320
    },
    {
      "epoch": 0.4264,
      "grad_norm": 73.74376678466797,
      "learning_rate": 0.00018578666666666668,
      "loss": -117.5593,
      "step": 5330
    },
    {
      "epoch": 0.4272,
      "grad_norm": 51.00743865966797,
      "learning_rate": 0.00018576,
      "loss": -117.8915,
      "step": 5340
    },
    {
      "epoch": 0.428,
      "grad_norm": 70.27986145019531,
      "learning_rate": 0.00018573333333333334,
      "loss": -118.3456,
      "step": 5350
    },
    {
      "epoch": 0.4288,
      "grad_norm": 78.17018127441406,
      "learning_rate": 0.00018570666666666667,
      "loss": -117.5726,
      "step": 5360
    },
    {
      "epoch": 0.4296,
      "grad_norm": 81.59937286376953,
      "learning_rate": 0.00018568,
      "loss": -116.6868,
      "step": 5370
    },
    {
      "epoch": 0.4304,
      "grad_norm": 68.61656951904297,
      "learning_rate": 0.00018565333333333335,
      "loss": -118.418,
      "step": 5380
    },
    {
      "epoch": 0.4312,
      "grad_norm": 63.83388900756836,
      "learning_rate": 0.00018562666666666668,
      "loss": -117.9953,
      "step": 5390
    },
    {
      "epoch": 0.432,
      "grad_norm": 70.85557556152344,
      "learning_rate": 0.0001856,
      "loss": -118.0138,
      "step": 5400
    },
    {
      "epoch": 0.4328,
      "grad_norm": 45.842628479003906,
      "learning_rate": 0.00018557333333333334,
      "loss": -117.6881,
      "step": 5410
    },
    {
      "epoch": 0.4336,
      "grad_norm": 47.01387405395508,
      "learning_rate": 0.00018554666666666667,
      "loss": -118.2196,
      "step": 5420
    },
    {
      "epoch": 0.4344,
      "grad_norm": 66.80032348632812,
      "learning_rate": 0.00018552,
      "loss": -116.383,
      "step": 5430
    },
    {
      "epoch": 0.4352,
      "grad_norm": 67.99108123779297,
      "learning_rate": 0.00018549333333333333,
      "loss": -117.7609,
      "step": 5440
    },
    {
      "epoch": 0.436,
      "grad_norm": 89.03282928466797,
      "learning_rate": 0.00018546666666666668,
      "loss": -117.2178,
      "step": 5450
    },
    {
      "epoch": 0.4368,
      "grad_norm": 56.32305908203125,
      "learning_rate": 0.00018544,
      "loss": -117.6444,
      "step": 5460
    },
    {
      "epoch": 0.4376,
      "grad_norm": 58.26311111450195,
      "learning_rate": 0.00018541333333333334,
      "loss": -118.5915,
      "step": 5470
    },
    {
      "epoch": 0.4384,
      "grad_norm": 70.79578399658203,
      "learning_rate": 0.0001853866666666667,
      "loss": -117.2974,
      "step": 5480
    },
    {
      "epoch": 0.4392,
      "grad_norm": 73.35456848144531,
      "learning_rate": 0.00018536,
      "loss": -117.9935,
      "step": 5490
    },
    {
      "epoch": 0.44,
      "grad_norm": 64.85086059570312,
      "learning_rate": 0.00018533333333333333,
      "loss": -118.0302,
      "step": 5500
    },
    {
      "epoch": 0.4408,
      "grad_norm": 45.53139877319336,
      "learning_rate": 0.00018530666666666669,
      "loss": -118.3312,
      "step": 5510
    },
    {
      "epoch": 0.4416,
      "grad_norm": 68.62301635742188,
      "learning_rate": 0.00018528000000000001,
      "loss": -117.7433,
      "step": 5520
    },
    {
      "epoch": 0.4424,
      "grad_norm": 68.18976593017578,
      "learning_rate": 0.00018525333333333334,
      "loss": -118.7045,
      "step": 5530
    },
    {
      "epoch": 0.4432,
      "grad_norm": 64.1775894165039,
      "learning_rate": 0.00018522666666666667,
      "loss": -116.8935,
      "step": 5540
    },
    {
      "epoch": 0.444,
      "grad_norm": 66.92405700683594,
      "learning_rate": 0.00018520000000000003,
      "loss": -116.6579,
      "step": 5550
    },
    {
      "epoch": 0.4448,
      "grad_norm": 62.90961837768555,
      "learning_rate": 0.00018517333333333333,
      "loss": -117.2308,
      "step": 5560
    },
    {
      "epoch": 0.4456,
      "grad_norm": 209.1767578125,
      "learning_rate": 0.00018514666666666666,
      "loss": -117.6919,
      "step": 5570
    },
    {
      "epoch": 0.4464,
      "grad_norm": 503.7726135253906,
      "learning_rate": 0.00018512000000000002,
      "loss": -117.0176,
      "step": 5580
    },
    {
      "epoch": 0.4472,
      "grad_norm": 58.65449905395508,
      "learning_rate": 0.00018509333333333335,
      "loss": -115.6009,
      "step": 5590
    },
    {
      "epoch": 0.448,
      "grad_norm": 289.18756103515625,
      "learning_rate": 0.00018506666666666667,
      "loss": -114.2238,
      "step": 5600
    },
    {
      "epoch": 0.4488,
      "grad_norm": 72.02120971679688,
      "learning_rate": 0.00018504,
      "loss": -116.4714,
      "step": 5610
    },
    {
      "epoch": 0.4496,
      "grad_norm": 331.1381530761719,
      "learning_rate": 0.00018501333333333336,
      "loss": -115.0831,
      "step": 5620
    },
    {
      "epoch": 0.4504,
      "grad_norm": 304.258056640625,
      "learning_rate": 0.0001849866666666667,
      "loss": -109.1779,
      "step": 5630
    },
    {
      "epoch": 0.4512,
      "grad_norm": 144.8413543701172,
      "learning_rate": 0.00018496,
      "loss": -109.8873,
      "step": 5640
    },
    {
      "epoch": 0.452,
      "grad_norm": 210.05958557128906,
      "learning_rate": 0.00018493333333333335,
      "loss": -113.3766,
      "step": 5650
    },
    {
      "epoch": 0.4528,
      "grad_norm": 76.48722076416016,
      "learning_rate": 0.00018490666666666668,
      "loss": -113.4679,
      "step": 5660
    },
    {
      "epoch": 0.4536,
      "grad_norm": 100.2171630859375,
      "learning_rate": 0.00018488,
      "loss": -115.0325,
      "step": 5670
    },
    {
      "epoch": 0.4544,
      "grad_norm": 88.81282806396484,
      "learning_rate": 0.00018485333333333333,
      "loss": -115.3388,
      "step": 5680
    },
    {
      "epoch": 0.4552,
      "grad_norm": 61.74827194213867,
      "learning_rate": 0.0001848266666666667,
      "loss": -114.2699,
      "step": 5690
    },
    {
      "epoch": 0.456,
      "grad_norm": 68.52152252197266,
      "learning_rate": 0.00018480000000000002,
      "loss": -115.6651,
      "step": 5700
    },
    {
      "epoch": 0.4568,
      "grad_norm": 70.29524230957031,
      "learning_rate": 0.00018477333333333332,
      "loss": -114.7497,
      "step": 5710
    },
    {
      "epoch": 0.4576,
      "grad_norm": 140.2889862060547,
      "learning_rate": 0.00018474666666666668,
      "loss": -116.7624,
      "step": 5720
    },
    {
      "epoch": 0.4584,
      "grad_norm": 58.30044937133789,
      "learning_rate": 0.00018472,
      "loss": -117.1569,
      "step": 5730
    },
    {
      "epoch": 0.4592,
      "grad_norm": 64.7415771484375,
      "learning_rate": 0.00018469333333333334,
      "loss": -114.1699,
      "step": 5740
    },
    {
      "epoch": 0.46,
      "grad_norm": 65.69821166992188,
      "learning_rate": 0.00018466666666666666,
      "loss": -116.4005,
      "step": 5750
    },
    {
      "epoch": 0.4608,
      "grad_norm": 68.19248962402344,
      "learning_rate": 0.00018464000000000002,
      "loss": -115.2504,
      "step": 5760
    },
    {
      "epoch": 0.4616,
      "grad_norm": 44.06074905395508,
      "learning_rate": 0.00018461333333333335,
      "loss": -115.5051,
      "step": 5770
    },
    {
      "epoch": 0.4624,
      "grad_norm": 52.08860397338867,
      "learning_rate": 0.00018458666666666668,
      "loss": -115.4891,
      "step": 5780
    },
    {
      "epoch": 0.4632,
      "grad_norm": 39.836063385009766,
      "learning_rate": 0.00018456,
      "loss": -116.4783,
      "step": 5790
    },
    {
      "epoch": 0.464,
      "grad_norm": 44.27580642700195,
      "learning_rate": 0.00018453333333333334,
      "loss": -116.067,
      "step": 5800
    },
    {
      "epoch": 0.4648,
      "grad_norm": 55.441402435302734,
      "learning_rate": 0.00018450666666666667,
      "loss": -116.5588,
      "step": 5810
    },
    {
      "epoch": 0.4656,
      "grad_norm": 62.58677291870117,
      "learning_rate": 0.00018448,
      "loss": -116.268,
      "step": 5820
    },
    {
      "epoch": 0.4664,
      "grad_norm": 63.867332458496094,
      "learning_rate": 0.00018445333333333335,
      "loss": -116.8775,
      "step": 5830
    },
    {
      "epoch": 0.4672,
      "grad_norm": 58.55681228637695,
      "learning_rate": 0.00018442666666666668,
      "loss": -117.2855,
      "step": 5840
    },
    {
      "epoch": 0.468,
      "grad_norm": 42.382904052734375,
      "learning_rate": 0.0001844,
      "loss": -115.5458,
      "step": 5850
    },
    {
      "epoch": 0.4688,
      "grad_norm": 68.94525909423828,
      "learning_rate": 0.00018437333333333334,
      "loss": -115.7372,
      "step": 5860
    },
    {
      "epoch": 0.4696,
      "grad_norm": 56.43513107299805,
      "learning_rate": 0.00018434666666666667,
      "loss": -114.3526,
      "step": 5870
    },
    {
      "epoch": 0.4704,
      "grad_norm": 49.93012619018555,
      "learning_rate": 0.00018432,
      "loss": -115.876,
      "step": 5880
    },
    {
      "epoch": 0.4712,
      "grad_norm": 38.751556396484375,
      "learning_rate": 0.00018429333333333335,
      "loss": -116.497,
      "step": 5890
    },
    {
      "epoch": 0.472,
      "grad_norm": 45.523067474365234,
      "learning_rate": 0.00018426666666666668,
      "loss": -116.5287,
      "step": 5900
    },
    {
      "epoch": 0.4728,
      "grad_norm": 64.8694076538086,
      "learning_rate": 0.00018424,
      "loss": -115.4963,
      "step": 5910
    },
    {
      "epoch": 0.4736,
      "grad_norm": 42.826419830322266,
      "learning_rate": 0.00018421333333333334,
      "loss": -117.4052,
      "step": 5920
    },
    {
      "epoch": 0.4744,
      "grad_norm": 52.41679000854492,
      "learning_rate": 0.0001841866666666667,
      "loss": -116.3543,
      "step": 5930
    },
    {
      "epoch": 0.4752,
      "grad_norm": 51.236148834228516,
      "learning_rate": 0.00018416,
      "loss": -115.9804,
      "step": 5940
    },
    {
      "epoch": 0.476,
      "grad_norm": 87.63955688476562,
      "learning_rate": 0.00018413333333333333,
      "loss": -116.3934,
      "step": 5950
    },
    {
      "epoch": 0.4768,
      "grad_norm": 79.54676818847656,
      "learning_rate": 0.00018410666666666668,
      "loss": -116.3705,
      "step": 5960
    },
    {
      "epoch": 0.4776,
      "grad_norm": 51.251583099365234,
      "learning_rate": 0.00018408,
      "loss": -116.9209,
      "step": 5970
    },
    {
      "epoch": 0.4784,
      "grad_norm": 50.485572814941406,
      "learning_rate": 0.00018405333333333334,
      "loss": -115.5145,
      "step": 5980
    },
    {
      "epoch": 0.4792,
      "grad_norm": 54.81180191040039,
      "learning_rate": 0.00018402666666666667,
      "loss": -116.4364,
      "step": 5990
    },
    {
      "epoch": 0.48,
      "grad_norm": 51.20444869995117,
      "learning_rate": 0.00018400000000000003,
      "loss": -115.7187,
      "step": 6000
    },
    {
      "epoch": 0.4808,
      "grad_norm": 45.4708251953125,
      "learning_rate": 0.00018397333333333336,
      "loss": -116.3517,
      "step": 6010
    },
    {
      "epoch": 0.4816,
      "grad_norm": 58.68874740600586,
      "learning_rate": 0.00018394666666666666,
      "loss": -116.9612,
      "step": 6020
    },
    {
      "epoch": 0.4824,
      "grad_norm": 50.43313217163086,
      "learning_rate": 0.00018392000000000001,
      "loss": -116.3708,
      "step": 6030
    },
    {
      "epoch": 0.4832,
      "grad_norm": 62.25627136230469,
      "learning_rate": 0.00018389333333333334,
      "loss": -115.5625,
      "step": 6040
    },
    {
      "epoch": 0.484,
      "grad_norm": 34.347625732421875,
      "learning_rate": 0.00018386666666666667,
      "loss": -117.1598,
      "step": 6050
    },
    {
      "epoch": 0.4848,
      "grad_norm": 36.23813247680664,
      "learning_rate": 0.00018384,
      "loss": -117.7682,
      "step": 6060
    },
    {
      "epoch": 0.4856,
      "grad_norm": 45.396541595458984,
      "learning_rate": 0.00018381333333333336,
      "loss": -115.8577,
      "step": 6070
    },
    {
      "epoch": 0.4864,
      "grad_norm": 65.1466064453125,
      "learning_rate": 0.0001837866666666667,
      "loss": -117.1323,
      "step": 6080
    },
    {
      "epoch": 0.4872,
      "grad_norm": 52.43524932861328,
      "learning_rate": 0.00018376,
      "loss": -116.1143,
      "step": 6090
    },
    {
      "epoch": 0.488,
      "grad_norm": 42.59843444824219,
      "learning_rate": 0.00018373333333333335,
      "loss": -116.3382,
      "step": 6100
    },
    {
      "epoch": 0.4888,
      "grad_norm": 46.32158279418945,
      "learning_rate": 0.00018370666666666667,
      "loss": -115.4008,
      "step": 6110
    },
    {
      "epoch": 0.4896,
      "grad_norm": 34.64784240722656,
      "learning_rate": 0.00018368,
      "loss": -117.1124,
      "step": 6120
    },
    {
      "epoch": 0.4904,
      "grad_norm": 44.88443374633789,
      "learning_rate": 0.00018365333333333333,
      "loss": -116.7588,
      "step": 6130
    },
    {
      "epoch": 0.4912,
      "grad_norm": 43.460670471191406,
      "learning_rate": 0.0001836266666666667,
      "loss": -116.1293,
      "step": 6140
    },
    {
      "epoch": 0.492,
      "grad_norm": 47.045555114746094,
      "learning_rate": 0.00018360000000000002,
      "loss": -116.8993,
      "step": 6150
    },
    {
      "epoch": 0.4928,
      "grad_norm": 55.406105041503906,
      "learning_rate": 0.00018357333333333335,
      "loss": -116.5655,
      "step": 6160
    },
    {
      "epoch": 0.4936,
      "grad_norm": 63.63386535644531,
      "learning_rate": 0.00018354666666666668,
      "loss": -115.8754,
      "step": 6170
    },
    {
      "epoch": 0.4944,
      "grad_norm": 43.17477798461914,
      "learning_rate": 0.00018352,
      "loss": -117.8289,
      "step": 6180
    },
    {
      "epoch": 0.4952,
      "grad_norm": 63.49277877807617,
      "learning_rate": 0.00018349333333333333,
      "loss": -117.1849,
      "step": 6190
    },
    {
      "epoch": 0.496,
      "grad_norm": 52.43095779418945,
      "learning_rate": 0.00018346666666666666,
      "loss": -114.8678,
      "step": 6200
    },
    {
      "epoch": 0.4968,
      "grad_norm": 61.856849670410156,
      "learning_rate": 0.00018344000000000002,
      "loss": -116.6945,
      "step": 6210
    },
    {
      "epoch": 0.4976,
      "grad_norm": 48.52446746826172,
      "learning_rate": 0.00018341333333333335,
      "loss": -115.6499,
      "step": 6220
    },
    {
      "epoch": 0.4984,
      "grad_norm": 65.83057403564453,
      "learning_rate": 0.00018338666666666668,
      "loss": -116.5362,
      "step": 6230
    },
    {
      "epoch": 0.4992,
      "grad_norm": 41.54655456542969,
      "learning_rate": 0.00018336,
      "loss": -116.3769,
      "step": 6240
    },
    {
      "epoch": 0.5,
      "grad_norm": 49.825599670410156,
      "learning_rate": 0.00018333333333333334,
      "loss": -117.1861,
      "step": 6250
    },
    {
      "epoch": 0.5008,
      "grad_norm": 47.166263580322266,
      "learning_rate": 0.00018330666666666666,
      "loss": -116.4852,
      "step": 6260
    },
    {
      "epoch": 0.5016,
      "grad_norm": 38.58407211303711,
      "learning_rate": 0.00018328000000000002,
      "loss": -116.2399,
      "step": 6270
    },
    {
      "epoch": 0.5024,
      "grad_norm": 46.6326904296875,
      "learning_rate": 0.00018325333333333335,
      "loss": -116.2185,
      "step": 6280
    },
    {
      "epoch": 0.5032,
      "grad_norm": 48.19611740112305,
      "learning_rate": 0.00018322666666666668,
      "loss": -114.4562,
      "step": 6290
    },
    {
      "epoch": 0.504,
      "grad_norm": 37.34022903442383,
      "learning_rate": 0.0001832,
      "loss": -116.6803,
      "step": 6300
    },
    {
      "epoch": 0.5048,
      "grad_norm": 45.492740631103516,
      "learning_rate": 0.00018317333333333336,
      "loss": -115.7884,
      "step": 6310
    },
    {
      "epoch": 0.5056,
      "grad_norm": 63.233306884765625,
      "learning_rate": 0.00018314666666666667,
      "loss": -114.8778,
      "step": 6320
    },
    {
      "epoch": 0.5064,
      "grad_norm": 69.35123443603516,
      "learning_rate": 0.00018312,
      "loss": -116.1393,
      "step": 6330
    },
    {
      "epoch": 0.5072,
      "grad_norm": 53.96925735473633,
      "learning_rate": 0.00018309333333333335,
      "loss": -116.7452,
      "step": 6340
    },
    {
      "epoch": 0.508,
      "grad_norm": 46.314266204833984,
      "learning_rate": 0.00018306666666666668,
      "loss": -116.6979,
      "step": 6350
    },
    {
      "epoch": 0.5088,
      "grad_norm": 44.10148620605469,
      "learning_rate": 0.00018304,
      "loss": -116.3622,
      "step": 6360
    },
    {
      "epoch": 0.5096,
      "grad_norm": 39.20956039428711,
      "learning_rate": 0.00018301333333333334,
      "loss": -115.929,
      "step": 6370
    },
    {
      "epoch": 0.5104,
      "grad_norm": 37.32683563232422,
      "learning_rate": 0.0001829866666666667,
      "loss": -116.7858,
      "step": 6380
    },
    {
      "epoch": 0.5112,
      "grad_norm": 55.832115173339844,
      "learning_rate": 0.00018296,
      "loss": -115.8496,
      "step": 6390
    },
    {
      "epoch": 0.512,
      "grad_norm": 48.543983459472656,
      "learning_rate": 0.00018293333333333333,
      "loss": -116.7262,
      "step": 6400
    },
    {
      "epoch": 0.5128,
      "grad_norm": 51.20538330078125,
      "learning_rate": 0.00018290666666666668,
      "loss": -117.2329,
      "step": 6410
    },
    {
      "epoch": 0.5136,
      "grad_norm": 42.97933578491211,
      "learning_rate": 0.00018288,
      "loss": -117.1459,
      "step": 6420
    },
    {
      "epoch": 0.5144,
      "grad_norm": 55.37627029418945,
      "learning_rate": 0.00018285333333333334,
      "loss": -117.7163,
      "step": 6430
    },
    {
      "epoch": 0.5152,
      "grad_norm": 64.1421127319336,
      "learning_rate": 0.00018282666666666667,
      "loss": -117.5322,
      "step": 6440
    },
    {
      "epoch": 0.516,
      "grad_norm": 45.550682067871094,
      "learning_rate": 0.00018280000000000003,
      "loss": -117.0367,
      "step": 6450
    },
    {
      "epoch": 0.5168,
      "grad_norm": 47.14752960205078,
      "learning_rate": 0.00018277333333333335,
      "loss": -116.6271,
      "step": 6460
    },
    {
      "epoch": 0.5176,
      "grad_norm": 40.93819808959961,
      "learning_rate": 0.00018274666666666666,
      "loss": -116.5648,
      "step": 6470
    },
    {
      "epoch": 0.5184,
      "grad_norm": 50.87745666503906,
      "learning_rate": 0.00018272,
      "loss": -115.6418,
      "step": 6480
    },
    {
      "epoch": 0.5192,
      "grad_norm": 50.774967193603516,
      "learning_rate": 0.00018269333333333334,
      "loss": -117.5829,
      "step": 6490
    },
    {
      "epoch": 0.52,
      "grad_norm": 45.96113967895508,
      "learning_rate": 0.00018266666666666667,
      "loss": -114.8246,
      "step": 6500
    },
    {
      "epoch": 0.5208,
      "grad_norm": 47.91546630859375,
      "learning_rate": 0.00018264,
      "loss": -117.4463,
      "step": 6510
    },
    {
      "epoch": 0.5216,
      "grad_norm": 49.19036102294922,
      "learning_rate": 0.00018261333333333336,
      "loss": -116.7511,
      "step": 6520
    },
    {
      "epoch": 0.5224,
      "grad_norm": 46.935420989990234,
      "learning_rate": 0.00018258666666666668,
      "loss": -116.4588,
      "step": 6530
    },
    {
      "epoch": 0.5232,
      "grad_norm": 42.20124053955078,
      "learning_rate": 0.00018256,
      "loss": -116.4041,
      "step": 6540
    },
    {
      "epoch": 0.524,
      "grad_norm": 48.649757385253906,
      "learning_rate": 0.00018253333333333334,
      "loss": -116.9226,
      "step": 6550
    },
    {
      "epoch": 0.5248,
      "grad_norm": 44.606258392333984,
      "learning_rate": 0.00018250666666666667,
      "loss": -116.7273,
      "step": 6560
    },
    {
      "epoch": 0.5256,
      "grad_norm": 46.428550720214844,
      "learning_rate": 0.00018248,
      "loss": -116.6197,
      "step": 6570
    },
    {
      "epoch": 0.5264,
      "grad_norm": 48.56486129760742,
      "learning_rate": 0.00018245333333333333,
      "loss": -117.0523,
      "step": 6580
    },
    {
      "epoch": 0.5272,
      "grad_norm": 46.10417556762695,
      "learning_rate": 0.00018242666666666669,
      "loss": -115.8413,
      "step": 6590
    },
    {
      "epoch": 0.528,
      "grad_norm": 53.22801971435547,
      "learning_rate": 0.00018240000000000002,
      "loss": -116.6773,
      "step": 6600
    },
    {
      "epoch": 0.5288,
      "grad_norm": 54.42750549316406,
      "learning_rate": 0.00018237333333333334,
      "loss": -116.3678,
      "step": 6610
    },
    {
      "epoch": 0.5296,
      "grad_norm": 53.80844497680664,
      "learning_rate": 0.00018234666666666667,
      "loss": -115.8328,
      "step": 6620
    },
    {
      "epoch": 0.5304,
      "grad_norm": 48.40088653564453,
      "learning_rate": 0.00018232,
      "loss": -117.2703,
      "step": 6630
    },
    {
      "epoch": 0.5312,
      "grad_norm": 40.11808776855469,
      "learning_rate": 0.00018229333333333333,
      "loss": -117.6592,
      "step": 6640
    },
    {
      "epoch": 0.532,
      "grad_norm": 61.12046432495117,
      "learning_rate": 0.0001822666666666667,
      "loss": -117.4192,
      "step": 6650
    },
    {
      "epoch": 0.5328,
      "grad_norm": 48.356170654296875,
      "learning_rate": 0.00018224000000000002,
      "loss": -117.2211,
      "step": 6660
    },
    {
      "epoch": 0.5336,
      "grad_norm": 45.098384857177734,
      "learning_rate": 0.00018221333333333335,
      "loss": -116.0394,
      "step": 6670
    },
    {
      "epoch": 0.5344,
      "grad_norm": 48.34357452392578,
      "learning_rate": 0.00018218666666666668,
      "loss": -117.1676,
      "step": 6680
    },
    {
      "epoch": 0.5352,
      "grad_norm": 53.26000213623047,
      "learning_rate": 0.00018216000000000003,
      "loss": -116.0113,
      "step": 6690
    },
    {
      "epoch": 0.536,
      "grad_norm": 55.31258010864258,
      "learning_rate": 0.00018213333333333333,
      "loss": -116.2731,
      "step": 6700
    },
    {
      "epoch": 0.5368,
      "grad_norm": 43.92825698852539,
      "learning_rate": 0.00018210666666666666,
      "loss": -116.4098,
      "step": 6710
    },
    {
      "epoch": 0.5376,
      "grad_norm": 42.69938278198242,
      "learning_rate": 0.00018208000000000002,
      "loss": -116.743,
      "step": 6720
    },
    {
      "epoch": 0.5384,
      "grad_norm": 56.44179916381836,
      "learning_rate": 0.00018205333333333335,
      "loss": -115.9815,
      "step": 6730
    },
    {
      "epoch": 0.5392,
      "grad_norm": 57.73887252807617,
      "learning_rate": 0.00018202666666666668,
      "loss": -116.2379,
      "step": 6740
    },
    {
      "epoch": 0.54,
      "grad_norm": 45.61983108520508,
      "learning_rate": 0.000182,
      "loss": -117.1034,
      "step": 6750
    },
    {
      "epoch": 0.5408,
      "grad_norm": 40.802791595458984,
      "learning_rate": 0.00018197333333333336,
      "loss": -116.9061,
      "step": 6760
    },
    {
      "epoch": 0.5416,
      "grad_norm": 46.17789840698242,
      "learning_rate": 0.00018194666666666666,
      "loss": -116.395,
      "step": 6770
    },
    {
      "epoch": 0.5424,
      "grad_norm": 66.211181640625,
      "learning_rate": 0.00018192,
      "loss": -117.0488,
      "step": 6780
    },
    {
      "epoch": 0.5432,
      "grad_norm": 45.40535354614258,
      "learning_rate": 0.00018189333333333335,
      "loss": -117.1853,
      "step": 6790
    },
    {
      "epoch": 0.544,
      "grad_norm": 55.419830322265625,
      "learning_rate": 0.00018186666666666668,
      "loss": -116.8891,
      "step": 6800
    },
    {
      "epoch": 0.5448,
      "grad_norm": 39.36368179321289,
      "learning_rate": 0.00018184,
      "loss": -116.8428,
      "step": 6810
    },
    {
      "epoch": 0.5456,
      "grad_norm": 52.30836486816406,
      "learning_rate": 0.00018181333333333334,
      "loss": -117.0873,
      "step": 6820
    },
    {
      "epoch": 0.5464,
      "grad_norm": 53.93484115600586,
      "learning_rate": 0.0001817866666666667,
      "loss": -115.1083,
      "step": 6830
    },
    {
      "epoch": 0.5472,
      "grad_norm": 56.295711517333984,
      "learning_rate": 0.00018176000000000002,
      "loss": -116.0054,
      "step": 6840
    },
    {
      "epoch": 0.548,
      "grad_norm": 48.59779739379883,
      "learning_rate": 0.00018173333333333332,
      "loss": -115.3479,
      "step": 6850
    },
    {
      "epoch": 0.5488,
      "grad_norm": 39.447425842285156,
      "learning_rate": 0.00018170666666666668,
      "loss": -116.2097,
      "step": 6860
    },
    {
      "epoch": 0.5496,
      "grad_norm": 35.570987701416016,
      "learning_rate": 0.00018168,
      "loss": -116.6985,
      "step": 6870
    },
    {
      "epoch": 0.5504,
      "grad_norm": 34.70112609863281,
      "learning_rate": 0.00018165333333333334,
      "loss": -116.0671,
      "step": 6880
    },
    {
      "epoch": 0.5512,
      "grad_norm": 41.29144287109375,
      "learning_rate": 0.00018162666666666667,
      "loss": -117.6045,
      "step": 6890
    },
    {
      "epoch": 0.552,
      "grad_norm": 59.58719253540039,
      "learning_rate": 0.00018160000000000002,
      "loss": -117.2098,
      "step": 6900
    },
    {
      "epoch": 0.5528,
      "grad_norm": 47.132362365722656,
      "learning_rate": 0.00018157333333333335,
      "loss": -117.4133,
      "step": 6910
    },
    {
      "epoch": 0.5536,
      "grad_norm": 47.70840835571289,
      "learning_rate": 0.00018154666666666665,
      "loss": -116.918,
      "step": 6920
    },
    {
      "epoch": 0.5544,
      "grad_norm": 40.254676818847656,
      "learning_rate": 0.00018152,
      "loss": -117.007,
      "step": 6930
    },
    {
      "epoch": 0.5552,
      "grad_norm": 41.702632904052734,
      "learning_rate": 0.00018149333333333334,
      "loss": -117.749,
      "step": 6940
    },
    {
      "epoch": 0.556,
      "grad_norm": 57.81248474121094,
      "learning_rate": 0.00018146666666666667,
      "loss": -116.3017,
      "step": 6950
    },
    {
      "epoch": 0.5568,
      "grad_norm": 48.701385498046875,
      "learning_rate": 0.00018144,
      "loss": -116.3376,
      "step": 6960
    },
    {
      "epoch": 0.5576,
      "grad_norm": 43.65536880493164,
      "learning_rate": 0.00018141333333333335,
      "loss": -116.826,
      "step": 6970
    },
    {
      "epoch": 0.5584,
      "grad_norm": 34.90165328979492,
      "learning_rate": 0.00018138666666666668,
      "loss": -116.9659,
      "step": 6980
    },
    {
      "epoch": 0.5592,
      "grad_norm": 32.42364501953125,
      "learning_rate": 0.00018136,
      "loss": -117.0347,
      "step": 6990
    },
    {
      "epoch": 0.56,
      "grad_norm": 32.579349517822266,
      "learning_rate": 0.00018133333333333334,
      "loss": -116.7354,
      "step": 7000
    },
    {
      "epoch": 0.5608,
      "grad_norm": 42.064395904541016,
      "learning_rate": 0.00018130666666666667,
      "loss": -114.9828,
      "step": 7010
    },
    {
      "epoch": 0.5616,
      "grad_norm": 42.948299407958984,
      "learning_rate": 0.00018128,
      "loss": -117.4066,
      "step": 7020
    },
    {
      "epoch": 0.5624,
      "grad_norm": 46.69017791748047,
      "learning_rate": 0.00018125333333333333,
      "loss": -117.4523,
      "step": 7030
    },
    {
      "epoch": 0.5632,
      "grad_norm": 37.10026550292969,
      "learning_rate": 0.00018122666666666668,
      "loss": -117.9312,
      "step": 7040
    },
    {
      "epoch": 0.564,
      "grad_norm": 46.52939224243164,
      "learning_rate": 0.0001812,
      "loss": -117.4407,
      "step": 7050
    },
    {
      "epoch": 0.5648,
      "grad_norm": 55.520957946777344,
      "learning_rate": 0.00018117333333333334,
      "loss": -116.5058,
      "step": 7060
    },
    {
      "epoch": 0.5656,
      "grad_norm": 44.18156433105469,
      "learning_rate": 0.00018114666666666667,
      "loss": -116.0443,
      "step": 7070
    },
    {
      "epoch": 0.5664,
      "grad_norm": 38.81914520263672,
      "learning_rate": 0.00018112,
      "loss": -116.2994,
      "step": 7080
    },
    {
      "epoch": 0.5672,
      "grad_norm": 44.79676818847656,
      "learning_rate": 0.00018109333333333333,
      "loss": -116.6726,
      "step": 7090
    },
    {
      "epoch": 0.568,
      "grad_norm": 60.41757583618164,
      "learning_rate": 0.00018106666666666669,
      "loss": -116.6289,
      "step": 7100
    },
    {
      "epoch": 0.5688,
      "grad_norm": 39.87009811401367,
      "learning_rate": 0.00018104000000000001,
      "loss": -116.0777,
      "step": 7110
    },
    {
      "epoch": 0.5696,
      "grad_norm": 45.78936767578125,
      "learning_rate": 0.00018101333333333334,
      "loss": -116.6606,
      "step": 7120
    },
    {
      "epoch": 0.5704,
      "grad_norm": 35.33668899536133,
      "learning_rate": 0.00018098666666666667,
      "loss": -116.3044,
      "step": 7130
    },
    {
      "epoch": 0.5712,
      "grad_norm": 48.87227249145508,
      "learning_rate": 0.00018096000000000003,
      "loss": -117.4768,
      "step": 7140
    },
    {
      "epoch": 0.572,
      "grad_norm": 41.416175842285156,
      "learning_rate": 0.00018093333333333333,
      "loss": -115.8566,
      "step": 7150
    },
    {
      "epoch": 0.5728,
      "grad_norm": 41.947967529296875,
      "learning_rate": 0.00018090666666666666,
      "loss": -116.6279,
      "step": 7160
    },
    {
      "epoch": 0.5736,
      "grad_norm": 43.00808334350586,
      "learning_rate": 0.00018088000000000002,
      "loss": -115.8292,
      "step": 7170
    },
    {
      "epoch": 0.5744,
      "grad_norm": 51.22386169433594,
      "learning_rate": 0.00018085333333333335,
      "loss": -116.9219,
      "step": 7180
    },
    {
      "epoch": 0.5752,
      "grad_norm": 39.6005859375,
      "learning_rate": 0.00018082666666666667,
      "loss": -117.1109,
      "step": 7190
    },
    {
      "epoch": 0.576,
      "grad_norm": 41.917884826660156,
      "learning_rate": 0.0001808,
      "loss": -117.3129,
      "step": 7200
    },
    {
      "epoch": 0.5768,
      "grad_norm": 36.19621658325195,
      "learning_rate": 0.00018077333333333336,
      "loss": -115.5164,
      "step": 7210
    },
    {
      "epoch": 0.5776,
      "grad_norm": 44.8084602355957,
      "learning_rate": 0.0001807466666666667,
      "loss": -115.4558,
      "step": 7220
    },
    {
      "epoch": 0.5784,
      "grad_norm": 58.22062301635742,
      "learning_rate": 0.00018072,
      "loss": -116.3725,
      "step": 7230
    },
    {
      "epoch": 0.5792,
      "grad_norm": 57.924468994140625,
      "learning_rate": 0.00018069333333333335,
      "loss": -116.0147,
      "step": 7240
    },
    {
      "epoch": 0.58,
      "grad_norm": 63.88901901245117,
      "learning_rate": 0.00018066666666666668,
      "loss": -115.3961,
      "step": 7250
    },
    {
      "epoch": 0.5808,
      "grad_norm": 32.50226974487305,
      "learning_rate": 0.00018064,
      "loss": -115.9636,
      "step": 7260
    },
    {
      "epoch": 0.5816,
      "grad_norm": 35.54066467285156,
      "learning_rate": 0.00018061333333333333,
      "loss": -117.2015,
      "step": 7270
    },
    {
      "epoch": 0.5824,
      "grad_norm": 46.701759338378906,
      "learning_rate": 0.0001805866666666667,
      "loss": -117.1544,
      "step": 7280
    },
    {
      "epoch": 0.5832,
      "grad_norm": 45.865230560302734,
      "learning_rate": 0.00018056000000000002,
      "loss": -117.0609,
      "step": 7290
    },
    {
      "epoch": 0.584,
      "grad_norm": 46.22263717651367,
      "learning_rate": 0.00018053333333333332,
      "loss": -118.0108,
      "step": 7300
    },
    {
      "epoch": 0.5848,
      "grad_norm": 43.880123138427734,
      "learning_rate": 0.00018050666666666668,
      "loss": -115.7819,
      "step": 7310
    },
    {
      "epoch": 0.5856,
      "grad_norm": 47.39733123779297,
      "learning_rate": 0.00018048,
      "loss": -117.6116,
      "step": 7320
    },
    {
      "epoch": 0.5864,
      "grad_norm": 47.9794807434082,
      "learning_rate": 0.00018045333333333334,
      "loss": -116.6503,
      "step": 7330
    },
    {
      "epoch": 0.5872,
      "grad_norm": 35.93383026123047,
      "learning_rate": 0.00018042666666666666,
      "loss": -116.8597,
      "step": 7340
    },
    {
      "epoch": 0.588,
      "grad_norm": 46.510955810546875,
      "learning_rate": 0.00018040000000000002,
      "loss": -117.6313,
      "step": 7350
    },
    {
      "epoch": 0.5888,
      "grad_norm": 28.3170108795166,
      "learning_rate": 0.00018037333333333335,
      "loss": -116.5784,
      "step": 7360
    },
    {
      "epoch": 0.5896,
      "grad_norm": 38.01980209350586,
      "learning_rate": 0.00018034666666666668,
      "loss": -116.1342,
      "step": 7370
    },
    {
      "epoch": 0.5904,
      "grad_norm": 42.82265090942383,
      "learning_rate": 0.00018032,
      "loss": -117.0767,
      "step": 7380
    },
    {
      "epoch": 0.5912,
      "grad_norm": 34.74549102783203,
      "learning_rate": 0.00018029333333333334,
      "loss": -117.8968,
      "step": 7390
    },
    {
      "epoch": 0.592,
      "grad_norm": 66.24939727783203,
      "learning_rate": 0.00018026666666666667,
      "loss": -117.5779,
      "step": 7400
    },
    {
      "epoch": 0.5928,
      "grad_norm": 39.25239562988281,
      "learning_rate": 0.00018024,
      "loss": -117.2855,
      "step": 7410
    },
    {
      "epoch": 0.5936,
      "grad_norm": 36.88819122314453,
      "learning_rate": 0.00018021333333333335,
      "loss": -116.3457,
      "step": 7420
    },
    {
      "epoch": 0.5944,
      "grad_norm": 41.39708709716797,
      "learning_rate": 0.00018018666666666668,
      "loss": -117.3655,
      "step": 7430
    },
    {
      "epoch": 0.5952,
      "grad_norm": 49.122520446777344,
      "learning_rate": 0.00018016,
      "loss": -117.8375,
      "step": 7440
    },
    {
      "epoch": 0.596,
      "grad_norm": 78.77855682373047,
      "learning_rate": 0.00018013333333333334,
      "loss": -118.1002,
      "step": 7450
    },
    {
      "epoch": 0.5968,
      "grad_norm": 66.4359359741211,
      "learning_rate": 0.00018010666666666667,
      "loss": -117.0915,
      "step": 7460
    },
    {
      "epoch": 0.5976,
      "grad_norm": 36.20977783203125,
      "learning_rate": 0.00018008,
      "loss": -117.064,
      "step": 7470
    },
    {
      "epoch": 0.5984,
      "grad_norm": 36.05269241333008,
      "learning_rate": 0.00018005333333333335,
      "loss": -116.2517,
      "step": 7480
    },
    {
      "epoch": 0.5992,
      "grad_norm": 39.3780632019043,
      "learning_rate": 0.00018002666666666668,
      "loss": -116.5394,
      "step": 7490
    },
    {
      "epoch": 0.6,
      "grad_norm": 57.44963836669922,
      "learning_rate": 0.00018,
      "loss": -116.4889,
      "step": 7500
    },
    {
      "epoch": 0.6008,
      "grad_norm": 34.02549743652344,
      "learning_rate": 0.00017997333333333334,
      "loss": -116.8412,
      "step": 7510
    },
    {
      "epoch": 0.6016,
      "grad_norm": 48.246761322021484,
      "learning_rate": 0.0001799466666666667,
      "loss": -116.5046,
      "step": 7520
    },
    {
      "epoch": 0.6024,
      "grad_norm": 47.6602783203125,
      "learning_rate": 0.00017992,
      "loss": -117.1374,
      "step": 7530
    },
    {
      "epoch": 0.6032,
      "grad_norm": 46.4791145324707,
      "learning_rate": 0.00017989333333333333,
      "loss": -116.2337,
      "step": 7540
    },
    {
      "epoch": 0.604,
      "grad_norm": 46.95732498168945,
      "learning_rate": 0.00017986666666666668,
      "loss": -116.3045,
      "step": 7550
    },
    {
      "epoch": 0.6048,
      "grad_norm": 41.426795959472656,
      "learning_rate": 0.00017984,
      "loss": -116.9569,
      "step": 7560
    },
    {
      "epoch": 0.6056,
      "grad_norm": 51.146751403808594,
      "learning_rate": 0.00017981333333333334,
      "loss": -116.2387,
      "step": 7570
    },
    {
      "epoch": 0.6064,
      "grad_norm": 44.228153228759766,
      "learning_rate": 0.00017978666666666667,
      "loss": -117.5371,
      "step": 7580
    },
    {
      "epoch": 0.6072,
      "grad_norm": 48.93086624145508,
      "learning_rate": 0.00017976000000000003,
      "loss": -117.1457,
      "step": 7590
    },
    {
      "epoch": 0.608,
      "grad_norm": 33.70541763305664,
      "learning_rate": 0.00017973333333333333,
      "loss": -116.5699,
      "step": 7600
    },
    {
      "epoch": 0.6088,
      "grad_norm": 44.03140640258789,
      "learning_rate": 0.00017970666666666666,
      "loss": -117.7898,
      "step": 7610
    },
    {
      "epoch": 0.6096,
      "grad_norm": 43.485652923583984,
      "learning_rate": 0.00017968000000000001,
      "loss": -116.5696,
      "step": 7620
    },
    {
      "epoch": 0.6104,
      "grad_norm": 47.08292007446289,
      "learning_rate": 0.00017965333333333334,
      "loss": -116.7182,
      "step": 7630
    },
    {
      "epoch": 0.6112,
      "grad_norm": 35.51005172729492,
      "learning_rate": 0.00017962666666666667,
      "loss": -117.1939,
      "step": 7640
    },
    {
      "epoch": 0.612,
      "grad_norm": 34.9428596496582,
      "learning_rate": 0.0001796,
      "loss": -116.2897,
      "step": 7650
    },
    {
      "epoch": 0.6128,
      "grad_norm": 36.63413619995117,
      "learning_rate": 0.00017957333333333336,
      "loss": -116.8844,
      "step": 7660
    },
    {
      "epoch": 0.6136,
      "grad_norm": 42.40555191040039,
      "learning_rate": 0.0001795466666666667,
      "loss": -117.2214,
      "step": 7670
    },
    {
      "epoch": 0.6144,
      "grad_norm": 44.988037109375,
      "learning_rate": 0.00017952,
      "loss": -117.7959,
      "step": 7680
    },
    {
      "epoch": 0.6152,
      "grad_norm": 52.753849029541016,
      "learning_rate": 0.00017949333333333335,
      "loss": -116.485,
      "step": 7690
    },
    {
      "epoch": 0.616,
      "grad_norm": 47.51270294189453,
      "learning_rate": 0.00017946666666666667,
      "loss": -116.3552,
      "step": 7700
    },
    {
      "epoch": 0.6168,
      "grad_norm": 42.75471115112305,
      "learning_rate": 0.00017944,
      "loss": -117.8938,
      "step": 7710
    },
    {
      "epoch": 0.6176,
      "grad_norm": 29.290790557861328,
      "learning_rate": 0.00017941333333333333,
      "loss": -117.4619,
      "step": 7720
    },
    {
      "epoch": 0.6184,
      "grad_norm": 50.05339813232422,
      "learning_rate": 0.0001793866666666667,
      "loss": -116.25,
      "step": 7730
    },
    {
      "epoch": 0.6192,
      "grad_norm": 35.6251335144043,
      "learning_rate": 0.00017936000000000002,
      "loss": -116.6741,
      "step": 7740
    },
    {
      "epoch": 0.62,
      "grad_norm": 46.092987060546875,
      "learning_rate": 0.00017933333333333332,
      "loss": -116.0879,
      "step": 7750
    },
    {
      "epoch": 0.6208,
      "grad_norm": 43.09198760986328,
      "learning_rate": 0.00017930666666666668,
      "loss": -116.9907,
      "step": 7760
    },
    {
      "epoch": 0.6216,
      "grad_norm": 45.09026336669922,
      "learning_rate": 0.00017928,
      "loss": -116.6262,
      "step": 7770
    },
    {
      "epoch": 0.6224,
      "grad_norm": 48.55646896362305,
      "learning_rate": 0.00017925333333333333,
      "loss": -117.7338,
      "step": 7780
    },
    {
      "epoch": 0.6232,
      "grad_norm": 37.72053527832031,
      "learning_rate": 0.00017922666666666666,
      "loss": -115.9088,
      "step": 7790
    },
    {
      "epoch": 0.624,
      "grad_norm": 46.23629379272461,
      "learning_rate": 0.00017920000000000002,
      "loss": -117.9448,
      "step": 7800
    },
    {
      "epoch": 0.6248,
      "grad_norm": 51.24875259399414,
      "learning_rate": 0.00017917333333333335,
      "loss": -116.4868,
      "step": 7810
    },
    {
      "epoch": 0.6256,
      "grad_norm": 45.128257751464844,
      "learning_rate": 0.00017914666666666668,
      "loss": -116.6464,
      "step": 7820
    },
    {
      "epoch": 0.6264,
      "grad_norm": 76.88504028320312,
      "learning_rate": 0.00017912,
      "loss": -116.5213,
      "step": 7830
    },
    {
      "epoch": 0.6272,
      "grad_norm": 37.46649169921875,
      "learning_rate": 0.00017909333333333334,
      "loss": -116.4843,
      "step": 7840
    },
    {
      "epoch": 0.628,
      "grad_norm": 43.57767868041992,
      "learning_rate": 0.00017906666666666666,
      "loss": -116.5198,
      "step": 7850
    },
    {
      "epoch": 0.6288,
      "grad_norm": 44.009864807128906,
      "learning_rate": 0.00017904000000000002,
      "loss": -117.3353,
      "step": 7860
    },
    {
      "epoch": 0.6296,
      "grad_norm": 35.634647369384766,
      "learning_rate": 0.00017901333333333335,
      "loss": -117.6926,
      "step": 7870
    },
    {
      "epoch": 0.6304,
      "grad_norm": 34.402427673339844,
      "learning_rate": 0.00017898666666666668,
      "loss": -116.2287,
      "step": 7880
    },
    {
      "epoch": 0.6312,
      "grad_norm": 53.60330581665039,
      "learning_rate": 0.00017896,
      "loss": -116.9334,
      "step": 7890
    },
    {
      "epoch": 0.632,
      "grad_norm": 44.69499969482422,
      "learning_rate": 0.00017893333333333336,
      "loss": -116.3592,
      "step": 7900
    },
    {
      "epoch": 0.6328,
      "grad_norm": 64.42022705078125,
      "learning_rate": 0.00017890666666666667,
      "loss": -116.9894,
      "step": 7910
    },
    {
      "epoch": 0.6336,
      "grad_norm": 43.38853073120117,
      "learning_rate": 0.00017888,
      "loss": -115.3559,
      "step": 7920
    },
    {
      "epoch": 0.6344,
      "grad_norm": 33.260589599609375,
      "learning_rate": 0.00017885333333333335,
      "loss": -117.6581,
      "step": 7930
    },
    {
      "epoch": 0.6352,
      "grad_norm": 40.554141998291016,
      "learning_rate": 0.00017882666666666668,
      "loss": -117.6118,
      "step": 7940
    },
    {
      "epoch": 0.636,
      "grad_norm": 51.8065185546875,
      "learning_rate": 0.0001788,
      "loss": -115.2736,
      "step": 7950
    },
    {
      "epoch": 0.6368,
      "grad_norm": 47.713802337646484,
      "learning_rate": 0.00017877333333333334,
      "loss": -116.9253,
      "step": 7960
    },
    {
      "epoch": 0.6376,
      "grad_norm": 50.183555603027344,
      "learning_rate": 0.0001787466666666667,
      "loss": -118.1005,
      "step": 7970
    },
    {
      "epoch": 0.6384,
      "grad_norm": 37.070091247558594,
      "learning_rate": 0.00017872,
      "loss": -116.5021,
      "step": 7980
    },
    {
      "epoch": 0.6392,
      "grad_norm": 54.4720573425293,
      "learning_rate": 0.00017869333333333333,
      "loss": -118.0732,
      "step": 7990
    },
    {
      "epoch": 0.64,
      "grad_norm": 38.36760330200195,
      "learning_rate": 0.00017866666666666668,
      "loss": -116.0683,
      "step": 8000
    },
    {
      "epoch": 0.6408,
      "grad_norm": 46.75566101074219,
      "learning_rate": 0.00017864,
      "loss": -117.4946,
      "step": 8010
    },
    {
      "epoch": 0.6416,
      "grad_norm": 46.86385726928711,
      "learning_rate": 0.00017861333333333334,
      "loss": -116.7693,
      "step": 8020
    },
    {
      "epoch": 0.6424,
      "grad_norm": 39.511802673339844,
      "learning_rate": 0.00017858666666666667,
      "loss": -116.5529,
      "step": 8030
    },
    {
      "epoch": 0.6432,
      "grad_norm": 38.33841323852539,
      "learning_rate": 0.00017856000000000003,
      "loss": -115.3659,
      "step": 8040
    },
    {
      "epoch": 0.644,
      "grad_norm": 56.69696807861328,
      "learning_rate": 0.00017853333333333335,
      "loss": -117.1656,
      "step": 8050
    },
    {
      "epoch": 0.6448,
      "grad_norm": 41.945037841796875,
      "learning_rate": 0.00017850666666666666,
      "loss": -114.8363,
      "step": 8060
    },
    {
      "epoch": 0.6456,
      "grad_norm": 53.45170593261719,
      "learning_rate": 0.00017848,
      "loss": -115.7865,
      "step": 8070
    },
    {
      "epoch": 0.6464,
      "grad_norm": 59.19235610961914,
      "learning_rate": 0.00017845333333333334,
      "loss": -117.3003,
      "step": 8080
    },
    {
      "epoch": 0.6472,
      "grad_norm": 51.13331604003906,
      "learning_rate": 0.00017842666666666667,
      "loss": -115.6961,
      "step": 8090
    },
    {
      "epoch": 0.648,
      "grad_norm": 49.789833068847656,
      "learning_rate": 0.0001784,
      "loss": -116.2824,
      "step": 8100
    },
    {
      "epoch": 0.6488,
      "grad_norm": 45.11083221435547,
      "learning_rate": 0.00017837333333333336,
      "loss": -117.308,
      "step": 8110
    },
    {
      "epoch": 0.6496,
      "grad_norm": 44.691226959228516,
      "learning_rate": 0.00017834666666666668,
      "loss": -115.8684,
      "step": 8120
    },
    {
      "epoch": 0.6504,
      "grad_norm": 41.87520980834961,
      "learning_rate": 0.00017832,
      "loss": -117.5663,
      "step": 8130
    },
    {
      "epoch": 0.6512,
      "grad_norm": 32.527652740478516,
      "learning_rate": 0.00017829333333333334,
      "loss": -116.7983,
      "step": 8140
    },
    {
      "epoch": 0.652,
      "grad_norm": 38.96406936645508,
      "learning_rate": 0.00017826666666666667,
      "loss": -117.0484,
      "step": 8150
    },
    {
      "epoch": 0.6528,
      "grad_norm": 32.98625946044922,
      "learning_rate": 0.00017824,
      "loss": -117.2295,
      "step": 8160
    },
    {
      "epoch": 0.6536,
      "grad_norm": 56.24735641479492,
      "learning_rate": 0.00017821333333333333,
      "loss": -116.7363,
      "step": 8170
    },
    {
      "epoch": 0.6544,
      "grad_norm": 61.56997299194336,
      "learning_rate": 0.00017818666666666669,
      "loss": -117.373,
      "step": 8180
    },
    {
      "epoch": 0.6552,
      "grad_norm": 55.31660079956055,
      "learning_rate": 0.00017816000000000002,
      "loss": -116.2382,
      "step": 8190
    },
    {
      "epoch": 0.656,
      "grad_norm": 50.214683532714844,
      "learning_rate": 0.00017813333333333334,
      "loss": -116.5147,
      "step": 8200
    },
    {
      "epoch": 0.6568,
      "grad_norm": 27.964555740356445,
      "learning_rate": 0.00017810666666666667,
      "loss": -116.3624,
      "step": 8210
    },
    {
      "epoch": 0.6576,
      "grad_norm": 44.756526947021484,
      "learning_rate": 0.00017808,
      "loss": -116.5015,
      "step": 8220
    },
    {
      "epoch": 0.6584,
      "grad_norm": 44.36581802368164,
      "learning_rate": 0.00017805333333333333,
      "loss": -115.8949,
      "step": 8230
    },
    {
      "epoch": 0.6592,
      "grad_norm": 35.15604019165039,
      "learning_rate": 0.0001780266666666667,
      "loss": -116.1405,
      "step": 8240
    },
    {
      "epoch": 0.66,
      "grad_norm": 51.24054718017578,
      "learning_rate": 0.00017800000000000002,
      "loss": -116.5141,
      "step": 8250
    },
    {
      "epoch": 0.6608,
      "grad_norm": 34.18670654296875,
      "learning_rate": 0.00017797333333333335,
      "loss": -115.8745,
      "step": 8260
    },
    {
      "epoch": 0.6616,
      "grad_norm": 46.126312255859375,
      "learning_rate": 0.00017794666666666668,
      "loss": -115.6763,
      "step": 8270
    },
    {
      "epoch": 0.6624,
      "grad_norm": 34.94189453125,
      "learning_rate": 0.00017792,
      "loss": -118.0447,
      "step": 8280
    },
    {
      "epoch": 0.6632,
      "grad_norm": 31.322267532348633,
      "learning_rate": 0.00017789333333333333,
      "loss": -116.9949,
      "step": 8290
    },
    {
      "epoch": 0.664,
      "grad_norm": 35.45851135253906,
      "learning_rate": 0.00017786666666666666,
      "loss": -117.3494,
      "step": 8300
    },
    {
      "epoch": 0.6648,
      "grad_norm": 35.05217742919922,
      "learning_rate": 0.00017784000000000002,
      "loss": -117.5676,
      "step": 8310
    },
    {
      "epoch": 0.6656,
      "grad_norm": 48.82609558105469,
      "learning_rate": 0.00017781333333333335,
      "loss": -116.975,
      "step": 8320
    },
    {
      "epoch": 0.6664,
      "grad_norm": 40.97316360473633,
      "learning_rate": 0.00017778666666666668,
      "loss": -116.6808,
      "step": 8330
    },
    {
      "epoch": 0.6672,
      "grad_norm": 37.475433349609375,
      "learning_rate": 0.00017776,
      "loss": -116.7773,
      "step": 8340
    },
    {
      "epoch": 0.668,
      "grad_norm": 32.0035285949707,
      "learning_rate": 0.00017773333333333336,
      "loss": -116.5331,
      "step": 8350
    },
    {
      "epoch": 0.6688,
      "grad_norm": 45.81812286376953,
      "learning_rate": 0.00017770666666666666,
      "loss": -116.0437,
      "step": 8360
    },
    {
      "epoch": 0.6696,
      "grad_norm": 32.23917007446289,
      "learning_rate": 0.00017768,
      "loss": -118.425,
      "step": 8370
    },
    {
      "epoch": 0.6704,
      "grad_norm": 34.261878967285156,
      "learning_rate": 0.00017765333333333335,
      "loss": -116.5344,
      "step": 8380
    },
    {
      "epoch": 0.6712,
      "grad_norm": 49.778499603271484,
      "learning_rate": 0.00017762666666666668,
      "loss": -116.2209,
      "step": 8390
    },
    {
      "epoch": 0.672,
      "grad_norm": 44.060821533203125,
      "learning_rate": 0.0001776,
      "loss": -115.2077,
      "step": 8400
    },
    {
      "epoch": 0.6728,
      "grad_norm": 39.106258392333984,
      "learning_rate": 0.00017757333333333334,
      "loss": -117.6046,
      "step": 8410
    },
    {
      "epoch": 0.6736,
      "grad_norm": 35.700721740722656,
      "learning_rate": 0.0001775466666666667,
      "loss": -116.6779,
      "step": 8420
    },
    {
      "epoch": 0.6744,
      "grad_norm": 38.49739456176758,
      "learning_rate": 0.00017752,
      "loss": -116.8539,
      "step": 8430
    },
    {
      "epoch": 0.6752,
      "grad_norm": 36.63945388793945,
      "learning_rate": 0.00017749333333333332,
      "loss": -117.2094,
      "step": 8440
    },
    {
      "epoch": 0.676,
      "grad_norm": 179.92340087890625,
      "learning_rate": 0.00017746666666666668,
      "loss": -117.8736,
      "step": 8450
    },
    {
      "epoch": 0.6768,
      "grad_norm": 43.007362365722656,
      "learning_rate": 0.00017744,
      "loss": -116.5265,
      "step": 8460
    },
    {
      "epoch": 0.6776,
      "grad_norm": 37.76871109008789,
      "learning_rate": 0.00017741333333333334,
      "loss": -116.0239,
      "step": 8470
    },
    {
      "epoch": 0.6784,
      "grad_norm": 54.19415283203125,
      "learning_rate": 0.00017738666666666667,
      "loss": -117.2823,
      "step": 8480
    },
    {
      "epoch": 0.6792,
      "grad_norm": 45.628604888916016,
      "learning_rate": 0.00017736000000000002,
      "loss": -117.2208,
      "step": 8490
    },
    {
      "epoch": 0.68,
      "grad_norm": 37.583499908447266,
      "learning_rate": 0.00017733333333333335,
      "loss": -116.9004,
      "step": 8500
    },
    {
      "epoch": 0.6808,
      "grad_norm": 47.783626556396484,
      "learning_rate": 0.00017730666666666665,
      "loss": -117.8704,
      "step": 8510
    },
    {
      "epoch": 0.6816,
      "grad_norm": 37.981529235839844,
      "learning_rate": 0.00017728,
      "loss": -117.1079,
      "step": 8520
    },
    {
      "epoch": 0.6824,
      "grad_norm": 41.569759368896484,
      "learning_rate": 0.00017725333333333334,
      "loss": -117.124,
      "step": 8530
    },
    {
      "epoch": 0.6832,
      "grad_norm": 44.38874816894531,
      "learning_rate": 0.00017722666666666667,
      "loss": -116.388,
      "step": 8540
    },
    {
      "epoch": 0.684,
      "grad_norm": 30.202728271484375,
      "learning_rate": 0.0001772,
      "loss": -117.1989,
      "step": 8550
    },
    {
      "epoch": 0.6848,
      "grad_norm": 35.20930862426758,
      "learning_rate": 0.00017717333333333335,
      "loss": -115.8835,
      "step": 8560
    },
    {
      "epoch": 0.6856,
      "grad_norm": 36.52688217163086,
      "learning_rate": 0.00017714666666666668,
      "loss": -118.0904,
      "step": 8570
    },
    {
      "epoch": 0.6864,
      "grad_norm": 34.76478958129883,
      "learning_rate": 0.00017712,
      "loss": -117.7977,
      "step": 8580
    },
    {
      "epoch": 0.6872,
      "grad_norm": 40.544227600097656,
      "learning_rate": 0.00017709333333333334,
      "loss": -115.5086,
      "step": 8590
    },
    {
      "epoch": 0.688,
      "grad_norm": 38.520450592041016,
      "learning_rate": 0.00017706666666666667,
      "loss": -117.872,
      "step": 8600
    },
    {
      "epoch": 0.6888,
      "grad_norm": 130.09129333496094,
      "learning_rate": 0.00017704,
      "loss": -118.113,
      "step": 8610
    },
    {
      "epoch": 0.6896,
      "grad_norm": 43.78343200683594,
      "learning_rate": 0.00017701333333333336,
      "loss": -116.0582,
      "step": 8620
    },
    {
      "epoch": 0.6904,
      "grad_norm": 40.78398513793945,
      "learning_rate": 0.00017698666666666668,
      "loss": -116.8421,
      "step": 8630
    },
    {
      "epoch": 0.6912,
      "grad_norm": 33.13228225708008,
      "learning_rate": 0.00017696,
      "loss": -114.556,
      "step": 8640
    },
    {
      "epoch": 0.692,
      "grad_norm": 39.38179016113281,
      "learning_rate": 0.00017693333333333334,
      "loss": -116.4203,
      "step": 8650
    },
    {
      "epoch": 0.6928,
      "grad_norm": 54.1925163269043,
      "learning_rate": 0.00017690666666666667,
      "loss": -116.6408,
      "step": 8660
    },
    {
      "epoch": 0.6936,
      "grad_norm": 35.46259307861328,
      "learning_rate": 0.00017688,
      "loss": -117.6413,
      "step": 8670
    },
    {
      "epoch": 0.6944,
      "grad_norm": 40.612125396728516,
      "learning_rate": 0.00017685333333333333,
      "loss": -117.3251,
      "step": 8680
    },
    {
      "epoch": 0.6952,
      "grad_norm": 32.79493713378906,
      "learning_rate": 0.00017682666666666669,
      "loss": -116.3192,
      "step": 8690
    },
    {
      "epoch": 0.696,
      "grad_norm": 34.6302490234375,
      "learning_rate": 0.00017680000000000001,
      "loss": -117.6763,
      "step": 8700
    },
    {
      "epoch": 0.6968,
      "grad_norm": 37.50491714477539,
      "learning_rate": 0.00017677333333333334,
      "loss": -116.9201,
      "step": 8710
    },
    {
      "epoch": 0.6976,
      "grad_norm": 36.83292770385742,
      "learning_rate": 0.00017674666666666667,
      "loss": -117.813,
      "step": 8720
    },
    {
      "epoch": 0.6984,
      "grad_norm": 42.22932052612305,
      "learning_rate": 0.00017672000000000003,
      "loss": -116.1193,
      "step": 8730
    },
    {
      "epoch": 0.6992,
      "grad_norm": 36.96061325073242,
      "learning_rate": 0.00017669333333333333,
      "loss": -116.2352,
      "step": 8740
    },
    {
      "epoch": 0.7,
      "grad_norm": 36.22679901123047,
      "learning_rate": 0.00017666666666666666,
      "loss": -115.5243,
      "step": 8750
    },
    {
      "epoch": 0.7008,
      "grad_norm": 40.591957092285156,
      "learning_rate": 0.00017664000000000002,
      "loss": -117.5236,
      "step": 8760
    },
    {
      "epoch": 0.7016,
      "grad_norm": 51.056861877441406,
      "learning_rate": 0.00017661333333333335,
      "loss": -116.7633,
      "step": 8770
    },
    {
      "epoch": 0.7024,
      "grad_norm": 36.088687896728516,
      "learning_rate": 0.00017658666666666667,
      "loss": -116.8765,
      "step": 8780
    },
    {
      "epoch": 0.7032,
      "grad_norm": 49.614891052246094,
      "learning_rate": 0.00017656,
      "loss": -118.246,
      "step": 8790
    },
    {
      "epoch": 0.704,
      "grad_norm": 32.98260498046875,
      "learning_rate": 0.00017653333333333336,
      "loss": -116.8395,
      "step": 8800
    },
    {
      "epoch": 0.7048,
      "grad_norm": 37.58303451538086,
      "learning_rate": 0.00017650666666666666,
      "loss": -116.2576,
      "step": 8810
    },
    {
      "epoch": 0.7056,
      "grad_norm": 61.086238861083984,
      "learning_rate": 0.00017648,
      "loss": -116.5773,
      "step": 8820
    },
    {
      "epoch": 0.7064,
      "grad_norm": 31.73227310180664,
      "learning_rate": 0.00017645333333333335,
      "loss": -117.3061,
      "step": 8830
    },
    {
      "epoch": 0.7072,
      "grad_norm": 27.5671329498291,
      "learning_rate": 0.00017642666666666668,
      "loss": -117.2918,
      "step": 8840
    },
    {
      "epoch": 0.708,
      "grad_norm": 156.36019897460938,
      "learning_rate": 0.0001764,
      "loss": -117.2204,
      "step": 8850
    },
    {
      "epoch": 0.7088,
      "grad_norm": 48.73027801513672,
      "learning_rate": 0.00017637333333333333,
      "loss": -116.2298,
      "step": 8860
    },
    {
      "epoch": 0.7096,
      "grad_norm": 33.24005126953125,
      "learning_rate": 0.0001763466666666667,
      "loss": -117.226,
      "step": 8870
    },
    {
      "epoch": 0.7104,
      "grad_norm": 36.19174575805664,
      "learning_rate": 0.00017632000000000002,
      "loss": -115.1355,
      "step": 8880
    },
    {
      "epoch": 0.7112,
      "grad_norm": 32.27180862426758,
      "learning_rate": 0.00017629333333333332,
      "loss": -117.7675,
      "step": 8890
    },
    {
      "epoch": 0.712,
      "grad_norm": 42.77313995361328,
      "learning_rate": 0.00017626666666666668,
      "loss": -117.2102,
      "step": 8900
    },
    {
      "epoch": 0.7128,
      "grad_norm": 45.13800048828125,
      "learning_rate": 0.00017624,
      "loss": -116.2951,
      "step": 8910
    },
    {
      "epoch": 0.7136,
      "grad_norm": 62.660648345947266,
      "learning_rate": 0.00017621333333333334,
      "loss": -116.3383,
      "step": 8920
    },
    {
      "epoch": 0.7144,
      "grad_norm": 47.011653900146484,
      "learning_rate": 0.00017618666666666666,
      "loss": -117.1264,
      "step": 8930
    },
    {
      "epoch": 0.7152,
      "grad_norm": 33.322303771972656,
      "learning_rate": 0.00017616000000000002,
      "loss": -117.647,
      "step": 8940
    },
    {
      "epoch": 0.716,
      "grad_norm": 41.159523010253906,
      "learning_rate": 0.00017613333333333335,
      "loss": -117.6452,
      "step": 8950
    },
    {
      "epoch": 0.7168,
      "grad_norm": 41.147586822509766,
      "learning_rate": 0.00017610666666666665,
      "loss": -116.6981,
      "step": 8960
    },
    {
      "epoch": 0.7176,
      "grad_norm": 47.418766021728516,
      "learning_rate": 0.00017608,
      "loss": -116.6269,
      "step": 8970
    },
    {
      "epoch": 0.7184,
      "grad_norm": 41.500022888183594,
      "learning_rate": 0.00017605333333333334,
      "loss": -116.465,
      "step": 8980
    },
    {
      "epoch": 0.7192,
      "grad_norm": 28.2304744720459,
      "learning_rate": 0.00017602666666666667,
      "loss": -115.6281,
      "step": 8990
    },
    {
      "epoch": 0.72,
      "grad_norm": 37.49712371826172,
      "learning_rate": 0.00017600000000000002,
      "loss": -116.0471,
      "step": 9000
    },
    {
      "epoch": 0.7208,
      "grad_norm": 32.3600959777832,
      "learning_rate": 0.00017597333333333335,
      "loss": -117.6102,
      "step": 9010
    },
    {
      "epoch": 0.7216,
      "grad_norm": 32.07460403442383,
      "learning_rate": 0.00017594666666666668,
      "loss": -116.0979,
      "step": 9020
    },
    {
      "epoch": 0.7224,
      "grad_norm": 42.53641891479492,
      "learning_rate": 0.00017592,
      "loss": -116.4295,
      "step": 9030
    },
    {
      "epoch": 0.7232,
      "grad_norm": 43.53851318359375,
      "learning_rate": 0.00017589333333333334,
      "loss": -117.6577,
      "step": 9040
    },
    {
      "epoch": 0.724,
      "grad_norm": 45.58750915527344,
      "learning_rate": 0.00017586666666666667,
      "loss": -116.8985,
      "step": 9050
    },
    {
      "epoch": 0.7248,
      "grad_norm": 36.50088119506836,
      "learning_rate": 0.00017584,
      "loss": -117.1226,
      "step": 9060
    },
    {
      "epoch": 0.7256,
      "grad_norm": 35.25590896606445,
      "learning_rate": 0.00017581333333333335,
      "loss": -117.053,
      "step": 9070
    },
    {
      "epoch": 0.7264,
      "grad_norm": 46.611083984375,
      "learning_rate": 0.00017578666666666668,
      "loss": -117.6817,
      "step": 9080
    },
    {
      "epoch": 0.7272,
      "grad_norm": 37.277313232421875,
      "learning_rate": 0.00017576,
      "loss": -118.4638,
      "step": 9090
    },
    {
      "epoch": 0.728,
      "grad_norm": 38.4487419128418,
      "learning_rate": 0.00017573333333333334,
      "loss": -117.8905,
      "step": 9100
    },
    {
      "epoch": 0.7288,
      "grad_norm": 42.57809066772461,
      "learning_rate": 0.0001757066666666667,
      "loss": -117.9848,
      "step": 9110
    },
    {
      "epoch": 0.7296,
      "grad_norm": 38.56827163696289,
      "learning_rate": 0.00017568,
      "loss": -116.8407,
      "step": 9120
    },
    {
      "epoch": 0.7304,
      "grad_norm": 26.96900177001953,
      "learning_rate": 0.00017565333333333333,
      "loss": -117.8851,
      "step": 9130
    },
    {
      "epoch": 0.7312,
      "grad_norm": 36.88459396362305,
      "learning_rate": 0.00017562666666666668,
      "loss": -115.8911,
      "step": 9140
    },
    {
      "epoch": 0.732,
      "grad_norm": 34.27349090576172,
      "learning_rate": 0.0001756,
      "loss": -117.159,
      "step": 9150
    },
    {
      "epoch": 0.7328,
      "grad_norm": 34.07572937011719,
      "learning_rate": 0.00017557333333333334,
      "loss": -116.1964,
      "step": 9160
    },
    {
      "epoch": 0.7336,
      "grad_norm": 46.15312194824219,
      "learning_rate": 0.00017554666666666667,
      "loss": -117.7465,
      "step": 9170
    },
    {
      "epoch": 0.7344,
      "grad_norm": 36.199012756347656,
      "learning_rate": 0.00017552000000000003,
      "loss": -116.5112,
      "step": 9180
    },
    {
      "epoch": 0.7352,
      "grad_norm": 28.545446395874023,
      "learning_rate": 0.00017549333333333333,
      "loss": -118.1452,
      "step": 9190
    },
    {
      "epoch": 0.736,
      "grad_norm": 32.911617279052734,
      "learning_rate": 0.00017546666666666666,
      "loss": -116.1325,
      "step": 9200
    },
    {
      "epoch": 0.7368,
      "grad_norm": 37.589996337890625,
      "learning_rate": 0.00017544000000000001,
      "loss": -117.8555,
      "step": 9210
    },
    {
      "epoch": 0.7376,
      "grad_norm": 43.62628173828125,
      "learning_rate": 0.00017541333333333334,
      "loss": -116.2054,
      "step": 9220
    },
    {
      "epoch": 0.7384,
      "grad_norm": 40.8306999206543,
      "learning_rate": 0.00017538666666666667,
      "loss": -116.0067,
      "step": 9230
    },
    {
      "epoch": 0.7392,
      "grad_norm": 39.630210876464844,
      "learning_rate": 0.00017536,
      "loss": -116.7189,
      "step": 9240
    },
    {
      "epoch": 0.74,
      "grad_norm": 37.813682556152344,
      "learning_rate": 0.00017533333333333336,
      "loss": -118.126,
      "step": 9250
    },
    {
      "epoch": 0.7408,
      "grad_norm": 29.68982696533203,
      "learning_rate": 0.0001753066666666667,
      "loss": -117.0832,
      "step": 9260
    },
    {
      "epoch": 0.7416,
      "grad_norm": 34.11018753051758,
      "learning_rate": 0.00017528,
      "loss": -114.9562,
      "step": 9270
    },
    {
      "epoch": 0.7424,
      "grad_norm": 38.50886154174805,
      "learning_rate": 0.00017525333333333334,
      "loss": -115.7195,
      "step": 9280
    },
    {
      "epoch": 0.7432,
      "grad_norm": 43.83024215698242,
      "learning_rate": 0.00017522666666666667,
      "loss": -117.0039,
      "step": 9290
    },
    {
      "epoch": 0.744,
      "grad_norm": 44.01445007324219,
      "learning_rate": 0.0001752,
      "loss": -117.5827,
      "step": 9300
    },
    {
      "epoch": 0.7448,
      "grad_norm": 42.317962646484375,
      "learning_rate": 0.00017517333333333333,
      "loss": -117.1248,
      "step": 9310
    },
    {
      "epoch": 0.7456,
      "grad_norm": 41.37525939941406,
      "learning_rate": 0.0001751466666666667,
      "loss": -117.1991,
      "step": 9320
    },
    {
      "epoch": 0.7464,
      "grad_norm": 35.179283142089844,
      "learning_rate": 0.00017512000000000002,
      "loss": -116.7185,
      "step": 9330
    },
    {
      "epoch": 0.7472,
      "grad_norm": 29.872228622436523,
      "learning_rate": 0.00017509333333333332,
      "loss": -117.3814,
      "step": 9340
    },
    {
      "epoch": 0.748,
      "grad_norm": 36.73237609863281,
      "learning_rate": 0.00017506666666666668,
      "loss": -117.6131,
      "step": 9350
    },
    {
      "epoch": 0.7488,
      "grad_norm": 39.22553253173828,
      "learning_rate": 0.00017504,
      "loss": -116.827,
      "step": 9360
    },
    {
      "epoch": 0.7496,
      "grad_norm": 35.09416580200195,
      "learning_rate": 0.00017501333333333333,
      "loss": -116.2173,
      "step": 9370
    },
    {
      "epoch": 0.7504,
      "grad_norm": 39.46403121948242,
      "learning_rate": 0.0001749866666666667,
      "loss": -116.7392,
      "step": 9380
    },
    {
      "epoch": 0.7512,
      "grad_norm": 51.719024658203125,
      "learning_rate": 0.00017496000000000002,
      "loss": -117.0319,
      "step": 9390
    },
    {
      "epoch": 0.752,
      "grad_norm": 32.57200622558594,
      "learning_rate": 0.00017493333333333335,
      "loss": -115.1694,
      "step": 9400
    },
    {
      "epoch": 0.7528,
      "grad_norm": 45.162742614746094,
      "learning_rate": 0.00017490666666666668,
      "loss": -115.714,
      "step": 9410
    },
    {
      "epoch": 0.7536,
      "grad_norm": 37.24314880371094,
      "learning_rate": 0.00017488,
      "loss": -117.3681,
      "step": 9420
    },
    {
      "epoch": 0.7544,
      "grad_norm": 40.98931121826172,
      "learning_rate": 0.00017485333333333334,
      "loss": -117.7077,
      "step": 9430
    },
    {
      "epoch": 0.7552,
      "grad_norm": 37.85551834106445,
      "learning_rate": 0.00017482666666666666,
      "loss": -116.4321,
      "step": 9440
    },
    {
      "epoch": 0.756,
      "grad_norm": 56.13038635253906,
      "learning_rate": 0.00017480000000000002,
      "loss": -118.009,
      "step": 9450
    },
    {
      "epoch": 0.7568,
      "grad_norm": 40.8016357421875,
      "learning_rate": 0.00017477333333333335,
      "loss": -119.1305,
      "step": 9460
    },
    {
      "epoch": 0.7576,
      "grad_norm": 30.534414291381836,
      "learning_rate": 0.00017474666666666668,
      "loss": -117.2079,
      "step": 9470
    },
    {
      "epoch": 0.7584,
      "grad_norm": 37.898746490478516,
      "learning_rate": 0.00017472,
      "loss": -117.0391,
      "step": 9480
    },
    {
      "epoch": 0.7592,
      "grad_norm": 42.81519317626953,
      "learning_rate": 0.00017469333333333334,
      "loss": -116.5269,
      "step": 9490
    },
    {
      "epoch": 0.76,
      "grad_norm": 35.926368713378906,
      "learning_rate": 0.00017466666666666667,
      "loss": -116.5483,
      "step": 9500
    },
    {
      "epoch": 0.7608,
      "grad_norm": 35.566001892089844,
      "learning_rate": 0.00017464,
      "loss": -117.7541,
      "step": 9510
    },
    {
      "epoch": 0.7616,
      "grad_norm": 29.93995475769043,
      "learning_rate": 0.00017461333333333335,
      "loss": -117.3806,
      "step": 9520
    },
    {
      "epoch": 0.7624,
      "grad_norm": 28.539628982543945,
      "learning_rate": 0.00017458666666666668,
      "loss": -116.825,
      "step": 9530
    },
    {
      "epoch": 0.7632,
      "grad_norm": 50.70758056640625,
      "learning_rate": 0.00017456,
      "loss": -117.6176,
      "step": 9540
    },
    {
      "epoch": 0.764,
      "grad_norm": 36.3371696472168,
      "learning_rate": 0.00017453333333333334,
      "loss": -117.0015,
      "step": 9550
    },
    {
      "epoch": 0.7648,
      "grad_norm": 32.858863830566406,
      "learning_rate": 0.0001745066666666667,
      "loss": -117.6041,
      "step": 9560
    },
    {
      "epoch": 0.7656,
      "grad_norm": 42.86309051513672,
      "learning_rate": 0.00017448,
      "loss": -116.3444,
      "step": 9570
    },
    {
      "epoch": 0.7664,
      "grad_norm": 53.366912841796875,
      "learning_rate": 0.00017445333333333333,
      "loss": -116.296,
      "step": 9580
    },
    {
      "epoch": 0.7672,
      "grad_norm": 37.78923797607422,
      "learning_rate": 0.00017442666666666668,
      "loss": -117.2076,
      "step": 9590
    },
    {
      "epoch": 0.768,
      "grad_norm": 42.18913269042969,
      "learning_rate": 0.0001744,
      "loss": -117.1174,
      "step": 9600
    },
    {
      "epoch": 0.7688,
      "grad_norm": 42.92565155029297,
      "learning_rate": 0.00017437333333333334,
      "loss": -116.2135,
      "step": 9610
    },
    {
      "epoch": 0.7696,
      "grad_norm": 48.124366760253906,
      "learning_rate": 0.00017434666666666667,
      "loss": -115.8326,
      "step": 9620
    },
    {
      "epoch": 0.7704,
      "grad_norm": 36.04734420776367,
      "learning_rate": 0.00017432000000000003,
      "loss": -117.1097,
      "step": 9630
    },
    {
      "epoch": 0.7712,
      "grad_norm": 37.95924758911133,
      "learning_rate": 0.00017429333333333333,
      "loss": -117.987,
      "step": 9640
    },
    {
      "epoch": 0.772,
      "grad_norm": 37.06597137451172,
      "learning_rate": 0.00017426666666666666,
      "loss": -117.3225,
      "step": 9650
    },
    {
      "epoch": 0.7728,
      "grad_norm": 33.998260498046875,
      "learning_rate": 0.00017424,
      "loss": -117.0352,
      "step": 9660
    },
    {
      "epoch": 0.7736,
      "grad_norm": 32.9652099609375,
      "learning_rate": 0.00017421333333333334,
      "loss": -118.2414,
      "step": 9670
    },
    {
      "epoch": 0.7744,
      "grad_norm": 36.97661590576172,
      "learning_rate": 0.00017418666666666667,
      "loss": -115.7202,
      "step": 9680
    },
    {
      "epoch": 0.7752,
      "grad_norm": 39.901695251464844,
      "learning_rate": 0.00017416,
      "loss": -117.2575,
      "step": 9690
    },
    {
      "epoch": 0.776,
      "grad_norm": 57.448856353759766,
      "learning_rate": 0.00017413333333333336,
      "loss": -117.2167,
      "step": 9700
    },
    {
      "epoch": 0.7768,
      "grad_norm": 45.93378829956055,
      "learning_rate": 0.00017410666666666668,
      "loss": -118.1347,
      "step": 9710
    },
    {
      "epoch": 0.7776,
      "grad_norm": 51.501529693603516,
      "learning_rate": 0.00017408,
      "loss": -115.721,
      "step": 9720
    },
    {
      "epoch": 0.7784,
      "grad_norm": 36.28866958618164,
      "learning_rate": 0.00017405333333333334,
      "loss": -118.1411,
      "step": 9730
    },
    {
      "epoch": 0.7792,
      "grad_norm": 36.06340789794922,
      "learning_rate": 0.00017402666666666667,
      "loss": -118.0621,
      "step": 9740
    },
    {
      "epoch": 0.78,
      "grad_norm": 30.802003860473633,
      "learning_rate": 0.000174,
      "loss": -117.9844,
      "step": 9750
    },
    {
      "epoch": 0.7808,
      "grad_norm": 33.037010192871094,
      "learning_rate": 0.00017397333333333336,
      "loss": -117.3135,
      "step": 9760
    },
    {
      "epoch": 0.7816,
      "grad_norm": 50.03852844238281,
      "learning_rate": 0.00017394666666666669,
      "loss": -117.3165,
      "step": 9770
    },
    {
      "epoch": 0.7824,
      "grad_norm": 50.68558120727539,
      "learning_rate": 0.00017392000000000002,
      "loss": -117.2146,
      "step": 9780
    },
    {
      "epoch": 0.7832,
      "grad_norm": 38.66961669921875,
      "learning_rate": 0.00017389333333333334,
      "loss": -116.7565,
      "step": 9790
    },
    {
      "epoch": 0.784,
      "grad_norm": 37.93056869506836,
      "learning_rate": 0.00017386666666666667,
      "loss": -115.32,
      "step": 9800
    },
    {
      "epoch": 0.7848,
      "grad_norm": 31.89476776123047,
      "learning_rate": 0.00017384,
      "loss": -117.7201,
      "step": 9810
    },
    {
      "epoch": 0.7856,
      "grad_norm": 37.77830505371094,
      "learning_rate": 0.00017381333333333333,
      "loss": -116.4914,
      "step": 9820
    },
    {
      "epoch": 0.7864,
      "grad_norm": 33.47239685058594,
      "learning_rate": 0.0001737866666666667,
      "loss": -117.0523,
      "step": 9830
    },
    {
      "epoch": 0.7872,
      "grad_norm": 47.43251419067383,
      "learning_rate": 0.00017376000000000002,
      "loss": -117.3681,
      "step": 9840
    },
    {
      "epoch": 0.788,
      "grad_norm": 37.15602111816406,
      "learning_rate": 0.00017373333333333335,
      "loss": -117.6162,
      "step": 9850
    },
    {
      "epoch": 0.7888,
      "grad_norm": 31.92695426940918,
      "learning_rate": 0.00017370666666666668,
      "loss": -117.0347,
      "step": 9860
    },
    {
      "epoch": 0.7896,
      "grad_norm": 34.538578033447266,
      "learning_rate": 0.00017368,
      "loss": -116.8584,
      "step": 9870
    },
    {
      "epoch": 0.7904,
      "grad_norm": 44.177120208740234,
      "learning_rate": 0.00017365333333333333,
      "loss": -117.6602,
      "step": 9880
    },
    {
      "epoch": 0.7912,
      "grad_norm": 41.836727142333984,
      "learning_rate": 0.00017362666666666666,
      "loss": -117.2714,
      "step": 9890
    },
    {
      "epoch": 0.792,
      "grad_norm": 39.481483459472656,
      "learning_rate": 0.00017360000000000002,
      "loss": -118.0631,
      "step": 9900
    },
    {
      "epoch": 0.7928,
      "grad_norm": 39.051300048828125,
      "learning_rate": 0.00017357333333333335,
      "loss": -116.6002,
      "step": 9910
    },
    {
      "epoch": 0.7936,
      "grad_norm": 30.939611434936523,
      "learning_rate": 0.00017354666666666668,
      "loss": -116.9089,
      "step": 9920
    },
    {
      "epoch": 0.7944,
      "grad_norm": 41.90079116821289,
      "learning_rate": 0.00017352,
      "loss": -117.4229,
      "step": 9930
    },
    {
      "epoch": 0.7952,
      "grad_norm": 45.26663589477539,
      "learning_rate": 0.00017349333333333336,
      "loss": -117.4943,
      "step": 9940
    },
    {
      "epoch": 0.796,
      "grad_norm": 37.87739944458008,
      "learning_rate": 0.00017346666666666666,
      "loss": -117.2341,
      "step": 9950
    },
    {
      "epoch": 0.7968,
      "grad_norm": 33.9995002746582,
      "learning_rate": 0.00017344,
      "loss": -117.9946,
      "step": 9960
    },
    {
      "epoch": 0.7976,
      "grad_norm": 32.17803955078125,
      "learning_rate": 0.00017341333333333335,
      "loss": -118.1242,
      "step": 9970
    },
    {
      "epoch": 0.7984,
      "grad_norm": 33.92988967895508,
      "learning_rate": 0.00017338666666666668,
      "loss": -117.4049,
      "step": 9980
    },
    {
      "epoch": 0.7992,
      "grad_norm": 51.09221649169922,
      "learning_rate": 0.00017336,
      "loss": -116.9355,
      "step": 9990
    },
    {
      "epoch": 0.8,
      "grad_norm": 37.503170013427734,
      "learning_rate": 0.00017333333333333334,
      "loss": -117.2671,
      "step": 10000
    },
    {
      "epoch": 0.8008,
      "grad_norm": 37.772159576416016,
      "learning_rate": 0.0001733066666666667,
      "loss": -116.6958,
      "step": 10010
    },
    {
      "epoch": 0.8016,
      "grad_norm": 36.61260223388672,
      "learning_rate": 0.00017328,
      "loss": -116.3866,
      "step": 10020
    },
    {
      "epoch": 0.8024,
      "grad_norm": 39.40378189086914,
      "learning_rate": 0.00017325333333333332,
      "loss": -117.2326,
      "step": 10030
    },
    {
      "epoch": 0.8032,
      "grad_norm": 47.54624557495117,
      "learning_rate": 0.00017322666666666668,
      "loss": -117.5456,
      "step": 10040
    },
    {
      "epoch": 0.804,
      "grad_norm": 41.16878128051758,
      "learning_rate": 0.0001732,
      "loss": -117.0015,
      "step": 10050
    },
    {
      "epoch": 0.8048,
      "grad_norm": 39.90586471557617,
      "learning_rate": 0.00017317333333333334,
      "loss": -117.7439,
      "step": 10060
    },
    {
      "epoch": 0.8056,
      "grad_norm": 45.470333099365234,
      "learning_rate": 0.00017314666666666667,
      "loss": -117.4301,
      "step": 10070
    },
    {
      "epoch": 0.8064,
      "grad_norm": 43.886932373046875,
      "learning_rate": 0.00017312000000000002,
      "loss": -117.3564,
      "step": 10080
    },
    {
      "epoch": 0.8072,
      "grad_norm": 49.464515686035156,
      "learning_rate": 0.00017309333333333335,
      "loss": -117.2263,
      "step": 10090
    },
    {
      "epoch": 0.808,
      "grad_norm": 40.31156921386719,
      "learning_rate": 0.00017306666666666665,
      "loss": -117.438,
      "step": 10100
    },
    {
      "epoch": 0.8088,
      "grad_norm": 36.24325942993164,
      "learning_rate": 0.00017304,
      "loss": -116.7521,
      "step": 10110
    },
    {
      "epoch": 0.8096,
      "grad_norm": 45.666664123535156,
      "learning_rate": 0.00017301333333333334,
      "loss": -116.3852,
      "step": 10120
    },
    {
      "epoch": 0.8104,
      "grad_norm": 36.255680084228516,
      "learning_rate": 0.00017298666666666667,
      "loss": -117.3309,
      "step": 10130
    },
    {
      "epoch": 0.8112,
      "grad_norm": 42.89284133911133,
      "learning_rate": 0.00017296,
      "loss": -117.3657,
      "step": 10140
    },
    {
      "epoch": 0.812,
      "grad_norm": 40.64197540283203,
      "learning_rate": 0.00017293333333333335,
      "loss": -116.6611,
      "step": 10150
    },
    {
      "epoch": 0.8128,
      "grad_norm": 41.116615295410156,
      "learning_rate": 0.00017290666666666668,
      "loss": -116.1314,
      "step": 10160
    },
    {
      "epoch": 0.8136,
      "grad_norm": 43.88105392456055,
      "learning_rate": 0.00017287999999999998,
      "loss": -115.3332,
      "step": 10170
    },
    {
      "epoch": 0.8144,
      "grad_norm": 37.77107620239258,
      "learning_rate": 0.00017285333333333334,
      "loss": -117.3244,
      "step": 10180
    },
    {
      "epoch": 0.8152,
      "grad_norm": 42.688419342041016,
      "learning_rate": 0.00017282666666666667,
      "loss": -117.143,
      "step": 10190
    },
    {
      "epoch": 0.816,
      "grad_norm": 42.0047607421875,
      "learning_rate": 0.0001728,
      "loss": -116.3286,
      "step": 10200
    },
    {
      "epoch": 0.8168,
      "grad_norm": 35.11088180541992,
      "learning_rate": 0.00017277333333333336,
      "loss": -117.6834,
      "step": 10210
    },
    {
      "epoch": 0.8176,
      "grad_norm": 46.41966247558594,
      "learning_rate": 0.00017274666666666668,
      "loss": -117.7795,
      "step": 10220
    },
    {
      "epoch": 0.8184,
      "grad_norm": 39.221378326416016,
      "learning_rate": 0.00017272,
      "loss": -116.1563,
      "step": 10230
    },
    {
      "epoch": 0.8192,
      "grad_norm": 58.53653335571289,
      "learning_rate": 0.00017269333333333334,
      "loss": -116.3387,
      "step": 10240
    },
    {
      "epoch": 0.82,
      "grad_norm": 87.43763732910156,
      "learning_rate": 0.00017266666666666667,
      "loss": -117.8699,
      "step": 10250
    },
    {
      "epoch": 0.8208,
      "grad_norm": 38.228267669677734,
      "learning_rate": 0.00017264,
      "loss": -117.0538,
      "step": 10260
    },
    {
      "epoch": 0.8216,
      "grad_norm": 42.80567169189453,
      "learning_rate": 0.00017261333333333333,
      "loss": -117.4334,
      "step": 10270
    },
    {
      "epoch": 0.8224,
      "grad_norm": 39.02827072143555,
      "learning_rate": 0.00017258666666666669,
      "loss": -118.1673,
      "step": 10280
    },
    {
      "epoch": 0.8232,
      "grad_norm": 60.59226989746094,
      "learning_rate": 0.00017256000000000001,
      "loss": -116.5187,
      "step": 10290
    },
    {
      "epoch": 0.824,
      "grad_norm": 41.76185607910156,
      "learning_rate": 0.00017253333333333334,
      "loss": -116.8979,
      "step": 10300
    },
    {
      "epoch": 0.8248,
      "grad_norm": 37.19527816772461,
      "learning_rate": 0.00017250666666666667,
      "loss": -115.4639,
      "step": 10310
    },
    {
      "epoch": 0.8256,
      "grad_norm": 45.12327575683594,
      "learning_rate": 0.00017248000000000003,
      "loss": -116.9445,
      "step": 10320
    },
    {
      "epoch": 0.8264,
      "grad_norm": 52.68502426147461,
      "learning_rate": 0.00017245333333333333,
      "loss": -117.5002,
      "step": 10330
    },
    {
      "epoch": 0.8272,
      "grad_norm": 44.341888427734375,
      "learning_rate": 0.00017242666666666666,
      "loss": -116.784,
      "step": 10340
    },
    {
      "epoch": 0.828,
      "grad_norm": 38.46736145019531,
      "learning_rate": 0.00017240000000000002,
      "loss": -116.4557,
      "step": 10350
    },
    {
      "epoch": 0.8288,
      "grad_norm": 40.9367561340332,
      "learning_rate": 0.00017237333333333335,
      "loss": -116.2437,
      "step": 10360
    },
    {
      "epoch": 0.8296,
      "grad_norm": 37.091243743896484,
      "learning_rate": 0.00017234666666666667,
      "loss": -117.0054,
      "step": 10370
    },
    {
      "epoch": 0.8304,
      "grad_norm": 42.10951232910156,
      "learning_rate": 0.00017232,
      "loss": -117.3552,
      "step": 10380
    },
    {
      "epoch": 0.8312,
      "grad_norm": 44.67382049560547,
      "learning_rate": 0.00017229333333333336,
      "loss": -116.5488,
      "step": 10390
    },
    {
      "epoch": 0.832,
      "grad_norm": 46.90599060058594,
      "learning_rate": 0.00017226666666666666,
      "loss": -116.997,
      "step": 10400
    },
    {
      "epoch": 0.8328,
      "grad_norm": 43.2051887512207,
      "learning_rate": 0.00017224,
      "loss": -117.8617,
      "step": 10410
    },
    {
      "epoch": 0.8336,
      "grad_norm": 49.99418258666992,
      "learning_rate": 0.00017221333333333335,
      "loss": -117.4969,
      "step": 10420
    },
    {
      "epoch": 0.8344,
      "grad_norm": 47.406593322753906,
      "learning_rate": 0.00017218666666666668,
      "loss": -118.1681,
      "step": 10430
    },
    {
      "epoch": 0.8352,
      "grad_norm": 49.753013610839844,
      "learning_rate": 0.00017216,
      "loss": -116.1992,
      "step": 10440
    },
    {
      "epoch": 0.836,
      "grad_norm": 49.02772521972656,
      "learning_rate": 0.00017213333333333333,
      "loss": -116.1705,
      "step": 10450
    },
    {
      "epoch": 0.8368,
      "grad_norm": 43.43007278442383,
      "learning_rate": 0.0001721066666666667,
      "loss": -116.5815,
      "step": 10460
    },
    {
      "epoch": 0.8376,
      "grad_norm": 31.58871078491211,
      "learning_rate": 0.00017208000000000002,
      "loss": -117.5287,
      "step": 10470
    },
    {
      "epoch": 0.8384,
      "grad_norm": 49.23271942138672,
      "learning_rate": 0.00017205333333333332,
      "loss": -117.0649,
      "step": 10480
    },
    {
      "epoch": 0.8392,
      "grad_norm": 53.49134826660156,
      "learning_rate": 0.00017202666666666668,
      "loss": -116.4555,
      "step": 10490
    },
    {
      "epoch": 0.84,
      "grad_norm": 49.910499572753906,
      "learning_rate": 0.000172,
      "loss": -116.1321,
      "step": 10500
    },
    {
      "epoch": 0.8408,
      "grad_norm": 44.502891540527344,
      "learning_rate": 0.00017197333333333334,
      "loss": -117.3347,
      "step": 10510
    },
    {
      "epoch": 0.8416,
      "grad_norm": 34.5551872253418,
      "learning_rate": 0.00017194666666666666,
      "loss": -117.7789,
      "step": 10520
    },
    {
      "epoch": 0.8424,
      "grad_norm": 53.7381477355957,
      "learning_rate": 0.00017192000000000002,
      "loss": -116.8215,
      "step": 10530
    },
    {
      "epoch": 0.8432,
      "grad_norm": 41.34278106689453,
      "learning_rate": 0.00017189333333333335,
      "loss": -117.2384,
      "step": 10540
    },
    {
      "epoch": 0.844,
      "grad_norm": 43.545230865478516,
      "learning_rate": 0.00017186666666666665,
      "loss": -117.1495,
      "step": 10550
    },
    {
      "epoch": 0.8448,
      "grad_norm": 41.03671646118164,
      "learning_rate": 0.00017184,
      "loss": -117.6649,
      "step": 10560
    },
    {
      "epoch": 0.8456,
      "grad_norm": 49.4794807434082,
      "learning_rate": 0.00017181333333333334,
      "loss": -116.336,
      "step": 10570
    },
    {
      "epoch": 0.8464,
      "grad_norm": 50.667442321777344,
      "learning_rate": 0.00017178666666666667,
      "loss": -116.4479,
      "step": 10580
    },
    {
      "epoch": 0.8472,
      "grad_norm": 38.7114143371582,
      "learning_rate": 0.00017176000000000002,
      "loss": -116.2674,
      "step": 10590
    },
    {
      "epoch": 0.848,
      "grad_norm": 37.19160842895508,
      "learning_rate": 0.00017173333333333335,
      "loss": -116.617,
      "step": 10600
    },
    {
      "epoch": 0.8488,
      "grad_norm": 44.786376953125,
      "learning_rate": 0.00017170666666666668,
      "loss": -117.2163,
      "step": 10610
    },
    {
      "epoch": 0.8496,
      "grad_norm": 34.55177688598633,
      "learning_rate": 0.00017168,
      "loss": -118.1401,
      "step": 10620
    },
    {
      "epoch": 0.8504,
      "grad_norm": 40.394126892089844,
      "learning_rate": 0.00017165333333333334,
      "loss": -117.2233,
      "step": 10630
    },
    {
      "epoch": 0.8512,
      "grad_norm": 40.517032623291016,
      "learning_rate": 0.00017162666666666667,
      "loss": -117.2076,
      "step": 10640
    },
    {
      "epoch": 0.852,
      "grad_norm": 47.97421646118164,
      "learning_rate": 0.0001716,
      "loss": -116.997,
      "step": 10650
    },
    {
      "epoch": 0.8528,
      "grad_norm": 76.55226135253906,
      "learning_rate": 0.00017157333333333335,
      "loss": -115.8859,
      "step": 10660
    },
    {
      "epoch": 0.8536,
      "grad_norm": 43.8157844543457,
      "learning_rate": 0.00017154666666666668,
      "loss": -116.2482,
      "step": 10670
    },
    {
      "epoch": 0.8544,
      "grad_norm": 45.0828857421875,
      "learning_rate": 0.00017152,
      "loss": -117.9285,
      "step": 10680
    },
    {
      "epoch": 0.8552,
      "grad_norm": 55.63522720336914,
      "learning_rate": 0.00017149333333333334,
      "loss": -117.4379,
      "step": 10690
    },
    {
      "epoch": 0.856,
      "grad_norm": 39.49064254760742,
      "learning_rate": 0.00017146666666666667,
      "loss": -117.9574,
      "step": 10700
    },
    {
      "epoch": 0.8568,
      "grad_norm": 50.40845489501953,
      "learning_rate": 0.00017144,
      "loss": -117.0945,
      "step": 10710
    },
    {
      "epoch": 0.8576,
      "grad_norm": 40.5634765625,
      "learning_rate": 0.00017141333333333333,
      "loss": -116.5181,
      "step": 10720
    },
    {
      "epoch": 0.8584,
      "grad_norm": 39.477516174316406,
      "learning_rate": 0.00017138666666666668,
      "loss": -116.8594,
      "step": 10730
    },
    {
      "epoch": 0.8592,
      "grad_norm": 36.81827926635742,
      "learning_rate": 0.00017136,
      "loss": -117.475,
      "step": 10740
    },
    {
      "epoch": 0.86,
      "grad_norm": 44.508724212646484,
      "learning_rate": 0.00017133333333333334,
      "loss": -116.8335,
      "step": 10750
    },
    {
      "epoch": 0.8608,
      "grad_norm": 62.364986419677734,
      "learning_rate": 0.00017130666666666667,
      "loss": -117.4392,
      "step": 10760
    },
    {
      "epoch": 0.8616,
      "grad_norm": 43.73081588745117,
      "learning_rate": 0.00017128000000000003,
      "loss": -117.0599,
      "step": 10770
    },
    {
      "epoch": 0.8624,
      "grad_norm": 66.94947052001953,
      "learning_rate": 0.00017125333333333333,
      "loss": -116.784,
      "step": 10780
    },
    {
      "epoch": 0.8632,
      "grad_norm": 52.402645111083984,
      "learning_rate": 0.00017122666666666666,
      "loss": -117.4685,
      "step": 10790
    },
    {
      "epoch": 0.864,
      "grad_norm": 54.24318313598633,
      "learning_rate": 0.00017120000000000001,
      "loss": -116.8503,
      "step": 10800
    },
    {
      "epoch": 0.8648,
      "grad_norm": 32.41240310668945,
      "learning_rate": 0.00017117333333333334,
      "loss": -115.5198,
      "step": 10810
    },
    {
      "epoch": 0.8656,
      "grad_norm": 60.55221939086914,
      "learning_rate": 0.00017114666666666667,
      "loss": -117.6249,
      "step": 10820
    },
    {
      "epoch": 0.8664,
      "grad_norm": 45.93354415893555,
      "learning_rate": 0.00017112,
      "loss": -118.1,
      "step": 10830
    },
    {
      "epoch": 0.8672,
      "grad_norm": 32.65840148925781,
      "learning_rate": 0.00017109333333333336,
      "loss": -118.149,
      "step": 10840
    },
    {
      "epoch": 0.868,
      "grad_norm": 46.73919677734375,
      "learning_rate": 0.00017106666666666666,
      "loss": -117.4324,
      "step": 10850
    },
    {
      "epoch": 0.8688,
      "grad_norm": 68.0079116821289,
      "learning_rate": 0.00017104,
      "loss": -116.2782,
      "step": 10860
    },
    {
      "epoch": 0.8696,
      "grad_norm": 40.47300720214844,
      "learning_rate": 0.00017101333333333334,
      "loss": -116.9724,
      "step": 10870
    },
    {
      "epoch": 0.8704,
      "grad_norm": 45.03017044067383,
      "learning_rate": 0.00017098666666666667,
      "loss": -118.548,
      "step": 10880
    },
    {
      "epoch": 0.8712,
      "grad_norm": 54.10895919799805,
      "learning_rate": 0.00017096,
      "loss": -117.3672,
      "step": 10890
    },
    {
      "epoch": 0.872,
      "grad_norm": 52.7243537902832,
      "learning_rate": 0.00017093333333333333,
      "loss": -116.5961,
      "step": 10900
    },
    {
      "epoch": 0.8728,
      "grad_norm": 57.79999923706055,
      "learning_rate": 0.0001709066666666667,
      "loss": -116.2746,
      "step": 10910
    },
    {
      "epoch": 0.8736,
      "grad_norm": 40.860260009765625,
      "learning_rate": 0.00017088000000000002,
      "loss": -116.9371,
      "step": 10920
    },
    {
      "epoch": 0.8744,
      "grad_norm": 46.69642639160156,
      "learning_rate": 0.00017085333333333332,
      "loss": -116.1149,
      "step": 10930
    },
    {
      "epoch": 0.8752,
      "grad_norm": 53.12354278564453,
      "learning_rate": 0.00017082666666666668,
      "loss": -116.8472,
      "step": 10940
    },
    {
      "epoch": 0.876,
      "grad_norm": 47.51912307739258,
      "learning_rate": 0.0001708,
      "loss": -117.569,
      "step": 10950
    },
    {
      "epoch": 0.8768,
      "grad_norm": 42.66727066040039,
      "learning_rate": 0.00017077333333333333,
      "loss": -116.5379,
      "step": 10960
    },
    {
      "epoch": 0.8776,
      "grad_norm": 78.94515228271484,
      "learning_rate": 0.0001707466666666667,
      "loss": -115.5433,
      "step": 10970
    },
    {
      "epoch": 0.8784,
      "grad_norm": 79.12973022460938,
      "learning_rate": 0.00017072000000000002,
      "loss": -116.6542,
      "step": 10980
    },
    {
      "epoch": 0.8792,
      "grad_norm": 66.27777862548828,
      "learning_rate": 0.00017069333333333335,
      "loss": -116.4489,
      "step": 10990
    },
    {
      "epoch": 0.88,
      "grad_norm": 38.99708938598633,
      "learning_rate": 0.00017066666666666668,
      "loss": -117.8698,
      "step": 11000
    },
    {
      "epoch": 0.8808,
      "grad_norm": 58.28094482421875,
      "learning_rate": 0.00017064,
      "loss": -118.0992,
      "step": 11010
    },
    {
      "epoch": 0.8816,
      "grad_norm": 33.74074172973633,
      "learning_rate": 0.00017061333333333334,
      "loss": -118.4569,
      "step": 11020
    },
    {
      "epoch": 0.8824,
      "grad_norm": 56.2979621887207,
      "learning_rate": 0.00017058666666666666,
      "loss": -117.1643,
      "step": 11030
    },
    {
      "epoch": 0.8832,
      "grad_norm": 73.16983032226562,
      "learning_rate": 0.00017056000000000002,
      "loss": -117.9697,
      "step": 11040
    },
    {
      "epoch": 0.884,
      "grad_norm": 2345.68310546875,
      "learning_rate": 0.00017053333333333335,
      "loss": -117.2909,
      "step": 11050
    },
    {
      "epoch": 0.8848,
      "grad_norm": 51.043251037597656,
      "learning_rate": 0.00017050666666666668,
      "loss": -116.749,
      "step": 11060
    },
    {
      "epoch": 0.8856,
      "grad_norm": 49.97084426879883,
      "learning_rate": 0.00017048,
      "loss": -116.8081,
      "step": 11070
    },
    {
      "epoch": 0.8864,
      "grad_norm": 84.06050109863281,
      "learning_rate": 0.00017045333333333334,
      "loss": -115.6229,
      "step": 11080
    },
    {
      "epoch": 0.8872,
      "grad_norm": 64.27342987060547,
      "learning_rate": 0.00017042666666666667,
      "loss": -117.589,
      "step": 11090
    },
    {
      "epoch": 0.888,
      "grad_norm": 43.46214294433594,
      "learning_rate": 0.0001704,
      "loss": -116.3023,
      "step": 11100
    },
    {
      "epoch": 0.8888,
      "grad_norm": 49.09113693237305,
      "learning_rate": 0.00017037333333333335,
      "loss": -116.7576,
      "step": 11110
    },
    {
      "epoch": 0.8896,
      "grad_norm": 44.83989715576172,
      "learning_rate": 0.00017034666666666668,
      "loss": -117.6646,
      "step": 11120
    },
    {
      "epoch": 0.8904,
      "grad_norm": 36.398983001708984,
      "learning_rate": 0.00017032,
      "loss": -117.3703,
      "step": 11130
    },
    {
      "epoch": 0.8912,
      "grad_norm": 64.61691284179688,
      "learning_rate": 0.00017029333333333334,
      "loss": -116.1187,
      "step": 11140
    },
    {
      "epoch": 0.892,
      "grad_norm": 52.2093391418457,
      "learning_rate": 0.0001702666666666667,
      "loss": -116.9979,
      "step": 11150
    },
    {
      "epoch": 0.8928,
      "grad_norm": 95.13056182861328,
      "learning_rate": 0.00017024,
      "loss": -116.2487,
      "step": 11160
    },
    {
      "epoch": 0.8936,
      "grad_norm": 106.93550872802734,
      "learning_rate": 0.00017021333333333333,
      "loss": -117.1441,
      "step": 11170
    },
    {
      "epoch": 0.8944,
      "grad_norm": 53.875919342041016,
      "learning_rate": 0.00017018666666666668,
      "loss": -117.663,
      "step": 11180
    },
    {
      "epoch": 0.8952,
      "grad_norm": 59.16569900512695,
      "learning_rate": 0.00017016,
      "loss": -116.8862,
      "step": 11190
    },
    {
      "epoch": 0.896,
      "grad_norm": 35.7396125793457,
      "learning_rate": 0.00017013333333333334,
      "loss": -118.0181,
      "step": 11200
    },
    {
      "epoch": 0.8968,
      "grad_norm": 60.68769073486328,
      "learning_rate": 0.00017010666666666667,
      "loss": -117.0843,
      "step": 11210
    },
    {
      "epoch": 0.8976,
      "grad_norm": 69.48706817626953,
      "learning_rate": 0.00017008000000000002,
      "loss": -116.4234,
      "step": 11220
    },
    {
      "epoch": 0.8984,
      "grad_norm": 53.78166198730469,
      "learning_rate": 0.00017005333333333333,
      "loss": -116.5045,
      "step": 11230
    },
    {
      "epoch": 0.8992,
      "grad_norm": 75.88245391845703,
      "learning_rate": 0.00017002666666666666,
      "loss": -117.0709,
      "step": 11240
    },
    {
      "epoch": 0.9,
      "grad_norm": 53.00850296020508,
      "learning_rate": 0.00017,
      "loss": -117.6511,
      "step": 11250
    },
    {
      "epoch": 0.9008,
      "grad_norm": 129.80845642089844,
      "learning_rate": 0.00016997333333333334,
      "loss": -117.225,
      "step": 11260
    },
    {
      "epoch": 0.9016,
      "grad_norm": 66.79264068603516,
      "learning_rate": 0.00016994666666666667,
      "loss": -116.9115,
      "step": 11270
    },
    {
      "epoch": 0.9024,
      "grad_norm": 50.26638412475586,
      "learning_rate": 0.00016992,
      "loss": -115.9179,
      "step": 11280
    },
    {
      "epoch": 0.9032,
      "grad_norm": 44.79767608642578,
      "learning_rate": 0.00016989333333333336,
      "loss": -117.7906,
      "step": 11290
    },
    {
      "epoch": 0.904,
      "grad_norm": 98.13970947265625,
      "learning_rate": 0.00016986666666666668,
      "loss": -117.3611,
      "step": 11300
    },
    {
      "epoch": 0.9048,
      "grad_norm": 50.60435485839844,
      "learning_rate": 0.00016984,
      "loss": -117.1874,
      "step": 11310
    },
    {
      "epoch": 0.9056,
      "grad_norm": 44.26810836791992,
      "learning_rate": 0.00016981333333333334,
      "loss": -117.956,
      "step": 11320
    },
    {
      "epoch": 0.9064,
      "grad_norm": 78.46216583251953,
      "learning_rate": 0.00016978666666666667,
      "loss": -116.0973,
      "step": 11330
    },
    {
      "epoch": 0.9072,
      "grad_norm": 31.51936149597168,
      "learning_rate": 0.00016976,
      "loss": -117.7792,
      "step": 11340
    },
    {
      "epoch": 0.908,
      "grad_norm": 65.59077453613281,
      "learning_rate": 0.00016973333333333336,
      "loss": -117.0417,
      "step": 11350
    },
    {
      "epoch": 0.9088,
      "grad_norm": 92.92515563964844,
      "learning_rate": 0.00016970666666666669,
      "loss": -117.3844,
      "step": 11360
    },
    {
      "epoch": 0.9096,
      "grad_norm": 93.76172637939453,
      "learning_rate": 0.00016968000000000002,
      "loss": -116.4693,
      "step": 11370
    },
    {
      "epoch": 0.9104,
      "grad_norm": 57.56781005859375,
      "learning_rate": 0.00016965333333333332,
      "loss": -117.3151,
      "step": 11380
    },
    {
      "epoch": 0.9112,
      "grad_norm": 88.88276672363281,
      "learning_rate": 0.00016962666666666667,
      "loss": -117.8213,
      "step": 11390
    },
    {
      "epoch": 0.912,
      "grad_norm": 41.182071685791016,
      "learning_rate": 0.0001696,
      "loss": -117.2842,
      "step": 11400
    },
    {
      "epoch": 0.9128,
      "grad_norm": 89.75202941894531,
      "learning_rate": 0.00016957333333333333,
      "loss": -117.8226,
      "step": 11410
    },
    {
      "epoch": 0.9136,
      "grad_norm": 79.86038208007812,
      "learning_rate": 0.0001695466666666667,
      "loss": -116.5803,
      "step": 11420
    },
    {
      "epoch": 0.9144,
      "grad_norm": 38.13020706176758,
      "learning_rate": 0.00016952000000000002,
      "loss": -117.9943,
      "step": 11430
    },
    {
      "epoch": 0.9152,
      "grad_norm": 47.87346649169922,
      "learning_rate": 0.00016949333333333335,
      "loss": -117.9595,
      "step": 11440
    },
    {
      "epoch": 0.916,
      "grad_norm": 53.99599838256836,
      "learning_rate": 0.00016946666666666667,
      "loss": -117.7559,
      "step": 11450
    },
    {
      "epoch": 0.9168,
      "grad_norm": 58.622257232666016,
      "learning_rate": 0.00016944,
      "loss": -115.8508,
      "step": 11460
    },
    {
      "epoch": 0.9176,
      "grad_norm": 47.04182052612305,
      "learning_rate": 0.00016941333333333333,
      "loss": -115.8703,
      "step": 11470
    },
    {
      "epoch": 0.9184,
      "grad_norm": 39.412559509277344,
      "learning_rate": 0.00016938666666666666,
      "loss": -116.5477,
      "step": 11480
    },
    {
      "epoch": 0.9192,
      "grad_norm": 68.67630767822266,
      "learning_rate": 0.00016936000000000002,
      "loss": -116.9294,
      "step": 11490
    },
    {
      "epoch": 0.92,
      "grad_norm": 87.48612976074219,
      "learning_rate": 0.00016933333333333335,
      "loss": -117.8866,
      "step": 11500
    },
    {
      "epoch": 0.9208,
      "grad_norm": 65.42521667480469,
      "learning_rate": 0.00016930666666666668,
      "loss": -117.9344,
      "step": 11510
    },
    {
      "epoch": 0.9216,
      "grad_norm": 94.50769805908203,
      "learning_rate": 0.00016928,
      "loss": -115.4215,
      "step": 11520
    },
    {
      "epoch": 0.9224,
      "grad_norm": 57.769710540771484,
      "learning_rate": 0.00016925333333333333,
      "loss": -116.5389,
      "step": 11530
    },
    {
      "epoch": 0.9232,
      "grad_norm": 67.38834381103516,
      "learning_rate": 0.00016922666666666666,
      "loss": -115.7775,
      "step": 11540
    },
    {
      "epoch": 0.924,
      "grad_norm": 96.54400634765625,
      "learning_rate": 0.0001692,
      "loss": -116.6125,
      "step": 11550
    },
    {
      "epoch": 0.9248,
      "grad_norm": 55.41862106323242,
      "learning_rate": 0.00016917333333333335,
      "loss": -118.0456,
      "step": 11560
    },
    {
      "epoch": 0.9256,
      "grad_norm": 62.771636962890625,
      "learning_rate": 0.00016914666666666668,
      "loss": -117.1667,
      "step": 11570
    },
    {
      "epoch": 0.9264,
      "grad_norm": 50.08879852294922,
      "learning_rate": 0.00016912,
      "loss": -118.1164,
      "step": 11580
    },
    {
      "epoch": 0.9272,
      "grad_norm": 65.00941467285156,
      "learning_rate": 0.00016909333333333334,
      "loss": -118.5503,
      "step": 11590
    },
    {
      "epoch": 0.928,
      "grad_norm": 85.84394073486328,
      "learning_rate": 0.0001690666666666667,
      "loss": -117.5815,
      "step": 11600
    },
    {
      "epoch": 0.9288,
      "grad_norm": 53.24177551269531,
      "learning_rate": 0.00016904,
      "loss": -117.4457,
      "step": 11610
    },
    {
      "epoch": 0.9296,
      "grad_norm": 61.08743667602539,
      "learning_rate": 0.00016901333333333332,
      "loss": -117.5816,
      "step": 11620
    },
    {
      "epoch": 0.9304,
      "grad_norm": 81.92003631591797,
      "learning_rate": 0.00016898666666666668,
      "loss": -116.8873,
      "step": 11630
    },
    {
      "epoch": 0.9312,
      "grad_norm": 90.92742919921875,
      "learning_rate": 0.00016896,
      "loss": -116.7108,
      "step": 11640
    },
    {
      "epoch": 0.932,
      "grad_norm": 48.70656967163086,
      "learning_rate": 0.00016893333333333334,
      "loss": -117.1813,
      "step": 11650
    },
    {
      "epoch": 0.9328,
      "grad_norm": 42.55426025390625,
      "learning_rate": 0.00016890666666666667,
      "loss": -116.9715,
      "step": 11660
    },
    {
      "epoch": 0.9336,
      "grad_norm": 60.878662109375,
      "learning_rate": 0.00016888000000000002,
      "loss": -116.6686,
      "step": 11670
    },
    {
      "epoch": 0.9344,
      "grad_norm": 60.07355880737305,
      "learning_rate": 0.00016885333333333335,
      "loss": -117.0254,
      "step": 11680
    },
    {
      "epoch": 0.9352,
      "grad_norm": 89.84535217285156,
      "learning_rate": 0.00016882666666666665,
      "loss": -116.4059,
      "step": 11690
    },
    {
      "epoch": 0.936,
      "grad_norm": 62.63066482543945,
      "learning_rate": 0.0001688,
      "loss": -117.2381,
      "step": 11700
    },
    {
      "epoch": 0.9368,
      "grad_norm": 68.66329956054688,
      "learning_rate": 0.00016877333333333334,
      "loss": -117.5925,
      "step": 11710
    },
    {
      "epoch": 0.9376,
      "grad_norm": 100.86043548583984,
      "learning_rate": 0.00016874666666666667,
      "loss": -116.4201,
      "step": 11720
    },
    {
      "epoch": 0.9384,
      "grad_norm": 97.3386001586914,
      "learning_rate": 0.00016872000000000002,
      "loss": -116.3398,
      "step": 11730
    },
    {
      "epoch": 0.9392,
      "grad_norm": 91.05880737304688,
      "learning_rate": 0.00016869333333333335,
      "loss": -117.6229,
      "step": 11740
    },
    {
      "epoch": 0.94,
      "grad_norm": 103.01835632324219,
      "learning_rate": 0.00016866666666666668,
      "loss": -116.9775,
      "step": 11750
    },
    {
      "epoch": 0.9408,
      "grad_norm": 48.371612548828125,
      "learning_rate": 0.00016863999999999998,
      "loss": -116.3242,
      "step": 11760
    },
    {
      "epoch": 0.9416,
      "grad_norm": 126.0789794921875,
      "learning_rate": 0.00016861333333333334,
      "loss": -116.5924,
      "step": 11770
    },
    {
      "epoch": 0.9424,
      "grad_norm": 95.73176574707031,
      "learning_rate": 0.00016858666666666667,
      "loss": -116.8206,
      "step": 11780
    },
    {
      "epoch": 0.9432,
      "grad_norm": 92.86315155029297,
      "learning_rate": 0.00016856,
      "loss": -116.7017,
      "step": 11790
    },
    {
      "epoch": 0.944,
      "grad_norm": 150.93795776367188,
      "learning_rate": 0.00016853333333333336,
      "loss": -116.8603,
      "step": 11800
    },
    {
      "epoch": 0.9448,
      "grad_norm": 67.49483489990234,
      "learning_rate": 0.00016850666666666668,
      "loss": -117.1851,
      "step": 11810
    },
    {
      "epoch": 0.9456,
      "grad_norm": 59.44088363647461,
      "learning_rate": 0.00016848,
      "loss": -117.1036,
      "step": 11820
    },
    {
      "epoch": 0.9464,
      "grad_norm": 70.77653503417969,
      "learning_rate": 0.00016845333333333334,
      "loss": -116.9822,
      "step": 11830
    },
    {
      "epoch": 0.9472,
      "grad_norm": 78.12226867675781,
      "learning_rate": 0.00016842666666666667,
      "loss": -116.9894,
      "step": 11840
    },
    {
      "epoch": 0.948,
      "grad_norm": 58.28801727294922,
      "learning_rate": 0.0001684,
      "loss": -117.5537,
      "step": 11850
    },
    {
      "epoch": 0.9488,
      "grad_norm": 57.58671569824219,
      "learning_rate": 0.00016837333333333333,
      "loss": -116.5528,
      "step": 11860
    },
    {
      "epoch": 0.9496,
      "grad_norm": 59.427703857421875,
      "learning_rate": 0.00016834666666666669,
      "loss": -116.8795,
      "step": 11870
    },
    {
      "epoch": 0.9504,
      "grad_norm": 53.4565544128418,
      "learning_rate": 0.00016832000000000001,
      "loss": -117.6356,
      "step": 11880
    },
    {
      "epoch": 0.9512,
      "grad_norm": 60.467281341552734,
      "learning_rate": 0.00016829333333333334,
      "loss": -115.928,
      "step": 11890
    },
    {
      "epoch": 0.952,
      "grad_norm": 67.98981475830078,
      "learning_rate": 0.00016826666666666667,
      "loss": -115.659,
      "step": 11900
    },
    {
      "epoch": 0.9528,
      "grad_norm": 94.52641296386719,
      "learning_rate": 0.00016824,
      "loss": -116.5728,
      "step": 11910
    },
    {
      "epoch": 0.9536,
      "grad_norm": 98.77193450927734,
      "learning_rate": 0.00016821333333333333,
      "loss": -116.9253,
      "step": 11920
    },
    {
      "epoch": 0.9544,
      "grad_norm": 75.30245208740234,
      "learning_rate": 0.00016818666666666666,
      "loss": -115.8313,
      "step": 11930
    },
    {
      "epoch": 0.9552,
      "grad_norm": 102.65462493896484,
      "learning_rate": 0.00016816000000000002,
      "loss": -116.5553,
      "step": 11940
    },
    {
      "epoch": 0.956,
      "grad_norm": 56.96399688720703,
      "learning_rate": 0.00016813333333333335,
      "loss": -117.7242,
      "step": 11950
    },
    {
      "epoch": 0.9568,
      "grad_norm": 174.66162109375,
      "learning_rate": 0.00016810666666666667,
      "loss": -115.0269,
      "step": 11960
    },
    {
      "epoch": 0.9576,
      "grad_norm": 282.7590026855469,
      "learning_rate": 0.00016808,
      "loss": -116.7868,
      "step": 11970
    },
    {
      "epoch": 0.9584,
      "grad_norm": 109.38117218017578,
      "learning_rate": 0.00016805333333333336,
      "loss": -116.1076,
      "step": 11980
    },
    {
      "epoch": 0.9592,
      "grad_norm": 93.59733581542969,
      "learning_rate": 0.00016802666666666666,
      "loss": -117.6869,
      "step": 11990
    },
    {
      "epoch": 0.96,
      "grad_norm": 123.62594604492188,
      "learning_rate": 0.000168,
      "loss": -116.7614,
      "step": 12000
    },
    {
      "epoch": 0.9608,
      "grad_norm": 130.962158203125,
      "learning_rate": 0.00016797333333333335,
      "loss": -116.4338,
      "step": 12010
    },
    {
      "epoch": 0.9616,
      "grad_norm": 86.5330581665039,
      "learning_rate": 0.00016794666666666668,
      "loss": -115.407,
      "step": 12020
    },
    {
      "epoch": 0.9624,
      "grad_norm": 78.27218627929688,
      "learning_rate": 0.00016792,
      "loss": -118.2721,
      "step": 12030
    },
    {
      "epoch": 0.9632,
      "grad_norm": 55.548583984375,
      "learning_rate": 0.00016789333333333333,
      "loss": -116.7706,
      "step": 12040
    },
    {
      "epoch": 0.964,
      "grad_norm": 118.6967544555664,
      "learning_rate": 0.0001678666666666667,
      "loss": -115.772,
      "step": 12050
    },
    {
      "epoch": 0.9648,
      "grad_norm": 73.47734069824219,
      "learning_rate": 0.00016784,
      "loss": -116.682,
      "step": 12060
    },
    {
      "epoch": 0.9656,
      "grad_norm": 313.8199462890625,
      "learning_rate": 0.00016781333333333332,
      "loss": -116.7422,
      "step": 12070
    },
    {
      "epoch": 0.9664,
      "grad_norm": 73.9266357421875,
      "learning_rate": 0.00016778666666666668,
      "loss": -116.4822,
      "step": 12080
    },
    {
      "epoch": 0.9672,
      "grad_norm": 55.15097427368164,
      "learning_rate": 0.00016776,
      "loss": -116.2024,
      "step": 12090
    },
    {
      "epoch": 0.968,
      "grad_norm": 67.08422088623047,
      "learning_rate": 0.00016773333333333334,
      "loss": -116.0222,
      "step": 12100
    },
    {
      "epoch": 0.9688,
      "grad_norm": 55.75651168823242,
      "learning_rate": 0.0001677066666666667,
      "loss": -116.8702,
      "step": 12110
    },
    {
      "epoch": 0.9696,
      "grad_norm": 67.22200775146484,
      "learning_rate": 0.00016768000000000002,
      "loss": -117.4585,
      "step": 12120
    },
    {
      "epoch": 0.9704,
      "grad_norm": 54.228614807128906,
      "learning_rate": 0.00016765333333333335,
      "loss": -116.8407,
      "step": 12130
    },
    {
      "epoch": 0.9712,
      "grad_norm": 73.72709655761719,
      "learning_rate": 0.00016762666666666665,
      "loss": -117.1802,
      "step": 12140
    },
    {
      "epoch": 0.972,
      "grad_norm": 111.2944564819336,
      "learning_rate": 0.0001676,
      "loss": -117.1577,
      "step": 12150
    },
    {
      "epoch": 0.9728,
      "grad_norm": 102.09548950195312,
      "learning_rate": 0.00016757333333333334,
      "loss": -116.9526,
      "step": 12160
    },
    {
      "epoch": 0.9736,
      "grad_norm": 54.8115348815918,
      "learning_rate": 0.00016754666666666667,
      "loss": -117.2896,
      "step": 12170
    },
    {
      "epoch": 0.9744,
      "grad_norm": 63.34724807739258,
      "learning_rate": 0.00016752000000000002,
      "loss": -116.6205,
      "step": 12180
    },
    {
      "epoch": 0.9752,
      "grad_norm": 102.85255432128906,
      "learning_rate": 0.00016749333333333335,
      "loss": -115.8241,
      "step": 12190
    },
    {
      "epoch": 0.976,
      "grad_norm": 153.78504943847656,
      "learning_rate": 0.00016746666666666668,
      "loss": -116.9328,
      "step": 12200
    },
    {
      "epoch": 0.9768,
      "grad_norm": 96.98687744140625,
      "learning_rate": 0.00016744,
      "loss": -117.1988,
      "step": 12210
    },
    {
      "epoch": 0.9776,
      "grad_norm": 53.04060745239258,
      "learning_rate": 0.00016741333333333334,
      "loss": -117.3927,
      "step": 12220
    },
    {
      "epoch": 0.9784,
      "grad_norm": 120.947265625,
      "learning_rate": 0.00016738666666666667,
      "loss": -118.79,
      "step": 12230
    },
    {
      "epoch": 0.9792,
      "grad_norm": 63.88637924194336,
      "learning_rate": 0.00016736,
      "loss": -118.1137,
      "step": 12240
    },
    {
      "epoch": 0.98,
      "grad_norm": 79.90894317626953,
      "learning_rate": 0.00016733333333333335,
      "loss": -117.0547,
      "step": 12250
    },
    {
      "epoch": 0.9808,
      "grad_norm": 89.6541748046875,
      "learning_rate": 0.00016730666666666668,
      "loss": -117.8245,
      "step": 12260
    },
    {
      "epoch": 0.9816,
      "grad_norm": 90.37093353271484,
      "learning_rate": 0.00016728,
      "loss": -117.3316,
      "step": 12270
    },
    {
      "epoch": 0.9824,
      "grad_norm": 109.24824523925781,
      "learning_rate": 0.00016725333333333334,
      "loss": -116.7387,
      "step": 12280
    },
    {
      "epoch": 0.9832,
      "grad_norm": 109.61309814453125,
      "learning_rate": 0.00016722666666666667,
      "loss": -115.8086,
      "step": 12290
    },
    {
      "epoch": 0.984,
      "grad_norm": 94.31756591796875,
      "learning_rate": 0.0001672,
      "loss": -117.0687,
      "step": 12300
    },
    {
      "epoch": 0.9848,
      "grad_norm": 54.325374603271484,
      "learning_rate": 0.00016717333333333333,
      "loss": -117.6413,
      "step": 12310
    },
    {
      "epoch": 0.9856,
      "grad_norm": 61.49561309814453,
      "learning_rate": 0.00016714666666666668,
      "loss": -116.8855,
      "step": 12320
    },
    {
      "epoch": 0.9864,
      "grad_norm": 75.02899169921875,
      "learning_rate": 0.00016712,
      "loss": -116.7139,
      "step": 12330
    },
    {
      "epoch": 0.9872,
      "grad_norm": 77.19142150878906,
      "learning_rate": 0.00016709333333333334,
      "loss": -115.8201,
      "step": 12340
    },
    {
      "epoch": 0.988,
      "grad_norm": 69.97565460205078,
      "learning_rate": 0.00016706666666666667,
      "loss": -116.5596,
      "step": 12350
    },
    {
      "epoch": 0.9888,
      "grad_norm": 275.8172607421875,
      "learning_rate": 0.00016704000000000003,
      "loss": -111.323,
      "step": 12360
    },
    {
      "epoch": 0.9896,
      "grad_norm": 235.21502685546875,
      "learning_rate": 0.00016701333333333333,
      "loss": -114.5424,
      "step": 12370
    },
    {
      "epoch": 0.9904,
      "grad_norm": 87.40684509277344,
      "learning_rate": 0.00016698666666666666,
      "loss": -114.9749,
      "step": 12380
    },
    {
      "epoch": 0.9912,
      "grad_norm": 123.42211151123047,
      "learning_rate": 0.00016696000000000001,
      "loss": -115.4471,
      "step": 12390
    },
    {
      "epoch": 0.992,
      "grad_norm": 76.83316040039062,
      "learning_rate": 0.00016693333333333334,
      "loss": -115.2606,
      "step": 12400
    },
    {
      "epoch": 0.9928,
      "grad_norm": 63.09186553955078,
      "learning_rate": 0.00016690666666666667,
      "loss": -115.8853,
      "step": 12410
    },
    {
      "epoch": 0.9936,
      "grad_norm": 60.785831451416016,
      "learning_rate": 0.00016688,
      "loss": -115.6949,
      "step": 12420
    },
    {
      "epoch": 0.9944,
      "grad_norm": 97.13720703125,
      "learning_rate": 0.00016685333333333336,
      "loss": -115.1071,
      "step": 12430
    },
    {
      "epoch": 0.9952,
      "grad_norm": 67.03722381591797,
      "learning_rate": 0.00016682666666666666,
      "loss": -117.1334,
      "step": 12440
    },
    {
      "epoch": 0.996,
      "grad_norm": 54.17840576171875,
      "learning_rate": 0.0001668,
      "loss": -116.3746,
      "step": 12450
    },
    {
      "epoch": 0.9968,
      "grad_norm": 43.18458557128906,
      "learning_rate": 0.00016677333333333334,
      "loss": -116.0513,
      "step": 12460
    },
    {
      "epoch": 0.9976,
      "grad_norm": 49.142059326171875,
      "learning_rate": 0.00016674666666666667,
      "loss": -116.3116,
      "step": 12470
    },
    {
      "epoch": 0.9984,
      "grad_norm": 41.380279541015625,
      "learning_rate": 0.00016672,
      "loss": -114.7757,
      "step": 12480
    },
    {
      "epoch": 0.9992,
      "grad_norm": 61.18212127685547,
      "learning_rate": 0.00016669333333333336,
      "loss": -116.3841,
      "step": 12490
    },
    {
      "epoch": 1.0,
      "grad_norm": 69.9844970703125,
      "learning_rate": 0.0001666666666666667,
      "loss": -115.4973,
      "step": 12500
    }
  ],
  "logging_steps": 10,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 7.4803052544e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
