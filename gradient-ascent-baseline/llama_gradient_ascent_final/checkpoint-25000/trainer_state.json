{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 25000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008,
      "grad_norm": 33.29362106323242,
      "learning_rate": 0.00019997333333333334,
      "loss": -11.7693,
      "step": 10
    },
    {
      "epoch": 0.0016,
      "grad_norm": 88.6811294555664,
      "learning_rate": 0.0001999466666666667,
      "loss": -19.5457,
      "step": 20
    },
    {
      "epoch": 0.0024,
      "grad_norm": 80.93682861328125,
      "learning_rate": 0.00019992000000000002,
      "loss": -32.6267,
      "step": 30
    },
    {
      "epoch": 0.0032,
      "grad_norm": 62.05029296875,
      "learning_rate": 0.00019989333333333333,
      "loss": -44.2532,
      "step": 40
    },
    {
      "epoch": 0.004,
      "grad_norm": 95.35437774658203,
      "learning_rate": 0.00019986666666666668,
      "loss": -54.6137,
      "step": 50
    },
    {
      "epoch": 0.0048,
      "grad_norm": 285.8903503417969,
      "learning_rate": 0.00019984,
      "loss": -67.3955,
      "step": 60
    },
    {
      "epoch": 0.0056,
      "grad_norm": 66.17249298095703,
      "learning_rate": 0.00019981333333333334,
      "loss": -77.2874,
      "step": 70
    },
    {
      "epoch": 0.0064,
      "grad_norm": 47.76024627685547,
      "learning_rate": 0.00019978666666666667,
      "loss": -83.9726,
      "step": 80
    },
    {
      "epoch": 0.0072,
      "grad_norm": 70.7253189086914,
      "learning_rate": 0.00019976000000000003,
      "loss": -89.4578,
      "step": 90
    },
    {
      "epoch": 0.008,
      "grad_norm": 62.08152770996094,
      "learning_rate": 0.00019973333333333335,
      "loss": -93.1687,
      "step": 100
    },
    {
      "epoch": 0.0088,
      "grad_norm": 36.339698791503906,
      "learning_rate": 0.00019970666666666668,
      "loss": -97.4488,
      "step": 110
    },
    {
      "epoch": 0.0096,
      "grad_norm": 47.71453857421875,
      "learning_rate": 0.00019968,
      "loss": -98.5919,
      "step": 120
    },
    {
      "epoch": 0.0104,
      "grad_norm": 56.312496185302734,
      "learning_rate": 0.00019965333333333334,
      "loss": -100.9339,
      "step": 130
    },
    {
      "epoch": 0.0112,
      "grad_norm": 55.961978912353516,
      "learning_rate": 0.00019962666666666667,
      "loss": -102.526,
      "step": 140
    },
    {
      "epoch": 0.012,
      "grad_norm": 50.273353576660156,
      "learning_rate": 0.0001996,
      "loss": -102.9351,
      "step": 150
    },
    {
      "epoch": 0.0128,
      "grad_norm": 63.39498519897461,
      "learning_rate": 0.00019957333333333336,
      "loss": -104.2318,
      "step": 160
    },
    {
      "epoch": 0.0136,
      "grad_norm": 80.68345642089844,
      "learning_rate": 0.00019954666666666669,
      "loss": -105.9233,
      "step": 170
    },
    {
      "epoch": 0.0144,
      "grad_norm": 101.63463592529297,
      "learning_rate": 0.00019952000000000001,
      "loss": -105.9457,
      "step": 180
    },
    {
      "epoch": 0.0152,
      "grad_norm": 99.14067840576172,
      "learning_rate": 0.00019949333333333334,
      "loss": -105.4273,
      "step": 190
    },
    {
      "epoch": 0.016,
      "grad_norm": 98.58280944824219,
      "learning_rate": 0.00019946666666666667,
      "loss": -107.6783,
      "step": 200
    },
    {
      "epoch": 0.0168,
      "grad_norm": 71.8197021484375,
      "learning_rate": 0.00019944,
      "loss": -108.0865,
      "step": 210
    },
    {
      "epoch": 0.0176,
      "grad_norm": 89.12177276611328,
      "learning_rate": 0.00019941333333333333,
      "loss": -108.4013,
      "step": 220
    },
    {
      "epoch": 0.0184,
      "grad_norm": 96.14568328857422,
      "learning_rate": 0.0001993866666666667,
      "loss": -108.9349,
      "step": 230
    },
    {
      "epoch": 0.0192,
      "grad_norm": 58.35210418701172,
      "learning_rate": 0.00019936000000000002,
      "loss": -110.3853,
      "step": 240
    },
    {
      "epoch": 0.02,
      "grad_norm": 63.979820251464844,
      "learning_rate": 0.00019933333333333334,
      "loss": -109.618,
      "step": 250
    },
    {
      "epoch": 0.0208,
      "grad_norm": 89.14130401611328,
      "learning_rate": 0.00019930666666666667,
      "loss": -110.2455,
      "step": 260
    },
    {
      "epoch": 0.0216,
      "grad_norm": 87.4437026977539,
      "learning_rate": 0.00019928,
      "loss": -111.7455,
      "step": 270
    },
    {
      "epoch": 0.0224,
      "grad_norm": 106.41680908203125,
      "learning_rate": 0.00019925333333333333,
      "loss": -110.5917,
      "step": 280
    },
    {
      "epoch": 0.0232,
      "grad_norm": 72.2378921508789,
      "learning_rate": 0.00019922666666666666,
      "loss": -110.4584,
      "step": 290
    },
    {
      "epoch": 0.024,
      "grad_norm": 90.60639953613281,
      "learning_rate": 0.00019920000000000002,
      "loss": -111.878,
      "step": 300
    },
    {
      "epoch": 0.0248,
      "grad_norm": 63.72541046142578,
      "learning_rate": 0.00019917333333333335,
      "loss": -110.814,
      "step": 310
    },
    {
      "epoch": 0.0256,
      "grad_norm": 112.77288818359375,
      "learning_rate": 0.00019914666666666668,
      "loss": -111.3397,
      "step": 320
    },
    {
      "epoch": 0.0264,
      "grad_norm": 91.1357650756836,
      "learning_rate": 0.00019912,
      "loss": -111.671,
      "step": 330
    },
    {
      "epoch": 0.0272,
      "grad_norm": 63.38945007324219,
      "learning_rate": 0.00019909333333333336,
      "loss": -112.2091,
      "step": 340
    },
    {
      "epoch": 0.028,
      "grad_norm": 59.81158447265625,
      "learning_rate": 0.00019906666666666666,
      "loss": -111.018,
      "step": 350
    },
    {
      "epoch": 0.0288,
      "grad_norm": 75.66832733154297,
      "learning_rate": 0.00019904,
      "loss": -112.6134,
      "step": 360
    },
    {
      "epoch": 0.0296,
      "grad_norm": 130.0281219482422,
      "learning_rate": 0.00019901333333333335,
      "loss": -112.6587,
      "step": 370
    },
    {
      "epoch": 0.0304,
      "grad_norm": 94.29589080810547,
      "learning_rate": 0.00019898666666666668,
      "loss": -111.5289,
      "step": 380
    },
    {
      "epoch": 0.0312,
      "grad_norm": 651.4603881835938,
      "learning_rate": 0.00019896,
      "loss": -112.362,
      "step": 390
    },
    {
      "epoch": 0.032,
      "grad_norm": 70.79876708984375,
      "learning_rate": 0.00019893333333333336,
      "loss": -112.3065,
      "step": 400
    },
    {
      "epoch": 0.0328,
      "grad_norm": 109.63276672363281,
      "learning_rate": 0.0001989066666666667,
      "loss": -112.4015,
      "step": 410
    },
    {
      "epoch": 0.0336,
      "grad_norm": 84.60466766357422,
      "learning_rate": 0.00019888,
      "loss": -112.1274,
      "step": 420
    },
    {
      "epoch": 0.0344,
      "grad_norm": 105.31868743896484,
      "learning_rate": 0.00019885333333333335,
      "loss": -113.687,
      "step": 430
    },
    {
      "epoch": 0.0352,
      "grad_norm": 59.35783767700195,
      "learning_rate": 0.00019882666666666668,
      "loss": -112.9827,
      "step": 440
    },
    {
      "epoch": 0.036,
      "grad_norm": 78.44389343261719,
      "learning_rate": 0.0001988,
      "loss": -113.8208,
      "step": 450
    },
    {
      "epoch": 0.0368,
      "grad_norm": 97.66646575927734,
      "learning_rate": 0.00019877333333333334,
      "loss": -114.2702,
      "step": 460
    },
    {
      "epoch": 0.0376,
      "grad_norm": 152.36741638183594,
      "learning_rate": 0.0001987466666666667,
      "loss": -111.9326,
      "step": 470
    },
    {
      "epoch": 0.0384,
      "grad_norm": 124.24365234375,
      "learning_rate": 0.00019872000000000002,
      "loss": -113.9961,
      "step": 480
    },
    {
      "epoch": 0.0392,
      "grad_norm": 90.356689453125,
      "learning_rate": 0.00019869333333333335,
      "loss": -114.2804,
      "step": 490
    },
    {
      "epoch": 0.04,
      "grad_norm": 77.0312271118164,
      "learning_rate": 0.00019866666666666668,
      "loss": -113.9764,
      "step": 500
    },
    {
      "epoch": 0.0408,
      "grad_norm": 69.40702056884766,
      "learning_rate": 0.00019864,
      "loss": -113.8124,
      "step": 510
    },
    {
      "epoch": 0.0416,
      "grad_norm": 87.39144134521484,
      "learning_rate": 0.00019861333333333334,
      "loss": -114.4427,
      "step": 520
    },
    {
      "epoch": 0.0424,
      "grad_norm": 62.119869232177734,
      "learning_rate": 0.00019858666666666667,
      "loss": -114.0859,
      "step": 530
    },
    {
      "epoch": 0.0432,
      "grad_norm": 77.51637268066406,
      "learning_rate": 0.00019856000000000002,
      "loss": -113.9016,
      "step": 540
    },
    {
      "epoch": 0.044,
      "grad_norm": 71.01506805419922,
      "learning_rate": 0.00019853333333333335,
      "loss": -115.0679,
      "step": 550
    },
    {
      "epoch": 0.0448,
      "grad_norm": 92.6697998046875,
      "learning_rate": 0.00019850666666666668,
      "loss": -113.6978,
      "step": 560
    },
    {
      "epoch": 0.0456,
      "grad_norm": 92.64653015136719,
      "learning_rate": 0.00019848,
      "loss": -115.451,
      "step": 570
    },
    {
      "epoch": 0.0464,
      "grad_norm": 104.21144104003906,
      "learning_rate": 0.00019845333333333334,
      "loss": -113.819,
      "step": 580
    },
    {
      "epoch": 0.0472,
      "grad_norm": 160.2574462890625,
      "learning_rate": 0.00019842666666666667,
      "loss": -114.2994,
      "step": 590
    },
    {
      "epoch": 0.048,
      "grad_norm": 77.76377868652344,
      "learning_rate": 0.0001984,
      "loss": -114.3774,
      "step": 600
    },
    {
      "epoch": 0.0488,
      "grad_norm": 80.45144653320312,
      "learning_rate": 0.00019837333333333335,
      "loss": -115.2262,
      "step": 610
    },
    {
      "epoch": 0.0496,
      "grad_norm": 127.11882019042969,
      "learning_rate": 0.00019834666666666668,
      "loss": -115.102,
      "step": 620
    },
    {
      "epoch": 0.0504,
      "grad_norm": 82.68360900878906,
      "learning_rate": 0.00019832,
      "loss": -114.2812,
      "step": 630
    },
    {
      "epoch": 0.0512,
      "grad_norm": 90.7738037109375,
      "learning_rate": 0.00019829333333333334,
      "loss": -114.5657,
      "step": 640
    },
    {
      "epoch": 0.052,
      "grad_norm": 100.32475280761719,
      "learning_rate": 0.00019826666666666667,
      "loss": -115.4703,
      "step": 650
    },
    {
      "epoch": 0.0528,
      "grad_norm": 94.75779724121094,
      "learning_rate": 0.00019824,
      "loss": -114.7218,
      "step": 660
    },
    {
      "epoch": 0.0536,
      "grad_norm": 85.77596282958984,
      "learning_rate": 0.00019821333333333333,
      "loss": -114.2901,
      "step": 670
    },
    {
      "epoch": 0.0544,
      "grad_norm": 81.40138244628906,
      "learning_rate": 0.00019818666666666668,
      "loss": -115.8699,
      "step": 680
    },
    {
      "epoch": 0.0552,
      "grad_norm": 126.37874603271484,
      "learning_rate": 0.00019816000000000001,
      "loss": -115.755,
      "step": 690
    },
    {
      "epoch": 0.056,
      "grad_norm": 146.80467224121094,
      "learning_rate": 0.00019813333333333334,
      "loss": -114.5479,
      "step": 700
    },
    {
      "epoch": 0.0568,
      "grad_norm": 90.47672271728516,
      "learning_rate": 0.00019810666666666667,
      "loss": -114.4874,
      "step": 710
    },
    {
      "epoch": 0.0576,
      "grad_norm": 178.48268127441406,
      "learning_rate": 0.00019808,
      "loss": -114.075,
      "step": 720
    },
    {
      "epoch": 0.0584,
      "grad_norm": 104.98505401611328,
      "learning_rate": 0.00019805333333333333,
      "loss": -115.8809,
      "step": 730
    },
    {
      "epoch": 0.0592,
      "grad_norm": 131.71875,
      "learning_rate": 0.00019802666666666666,
      "loss": -113.7449,
      "step": 740
    },
    {
      "epoch": 0.06,
      "grad_norm": 96.82987976074219,
      "learning_rate": 0.00019800000000000002,
      "loss": -114.128,
      "step": 750
    },
    {
      "epoch": 0.0608,
      "grad_norm": 136.55215454101562,
      "learning_rate": 0.00019797333333333334,
      "loss": -114.9512,
      "step": 760
    },
    {
      "epoch": 0.0616,
      "grad_norm": 78.0204086303711,
      "learning_rate": 0.00019794666666666667,
      "loss": -114.3488,
      "step": 770
    },
    {
      "epoch": 0.0624,
      "grad_norm": 79.91584014892578,
      "learning_rate": 0.00019792000000000003,
      "loss": -115.168,
      "step": 780
    },
    {
      "epoch": 0.0632,
      "grad_norm": 133.26724243164062,
      "learning_rate": 0.00019789333333333336,
      "loss": -115.1987,
      "step": 790
    },
    {
      "epoch": 0.064,
      "grad_norm": 118.00567626953125,
      "learning_rate": 0.00019786666666666666,
      "loss": -114.4543,
      "step": 800
    },
    {
      "epoch": 0.0648,
      "grad_norm": 162.2584991455078,
      "learning_rate": 0.00019784,
      "loss": -115.0464,
      "step": 810
    },
    {
      "epoch": 0.0656,
      "grad_norm": 154.41412353515625,
      "learning_rate": 0.00019781333333333335,
      "loss": -114.2008,
      "step": 820
    },
    {
      "epoch": 0.0664,
      "grad_norm": 93.78880310058594,
      "learning_rate": 0.00019778666666666667,
      "loss": -114.3747,
      "step": 830
    },
    {
      "epoch": 0.0672,
      "grad_norm": 185.42837524414062,
      "learning_rate": 0.00019776,
      "loss": -113.1197,
      "step": 840
    },
    {
      "epoch": 0.068,
      "grad_norm": 109.28009033203125,
      "learning_rate": 0.00019773333333333336,
      "loss": -114.077,
      "step": 850
    },
    {
      "epoch": 0.0688,
      "grad_norm": 81.7590560913086,
      "learning_rate": 0.0001977066666666667,
      "loss": -114.7538,
      "step": 860
    },
    {
      "epoch": 0.0696,
      "grad_norm": 103.29158782958984,
      "learning_rate": 0.00019768,
      "loss": -116.5384,
      "step": 870
    },
    {
      "epoch": 0.0704,
      "grad_norm": 95.2335205078125,
      "learning_rate": 0.00019765333333333335,
      "loss": -115.241,
      "step": 880
    },
    {
      "epoch": 0.0712,
      "grad_norm": 107.10286712646484,
      "learning_rate": 0.00019762666666666668,
      "loss": -114.4508,
      "step": 890
    },
    {
      "epoch": 0.072,
      "grad_norm": 95.78987121582031,
      "learning_rate": 0.0001976,
      "loss": -115.3888,
      "step": 900
    },
    {
      "epoch": 0.0728,
      "grad_norm": 101.52413940429688,
      "learning_rate": 0.00019757333333333333,
      "loss": -114.9042,
      "step": 910
    },
    {
      "epoch": 0.0736,
      "grad_norm": 93.3606948852539,
      "learning_rate": 0.0001975466666666667,
      "loss": -115.4693,
      "step": 920
    },
    {
      "epoch": 0.0744,
      "grad_norm": 77.70208740234375,
      "learning_rate": 0.00019752000000000002,
      "loss": -115.6664,
      "step": 930
    },
    {
      "epoch": 0.0752,
      "grad_norm": 129.92074584960938,
      "learning_rate": 0.00019749333333333335,
      "loss": -114.164,
      "step": 940
    },
    {
      "epoch": 0.076,
      "grad_norm": 129.43418884277344,
      "learning_rate": 0.00019746666666666668,
      "loss": -115.4093,
      "step": 950
    },
    {
      "epoch": 0.0768,
      "grad_norm": 161.49240112304688,
      "learning_rate": 0.00019744,
      "loss": -114.6987,
      "step": 960
    },
    {
      "epoch": 0.0776,
      "grad_norm": 95.78922271728516,
      "learning_rate": 0.00019741333333333334,
      "loss": -115.1326,
      "step": 970
    },
    {
      "epoch": 0.0784,
      "grad_norm": 137.501708984375,
      "learning_rate": 0.00019738666666666667,
      "loss": -117.0616,
      "step": 980
    },
    {
      "epoch": 0.0792,
      "grad_norm": 90.75619506835938,
      "learning_rate": 0.00019736000000000002,
      "loss": -114.6751,
      "step": 990
    },
    {
      "epoch": 0.08,
      "grad_norm": 99.48433685302734,
      "learning_rate": 0.00019733333333333335,
      "loss": -114.6571,
      "step": 1000
    },
    {
      "epoch": 0.0808,
      "grad_norm": 103.70498657226562,
      "learning_rate": 0.00019730666666666668,
      "loss": -114.7998,
      "step": 1010
    },
    {
      "epoch": 0.0816,
      "grad_norm": 115.91633605957031,
      "learning_rate": 0.00019728,
      "loss": -114.9305,
      "step": 1020
    },
    {
      "epoch": 0.0824,
      "grad_norm": 97.43905639648438,
      "learning_rate": 0.00019725333333333334,
      "loss": -114.9948,
      "step": 1030
    },
    {
      "epoch": 0.0832,
      "grad_norm": 98.72494506835938,
      "learning_rate": 0.00019722666666666667,
      "loss": -114.7399,
      "step": 1040
    },
    {
      "epoch": 0.084,
      "grad_norm": 100.03331756591797,
      "learning_rate": 0.0001972,
      "loss": -115.4478,
      "step": 1050
    },
    {
      "epoch": 0.0848,
      "grad_norm": 64.29741668701172,
      "learning_rate": 0.00019717333333333335,
      "loss": -114.6254,
      "step": 1060
    },
    {
      "epoch": 0.0856,
      "grad_norm": 100.44302368164062,
      "learning_rate": 0.00019714666666666668,
      "loss": -115.1388,
      "step": 1070
    },
    {
      "epoch": 0.0864,
      "grad_norm": 97.56311798095703,
      "learning_rate": 0.00019712,
      "loss": -116.1743,
      "step": 1080
    },
    {
      "epoch": 0.0872,
      "grad_norm": 112.4156265258789,
      "learning_rate": 0.00019709333333333334,
      "loss": -114.7417,
      "step": 1090
    },
    {
      "epoch": 0.088,
      "grad_norm": 78.50469970703125,
      "learning_rate": 0.00019706666666666667,
      "loss": -115.5503,
      "step": 1100
    },
    {
      "epoch": 0.0888,
      "grad_norm": 88.62971496582031,
      "learning_rate": 0.00019704,
      "loss": -116.8199,
      "step": 1110
    },
    {
      "epoch": 0.0896,
      "grad_norm": 82.18741607666016,
      "learning_rate": 0.00019701333333333333,
      "loss": -115.5558,
      "step": 1120
    },
    {
      "epoch": 0.0904,
      "grad_norm": 68.86174011230469,
      "learning_rate": 0.00019698666666666668,
      "loss": -116.2156,
      "step": 1130
    },
    {
      "epoch": 0.0912,
      "grad_norm": 78.0696029663086,
      "learning_rate": 0.00019696,
      "loss": -114.6772,
      "step": 1140
    },
    {
      "epoch": 0.092,
      "grad_norm": 124.79463958740234,
      "learning_rate": 0.00019693333333333334,
      "loss": -115.2518,
      "step": 1150
    },
    {
      "epoch": 0.0928,
      "grad_norm": 99.35363006591797,
      "learning_rate": 0.0001969066666666667,
      "loss": -117.0764,
      "step": 1160
    },
    {
      "epoch": 0.0936,
      "grad_norm": 109.3826675415039,
      "learning_rate": 0.00019688000000000003,
      "loss": -115.577,
      "step": 1170
    },
    {
      "epoch": 0.0944,
      "grad_norm": 134.3365936279297,
      "learning_rate": 0.00019685333333333333,
      "loss": -115.8222,
      "step": 1180
    },
    {
      "epoch": 0.0952,
      "grad_norm": 120.15205383300781,
      "learning_rate": 0.00019682666666666666,
      "loss": -116.0396,
      "step": 1190
    },
    {
      "epoch": 0.096,
      "grad_norm": 141.12403869628906,
      "learning_rate": 0.0001968,
      "loss": -116.0833,
      "step": 1200
    },
    {
      "epoch": 0.0968,
      "grad_norm": 115.08223724365234,
      "learning_rate": 0.00019677333333333334,
      "loss": -114.9823,
      "step": 1210
    },
    {
      "epoch": 0.0976,
      "grad_norm": 116.84838104248047,
      "learning_rate": 0.00019674666666666667,
      "loss": -114.8518,
      "step": 1220
    },
    {
      "epoch": 0.0984,
      "grad_norm": 108.42768859863281,
      "learning_rate": 0.00019672000000000003,
      "loss": -115.5387,
      "step": 1230
    },
    {
      "epoch": 0.0992,
      "grad_norm": 105.79828643798828,
      "learning_rate": 0.00019669333333333336,
      "loss": -116.0635,
      "step": 1240
    },
    {
      "epoch": 0.1,
      "grad_norm": 110.59992980957031,
      "learning_rate": 0.00019666666666666666,
      "loss": -115.2531,
      "step": 1250
    },
    {
      "epoch": 0.1008,
      "grad_norm": 154.4980926513672,
      "learning_rate": 0.00019664000000000001,
      "loss": -116.1368,
      "step": 1260
    },
    {
      "epoch": 0.1016,
      "grad_norm": 151.39822387695312,
      "learning_rate": 0.00019661333333333334,
      "loss": -115.4222,
      "step": 1270
    },
    {
      "epoch": 0.1024,
      "grad_norm": 121.9175033569336,
      "learning_rate": 0.00019658666666666667,
      "loss": -115.5649,
      "step": 1280
    },
    {
      "epoch": 0.1032,
      "grad_norm": 98.10501098632812,
      "learning_rate": 0.00019656,
      "loss": -116.006,
      "step": 1290
    },
    {
      "epoch": 0.104,
      "grad_norm": 111.61466217041016,
      "learning_rate": 0.00019653333333333336,
      "loss": -116.8658,
      "step": 1300
    },
    {
      "epoch": 0.1048,
      "grad_norm": 108.80281829833984,
      "learning_rate": 0.0001965066666666667,
      "loss": -116.3845,
      "step": 1310
    },
    {
      "epoch": 0.1056,
      "grad_norm": 99.57032775878906,
      "learning_rate": 0.00019648000000000002,
      "loss": -114.9959,
      "step": 1320
    },
    {
      "epoch": 0.1064,
      "grad_norm": 87.7214584350586,
      "learning_rate": 0.00019645333333333335,
      "loss": -116.0236,
      "step": 1330
    },
    {
      "epoch": 0.1072,
      "grad_norm": 98.60572052001953,
      "learning_rate": 0.00019642666666666667,
      "loss": -116.9043,
      "step": 1340
    },
    {
      "epoch": 0.108,
      "grad_norm": 70.14616394042969,
      "learning_rate": 0.0001964,
      "loss": -116.6236,
      "step": 1350
    },
    {
      "epoch": 0.1088,
      "grad_norm": 92.24253845214844,
      "learning_rate": 0.00019637333333333333,
      "loss": -116.6087,
      "step": 1360
    },
    {
      "epoch": 0.1096,
      "grad_norm": 71.57415771484375,
      "learning_rate": 0.0001963466666666667,
      "loss": -115.9293,
      "step": 1370
    },
    {
      "epoch": 0.1104,
      "grad_norm": 122.57061767578125,
      "learning_rate": 0.00019632000000000002,
      "loss": -116.9844,
      "step": 1380
    },
    {
      "epoch": 0.1112,
      "grad_norm": 81.31510925292969,
      "learning_rate": 0.00019629333333333335,
      "loss": -116.5014,
      "step": 1390
    },
    {
      "epoch": 0.112,
      "grad_norm": 71.13145446777344,
      "learning_rate": 0.00019626666666666668,
      "loss": -114.9878,
      "step": 1400
    },
    {
      "epoch": 0.1128,
      "grad_norm": 96.53685760498047,
      "learning_rate": 0.00019624,
      "loss": -116.9482,
      "step": 1410
    },
    {
      "epoch": 0.1136,
      "grad_norm": 88.7847900390625,
      "learning_rate": 0.00019621333333333333,
      "loss": -115.7338,
      "step": 1420
    },
    {
      "epoch": 0.1144,
      "grad_norm": 88.48904418945312,
      "learning_rate": 0.00019618666666666666,
      "loss": -116.507,
      "step": 1430
    },
    {
      "epoch": 0.1152,
      "grad_norm": 108.9176254272461,
      "learning_rate": 0.00019616000000000002,
      "loss": -116.542,
      "step": 1440
    },
    {
      "epoch": 0.116,
      "grad_norm": 94.55103302001953,
      "learning_rate": 0.00019613333333333335,
      "loss": -116.511,
      "step": 1450
    },
    {
      "epoch": 0.1168,
      "grad_norm": 117.89035034179688,
      "learning_rate": 0.00019610666666666668,
      "loss": -115.9994,
      "step": 1460
    },
    {
      "epoch": 0.1176,
      "grad_norm": 86.69703674316406,
      "learning_rate": 0.00019608,
      "loss": -116.4851,
      "step": 1470
    },
    {
      "epoch": 0.1184,
      "grad_norm": 87.14048767089844,
      "learning_rate": 0.00019605333333333334,
      "loss": -116.1789,
      "step": 1480
    },
    {
      "epoch": 0.1192,
      "grad_norm": 90.2332992553711,
      "learning_rate": 0.00019602666666666666,
      "loss": -115.4658,
      "step": 1490
    },
    {
      "epoch": 0.12,
      "grad_norm": 117.67054748535156,
      "learning_rate": 0.000196,
      "loss": -116.1913,
      "step": 1500
    },
    {
      "epoch": 0.1208,
      "grad_norm": 163.81149291992188,
      "learning_rate": 0.00019597333333333335,
      "loss": -117.0726,
      "step": 1510
    },
    {
      "epoch": 0.1216,
      "grad_norm": 134.6744842529297,
      "learning_rate": 0.00019594666666666668,
      "loss": -116.0074,
      "step": 1520
    },
    {
      "epoch": 0.1224,
      "grad_norm": 119.5706558227539,
      "learning_rate": 0.00019592,
      "loss": -115.8861,
      "step": 1530
    },
    {
      "epoch": 0.1232,
      "grad_norm": 142.79071044921875,
      "learning_rate": 0.00019589333333333336,
      "loss": -116.7827,
      "step": 1540
    },
    {
      "epoch": 0.124,
      "grad_norm": 105.36556243896484,
      "learning_rate": 0.00019586666666666667,
      "loss": -117.1157,
      "step": 1550
    },
    {
      "epoch": 0.1248,
      "grad_norm": 75.69544219970703,
      "learning_rate": 0.00019584,
      "loss": -114.6769,
      "step": 1560
    },
    {
      "epoch": 0.1256,
      "grad_norm": 95.05902862548828,
      "learning_rate": 0.00019581333333333332,
      "loss": -117.5647,
      "step": 1570
    },
    {
      "epoch": 0.1264,
      "grad_norm": 129.49606323242188,
      "learning_rate": 0.00019578666666666668,
      "loss": -116.6159,
      "step": 1580
    },
    {
      "epoch": 0.1272,
      "grad_norm": 111.02128601074219,
      "learning_rate": 0.00019576,
      "loss": -117.2219,
      "step": 1590
    },
    {
      "epoch": 0.128,
      "grad_norm": 122.70800018310547,
      "learning_rate": 0.00019573333333333334,
      "loss": -115.871,
      "step": 1600
    },
    {
      "epoch": 0.1288,
      "grad_norm": 85.11271667480469,
      "learning_rate": 0.0001957066666666667,
      "loss": -116.4287,
      "step": 1610
    },
    {
      "epoch": 0.1296,
      "grad_norm": 198.78150939941406,
      "learning_rate": 0.00019568000000000002,
      "loss": -116.5709,
      "step": 1620
    },
    {
      "epoch": 0.1304,
      "grad_norm": 117.51506042480469,
      "learning_rate": 0.00019565333333333333,
      "loss": -116.3797,
      "step": 1630
    },
    {
      "epoch": 0.1312,
      "grad_norm": 124.05500793457031,
      "learning_rate": 0.00019562666666666668,
      "loss": -115.9587,
      "step": 1640
    },
    {
      "epoch": 0.132,
      "grad_norm": 80.87190246582031,
      "learning_rate": 0.0001956,
      "loss": -115.9186,
      "step": 1650
    },
    {
      "epoch": 0.1328,
      "grad_norm": 84.8852310180664,
      "learning_rate": 0.00019557333333333334,
      "loss": -116.6733,
      "step": 1660
    },
    {
      "epoch": 0.1336,
      "grad_norm": 146.5045166015625,
      "learning_rate": 0.00019554666666666667,
      "loss": -114.6718,
      "step": 1670
    },
    {
      "epoch": 0.1344,
      "grad_norm": 68.75531005859375,
      "learning_rate": 0.00019552000000000003,
      "loss": -115.5139,
      "step": 1680
    },
    {
      "epoch": 0.1352,
      "grad_norm": 104.24674987792969,
      "learning_rate": 0.00019549333333333335,
      "loss": -115.1525,
      "step": 1690
    },
    {
      "epoch": 0.136,
      "grad_norm": 91.59434509277344,
      "learning_rate": 0.00019546666666666668,
      "loss": -116.5295,
      "step": 1700
    },
    {
      "epoch": 0.1368,
      "grad_norm": 155.1585693359375,
      "learning_rate": 0.00019544,
      "loss": -115.6815,
      "step": 1710
    },
    {
      "epoch": 0.1376,
      "grad_norm": 100.99142456054688,
      "learning_rate": 0.00019541333333333334,
      "loss": -116.4891,
      "step": 1720
    },
    {
      "epoch": 0.1384,
      "grad_norm": 120.42520141601562,
      "learning_rate": 0.00019538666666666667,
      "loss": -115.6239,
      "step": 1730
    },
    {
      "epoch": 0.1392,
      "grad_norm": 111.8515853881836,
      "learning_rate": 0.00019536,
      "loss": -116.2184,
      "step": 1740
    },
    {
      "epoch": 0.14,
      "grad_norm": 152.400390625,
      "learning_rate": 0.00019533333333333336,
      "loss": -116.7681,
      "step": 1750
    },
    {
      "epoch": 0.1408,
      "grad_norm": 92.07115936279297,
      "learning_rate": 0.00019530666666666669,
      "loss": -116.0309,
      "step": 1760
    },
    {
      "epoch": 0.1416,
      "grad_norm": 69.31975555419922,
      "learning_rate": 0.00019528000000000001,
      "loss": -115.8824,
      "step": 1770
    },
    {
      "epoch": 0.1424,
      "grad_norm": 81.69468688964844,
      "learning_rate": 0.00019525333333333334,
      "loss": -116.9399,
      "step": 1780
    },
    {
      "epoch": 0.1432,
      "grad_norm": 81.63911437988281,
      "learning_rate": 0.00019522666666666667,
      "loss": -117.6871,
      "step": 1790
    },
    {
      "epoch": 0.144,
      "grad_norm": 216.7011260986328,
      "learning_rate": 0.0001952,
      "loss": -116.508,
      "step": 1800
    },
    {
      "epoch": 0.1448,
      "grad_norm": 92.41412353515625,
      "learning_rate": 0.00019517333333333333,
      "loss": -116.3433,
      "step": 1810
    },
    {
      "epoch": 0.1456,
      "grad_norm": 100.36270904541016,
      "learning_rate": 0.0001951466666666667,
      "loss": -117.1633,
      "step": 1820
    },
    {
      "epoch": 0.1464,
      "grad_norm": 90.75164794921875,
      "learning_rate": 0.00019512000000000002,
      "loss": -116.9237,
      "step": 1830
    },
    {
      "epoch": 0.1472,
      "grad_norm": 104.2852783203125,
      "learning_rate": 0.00019509333333333334,
      "loss": -116.464,
      "step": 1840
    },
    {
      "epoch": 0.148,
      "grad_norm": 83.07512664794922,
      "learning_rate": 0.00019506666666666667,
      "loss": -115.2329,
      "step": 1850
    },
    {
      "epoch": 0.1488,
      "grad_norm": 128.6038818359375,
      "learning_rate": 0.00019504,
      "loss": -116.7028,
      "step": 1860
    },
    {
      "epoch": 0.1496,
      "grad_norm": 110.7585678100586,
      "learning_rate": 0.00019501333333333333,
      "loss": -117.946,
      "step": 1870
    },
    {
      "epoch": 0.1504,
      "grad_norm": 136.41221618652344,
      "learning_rate": 0.00019498666666666666,
      "loss": -115.3426,
      "step": 1880
    },
    {
      "epoch": 0.1512,
      "grad_norm": 79.72608184814453,
      "learning_rate": 0.00019496000000000002,
      "loss": -117.0876,
      "step": 1890
    },
    {
      "epoch": 0.152,
      "grad_norm": 122.6067123413086,
      "learning_rate": 0.00019493333333333335,
      "loss": -116.174,
      "step": 1900
    },
    {
      "epoch": 0.1528,
      "grad_norm": 111.66474151611328,
      "learning_rate": 0.00019490666666666668,
      "loss": -116.375,
      "step": 1910
    },
    {
      "epoch": 0.1536,
      "grad_norm": 123.6317367553711,
      "learning_rate": 0.00019488000000000003,
      "loss": -116.6466,
      "step": 1920
    },
    {
      "epoch": 0.1544,
      "grad_norm": 87.61900329589844,
      "learning_rate": 0.00019485333333333333,
      "loss": -117.6579,
      "step": 1930
    },
    {
      "epoch": 0.1552,
      "grad_norm": 82.45188903808594,
      "learning_rate": 0.00019482666666666666,
      "loss": -116.1819,
      "step": 1940
    },
    {
      "epoch": 0.156,
      "grad_norm": 103.1208267211914,
      "learning_rate": 0.0001948,
      "loss": -117.2946,
      "step": 1950
    },
    {
      "epoch": 0.1568,
      "grad_norm": 112.88037109375,
      "learning_rate": 0.00019477333333333335,
      "loss": -117.2511,
      "step": 1960
    },
    {
      "epoch": 0.1576,
      "grad_norm": 77.01392364501953,
      "learning_rate": 0.00019474666666666668,
      "loss": -116.9234,
      "step": 1970
    },
    {
      "epoch": 0.1584,
      "grad_norm": 74.04377746582031,
      "learning_rate": 0.00019472,
      "loss": -115.76,
      "step": 1980
    },
    {
      "epoch": 0.1592,
      "grad_norm": 73.6789321899414,
      "learning_rate": 0.00019469333333333336,
      "loss": -116.1822,
      "step": 1990
    },
    {
      "epoch": 0.16,
      "grad_norm": 94.78670501708984,
      "learning_rate": 0.0001946666666666667,
      "loss": -118.0996,
      "step": 2000
    },
    {
      "epoch": 0.1608,
      "grad_norm": 102.3202133178711,
      "learning_rate": 0.00019464,
      "loss": -116.683,
      "step": 2010
    },
    {
      "epoch": 0.1616,
      "grad_norm": 95.40208435058594,
      "learning_rate": 0.00019461333333333335,
      "loss": -115.7574,
      "step": 2020
    },
    {
      "epoch": 0.1624,
      "grad_norm": 100.09839630126953,
      "learning_rate": 0.00019458666666666668,
      "loss": -117.3755,
      "step": 2030
    },
    {
      "epoch": 0.1632,
      "grad_norm": 117.47175598144531,
      "learning_rate": 0.00019456,
      "loss": -116.6912,
      "step": 2040
    },
    {
      "epoch": 0.164,
      "grad_norm": 94.3270034790039,
      "learning_rate": 0.00019453333333333334,
      "loss": -116.4651,
      "step": 2050
    },
    {
      "epoch": 0.1648,
      "grad_norm": 103.73236846923828,
      "learning_rate": 0.0001945066666666667,
      "loss": -117.0672,
      "step": 2060
    },
    {
      "epoch": 0.1656,
      "grad_norm": 115.82574462890625,
      "learning_rate": 0.00019448000000000002,
      "loss": -117.8261,
      "step": 2070
    },
    {
      "epoch": 0.1664,
      "grad_norm": 75.38306427001953,
      "learning_rate": 0.00019445333333333332,
      "loss": -116.4981,
      "step": 2080
    },
    {
      "epoch": 0.1672,
      "grad_norm": 80.08241271972656,
      "learning_rate": 0.00019442666666666668,
      "loss": -116.2032,
      "step": 2090
    },
    {
      "epoch": 0.168,
      "grad_norm": 133.91014099121094,
      "learning_rate": 0.0001944,
      "loss": -117.7254,
      "step": 2100
    },
    {
      "epoch": 0.1688,
      "grad_norm": 95.9876480102539,
      "learning_rate": 0.00019437333333333334,
      "loss": -115.9967,
      "step": 2110
    },
    {
      "epoch": 0.1696,
      "grad_norm": 100.36714935302734,
      "learning_rate": 0.00019434666666666667,
      "loss": -117.9294,
      "step": 2120
    },
    {
      "epoch": 0.1704,
      "grad_norm": 106.18048858642578,
      "learning_rate": 0.00019432000000000002,
      "loss": -116.9587,
      "step": 2130
    },
    {
      "epoch": 0.1712,
      "grad_norm": 121.1848373413086,
      "learning_rate": 0.00019429333333333335,
      "loss": -115.8333,
      "step": 2140
    },
    {
      "epoch": 0.172,
      "grad_norm": 108.23075103759766,
      "learning_rate": 0.00019426666666666668,
      "loss": -118.0052,
      "step": 2150
    },
    {
      "epoch": 0.1728,
      "grad_norm": 95.86546325683594,
      "learning_rate": 0.00019424,
      "loss": -116.8358,
      "step": 2160
    },
    {
      "epoch": 0.1736,
      "grad_norm": 114.08641815185547,
      "learning_rate": 0.00019421333333333334,
      "loss": -116.7631,
      "step": 2170
    },
    {
      "epoch": 0.1744,
      "grad_norm": 66.86699676513672,
      "learning_rate": 0.00019418666666666667,
      "loss": -116.4661,
      "step": 2180
    },
    {
      "epoch": 0.1752,
      "grad_norm": 99.85564422607422,
      "learning_rate": 0.00019416,
      "loss": -116.3232,
      "step": 2190
    },
    {
      "epoch": 0.176,
      "grad_norm": 87.9434585571289,
      "learning_rate": 0.00019413333333333335,
      "loss": -117.6634,
      "step": 2200
    },
    {
      "epoch": 0.1768,
      "grad_norm": 110.09405517578125,
      "learning_rate": 0.00019410666666666668,
      "loss": -116.4121,
      "step": 2210
    },
    {
      "epoch": 0.1776,
      "grad_norm": 81.56073760986328,
      "learning_rate": 0.00019408,
      "loss": -116.647,
      "step": 2220
    },
    {
      "epoch": 0.1784,
      "grad_norm": 110.5086441040039,
      "learning_rate": 0.00019405333333333334,
      "loss": -116.3069,
      "step": 2230
    },
    {
      "epoch": 0.1792,
      "grad_norm": 96.45773315429688,
      "learning_rate": 0.00019402666666666667,
      "loss": -116.8556,
      "step": 2240
    },
    {
      "epoch": 0.18,
      "grad_norm": 115.26629638671875,
      "learning_rate": 0.000194,
      "loss": -115.8583,
      "step": 2250
    },
    {
      "epoch": 0.1808,
      "grad_norm": 111.51345825195312,
      "learning_rate": 0.00019397333333333333,
      "loss": -115.7597,
      "step": 2260
    },
    {
      "epoch": 0.1816,
      "grad_norm": 92.91079711914062,
      "learning_rate": 0.00019394666666666668,
      "loss": -116.8208,
      "step": 2270
    },
    {
      "epoch": 0.1824,
      "grad_norm": 81.81238555908203,
      "learning_rate": 0.00019392000000000001,
      "loss": -116.7149,
      "step": 2280
    },
    {
      "epoch": 0.1832,
      "grad_norm": 92.9188232421875,
      "learning_rate": 0.00019389333333333334,
      "loss": -116.3398,
      "step": 2290
    },
    {
      "epoch": 0.184,
      "grad_norm": 122.13356018066406,
      "learning_rate": 0.0001938666666666667,
      "loss": -115.5792,
      "step": 2300
    },
    {
      "epoch": 0.1848,
      "grad_norm": 113.79287719726562,
      "learning_rate": 0.00019384,
      "loss": -116.7411,
      "step": 2310
    },
    {
      "epoch": 0.1856,
      "grad_norm": 85.51744842529297,
      "learning_rate": 0.00019381333333333333,
      "loss": -116.8315,
      "step": 2320
    },
    {
      "epoch": 0.1864,
      "grad_norm": 110.90833282470703,
      "learning_rate": 0.00019378666666666666,
      "loss": -117.0648,
      "step": 2330
    },
    {
      "epoch": 0.1872,
      "grad_norm": 88.8200912475586,
      "learning_rate": 0.00019376000000000002,
      "loss": -116.8185,
      "step": 2340
    },
    {
      "epoch": 0.188,
      "grad_norm": 96.23922729492188,
      "learning_rate": 0.00019373333333333334,
      "loss": -116.912,
      "step": 2350
    },
    {
      "epoch": 0.1888,
      "grad_norm": 72.27816772460938,
      "learning_rate": 0.00019370666666666667,
      "loss": -117.1941,
      "step": 2360
    },
    {
      "epoch": 0.1896,
      "grad_norm": 95.32795715332031,
      "learning_rate": 0.00019368000000000003,
      "loss": -116.3282,
      "step": 2370
    },
    {
      "epoch": 0.1904,
      "grad_norm": 57.43355941772461,
      "learning_rate": 0.00019365333333333336,
      "loss": -117.7411,
      "step": 2380
    },
    {
      "epoch": 0.1912,
      "grad_norm": 112.93827056884766,
      "learning_rate": 0.00019362666666666666,
      "loss": -115.314,
      "step": 2390
    },
    {
      "epoch": 0.192,
      "grad_norm": 112.46293640136719,
      "learning_rate": 0.00019360000000000002,
      "loss": -117.9412,
      "step": 2400
    },
    {
      "epoch": 0.1928,
      "grad_norm": 117.99288177490234,
      "learning_rate": 0.00019357333333333335,
      "loss": -116.1822,
      "step": 2410
    },
    {
      "epoch": 0.1936,
      "grad_norm": 140.0335693359375,
      "learning_rate": 0.00019354666666666667,
      "loss": -116.8385,
      "step": 2420
    },
    {
      "epoch": 0.1944,
      "grad_norm": 96.07984161376953,
      "learning_rate": 0.00019352,
      "loss": -116.1797,
      "step": 2430
    },
    {
      "epoch": 0.1952,
      "grad_norm": 70.34224700927734,
      "learning_rate": 0.00019349333333333336,
      "loss": -117.2126,
      "step": 2440
    },
    {
      "epoch": 0.196,
      "grad_norm": 41.83427429199219,
      "learning_rate": 0.0001934666666666667,
      "loss": -116.769,
      "step": 2450
    },
    {
      "epoch": 0.1968,
      "grad_norm": 134.31002807617188,
      "learning_rate": 0.00019344,
      "loss": -117.0863,
      "step": 2460
    },
    {
      "epoch": 0.1976,
      "grad_norm": 73.14415740966797,
      "learning_rate": 0.00019341333333333335,
      "loss": -118.0746,
      "step": 2470
    },
    {
      "epoch": 0.1984,
      "grad_norm": 70.27688598632812,
      "learning_rate": 0.00019338666666666668,
      "loss": -116.6753,
      "step": 2480
    },
    {
      "epoch": 0.1992,
      "grad_norm": 84.46620178222656,
      "learning_rate": 0.00019336,
      "loss": -117.0398,
      "step": 2490
    },
    {
      "epoch": 0.2,
      "grad_norm": 90.57164764404297,
      "learning_rate": 0.00019333333333333333,
      "loss": -116.4089,
      "step": 2500
    },
    {
      "epoch": 0.2008,
      "grad_norm": 95.27186584472656,
      "learning_rate": 0.0001933066666666667,
      "loss": -116.9419,
      "step": 2510
    },
    {
      "epoch": 0.2016,
      "grad_norm": 105.79436492919922,
      "learning_rate": 0.00019328000000000002,
      "loss": -114.8314,
      "step": 2520
    },
    {
      "epoch": 0.2024,
      "grad_norm": 76.89646911621094,
      "learning_rate": 0.00019325333333333335,
      "loss": -116.8276,
      "step": 2530
    },
    {
      "epoch": 0.2032,
      "grad_norm": 101.41246032714844,
      "learning_rate": 0.00019322666666666668,
      "loss": -117.0191,
      "step": 2540
    },
    {
      "epoch": 0.204,
      "grad_norm": 66.12213897705078,
      "learning_rate": 0.0001932,
      "loss": -117.2247,
      "step": 2550
    },
    {
      "epoch": 0.2048,
      "grad_norm": 98.54371643066406,
      "learning_rate": 0.00019317333333333334,
      "loss": -117.6077,
      "step": 2560
    },
    {
      "epoch": 0.2056,
      "grad_norm": 107.4691390991211,
      "learning_rate": 0.00019314666666666667,
      "loss": -115.3108,
      "step": 2570
    },
    {
      "epoch": 0.2064,
      "grad_norm": 114.70890808105469,
      "learning_rate": 0.00019312000000000002,
      "loss": -116.9863,
      "step": 2580
    },
    {
      "epoch": 0.2072,
      "grad_norm": 135.0662078857422,
      "learning_rate": 0.00019309333333333335,
      "loss": -115.9663,
      "step": 2590
    },
    {
      "epoch": 0.208,
      "grad_norm": 98.53016662597656,
      "learning_rate": 0.00019306666666666668,
      "loss": -117.3134,
      "step": 2600
    },
    {
      "epoch": 0.2088,
      "grad_norm": 78.45132446289062,
      "learning_rate": 0.00019304,
      "loss": -117.065,
      "step": 2610
    },
    {
      "epoch": 0.2096,
      "grad_norm": 92.63209533691406,
      "learning_rate": 0.00019301333333333334,
      "loss": -117.3034,
      "step": 2620
    },
    {
      "epoch": 0.2104,
      "grad_norm": 79.07618713378906,
      "learning_rate": 0.00019298666666666667,
      "loss": -116.2444,
      "step": 2630
    },
    {
      "epoch": 0.2112,
      "grad_norm": 131.80841064453125,
      "learning_rate": 0.00019296,
      "loss": -116.0583,
      "step": 2640
    },
    {
      "epoch": 0.212,
      "grad_norm": 83.16053009033203,
      "learning_rate": 0.00019293333333333335,
      "loss": -116.8296,
      "step": 2650
    },
    {
      "epoch": 0.2128,
      "grad_norm": 90.58116149902344,
      "learning_rate": 0.00019290666666666668,
      "loss": -116.5885,
      "step": 2660
    },
    {
      "epoch": 0.2136,
      "grad_norm": 101.73612976074219,
      "learning_rate": 0.00019288,
      "loss": -117.2519,
      "step": 2670
    },
    {
      "epoch": 0.2144,
      "grad_norm": 63.02219772338867,
      "learning_rate": 0.00019285333333333334,
      "loss": -117.4051,
      "step": 2680
    },
    {
      "epoch": 0.2152,
      "grad_norm": 79.68089294433594,
      "learning_rate": 0.00019282666666666667,
      "loss": -116.7162,
      "step": 2690
    },
    {
      "epoch": 0.216,
      "grad_norm": 114.16490936279297,
      "learning_rate": 0.0001928,
      "loss": -116.7875,
      "step": 2700
    },
    {
      "epoch": 0.2168,
      "grad_norm": 67.18409729003906,
      "learning_rate": 0.00019277333333333333,
      "loss": -115.961,
      "step": 2710
    },
    {
      "epoch": 0.2176,
      "grad_norm": 99.48344421386719,
      "learning_rate": 0.00019274666666666668,
      "loss": -118.0206,
      "step": 2720
    },
    {
      "epoch": 0.2184,
      "grad_norm": 82.21388244628906,
      "learning_rate": 0.00019272,
      "loss": -116.6992,
      "step": 2730
    },
    {
      "epoch": 0.2192,
      "grad_norm": 90.50921630859375,
      "learning_rate": 0.00019269333333333334,
      "loss": -115.4256,
      "step": 2740
    },
    {
      "epoch": 0.22,
      "grad_norm": 91.21497344970703,
      "learning_rate": 0.0001926666666666667,
      "loss": -117.903,
      "step": 2750
    },
    {
      "epoch": 0.2208,
      "grad_norm": 82.18656921386719,
      "learning_rate": 0.00019264,
      "loss": -116.5947,
      "step": 2760
    },
    {
      "epoch": 0.2216,
      "grad_norm": 78.10005187988281,
      "learning_rate": 0.00019261333333333333,
      "loss": -117.01,
      "step": 2770
    },
    {
      "epoch": 0.2224,
      "grad_norm": 124.70529174804688,
      "learning_rate": 0.00019258666666666668,
      "loss": -117.577,
      "step": 2780
    },
    {
      "epoch": 0.2232,
      "grad_norm": 62.79027557373047,
      "learning_rate": 0.00019256,
      "loss": -117.6779,
      "step": 2790
    },
    {
      "epoch": 0.224,
      "grad_norm": 84.61380767822266,
      "learning_rate": 0.00019253333333333334,
      "loss": -117.1526,
      "step": 2800
    },
    {
      "epoch": 0.2248,
      "grad_norm": 135.67636108398438,
      "learning_rate": 0.00019250666666666667,
      "loss": -117.923,
      "step": 2810
    },
    {
      "epoch": 0.2256,
      "grad_norm": 97.17060089111328,
      "learning_rate": 0.00019248000000000003,
      "loss": -117.3728,
      "step": 2820
    },
    {
      "epoch": 0.2264,
      "grad_norm": 73.30915069580078,
      "learning_rate": 0.00019245333333333336,
      "loss": -117.3215,
      "step": 2830
    },
    {
      "epoch": 0.2272,
      "grad_norm": 64.7952880859375,
      "learning_rate": 0.00019242666666666666,
      "loss": -116.3177,
      "step": 2840
    },
    {
      "epoch": 0.228,
      "grad_norm": 103.53379821777344,
      "learning_rate": 0.00019240000000000001,
      "loss": -116.813,
      "step": 2850
    },
    {
      "epoch": 0.2288,
      "grad_norm": 86.75952911376953,
      "learning_rate": 0.00019237333333333334,
      "loss": -117.1092,
      "step": 2860
    },
    {
      "epoch": 0.2296,
      "grad_norm": 83.12065887451172,
      "learning_rate": 0.00019234666666666667,
      "loss": -115.701,
      "step": 2870
    },
    {
      "epoch": 0.2304,
      "grad_norm": 104.2449722290039,
      "learning_rate": 0.00019232,
      "loss": -117.5413,
      "step": 2880
    },
    {
      "epoch": 0.2312,
      "grad_norm": 136.86949157714844,
      "learning_rate": 0.00019229333333333336,
      "loss": -117.9015,
      "step": 2890
    },
    {
      "epoch": 0.232,
      "grad_norm": 84.23482513427734,
      "learning_rate": 0.0001922666666666667,
      "loss": -116.8577,
      "step": 2900
    },
    {
      "epoch": 0.2328,
      "grad_norm": 69.14227294921875,
      "learning_rate": 0.00019224000000000002,
      "loss": -116.9006,
      "step": 2910
    },
    {
      "epoch": 0.2336,
      "grad_norm": 118.1960220336914,
      "learning_rate": 0.00019221333333333335,
      "loss": -117.615,
      "step": 2920
    },
    {
      "epoch": 0.2344,
      "grad_norm": 93.56986236572266,
      "learning_rate": 0.00019218666666666667,
      "loss": -117.0039,
      "step": 2930
    },
    {
      "epoch": 0.2352,
      "grad_norm": 114.42630767822266,
      "learning_rate": 0.00019216,
      "loss": -116.9503,
      "step": 2940
    },
    {
      "epoch": 0.236,
      "grad_norm": 95.061279296875,
      "learning_rate": 0.00019213333333333333,
      "loss": -117.6494,
      "step": 2950
    },
    {
      "epoch": 0.2368,
      "grad_norm": 88.93921661376953,
      "learning_rate": 0.0001921066666666667,
      "loss": -117.2059,
      "step": 2960
    },
    {
      "epoch": 0.2376,
      "grad_norm": 72.67536163330078,
      "learning_rate": 0.00019208000000000002,
      "loss": -117.2899,
      "step": 2970
    },
    {
      "epoch": 0.2384,
      "grad_norm": 77.17446899414062,
      "learning_rate": 0.00019205333333333335,
      "loss": -117.7868,
      "step": 2980
    },
    {
      "epoch": 0.2392,
      "grad_norm": 82.28032684326172,
      "learning_rate": 0.00019202666666666668,
      "loss": -116.3687,
      "step": 2990
    },
    {
      "epoch": 0.24,
      "grad_norm": 114.46782684326172,
      "learning_rate": 0.000192,
      "loss": -117.6017,
      "step": 3000
    },
    {
      "epoch": 0.2408,
      "grad_norm": 77.67455291748047,
      "learning_rate": 0.00019197333333333333,
      "loss": -118.2281,
      "step": 3010
    },
    {
      "epoch": 0.2416,
      "grad_norm": 93.7262954711914,
      "learning_rate": 0.00019194666666666666,
      "loss": -116.5188,
      "step": 3020
    },
    {
      "epoch": 0.2424,
      "grad_norm": 133.79672241210938,
      "learning_rate": 0.00019192000000000002,
      "loss": -118.094,
      "step": 3030
    },
    {
      "epoch": 0.2432,
      "grad_norm": 98.30145263671875,
      "learning_rate": 0.00019189333333333335,
      "loss": -116.5864,
      "step": 3040
    },
    {
      "epoch": 0.244,
      "grad_norm": 90.44749450683594,
      "learning_rate": 0.00019186666666666668,
      "loss": -117.3334,
      "step": 3050
    },
    {
      "epoch": 0.2448,
      "grad_norm": 66.46038055419922,
      "learning_rate": 0.00019184,
      "loss": -116.9456,
      "step": 3060
    },
    {
      "epoch": 0.2456,
      "grad_norm": 83.99249267578125,
      "learning_rate": 0.00019181333333333334,
      "loss": -116.6775,
      "step": 3070
    },
    {
      "epoch": 0.2464,
      "grad_norm": 64.46127319335938,
      "learning_rate": 0.00019178666666666666,
      "loss": -117.0517,
      "step": 3080
    },
    {
      "epoch": 0.2472,
      "grad_norm": 80.6865463256836,
      "learning_rate": 0.00019176,
      "loss": -117.7743,
      "step": 3090
    },
    {
      "epoch": 0.248,
      "grad_norm": 64.85520935058594,
      "learning_rate": 0.00019173333333333335,
      "loss": -117.103,
      "step": 3100
    },
    {
      "epoch": 0.2488,
      "grad_norm": 78.61631774902344,
      "learning_rate": 0.00019170666666666668,
      "loss": -116.466,
      "step": 3110
    },
    {
      "epoch": 0.2496,
      "grad_norm": 59.374820709228516,
      "learning_rate": 0.00019168,
      "loss": -116.4758,
      "step": 3120
    },
    {
      "epoch": 0.2504,
      "grad_norm": 97.58914184570312,
      "learning_rate": 0.00019165333333333336,
      "loss": -117.4509,
      "step": 3130
    },
    {
      "epoch": 0.2512,
      "grad_norm": 135.64537048339844,
      "learning_rate": 0.00019162666666666667,
      "loss": -116.115,
      "step": 3140
    },
    {
      "epoch": 0.252,
      "grad_norm": 63.67714309692383,
      "learning_rate": 0.0001916,
      "loss": -116.8232,
      "step": 3150
    },
    {
      "epoch": 0.2528,
      "grad_norm": 49.535396575927734,
      "learning_rate": 0.00019157333333333335,
      "loss": -116.654,
      "step": 3160
    },
    {
      "epoch": 0.2536,
      "grad_norm": 96.02322387695312,
      "learning_rate": 0.00019154666666666668,
      "loss": -117.17,
      "step": 3170
    },
    {
      "epoch": 0.2544,
      "grad_norm": 89.24690246582031,
      "learning_rate": 0.00019152,
      "loss": -117.594,
      "step": 3180
    },
    {
      "epoch": 0.2552,
      "grad_norm": 84.67578125,
      "learning_rate": 0.00019149333333333334,
      "loss": -116.8177,
      "step": 3190
    },
    {
      "epoch": 0.256,
      "grad_norm": 66.39517974853516,
      "learning_rate": 0.0001914666666666667,
      "loss": -117.5244,
      "step": 3200
    },
    {
      "epoch": 0.2568,
      "grad_norm": 94.3587417602539,
      "learning_rate": 0.00019144000000000002,
      "loss": -116.7972,
      "step": 3210
    },
    {
      "epoch": 0.2576,
      "grad_norm": 104.29000091552734,
      "learning_rate": 0.00019141333333333333,
      "loss": -117.5728,
      "step": 3220
    },
    {
      "epoch": 0.2584,
      "grad_norm": 68.11003112792969,
      "learning_rate": 0.00019138666666666668,
      "loss": -116.6683,
      "step": 3230
    },
    {
      "epoch": 0.2592,
      "grad_norm": 105.76512908935547,
      "learning_rate": 0.00019136,
      "loss": -116.5012,
      "step": 3240
    },
    {
      "epoch": 0.26,
      "grad_norm": 104.28616333007812,
      "learning_rate": 0.00019133333333333334,
      "loss": -117.4536,
      "step": 3250
    },
    {
      "epoch": 0.2608,
      "grad_norm": 98.94849395751953,
      "learning_rate": 0.00019130666666666667,
      "loss": -117.4869,
      "step": 3260
    },
    {
      "epoch": 0.2616,
      "grad_norm": 107.40431213378906,
      "learning_rate": 0.00019128000000000003,
      "loss": -116.3212,
      "step": 3270
    },
    {
      "epoch": 0.2624,
      "grad_norm": 91.6134033203125,
      "learning_rate": 0.00019125333333333335,
      "loss": -118.3244,
      "step": 3280
    },
    {
      "epoch": 0.2632,
      "grad_norm": 56.32924270629883,
      "learning_rate": 0.00019122666666666666,
      "loss": -117.6362,
      "step": 3290
    },
    {
      "epoch": 0.264,
      "grad_norm": 74.73661041259766,
      "learning_rate": 0.0001912,
      "loss": -117.5714,
      "step": 3300
    },
    {
      "epoch": 0.2648,
      "grad_norm": 89.59890747070312,
      "learning_rate": 0.00019117333333333334,
      "loss": -118.0859,
      "step": 3310
    },
    {
      "epoch": 0.2656,
      "grad_norm": 58.61537551879883,
      "learning_rate": 0.00019114666666666667,
      "loss": -117.8918,
      "step": 3320
    },
    {
      "epoch": 0.2664,
      "grad_norm": 63.01579284667969,
      "learning_rate": 0.00019112,
      "loss": -117.6317,
      "step": 3330
    },
    {
      "epoch": 0.2672,
      "grad_norm": 54.17140579223633,
      "learning_rate": 0.00019109333333333336,
      "loss": -117.0268,
      "step": 3340
    },
    {
      "epoch": 0.268,
      "grad_norm": 86.9528579711914,
      "learning_rate": 0.00019106666666666668,
      "loss": -117.4514,
      "step": 3350
    },
    {
      "epoch": 0.2688,
      "grad_norm": 91.69364166259766,
      "learning_rate": 0.00019104000000000001,
      "loss": -117.0095,
      "step": 3360
    },
    {
      "epoch": 0.2696,
      "grad_norm": 141.9312286376953,
      "learning_rate": 0.00019101333333333334,
      "loss": -117.9608,
      "step": 3370
    },
    {
      "epoch": 0.2704,
      "grad_norm": 96.09416961669922,
      "learning_rate": 0.00019098666666666667,
      "loss": -117.7899,
      "step": 3380
    },
    {
      "epoch": 0.2712,
      "grad_norm": 59.9031867980957,
      "learning_rate": 0.00019096,
      "loss": -117.7108,
      "step": 3390
    },
    {
      "epoch": 0.272,
      "grad_norm": 68.49246978759766,
      "learning_rate": 0.00019093333333333333,
      "loss": -116.4381,
      "step": 3400
    },
    {
      "epoch": 0.2728,
      "grad_norm": 82.70870971679688,
      "learning_rate": 0.0001909066666666667,
      "loss": -115.9575,
      "step": 3410
    },
    {
      "epoch": 0.2736,
      "grad_norm": 84.50139617919922,
      "learning_rate": 0.00019088000000000002,
      "loss": -118.2018,
      "step": 3420
    },
    {
      "epoch": 0.2744,
      "grad_norm": 108.35196685791016,
      "learning_rate": 0.00019085333333333334,
      "loss": -117.5989,
      "step": 3430
    },
    {
      "epoch": 0.2752,
      "grad_norm": 89.82722473144531,
      "learning_rate": 0.00019082666666666667,
      "loss": -117.3292,
      "step": 3440
    },
    {
      "epoch": 0.276,
      "grad_norm": 108.0348129272461,
      "learning_rate": 0.0001908,
      "loss": -116.8116,
      "step": 3450
    },
    {
      "epoch": 0.2768,
      "grad_norm": 87.86354064941406,
      "learning_rate": 0.00019077333333333333,
      "loss": -117.4632,
      "step": 3460
    },
    {
      "epoch": 0.2776,
      "grad_norm": 82.20124053955078,
      "learning_rate": 0.00019074666666666666,
      "loss": -117.7412,
      "step": 3470
    },
    {
      "epoch": 0.2784,
      "grad_norm": 95.33695220947266,
      "learning_rate": 0.00019072000000000002,
      "loss": -116.692,
      "step": 3480
    },
    {
      "epoch": 0.2792,
      "grad_norm": 80.60599517822266,
      "learning_rate": 0.00019069333333333335,
      "loss": -117.069,
      "step": 3490
    },
    {
      "epoch": 0.28,
      "grad_norm": 101.62773132324219,
      "learning_rate": 0.00019066666666666668,
      "loss": -116.7947,
      "step": 3500
    },
    {
      "epoch": 0.2808,
      "grad_norm": 108.25862121582031,
      "learning_rate": 0.00019064000000000003,
      "loss": -116.9195,
      "step": 3510
    },
    {
      "epoch": 0.2816,
      "grad_norm": 91.21430206298828,
      "learning_rate": 0.00019061333333333333,
      "loss": -116.1421,
      "step": 3520
    },
    {
      "epoch": 0.2824,
      "grad_norm": 91.51296997070312,
      "learning_rate": 0.00019058666666666666,
      "loss": -116.8669,
      "step": 3530
    },
    {
      "epoch": 0.2832,
      "grad_norm": 90.5048828125,
      "learning_rate": 0.00019056000000000002,
      "loss": -118.335,
      "step": 3540
    },
    {
      "epoch": 0.284,
      "grad_norm": 74.15998077392578,
      "learning_rate": 0.00019053333333333335,
      "loss": -117.7986,
      "step": 3550
    },
    {
      "epoch": 0.2848,
      "grad_norm": 65.29568481445312,
      "learning_rate": 0.00019050666666666668,
      "loss": -117.5826,
      "step": 3560
    },
    {
      "epoch": 0.2856,
      "grad_norm": 86.4378433227539,
      "learning_rate": 0.00019048,
      "loss": -117.0385,
      "step": 3570
    },
    {
      "epoch": 0.2864,
      "grad_norm": 97.05860900878906,
      "learning_rate": 0.00019045333333333336,
      "loss": -118.2543,
      "step": 3580
    },
    {
      "epoch": 0.2872,
      "grad_norm": 80.80037689208984,
      "learning_rate": 0.0001904266666666667,
      "loss": -118.2416,
      "step": 3590
    },
    {
      "epoch": 0.288,
      "grad_norm": 62.77082443237305,
      "learning_rate": 0.0001904,
      "loss": -116.7474,
      "step": 3600
    },
    {
      "epoch": 0.2888,
      "grad_norm": 77.68909454345703,
      "learning_rate": 0.00019037333333333335,
      "loss": -118.114,
      "step": 3610
    },
    {
      "epoch": 0.2896,
      "grad_norm": 76.13875579833984,
      "learning_rate": 0.00019034666666666668,
      "loss": -117.0027,
      "step": 3620
    },
    {
      "epoch": 0.2904,
      "grad_norm": 81.9204330444336,
      "learning_rate": 0.00019032,
      "loss": -116.2253,
      "step": 3630
    },
    {
      "epoch": 0.2912,
      "grad_norm": 74.04994201660156,
      "learning_rate": 0.00019029333333333334,
      "loss": -117.8914,
      "step": 3640
    },
    {
      "epoch": 0.292,
      "grad_norm": 60.830421447753906,
      "learning_rate": 0.0001902666666666667,
      "loss": -116.3869,
      "step": 3650
    },
    {
      "epoch": 0.2928,
      "grad_norm": 89.9209213256836,
      "learning_rate": 0.00019024000000000002,
      "loss": -118.2393,
      "step": 3660
    },
    {
      "epoch": 0.2936,
      "grad_norm": 78.48104858398438,
      "learning_rate": 0.00019021333333333332,
      "loss": -117.9432,
      "step": 3670
    },
    {
      "epoch": 0.2944,
      "grad_norm": 91.11188507080078,
      "learning_rate": 0.00019018666666666668,
      "loss": -118.4355,
      "step": 3680
    },
    {
      "epoch": 0.2952,
      "grad_norm": 101.74385833740234,
      "learning_rate": 0.00019016,
      "loss": -116.879,
      "step": 3690
    },
    {
      "epoch": 0.296,
      "grad_norm": 59.35485076904297,
      "learning_rate": 0.00019013333333333334,
      "loss": -117.561,
      "step": 3700
    },
    {
      "epoch": 0.2968,
      "grad_norm": 102.1507797241211,
      "learning_rate": 0.00019010666666666667,
      "loss": -117.1112,
      "step": 3710
    },
    {
      "epoch": 0.2976,
      "grad_norm": 91.93021392822266,
      "learning_rate": 0.00019008000000000002,
      "loss": -117.3757,
      "step": 3720
    },
    {
      "epoch": 0.2984,
      "grad_norm": 94.06929016113281,
      "learning_rate": 0.00019005333333333335,
      "loss": -117.2906,
      "step": 3730
    },
    {
      "epoch": 0.2992,
      "grad_norm": 90.54251861572266,
      "learning_rate": 0.00019002666666666668,
      "loss": -117.5997,
      "step": 3740
    },
    {
      "epoch": 0.3,
      "grad_norm": 80.07730865478516,
      "learning_rate": 0.00019,
      "loss": -117.117,
      "step": 3750
    },
    {
      "epoch": 0.3008,
      "grad_norm": 90.19027709960938,
      "learning_rate": 0.00018997333333333334,
      "loss": -117.6849,
      "step": 3760
    },
    {
      "epoch": 0.3016,
      "grad_norm": 100.54865264892578,
      "learning_rate": 0.00018994666666666667,
      "loss": -116.2589,
      "step": 3770
    },
    {
      "epoch": 0.3024,
      "grad_norm": 76.18262481689453,
      "learning_rate": 0.00018992,
      "loss": -115.5237,
      "step": 3780
    },
    {
      "epoch": 0.3032,
      "grad_norm": 50.74720001220703,
      "learning_rate": 0.00018989333333333335,
      "loss": -117.5807,
      "step": 3790
    },
    {
      "epoch": 0.304,
      "grad_norm": 65.12234497070312,
      "learning_rate": 0.00018986666666666668,
      "loss": -118.6389,
      "step": 3800
    },
    {
      "epoch": 0.3048,
      "grad_norm": 70.130615234375,
      "learning_rate": 0.00018984,
      "loss": -117.7885,
      "step": 3810
    },
    {
      "epoch": 0.3056,
      "grad_norm": 91.68362426757812,
      "learning_rate": 0.00018981333333333334,
      "loss": -118.3332,
      "step": 3820
    },
    {
      "epoch": 0.3064,
      "grad_norm": 72.5172348022461,
      "learning_rate": 0.00018978666666666667,
      "loss": -118.2262,
      "step": 3830
    },
    {
      "epoch": 0.3072,
      "grad_norm": 90.45062255859375,
      "learning_rate": 0.00018976,
      "loss": -117.2808,
      "step": 3840
    },
    {
      "epoch": 0.308,
      "grad_norm": 78.4048080444336,
      "learning_rate": 0.00018973333333333333,
      "loss": -118.6012,
      "step": 3850
    },
    {
      "epoch": 0.3088,
      "grad_norm": 84.51206970214844,
      "learning_rate": 0.00018970666666666668,
      "loss": -117.4165,
      "step": 3860
    },
    {
      "epoch": 0.3096,
      "grad_norm": 75.72750091552734,
      "learning_rate": 0.00018968,
      "loss": -115.9967,
      "step": 3870
    },
    {
      "epoch": 0.3104,
      "grad_norm": 101.60262298583984,
      "learning_rate": 0.00018965333333333334,
      "loss": -117.82,
      "step": 3880
    },
    {
      "epoch": 0.3112,
      "grad_norm": 48.47683334350586,
      "learning_rate": 0.0001896266666666667,
      "loss": -116.8025,
      "step": 3890
    },
    {
      "epoch": 0.312,
      "grad_norm": 59.449317932128906,
      "learning_rate": 0.0001896,
      "loss": -117.2565,
      "step": 3900
    },
    {
      "epoch": 0.3128,
      "grad_norm": 57.14265060424805,
      "learning_rate": 0.00018957333333333333,
      "loss": -117.9141,
      "step": 3910
    },
    {
      "epoch": 0.3136,
      "grad_norm": 54.19742965698242,
      "learning_rate": 0.00018954666666666666,
      "loss": -115.9009,
      "step": 3920
    },
    {
      "epoch": 0.3144,
      "grad_norm": 101.91368103027344,
      "learning_rate": 0.00018952000000000002,
      "loss": -117.9766,
      "step": 3930
    },
    {
      "epoch": 0.3152,
      "grad_norm": 87.10079956054688,
      "learning_rate": 0.00018949333333333334,
      "loss": -118.4094,
      "step": 3940
    },
    {
      "epoch": 0.316,
      "grad_norm": 69.60458374023438,
      "learning_rate": 0.00018946666666666667,
      "loss": -116.8932,
      "step": 3950
    },
    {
      "epoch": 0.3168,
      "grad_norm": 77.1846923828125,
      "learning_rate": 0.00018944000000000003,
      "loss": -116.9866,
      "step": 3960
    },
    {
      "epoch": 0.3176,
      "grad_norm": 55.113372802734375,
      "learning_rate": 0.00018941333333333333,
      "loss": -117.105,
      "step": 3970
    },
    {
      "epoch": 0.3184,
      "grad_norm": 75.24275207519531,
      "learning_rate": 0.00018938666666666666,
      "loss": -117.9927,
      "step": 3980
    },
    {
      "epoch": 0.3192,
      "grad_norm": 79.59107208251953,
      "learning_rate": 0.00018936000000000002,
      "loss": -116.6788,
      "step": 3990
    },
    {
      "epoch": 0.32,
      "grad_norm": 77.52447509765625,
      "learning_rate": 0.00018933333333333335,
      "loss": -116.7331,
      "step": 4000
    },
    {
      "epoch": 0.3208,
      "grad_norm": 66.54534912109375,
      "learning_rate": 0.00018930666666666667,
      "loss": -117.8879,
      "step": 4010
    },
    {
      "epoch": 0.3216,
      "grad_norm": 61.32707214355469,
      "learning_rate": 0.00018928,
      "loss": -117.8043,
      "step": 4020
    },
    {
      "epoch": 0.3224,
      "grad_norm": 58.39420700073242,
      "learning_rate": 0.00018925333333333336,
      "loss": -117.9269,
      "step": 4030
    },
    {
      "epoch": 0.3232,
      "grad_norm": 72.73858642578125,
      "learning_rate": 0.0001892266666666667,
      "loss": -117.2198,
      "step": 4040
    },
    {
      "epoch": 0.324,
      "grad_norm": 64.34188079833984,
      "learning_rate": 0.0001892,
      "loss": -118.7229,
      "step": 4050
    },
    {
      "epoch": 0.3248,
      "grad_norm": 73.39701843261719,
      "learning_rate": 0.00018917333333333335,
      "loss": -117.2468,
      "step": 4060
    },
    {
      "epoch": 0.3256,
      "grad_norm": 58.50059509277344,
      "learning_rate": 0.00018914666666666668,
      "loss": -118.2619,
      "step": 4070
    },
    {
      "epoch": 0.3264,
      "grad_norm": 68.39834594726562,
      "learning_rate": 0.00018912,
      "loss": -116.2637,
      "step": 4080
    },
    {
      "epoch": 0.3272,
      "grad_norm": 67.35710144042969,
      "learning_rate": 0.00018909333333333333,
      "loss": -118.8408,
      "step": 4090
    },
    {
      "epoch": 0.328,
      "grad_norm": 50.09440231323242,
      "learning_rate": 0.0001890666666666667,
      "loss": -117.9853,
      "step": 4100
    },
    {
      "epoch": 0.3288,
      "grad_norm": 91.46844482421875,
      "learning_rate": 0.00018904000000000002,
      "loss": -117.205,
      "step": 4110
    },
    {
      "epoch": 0.3296,
      "grad_norm": 60.12696838378906,
      "learning_rate": 0.00018901333333333335,
      "loss": -119.073,
      "step": 4120
    },
    {
      "epoch": 0.3304,
      "grad_norm": 74.77214813232422,
      "learning_rate": 0.00018898666666666668,
      "loss": -116.5203,
      "step": 4130
    },
    {
      "epoch": 0.3312,
      "grad_norm": 114.73848724365234,
      "learning_rate": 0.00018896,
      "loss": -116.171,
      "step": 4140
    },
    {
      "epoch": 0.332,
      "grad_norm": 55.35664749145508,
      "learning_rate": 0.00018893333333333334,
      "loss": -116.291,
      "step": 4150
    },
    {
      "epoch": 0.3328,
      "grad_norm": 55.78053665161133,
      "learning_rate": 0.00018890666666666667,
      "loss": -117.0785,
      "step": 4160
    },
    {
      "epoch": 0.3336,
      "grad_norm": 53.554447174072266,
      "learning_rate": 0.00018888000000000002,
      "loss": -117.9424,
      "step": 4170
    },
    {
      "epoch": 0.3344,
      "grad_norm": 66.32056427001953,
      "learning_rate": 0.00018885333333333335,
      "loss": -116.3762,
      "step": 4180
    },
    {
      "epoch": 0.3352,
      "grad_norm": 69.76102447509766,
      "learning_rate": 0.00018882666666666668,
      "loss": -116.6085,
      "step": 4190
    },
    {
      "epoch": 0.336,
      "grad_norm": 75.03665161132812,
      "learning_rate": 0.0001888,
      "loss": -117.708,
      "step": 4200
    },
    {
      "epoch": 0.3368,
      "grad_norm": 79.42057800292969,
      "learning_rate": 0.00018877333333333334,
      "loss": -117.3296,
      "step": 4210
    },
    {
      "epoch": 0.3376,
      "grad_norm": 76.7390365600586,
      "learning_rate": 0.00018874666666666667,
      "loss": -116.4856,
      "step": 4220
    },
    {
      "epoch": 0.3384,
      "grad_norm": 66.21896362304688,
      "learning_rate": 0.00018872,
      "loss": -118.2399,
      "step": 4230
    },
    {
      "epoch": 0.3392,
      "grad_norm": 93.60064697265625,
      "learning_rate": 0.00018869333333333335,
      "loss": -116.7912,
      "step": 4240
    },
    {
      "epoch": 0.34,
      "grad_norm": 95.3961181640625,
      "learning_rate": 0.00018866666666666668,
      "loss": -117.4466,
      "step": 4250
    },
    {
      "epoch": 0.3408,
      "grad_norm": 61.86552810668945,
      "learning_rate": 0.00018864,
      "loss": -116.6594,
      "step": 4260
    },
    {
      "epoch": 0.3416,
      "grad_norm": 61.491058349609375,
      "learning_rate": 0.00018861333333333337,
      "loss": -116.4522,
      "step": 4270
    },
    {
      "epoch": 0.3424,
      "grad_norm": 66.52474975585938,
      "learning_rate": 0.00018858666666666667,
      "loss": -117.536,
      "step": 4280
    },
    {
      "epoch": 0.3432,
      "grad_norm": 82.46476745605469,
      "learning_rate": 0.00018856,
      "loss": -117.8784,
      "step": 4290
    },
    {
      "epoch": 0.344,
      "grad_norm": 91.26985931396484,
      "learning_rate": 0.00018853333333333333,
      "loss": -117.48,
      "step": 4300
    },
    {
      "epoch": 0.3448,
      "grad_norm": 73.15975952148438,
      "learning_rate": 0.00018850666666666668,
      "loss": -117.187,
      "step": 4310
    },
    {
      "epoch": 0.3456,
      "grad_norm": 58.43321990966797,
      "learning_rate": 0.00018848,
      "loss": -117.3293,
      "step": 4320
    },
    {
      "epoch": 0.3464,
      "grad_norm": 73.56160736083984,
      "learning_rate": 0.00018845333333333334,
      "loss": -116.6048,
      "step": 4330
    },
    {
      "epoch": 0.3472,
      "grad_norm": 51.17985916137695,
      "learning_rate": 0.0001884266666666667,
      "loss": -118.1112,
      "step": 4340
    },
    {
      "epoch": 0.348,
      "grad_norm": 70.79485321044922,
      "learning_rate": 0.0001884,
      "loss": -117.7149,
      "step": 4350
    },
    {
      "epoch": 0.3488,
      "grad_norm": 70.31846618652344,
      "learning_rate": 0.00018837333333333333,
      "loss": -117.7418,
      "step": 4360
    },
    {
      "epoch": 0.3496,
      "grad_norm": 61.23720932006836,
      "learning_rate": 0.00018834666666666668,
      "loss": -117.6812,
      "step": 4370
    },
    {
      "epoch": 0.3504,
      "grad_norm": 64.12623596191406,
      "learning_rate": 0.00018832,
      "loss": -118.773,
      "step": 4380
    },
    {
      "epoch": 0.3512,
      "grad_norm": 73.33262634277344,
      "learning_rate": 0.00018829333333333334,
      "loss": -118.2909,
      "step": 4390
    },
    {
      "epoch": 0.352,
      "grad_norm": 62.65521240234375,
      "learning_rate": 0.00018826666666666667,
      "loss": -119.0553,
      "step": 4400
    },
    {
      "epoch": 0.3528,
      "grad_norm": 87.64215087890625,
      "learning_rate": 0.00018824000000000003,
      "loss": -118.2292,
      "step": 4410
    },
    {
      "epoch": 0.3536,
      "grad_norm": 82.80455017089844,
      "learning_rate": 0.00018821333333333336,
      "loss": -116.9786,
      "step": 4420
    },
    {
      "epoch": 0.3544,
      "grad_norm": 57.36896896362305,
      "learning_rate": 0.00018818666666666666,
      "loss": -117.8281,
      "step": 4430
    },
    {
      "epoch": 0.3552,
      "grad_norm": 62.05329895019531,
      "learning_rate": 0.00018816000000000001,
      "loss": -116.1513,
      "step": 4440
    },
    {
      "epoch": 0.356,
      "grad_norm": 88.54759979248047,
      "learning_rate": 0.00018813333333333334,
      "loss": -118.5358,
      "step": 4450
    },
    {
      "epoch": 0.3568,
      "grad_norm": 63.52227020263672,
      "learning_rate": 0.00018810666666666667,
      "loss": -118.5384,
      "step": 4460
    },
    {
      "epoch": 0.3576,
      "grad_norm": 52.25962829589844,
      "learning_rate": 0.00018808,
      "loss": -117.7742,
      "step": 4470
    },
    {
      "epoch": 0.3584,
      "grad_norm": 96.7466049194336,
      "learning_rate": 0.00018805333333333336,
      "loss": -118.7534,
      "step": 4480
    },
    {
      "epoch": 0.3592,
      "grad_norm": 79.41165924072266,
      "learning_rate": 0.0001880266666666667,
      "loss": -116.3055,
      "step": 4490
    },
    {
      "epoch": 0.36,
      "grad_norm": 102.44923400878906,
      "learning_rate": 0.000188,
      "loss": -117.312,
      "step": 4500
    },
    {
      "epoch": 0.3608,
      "grad_norm": 68.21475982666016,
      "learning_rate": 0.00018797333333333335,
      "loss": -116.1211,
      "step": 4510
    },
    {
      "epoch": 0.3616,
      "grad_norm": 53.773834228515625,
      "learning_rate": 0.00018794666666666667,
      "loss": -117.0457,
      "step": 4520
    },
    {
      "epoch": 0.3624,
      "grad_norm": 69.13665771484375,
      "learning_rate": 0.00018792,
      "loss": -117.9965,
      "step": 4530
    },
    {
      "epoch": 0.3632,
      "grad_norm": 60.934356689453125,
      "learning_rate": 0.00018789333333333333,
      "loss": -118.7036,
      "step": 4540
    },
    {
      "epoch": 0.364,
      "grad_norm": 59.8915901184082,
      "learning_rate": 0.0001878666666666667,
      "loss": -117.4698,
      "step": 4550
    },
    {
      "epoch": 0.3648,
      "grad_norm": 65.64076232910156,
      "learning_rate": 0.00018784000000000002,
      "loss": -118.0582,
      "step": 4560
    },
    {
      "epoch": 0.3656,
      "grad_norm": 101.34564971923828,
      "learning_rate": 0.00018781333333333335,
      "loss": -117.8591,
      "step": 4570
    },
    {
      "epoch": 0.3664,
      "grad_norm": 87.6083984375,
      "learning_rate": 0.00018778666666666668,
      "loss": -118.6754,
      "step": 4580
    },
    {
      "epoch": 0.3672,
      "grad_norm": 56.80071258544922,
      "learning_rate": 0.00018776,
      "loss": -116.879,
      "step": 4590
    },
    {
      "epoch": 0.368,
      "grad_norm": 63.74310302734375,
      "learning_rate": 0.00018773333333333333,
      "loss": -116.4777,
      "step": 4600
    },
    {
      "epoch": 0.3688,
      "grad_norm": 63.60380172729492,
      "learning_rate": 0.00018770666666666666,
      "loss": -117.5199,
      "step": 4610
    },
    {
      "epoch": 0.3696,
      "grad_norm": 59.35285949707031,
      "learning_rate": 0.00018768000000000002,
      "loss": -118.585,
      "step": 4620
    },
    {
      "epoch": 0.3704,
      "grad_norm": 75.16741943359375,
      "learning_rate": 0.00018765333333333335,
      "loss": -117.9438,
      "step": 4630
    },
    {
      "epoch": 0.3712,
      "grad_norm": 63.400657653808594,
      "learning_rate": 0.00018762666666666668,
      "loss": -117.0587,
      "step": 4640
    },
    {
      "epoch": 0.372,
      "grad_norm": 51.25811767578125,
      "learning_rate": 0.0001876,
      "loss": -117.5519,
      "step": 4650
    },
    {
      "epoch": 0.3728,
      "grad_norm": 94.19110107421875,
      "learning_rate": 0.00018757333333333334,
      "loss": -117.2351,
      "step": 4660
    },
    {
      "epoch": 0.3736,
      "grad_norm": 75.27001953125,
      "learning_rate": 0.00018754666666666666,
      "loss": -117.7651,
      "step": 4670
    },
    {
      "epoch": 0.3744,
      "grad_norm": 67.87802124023438,
      "learning_rate": 0.00018752,
      "loss": -118.1247,
      "step": 4680
    },
    {
      "epoch": 0.3752,
      "grad_norm": 60.08438491821289,
      "learning_rate": 0.00018749333333333335,
      "loss": -118.38,
      "step": 4690
    },
    {
      "epoch": 0.376,
      "grad_norm": 66.25332641601562,
      "learning_rate": 0.00018746666666666668,
      "loss": -117.9075,
      "step": 4700
    },
    {
      "epoch": 0.3768,
      "grad_norm": 67.30878448486328,
      "learning_rate": 0.00018744,
      "loss": -118.4781,
      "step": 4710
    },
    {
      "epoch": 0.3776,
      "grad_norm": 65.8761215209961,
      "learning_rate": 0.00018741333333333336,
      "loss": -117.2142,
      "step": 4720
    },
    {
      "epoch": 0.3784,
      "grad_norm": 58.27385711669922,
      "learning_rate": 0.00018738666666666667,
      "loss": -118.2746,
      "step": 4730
    },
    {
      "epoch": 0.3792,
      "grad_norm": 53.382606506347656,
      "learning_rate": 0.00018736,
      "loss": -117.6923,
      "step": 4740
    },
    {
      "epoch": 0.38,
      "grad_norm": 60.920108795166016,
      "learning_rate": 0.00018733333333333335,
      "loss": -117.7123,
      "step": 4750
    },
    {
      "epoch": 0.3808,
      "grad_norm": 65.83172607421875,
      "learning_rate": 0.00018730666666666668,
      "loss": -118.494,
      "step": 4760
    },
    {
      "epoch": 0.3816,
      "grad_norm": 70.22647094726562,
      "learning_rate": 0.00018728,
      "loss": -117.3532,
      "step": 4770
    },
    {
      "epoch": 0.3824,
      "grad_norm": 58.18931579589844,
      "learning_rate": 0.00018725333333333334,
      "loss": -117.0561,
      "step": 4780
    },
    {
      "epoch": 0.3832,
      "grad_norm": 60.22052001953125,
      "learning_rate": 0.0001872266666666667,
      "loss": -117.3075,
      "step": 4790
    },
    {
      "epoch": 0.384,
      "grad_norm": 72.55534362792969,
      "learning_rate": 0.00018720000000000002,
      "loss": -118.9561,
      "step": 4800
    },
    {
      "epoch": 0.3848,
      "grad_norm": 71.63127136230469,
      "learning_rate": 0.00018717333333333333,
      "loss": -118.3494,
      "step": 4810
    },
    {
      "epoch": 0.3856,
      "grad_norm": 71.6113052368164,
      "learning_rate": 0.00018714666666666668,
      "loss": -118.5831,
      "step": 4820
    },
    {
      "epoch": 0.3864,
      "grad_norm": 71.90507507324219,
      "learning_rate": 0.00018712,
      "loss": -117.8411,
      "step": 4830
    },
    {
      "epoch": 0.3872,
      "grad_norm": 73.09596252441406,
      "learning_rate": 0.00018709333333333334,
      "loss": -118.1457,
      "step": 4840
    },
    {
      "epoch": 0.388,
      "grad_norm": 58.62297058105469,
      "learning_rate": 0.00018706666666666667,
      "loss": -117.177,
      "step": 4850
    },
    {
      "epoch": 0.3888,
      "grad_norm": 59.84435272216797,
      "learning_rate": 0.00018704000000000003,
      "loss": -117.9997,
      "step": 4860
    },
    {
      "epoch": 0.3896,
      "grad_norm": 51.79825973510742,
      "learning_rate": 0.00018701333333333335,
      "loss": -118.0983,
      "step": 4870
    },
    {
      "epoch": 0.3904,
      "grad_norm": 68.60951232910156,
      "learning_rate": 0.00018698666666666666,
      "loss": -117.2107,
      "step": 4880
    },
    {
      "epoch": 0.3912,
      "grad_norm": 58.92770767211914,
      "learning_rate": 0.00018696,
      "loss": -117.2655,
      "step": 4890
    },
    {
      "epoch": 0.392,
      "grad_norm": 69.91917419433594,
      "learning_rate": 0.00018693333333333334,
      "loss": -117.1192,
      "step": 4900
    },
    {
      "epoch": 0.3928,
      "grad_norm": 85.4964828491211,
      "learning_rate": 0.00018690666666666667,
      "loss": -118.1314,
      "step": 4910
    },
    {
      "epoch": 0.3936,
      "grad_norm": 60.05213165283203,
      "learning_rate": 0.00018688,
      "loss": -117.9704,
      "step": 4920
    },
    {
      "epoch": 0.3944,
      "grad_norm": 71.92659759521484,
      "learning_rate": 0.00018685333333333336,
      "loss": -118.7735,
      "step": 4930
    },
    {
      "epoch": 0.3952,
      "grad_norm": 66.6703109741211,
      "learning_rate": 0.00018682666666666668,
      "loss": -118.9531,
      "step": 4940
    },
    {
      "epoch": 0.396,
      "grad_norm": 85.85305786132812,
      "learning_rate": 0.00018680000000000001,
      "loss": -119.1412,
      "step": 4950
    },
    {
      "epoch": 0.3968,
      "grad_norm": 76.76334381103516,
      "learning_rate": 0.00018677333333333334,
      "loss": -117.604,
      "step": 4960
    },
    {
      "epoch": 0.3976,
      "grad_norm": 73.1579818725586,
      "learning_rate": 0.00018674666666666667,
      "loss": -118.1296,
      "step": 4970
    },
    {
      "epoch": 0.3984,
      "grad_norm": 56.71463394165039,
      "learning_rate": 0.00018672,
      "loss": -117.4059,
      "step": 4980
    },
    {
      "epoch": 0.3992,
      "grad_norm": 72.11493682861328,
      "learning_rate": 0.00018669333333333333,
      "loss": -118.1254,
      "step": 4990
    },
    {
      "epoch": 0.4,
      "grad_norm": 59.09806823730469,
      "learning_rate": 0.0001866666666666667,
      "loss": -117.6472,
      "step": 5000
    },
    {
      "epoch": 0.4008,
      "grad_norm": 81.44220733642578,
      "learning_rate": 0.00018664000000000002,
      "loss": -118.5603,
      "step": 5010
    },
    {
      "epoch": 0.4016,
      "grad_norm": 50.14027404785156,
      "learning_rate": 0.00018661333333333334,
      "loss": -117.9923,
      "step": 5020
    },
    {
      "epoch": 0.4024,
      "grad_norm": 52.13078689575195,
      "learning_rate": 0.00018658666666666667,
      "loss": -117.026,
      "step": 5030
    },
    {
      "epoch": 0.4032,
      "grad_norm": 86.62969207763672,
      "learning_rate": 0.00018656,
      "loss": -118.3571,
      "step": 5040
    },
    {
      "epoch": 0.404,
      "grad_norm": 54.009525299072266,
      "learning_rate": 0.00018653333333333333,
      "loss": -118.1159,
      "step": 5050
    },
    {
      "epoch": 0.4048,
      "grad_norm": 51.501773834228516,
      "learning_rate": 0.00018650666666666666,
      "loss": -118.2261,
      "step": 5060
    },
    {
      "epoch": 0.4056,
      "grad_norm": 78.20857238769531,
      "learning_rate": 0.00018648000000000002,
      "loss": -117.3095,
      "step": 5070
    },
    {
      "epoch": 0.4064,
      "grad_norm": 70.96256256103516,
      "learning_rate": 0.00018645333333333335,
      "loss": -117.8302,
      "step": 5080
    },
    {
      "epoch": 0.4072,
      "grad_norm": 81.35531616210938,
      "learning_rate": 0.00018642666666666668,
      "loss": -117.8703,
      "step": 5090
    },
    {
      "epoch": 0.408,
      "grad_norm": 59.57603454589844,
      "learning_rate": 0.00018640000000000003,
      "loss": -118.7605,
      "step": 5100
    },
    {
      "epoch": 0.4088,
      "grad_norm": 52.101078033447266,
      "learning_rate": 0.00018637333333333333,
      "loss": -118.234,
      "step": 5110
    },
    {
      "epoch": 0.4096,
      "grad_norm": 79.62602996826172,
      "learning_rate": 0.00018634666666666666,
      "loss": -118.0338,
      "step": 5120
    },
    {
      "epoch": 0.4104,
      "grad_norm": 57.989437103271484,
      "learning_rate": 0.00018632000000000002,
      "loss": -118.0222,
      "step": 5130
    },
    {
      "epoch": 0.4112,
      "grad_norm": 97.660888671875,
      "learning_rate": 0.00018629333333333335,
      "loss": -118.4978,
      "step": 5140
    },
    {
      "epoch": 0.412,
      "grad_norm": 64.05035400390625,
      "learning_rate": 0.00018626666666666668,
      "loss": -118.6407,
      "step": 5150
    },
    {
      "epoch": 0.4128,
      "grad_norm": 67.84925079345703,
      "learning_rate": 0.00018624,
      "loss": -117.9959,
      "step": 5160
    },
    {
      "epoch": 0.4136,
      "grad_norm": 61.76091003417969,
      "learning_rate": 0.00018621333333333336,
      "loss": -117.6654,
      "step": 5170
    },
    {
      "epoch": 0.4144,
      "grad_norm": 71.74740600585938,
      "learning_rate": 0.00018618666666666666,
      "loss": -118.2965,
      "step": 5180
    },
    {
      "epoch": 0.4152,
      "grad_norm": 58.21009063720703,
      "learning_rate": 0.00018616,
      "loss": -117.7312,
      "step": 5190
    },
    {
      "epoch": 0.416,
      "grad_norm": 53.4370231628418,
      "learning_rate": 0.00018613333333333335,
      "loss": -119.2453,
      "step": 5200
    },
    {
      "epoch": 0.4168,
      "grad_norm": 56.698829650878906,
      "learning_rate": 0.00018610666666666668,
      "loss": -116.519,
      "step": 5210
    },
    {
      "epoch": 0.4176,
      "grad_norm": 55.16939163208008,
      "learning_rate": 0.00018608,
      "loss": -118.0519,
      "step": 5220
    },
    {
      "epoch": 0.4184,
      "grad_norm": 55.251041412353516,
      "learning_rate": 0.00018605333333333334,
      "loss": -117.6699,
      "step": 5230
    },
    {
      "epoch": 0.4192,
      "grad_norm": 57.12929153442383,
      "learning_rate": 0.0001860266666666667,
      "loss": -117.4114,
      "step": 5240
    },
    {
      "epoch": 0.42,
      "grad_norm": 61.62242889404297,
      "learning_rate": 0.00018600000000000002,
      "loss": -117.9552,
      "step": 5250
    },
    {
      "epoch": 0.4208,
      "grad_norm": 68.20054626464844,
      "learning_rate": 0.00018597333333333332,
      "loss": -116.5247,
      "step": 5260
    },
    {
      "epoch": 0.4216,
      "grad_norm": 61.62418746948242,
      "learning_rate": 0.00018594666666666668,
      "loss": -117.8296,
      "step": 5270
    },
    {
      "epoch": 0.4224,
      "grad_norm": 94.54126739501953,
      "learning_rate": 0.00018592,
      "loss": -117.4134,
      "step": 5280
    },
    {
      "epoch": 0.4232,
      "grad_norm": 50.19167709350586,
      "learning_rate": 0.00018589333333333334,
      "loss": -117.1484,
      "step": 5290
    },
    {
      "epoch": 0.424,
      "grad_norm": 51.50139236450195,
      "learning_rate": 0.00018586666666666667,
      "loss": -117.636,
      "step": 5300
    },
    {
      "epoch": 0.4248,
      "grad_norm": 48.563392639160156,
      "learning_rate": 0.00018584000000000002,
      "loss": -117.1007,
      "step": 5310
    },
    {
      "epoch": 0.4256,
      "grad_norm": 61.08380126953125,
      "learning_rate": 0.00018581333333333335,
      "loss": -118.976,
      "step": 5320
    },
    {
      "epoch": 0.4264,
      "grad_norm": 73.74376678466797,
      "learning_rate": 0.00018578666666666668,
      "loss": -117.5593,
      "step": 5330
    },
    {
      "epoch": 0.4272,
      "grad_norm": 51.00743865966797,
      "learning_rate": 0.00018576,
      "loss": -117.8915,
      "step": 5340
    },
    {
      "epoch": 0.428,
      "grad_norm": 70.27986145019531,
      "learning_rate": 0.00018573333333333334,
      "loss": -118.3456,
      "step": 5350
    },
    {
      "epoch": 0.4288,
      "grad_norm": 78.17018127441406,
      "learning_rate": 0.00018570666666666667,
      "loss": -117.5726,
      "step": 5360
    },
    {
      "epoch": 0.4296,
      "grad_norm": 81.59937286376953,
      "learning_rate": 0.00018568,
      "loss": -116.6868,
      "step": 5370
    },
    {
      "epoch": 0.4304,
      "grad_norm": 68.61656951904297,
      "learning_rate": 0.00018565333333333335,
      "loss": -118.418,
      "step": 5380
    },
    {
      "epoch": 0.4312,
      "grad_norm": 63.83388900756836,
      "learning_rate": 0.00018562666666666668,
      "loss": -117.9953,
      "step": 5390
    },
    {
      "epoch": 0.432,
      "grad_norm": 70.85557556152344,
      "learning_rate": 0.0001856,
      "loss": -118.0138,
      "step": 5400
    },
    {
      "epoch": 0.4328,
      "grad_norm": 45.842628479003906,
      "learning_rate": 0.00018557333333333334,
      "loss": -117.6881,
      "step": 5410
    },
    {
      "epoch": 0.4336,
      "grad_norm": 47.01387405395508,
      "learning_rate": 0.00018554666666666667,
      "loss": -118.2196,
      "step": 5420
    },
    {
      "epoch": 0.4344,
      "grad_norm": 66.80032348632812,
      "learning_rate": 0.00018552,
      "loss": -116.383,
      "step": 5430
    },
    {
      "epoch": 0.4352,
      "grad_norm": 67.99108123779297,
      "learning_rate": 0.00018549333333333333,
      "loss": -117.7609,
      "step": 5440
    },
    {
      "epoch": 0.436,
      "grad_norm": 89.03282928466797,
      "learning_rate": 0.00018546666666666668,
      "loss": -117.2178,
      "step": 5450
    },
    {
      "epoch": 0.4368,
      "grad_norm": 56.32305908203125,
      "learning_rate": 0.00018544,
      "loss": -117.6444,
      "step": 5460
    },
    {
      "epoch": 0.4376,
      "grad_norm": 58.26311111450195,
      "learning_rate": 0.00018541333333333334,
      "loss": -118.5915,
      "step": 5470
    },
    {
      "epoch": 0.4384,
      "grad_norm": 70.79578399658203,
      "learning_rate": 0.0001853866666666667,
      "loss": -117.2974,
      "step": 5480
    },
    {
      "epoch": 0.4392,
      "grad_norm": 73.35456848144531,
      "learning_rate": 0.00018536,
      "loss": -117.9935,
      "step": 5490
    },
    {
      "epoch": 0.44,
      "grad_norm": 64.85086059570312,
      "learning_rate": 0.00018533333333333333,
      "loss": -118.0302,
      "step": 5500
    },
    {
      "epoch": 0.4408,
      "grad_norm": 45.53139877319336,
      "learning_rate": 0.00018530666666666669,
      "loss": -118.3312,
      "step": 5510
    },
    {
      "epoch": 0.4416,
      "grad_norm": 68.62301635742188,
      "learning_rate": 0.00018528000000000001,
      "loss": -117.7433,
      "step": 5520
    },
    {
      "epoch": 0.4424,
      "grad_norm": 68.18976593017578,
      "learning_rate": 0.00018525333333333334,
      "loss": -118.7045,
      "step": 5530
    },
    {
      "epoch": 0.4432,
      "grad_norm": 64.1775894165039,
      "learning_rate": 0.00018522666666666667,
      "loss": -116.8935,
      "step": 5540
    },
    {
      "epoch": 0.444,
      "grad_norm": 66.92405700683594,
      "learning_rate": 0.00018520000000000003,
      "loss": -116.6579,
      "step": 5550
    },
    {
      "epoch": 0.4448,
      "grad_norm": 62.90961837768555,
      "learning_rate": 0.00018517333333333333,
      "loss": -117.2308,
      "step": 5560
    },
    {
      "epoch": 0.4456,
      "grad_norm": 209.1767578125,
      "learning_rate": 0.00018514666666666666,
      "loss": -117.6919,
      "step": 5570
    },
    {
      "epoch": 0.4464,
      "grad_norm": 503.7726135253906,
      "learning_rate": 0.00018512000000000002,
      "loss": -117.0176,
      "step": 5580
    },
    {
      "epoch": 0.4472,
      "grad_norm": 58.65449905395508,
      "learning_rate": 0.00018509333333333335,
      "loss": -115.6009,
      "step": 5590
    },
    {
      "epoch": 0.448,
      "grad_norm": 289.18756103515625,
      "learning_rate": 0.00018506666666666667,
      "loss": -114.2238,
      "step": 5600
    },
    {
      "epoch": 0.4488,
      "grad_norm": 72.02120971679688,
      "learning_rate": 0.00018504,
      "loss": -116.4714,
      "step": 5610
    },
    {
      "epoch": 0.4496,
      "grad_norm": 331.1381530761719,
      "learning_rate": 0.00018501333333333336,
      "loss": -115.0831,
      "step": 5620
    },
    {
      "epoch": 0.4504,
      "grad_norm": 304.258056640625,
      "learning_rate": 0.0001849866666666667,
      "loss": -109.1779,
      "step": 5630
    },
    {
      "epoch": 0.4512,
      "grad_norm": 144.8413543701172,
      "learning_rate": 0.00018496,
      "loss": -109.8873,
      "step": 5640
    },
    {
      "epoch": 0.452,
      "grad_norm": 210.05958557128906,
      "learning_rate": 0.00018493333333333335,
      "loss": -113.3766,
      "step": 5650
    },
    {
      "epoch": 0.4528,
      "grad_norm": 76.48722076416016,
      "learning_rate": 0.00018490666666666668,
      "loss": -113.4679,
      "step": 5660
    },
    {
      "epoch": 0.4536,
      "grad_norm": 100.2171630859375,
      "learning_rate": 0.00018488,
      "loss": -115.0325,
      "step": 5670
    },
    {
      "epoch": 0.4544,
      "grad_norm": 88.81282806396484,
      "learning_rate": 0.00018485333333333333,
      "loss": -115.3388,
      "step": 5680
    },
    {
      "epoch": 0.4552,
      "grad_norm": 61.74827194213867,
      "learning_rate": 0.0001848266666666667,
      "loss": -114.2699,
      "step": 5690
    },
    {
      "epoch": 0.456,
      "grad_norm": 68.52152252197266,
      "learning_rate": 0.00018480000000000002,
      "loss": -115.6651,
      "step": 5700
    },
    {
      "epoch": 0.4568,
      "grad_norm": 70.29524230957031,
      "learning_rate": 0.00018477333333333332,
      "loss": -114.7497,
      "step": 5710
    },
    {
      "epoch": 0.4576,
      "grad_norm": 140.2889862060547,
      "learning_rate": 0.00018474666666666668,
      "loss": -116.7624,
      "step": 5720
    },
    {
      "epoch": 0.4584,
      "grad_norm": 58.30044937133789,
      "learning_rate": 0.00018472,
      "loss": -117.1569,
      "step": 5730
    },
    {
      "epoch": 0.4592,
      "grad_norm": 64.7415771484375,
      "learning_rate": 0.00018469333333333334,
      "loss": -114.1699,
      "step": 5740
    },
    {
      "epoch": 0.46,
      "grad_norm": 65.69821166992188,
      "learning_rate": 0.00018466666666666666,
      "loss": -116.4005,
      "step": 5750
    },
    {
      "epoch": 0.4608,
      "grad_norm": 68.19248962402344,
      "learning_rate": 0.00018464000000000002,
      "loss": -115.2504,
      "step": 5760
    },
    {
      "epoch": 0.4616,
      "grad_norm": 44.06074905395508,
      "learning_rate": 0.00018461333333333335,
      "loss": -115.5051,
      "step": 5770
    },
    {
      "epoch": 0.4624,
      "grad_norm": 52.08860397338867,
      "learning_rate": 0.00018458666666666668,
      "loss": -115.4891,
      "step": 5780
    },
    {
      "epoch": 0.4632,
      "grad_norm": 39.836063385009766,
      "learning_rate": 0.00018456,
      "loss": -116.4783,
      "step": 5790
    },
    {
      "epoch": 0.464,
      "grad_norm": 44.27580642700195,
      "learning_rate": 0.00018453333333333334,
      "loss": -116.067,
      "step": 5800
    },
    {
      "epoch": 0.4648,
      "grad_norm": 55.441402435302734,
      "learning_rate": 0.00018450666666666667,
      "loss": -116.5588,
      "step": 5810
    },
    {
      "epoch": 0.4656,
      "grad_norm": 62.58677291870117,
      "learning_rate": 0.00018448,
      "loss": -116.268,
      "step": 5820
    },
    {
      "epoch": 0.4664,
      "grad_norm": 63.867332458496094,
      "learning_rate": 0.00018445333333333335,
      "loss": -116.8775,
      "step": 5830
    },
    {
      "epoch": 0.4672,
      "grad_norm": 58.55681228637695,
      "learning_rate": 0.00018442666666666668,
      "loss": -117.2855,
      "step": 5840
    },
    {
      "epoch": 0.468,
      "grad_norm": 42.382904052734375,
      "learning_rate": 0.0001844,
      "loss": -115.5458,
      "step": 5850
    },
    {
      "epoch": 0.4688,
      "grad_norm": 68.94525909423828,
      "learning_rate": 0.00018437333333333334,
      "loss": -115.7372,
      "step": 5860
    },
    {
      "epoch": 0.4696,
      "grad_norm": 56.43513107299805,
      "learning_rate": 0.00018434666666666667,
      "loss": -114.3526,
      "step": 5870
    },
    {
      "epoch": 0.4704,
      "grad_norm": 49.93012619018555,
      "learning_rate": 0.00018432,
      "loss": -115.876,
      "step": 5880
    },
    {
      "epoch": 0.4712,
      "grad_norm": 38.751556396484375,
      "learning_rate": 0.00018429333333333335,
      "loss": -116.497,
      "step": 5890
    },
    {
      "epoch": 0.472,
      "grad_norm": 45.523067474365234,
      "learning_rate": 0.00018426666666666668,
      "loss": -116.5287,
      "step": 5900
    },
    {
      "epoch": 0.4728,
      "grad_norm": 64.8694076538086,
      "learning_rate": 0.00018424,
      "loss": -115.4963,
      "step": 5910
    },
    {
      "epoch": 0.4736,
      "grad_norm": 42.826419830322266,
      "learning_rate": 0.00018421333333333334,
      "loss": -117.4052,
      "step": 5920
    },
    {
      "epoch": 0.4744,
      "grad_norm": 52.41679000854492,
      "learning_rate": 0.0001841866666666667,
      "loss": -116.3543,
      "step": 5930
    },
    {
      "epoch": 0.4752,
      "grad_norm": 51.236148834228516,
      "learning_rate": 0.00018416,
      "loss": -115.9804,
      "step": 5940
    },
    {
      "epoch": 0.476,
      "grad_norm": 87.63955688476562,
      "learning_rate": 0.00018413333333333333,
      "loss": -116.3934,
      "step": 5950
    },
    {
      "epoch": 0.4768,
      "grad_norm": 79.54676818847656,
      "learning_rate": 0.00018410666666666668,
      "loss": -116.3705,
      "step": 5960
    },
    {
      "epoch": 0.4776,
      "grad_norm": 51.251583099365234,
      "learning_rate": 0.00018408,
      "loss": -116.9209,
      "step": 5970
    },
    {
      "epoch": 0.4784,
      "grad_norm": 50.485572814941406,
      "learning_rate": 0.00018405333333333334,
      "loss": -115.5145,
      "step": 5980
    },
    {
      "epoch": 0.4792,
      "grad_norm": 54.81180191040039,
      "learning_rate": 0.00018402666666666667,
      "loss": -116.4364,
      "step": 5990
    },
    {
      "epoch": 0.48,
      "grad_norm": 51.20444869995117,
      "learning_rate": 0.00018400000000000003,
      "loss": -115.7187,
      "step": 6000
    },
    {
      "epoch": 0.4808,
      "grad_norm": 45.4708251953125,
      "learning_rate": 0.00018397333333333336,
      "loss": -116.3517,
      "step": 6010
    },
    {
      "epoch": 0.4816,
      "grad_norm": 58.68874740600586,
      "learning_rate": 0.00018394666666666666,
      "loss": -116.9612,
      "step": 6020
    },
    {
      "epoch": 0.4824,
      "grad_norm": 50.43313217163086,
      "learning_rate": 0.00018392000000000001,
      "loss": -116.3708,
      "step": 6030
    },
    {
      "epoch": 0.4832,
      "grad_norm": 62.25627136230469,
      "learning_rate": 0.00018389333333333334,
      "loss": -115.5625,
      "step": 6040
    },
    {
      "epoch": 0.484,
      "grad_norm": 34.347625732421875,
      "learning_rate": 0.00018386666666666667,
      "loss": -117.1598,
      "step": 6050
    },
    {
      "epoch": 0.4848,
      "grad_norm": 36.23813247680664,
      "learning_rate": 0.00018384,
      "loss": -117.7682,
      "step": 6060
    },
    {
      "epoch": 0.4856,
      "grad_norm": 45.396541595458984,
      "learning_rate": 0.00018381333333333336,
      "loss": -115.8577,
      "step": 6070
    },
    {
      "epoch": 0.4864,
      "grad_norm": 65.1466064453125,
      "learning_rate": 0.0001837866666666667,
      "loss": -117.1323,
      "step": 6080
    },
    {
      "epoch": 0.4872,
      "grad_norm": 52.43524932861328,
      "learning_rate": 0.00018376,
      "loss": -116.1143,
      "step": 6090
    },
    {
      "epoch": 0.488,
      "grad_norm": 42.59843444824219,
      "learning_rate": 0.00018373333333333335,
      "loss": -116.3382,
      "step": 6100
    },
    {
      "epoch": 0.4888,
      "grad_norm": 46.32158279418945,
      "learning_rate": 0.00018370666666666667,
      "loss": -115.4008,
      "step": 6110
    },
    {
      "epoch": 0.4896,
      "grad_norm": 34.64784240722656,
      "learning_rate": 0.00018368,
      "loss": -117.1124,
      "step": 6120
    },
    {
      "epoch": 0.4904,
      "grad_norm": 44.88443374633789,
      "learning_rate": 0.00018365333333333333,
      "loss": -116.7588,
      "step": 6130
    },
    {
      "epoch": 0.4912,
      "grad_norm": 43.460670471191406,
      "learning_rate": 0.0001836266666666667,
      "loss": -116.1293,
      "step": 6140
    },
    {
      "epoch": 0.492,
      "grad_norm": 47.045555114746094,
      "learning_rate": 0.00018360000000000002,
      "loss": -116.8993,
      "step": 6150
    },
    {
      "epoch": 0.4928,
      "grad_norm": 55.406105041503906,
      "learning_rate": 0.00018357333333333335,
      "loss": -116.5655,
      "step": 6160
    },
    {
      "epoch": 0.4936,
      "grad_norm": 63.63386535644531,
      "learning_rate": 0.00018354666666666668,
      "loss": -115.8754,
      "step": 6170
    },
    {
      "epoch": 0.4944,
      "grad_norm": 43.17477798461914,
      "learning_rate": 0.00018352,
      "loss": -117.8289,
      "step": 6180
    },
    {
      "epoch": 0.4952,
      "grad_norm": 63.49277877807617,
      "learning_rate": 0.00018349333333333333,
      "loss": -117.1849,
      "step": 6190
    },
    {
      "epoch": 0.496,
      "grad_norm": 52.43095779418945,
      "learning_rate": 0.00018346666666666666,
      "loss": -114.8678,
      "step": 6200
    },
    {
      "epoch": 0.4968,
      "grad_norm": 61.856849670410156,
      "learning_rate": 0.00018344000000000002,
      "loss": -116.6945,
      "step": 6210
    },
    {
      "epoch": 0.4976,
      "grad_norm": 48.52446746826172,
      "learning_rate": 0.00018341333333333335,
      "loss": -115.6499,
      "step": 6220
    },
    {
      "epoch": 0.4984,
      "grad_norm": 65.83057403564453,
      "learning_rate": 0.00018338666666666668,
      "loss": -116.5362,
      "step": 6230
    },
    {
      "epoch": 0.4992,
      "grad_norm": 41.54655456542969,
      "learning_rate": 0.00018336,
      "loss": -116.3769,
      "step": 6240
    },
    {
      "epoch": 0.5,
      "grad_norm": 49.825599670410156,
      "learning_rate": 0.00018333333333333334,
      "loss": -117.1861,
      "step": 6250
    },
    {
      "epoch": 0.5008,
      "grad_norm": 47.166263580322266,
      "learning_rate": 0.00018330666666666666,
      "loss": -116.4852,
      "step": 6260
    },
    {
      "epoch": 0.5016,
      "grad_norm": 38.58407211303711,
      "learning_rate": 0.00018328000000000002,
      "loss": -116.2399,
      "step": 6270
    },
    {
      "epoch": 0.5024,
      "grad_norm": 46.6326904296875,
      "learning_rate": 0.00018325333333333335,
      "loss": -116.2185,
      "step": 6280
    },
    {
      "epoch": 0.5032,
      "grad_norm": 48.19611740112305,
      "learning_rate": 0.00018322666666666668,
      "loss": -114.4562,
      "step": 6290
    },
    {
      "epoch": 0.504,
      "grad_norm": 37.34022903442383,
      "learning_rate": 0.0001832,
      "loss": -116.6803,
      "step": 6300
    },
    {
      "epoch": 0.5048,
      "grad_norm": 45.492740631103516,
      "learning_rate": 0.00018317333333333336,
      "loss": -115.7884,
      "step": 6310
    },
    {
      "epoch": 0.5056,
      "grad_norm": 63.233306884765625,
      "learning_rate": 0.00018314666666666667,
      "loss": -114.8778,
      "step": 6320
    },
    {
      "epoch": 0.5064,
      "grad_norm": 69.35123443603516,
      "learning_rate": 0.00018312,
      "loss": -116.1393,
      "step": 6330
    },
    {
      "epoch": 0.5072,
      "grad_norm": 53.96925735473633,
      "learning_rate": 0.00018309333333333335,
      "loss": -116.7452,
      "step": 6340
    },
    {
      "epoch": 0.508,
      "grad_norm": 46.314266204833984,
      "learning_rate": 0.00018306666666666668,
      "loss": -116.6979,
      "step": 6350
    },
    {
      "epoch": 0.5088,
      "grad_norm": 44.10148620605469,
      "learning_rate": 0.00018304,
      "loss": -116.3622,
      "step": 6360
    },
    {
      "epoch": 0.5096,
      "grad_norm": 39.20956039428711,
      "learning_rate": 0.00018301333333333334,
      "loss": -115.929,
      "step": 6370
    },
    {
      "epoch": 0.5104,
      "grad_norm": 37.32683563232422,
      "learning_rate": 0.0001829866666666667,
      "loss": -116.7858,
      "step": 6380
    },
    {
      "epoch": 0.5112,
      "grad_norm": 55.832115173339844,
      "learning_rate": 0.00018296,
      "loss": -115.8496,
      "step": 6390
    },
    {
      "epoch": 0.512,
      "grad_norm": 48.543983459472656,
      "learning_rate": 0.00018293333333333333,
      "loss": -116.7262,
      "step": 6400
    },
    {
      "epoch": 0.5128,
      "grad_norm": 51.20538330078125,
      "learning_rate": 0.00018290666666666668,
      "loss": -117.2329,
      "step": 6410
    },
    {
      "epoch": 0.5136,
      "grad_norm": 42.97933578491211,
      "learning_rate": 0.00018288,
      "loss": -117.1459,
      "step": 6420
    },
    {
      "epoch": 0.5144,
      "grad_norm": 55.37627029418945,
      "learning_rate": 0.00018285333333333334,
      "loss": -117.7163,
      "step": 6430
    },
    {
      "epoch": 0.5152,
      "grad_norm": 64.1421127319336,
      "learning_rate": 0.00018282666666666667,
      "loss": -117.5322,
      "step": 6440
    },
    {
      "epoch": 0.516,
      "grad_norm": 45.550682067871094,
      "learning_rate": 0.00018280000000000003,
      "loss": -117.0367,
      "step": 6450
    },
    {
      "epoch": 0.5168,
      "grad_norm": 47.14752960205078,
      "learning_rate": 0.00018277333333333335,
      "loss": -116.6271,
      "step": 6460
    },
    {
      "epoch": 0.5176,
      "grad_norm": 40.93819808959961,
      "learning_rate": 0.00018274666666666666,
      "loss": -116.5648,
      "step": 6470
    },
    {
      "epoch": 0.5184,
      "grad_norm": 50.87745666503906,
      "learning_rate": 0.00018272,
      "loss": -115.6418,
      "step": 6480
    },
    {
      "epoch": 0.5192,
      "grad_norm": 50.774967193603516,
      "learning_rate": 0.00018269333333333334,
      "loss": -117.5829,
      "step": 6490
    },
    {
      "epoch": 0.52,
      "grad_norm": 45.96113967895508,
      "learning_rate": 0.00018266666666666667,
      "loss": -114.8246,
      "step": 6500
    },
    {
      "epoch": 0.5208,
      "grad_norm": 47.91546630859375,
      "learning_rate": 0.00018264,
      "loss": -117.4463,
      "step": 6510
    },
    {
      "epoch": 0.5216,
      "grad_norm": 49.19036102294922,
      "learning_rate": 0.00018261333333333336,
      "loss": -116.7511,
      "step": 6520
    },
    {
      "epoch": 0.5224,
      "grad_norm": 46.935420989990234,
      "learning_rate": 0.00018258666666666668,
      "loss": -116.4588,
      "step": 6530
    },
    {
      "epoch": 0.5232,
      "grad_norm": 42.20124053955078,
      "learning_rate": 0.00018256,
      "loss": -116.4041,
      "step": 6540
    },
    {
      "epoch": 0.524,
      "grad_norm": 48.649757385253906,
      "learning_rate": 0.00018253333333333334,
      "loss": -116.9226,
      "step": 6550
    },
    {
      "epoch": 0.5248,
      "grad_norm": 44.606258392333984,
      "learning_rate": 0.00018250666666666667,
      "loss": -116.7273,
      "step": 6560
    },
    {
      "epoch": 0.5256,
      "grad_norm": 46.428550720214844,
      "learning_rate": 0.00018248,
      "loss": -116.6197,
      "step": 6570
    },
    {
      "epoch": 0.5264,
      "grad_norm": 48.56486129760742,
      "learning_rate": 0.00018245333333333333,
      "loss": -117.0523,
      "step": 6580
    },
    {
      "epoch": 0.5272,
      "grad_norm": 46.10417556762695,
      "learning_rate": 0.00018242666666666669,
      "loss": -115.8413,
      "step": 6590
    },
    {
      "epoch": 0.528,
      "grad_norm": 53.22801971435547,
      "learning_rate": 0.00018240000000000002,
      "loss": -116.6773,
      "step": 6600
    },
    {
      "epoch": 0.5288,
      "grad_norm": 54.42750549316406,
      "learning_rate": 0.00018237333333333334,
      "loss": -116.3678,
      "step": 6610
    },
    {
      "epoch": 0.5296,
      "grad_norm": 53.80844497680664,
      "learning_rate": 0.00018234666666666667,
      "loss": -115.8328,
      "step": 6620
    },
    {
      "epoch": 0.5304,
      "grad_norm": 48.40088653564453,
      "learning_rate": 0.00018232,
      "loss": -117.2703,
      "step": 6630
    },
    {
      "epoch": 0.5312,
      "grad_norm": 40.11808776855469,
      "learning_rate": 0.00018229333333333333,
      "loss": -117.6592,
      "step": 6640
    },
    {
      "epoch": 0.532,
      "grad_norm": 61.12046432495117,
      "learning_rate": 0.0001822666666666667,
      "loss": -117.4192,
      "step": 6650
    },
    {
      "epoch": 0.5328,
      "grad_norm": 48.356170654296875,
      "learning_rate": 0.00018224000000000002,
      "loss": -117.2211,
      "step": 6660
    },
    {
      "epoch": 0.5336,
      "grad_norm": 45.098384857177734,
      "learning_rate": 0.00018221333333333335,
      "loss": -116.0394,
      "step": 6670
    },
    {
      "epoch": 0.5344,
      "grad_norm": 48.34357452392578,
      "learning_rate": 0.00018218666666666668,
      "loss": -117.1676,
      "step": 6680
    },
    {
      "epoch": 0.5352,
      "grad_norm": 53.26000213623047,
      "learning_rate": 0.00018216000000000003,
      "loss": -116.0113,
      "step": 6690
    },
    {
      "epoch": 0.536,
      "grad_norm": 55.31258010864258,
      "learning_rate": 0.00018213333333333333,
      "loss": -116.2731,
      "step": 6700
    },
    {
      "epoch": 0.5368,
      "grad_norm": 43.92825698852539,
      "learning_rate": 0.00018210666666666666,
      "loss": -116.4098,
      "step": 6710
    },
    {
      "epoch": 0.5376,
      "grad_norm": 42.69938278198242,
      "learning_rate": 0.00018208000000000002,
      "loss": -116.743,
      "step": 6720
    },
    {
      "epoch": 0.5384,
      "grad_norm": 56.44179916381836,
      "learning_rate": 0.00018205333333333335,
      "loss": -115.9815,
      "step": 6730
    },
    {
      "epoch": 0.5392,
      "grad_norm": 57.73887252807617,
      "learning_rate": 0.00018202666666666668,
      "loss": -116.2379,
      "step": 6740
    },
    {
      "epoch": 0.54,
      "grad_norm": 45.61983108520508,
      "learning_rate": 0.000182,
      "loss": -117.1034,
      "step": 6750
    },
    {
      "epoch": 0.5408,
      "grad_norm": 40.802791595458984,
      "learning_rate": 0.00018197333333333336,
      "loss": -116.9061,
      "step": 6760
    },
    {
      "epoch": 0.5416,
      "grad_norm": 46.17789840698242,
      "learning_rate": 0.00018194666666666666,
      "loss": -116.395,
      "step": 6770
    },
    {
      "epoch": 0.5424,
      "grad_norm": 66.211181640625,
      "learning_rate": 0.00018192,
      "loss": -117.0488,
      "step": 6780
    },
    {
      "epoch": 0.5432,
      "grad_norm": 45.40535354614258,
      "learning_rate": 0.00018189333333333335,
      "loss": -117.1853,
      "step": 6790
    },
    {
      "epoch": 0.544,
      "grad_norm": 55.419830322265625,
      "learning_rate": 0.00018186666666666668,
      "loss": -116.8891,
      "step": 6800
    },
    {
      "epoch": 0.5448,
      "grad_norm": 39.36368179321289,
      "learning_rate": 0.00018184,
      "loss": -116.8428,
      "step": 6810
    },
    {
      "epoch": 0.5456,
      "grad_norm": 52.30836486816406,
      "learning_rate": 0.00018181333333333334,
      "loss": -117.0873,
      "step": 6820
    },
    {
      "epoch": 0.5464,
      "grad_norm": 53.93484115600586,
      "learning_rate": 0.0001817866666666667,
      "loss": -115.1083,
      "step": 6830
    },
    {
      "epoch": 0.5472,
      "grad_norm": 56.295711517333984,
      "learning_rate": 0.00018176000000000002,
      "loss": -116.0054,
      "step": 6840
    },
    {
      "epoch": 0.548,
      "grad_norm": 48.59779739379883,
      "learning_rate": 0.00018173333333333332,
      "loss": -115.3479,
      "step": 6850
    },
    {
      "epoch": 0.5488,
      "grad_norm": 39.447425842285156,
      "learning_rate": 0.00018170666666666668,
      "loss": -116.2097,
      "step": 6860
    },
    {
      "epoch": 0.5496,
      "grad_norm": 35.570987701416016,
      "learning_rate": 0.00018168,
      "loss": -116.6985,
      "step": 6870
    },
    {
      "epoch": 0.5504,
      "grad_norm": 34.70112609863281,
      "learning_rate": 0.00018165333333333334,
      "loss": -116.0671,
      "step": 6880
    },
    {
      "epoch": 0.5512,
      "grad_norm": 41.29144287109375,
      "learning_rate": 0.00018162666666666667,
      "loss": -117.6045,
      "step": 6890
    },
    {
      "epoch": 0.552,
      "grad_norm": 59.58719253540039,
      "learning_rate": 0.00018160000000000002,
      "loss": -117.2098,
      "step": 6900
    },
    {
      "epoch": 0.5528,
      "grad_norm": 47.132362365722656,
      "learning_rate": 0.00018157333333333335,
      "loss": -117.4133,
      "step": 6910
    },
    {
      "epoch": 0.5536,
      "grad_norm": 47.70840835571289,
      "learning_rate": 0.00018154666666666665,
      "loss": -116.918,
      "step": 6920
    },
    {
      "epoch": 0.5544,
      "grad_norm": 40.254676818847656,
      "learning_rate": 0.00018152,
      "loss": -117.007,
      "step": 6930
    },
    {
      "epoch": 0.5552,
      "grad_norm": 41.702632904052734,
      "learning_rate": 0.00018149333333333334,
      "loss": -117.749,
      "step": 6940
    },
    {
      "epoch": 0.556,
      "grad_norm": 57.81248474121094,
      "learning_rate": 0.00018146666666666667,
      "loss": -116.3017,
      "step": 6950
    },
    {
      "epoch": 0.5568,
      "grad_norm": 48.701385498046875,
      "learning_rate": 0.00018144,
      "loss": -116.3376,
      "step": 6960
    },
    {
      "epoch": 0.5576,
      "grad_norm": 43.65536880493164,
      "learning_rate": 0.00018141333333333335,
      "loss": -116.826,
      "step": 6970
    },
    {
      "epoch": 0.5584,
      "grad_norm": 34.90165328979492,
      "learning_rate": 0.00018138666666666668,
      "loss": -116.9659,
      "step": 6980
    },
    {
      "epoch": 0.5592,
      "grad_norm": 32.42364501953125,
      "learning_rate": 0.00018136,
      "loss": -117.0347,
      "step": 6990
    },
    {
      "epoch": 0.56,
      "grad_norm": 32.579349517822266,
      "learning_rate": 0.00018133333333333334,
      "loss": -116.7354,
      "step": 7000
    },
    {
      "epoch": 0.5608,
      "grad_norm": 42.064395904541016,
      "learning_rate": 0.00018130666666666667,
      "loss": -114.9828,
      "step": 7010
    },
    {
      "epoch": 0.5616,
      "grad_norm": 42.948299407958984,
      "learning_rate": 0.00018128,
      "loss": -117.4066,
      "step": 7020
    },
    {
      "epoch": 0.5624,
      "grad_norm": 46.69017791748047,
      "learning_rate": 0.00018125333333333333,
      "loss": -117.4523,
      "step": 7030
    },
    {
      "epoch": 0.5632,
      "grad_norm": 37.10026550292969,
      "learning_rate": 0.00018122666666666668,
      "loss": -117.9312,
      "step": 7040
    },
    {
      "epoch": 0.564,
      "grad_norm": 46.52939224243164,
      "learning_rate": 0.0001812,
      "loss": -117.4407,
      "step": 7050
    },
    {
      "epoch": 0.5648,
      "grad_norm": 55.520957946777344,
      "learning_rate": 0.00018117333333333334,
      "loss": -116.5058,
      "step": 7060
    },
    {
      "epoch": 0.5656,
      "grad_norm": 44.18156433105469,
      "learning_rate": 0.00018114666666666667,
      "loss": -116.0443,
      "step": 7070
    },
    {
      "epoch": 0.5664,
      "grad_norm": 38.81914520263672,
      "learning_rate": 0.00018112,
      "loss": -116.2994,
      "step": 7080
    },
    {
      "epoch": 0.5672,
      "grad_norm": 44.79676818847656,
      "learning_rate": 0.00018109333333333333,
      "loss": -116.6726,
      "step": 7090
    },
    {
      "epoch": 0.568,
      "grad_norm": 60.41757583618164,
      "learning_rate": 0.00018106666666666669,
      "loss": -116.6289,
      "step": 7100
    },
    {
      "epoch": 0.5688,
      "grad_norm": 39.87009811401367,
      "learning_rate": 0.00018104000000000001,
      "loss": -116.0777,
      "step": 7110
    },
    {
      "epoch": 0.5696,
      "grad_norm": 45.78936767578125,
      "learning_rate": 0.00018101333333333334,
      "loss": -116.6606,
      "step": 7120
    },
    {
      "epoch": 0.5704,
      "grad_norm": 35.33668899536133,
      "learning_rate": 0.00018098666666666667,
      "loss": -116.3044,
      "step": 7130
    },
    {
      "epoch": 0.5712,
      "grad_norm": 48.87227249145508,
      "learning_rate": 0.00018096000000000003,
      "loss": -117.4768,
      "step": 7140
    },
    {
      "epoch": 0.572,
      "grad_norm": 41.416175842285156,
      "learning_rate": 0.00018093333333333333,
      "loss": -115.8566,
      "step": 7150
    },
    {
      "epoch": 0.5728,
      "grad_norm": 41.947967529296875,
      "learning_rate": 0.00018090666666666666,
      "loss": -116.6279,
      "step": 7160
    },
    {
      "epoch": 0.5736,
      "grad_norm": 43.00808334350586,
      "learning_rate": 0.00018088000000000002,
      "loss": -115.8292,
      "step": 7170
    },
    {
      "epoch": 0.5744,
      "grad_norm": 51.22386169433594,
      "learning_rate": 0.00018085333333333335,
      "loss": -116.9219,
      "step": 7180
    },
    {
      "epoch": 0.5752,
      "grad_norm": 39.6005859375,
      "learning_rate": 0.00018082666666666667,
      "loss": -117.1109,
      "step": 7190
    },
    {
      "epoch": 0.576,
      "grad_norm": 41.917884826660156,
      "learning_rate": 0.0001808,
      "loss": -117.3129,
      "step": 7200
    },
    {
      "epoch": 0.5768,
      "grad_norm": 36.19621658325195,
      "learning_rate": 0.00018077333333333336,
      "loss": -115.5164,
      "step": 7210
    },
    {
      "epoch": 0.5776,
      "grad_norm": 44.8084602355957,
      "learning_rate": 0.0001807466666666667,
      "loss": -115.4558,
      "step": 7220
    },
    {
      "epoch": 0.5784,
      "grad_norm": 58.22062301635742,
      "learning_rate": 0.00018072,
      "loss": -116.3725,
      "step": 7230
    },
    {
      "epoch": 0.5792,
      "grad_norm": 57.924468994140625,
      "learning_rate": 0.00018069333333333335,
      "loss": -116.0147,
      "step": 7240
    },
    {
      "epoch": 0.58,
      "grad_norm": 63.88901901245117,
      "learning_rate": 0.00018066666666666668,
      "loss": -115.3961,
      "step": 7250
    },
    {
      "epoch": 0.5808,
      "grad_norm": 32.50226974487305,
      "learning_rate": 0.00018064,
      "loss": -115.9636,
      "step": 7260
    },
    {
      "epoch": 0.5816,
      "grad_norm": 35.54066467285156,
      "learning_rate": 0.00018061333333333333,
      "loss": -117.2015,
      "step": 7270
    },
    {
      "epoch": 0.5824,
      "grad_norm": 46.701759338378906,
      "learning_rate": 0.0001805866666666667,
      "loss": -117.1544,
      "step": 7280
    },
    {
      "epoch": 0.5832,
      "grad_norm": 45.865230560302734,
      "learning_rate": 0.00018056000000000002,
      "loss": -117.0609,
      "step": 7290
    },
    {
      "epoch": 0.584,
      "grad_norm": 46.22263717651367,
      "learning_rate": 0.00018053333333333332,
      "loss": -118.0108,
      "step": 7300
    },
    {
      "epoch": 0.5848,
      "grad_norm": 43.880123138427734,
      "learning_rate": 0.00018050666666666668,
      "loss": -115.7819,
      "step": 7310
    },
    {
      "epoch": 0.5856,
      "grad_norm": 47.39733123779297,
      "learning_rate": 0.00018048,
      "loss": -117.6116,
      "step": 7320
    },
    {
      "epoch": 0.5864,
      "grad_norm": 47.9794807434082,
      "learning_rate": 0.00018045333333333334,
      "loss": -116.6503,
      "step": 7330
    },
    {
      "epoch": 0.5872,
      "grad_norm": 35.93383026123047,
      "learning_rate": 0.00018042666666666666,
      "loss": -116.8597,
      "step": 7340
    },
    {
      "epoch": 0.588,
      "grad_norm": 46.510955810546875,
      "learning_rate": 0.00018040000000000002,
      "loss": -117.6313,
      "step": 7350
    },
    {
      "epoch": 0.5888,
      "grad_norm": 28.3170108795166,
      "learning_rate": 0.00018037333333333335,
      "loss": -116.5784,
      "step": 7360
    },
    {
      "epoch": 0.5896,
      "grad_norm": 38.01980209350586,
      "learning_rate": 0.00018034666666666668,
      "loss": -116.1342,
      "step": 7370
    },
    {
      "epoch": 0.5904,
      "grad_norm": 42.82265090942383,
      "learning_rate": 0.00018032,
      "loss": -117.0767,
      "step": 7380
    },
    {
      "epoch": 0.5912,
      "grad_norm": 34.74549102783203,
      "learning_rate": 0.00018029333333333334,
      "loss": -117.8968,
      "step": 7390
    },
    {
      "epoch": 0.592,
      "grad_norm": 66.24939727783203,
      "learning_rate": 0.00018026666666666667,
      "loss": -117.5779,
      "step": 7400
    },
    {
      "epoch": 0.5928,
      "grad_norm": 39.25239562988281,
      "learning_rate": 0.00018024,
      "loss": -117.2855,
      "step": 7410
    },
    {
      "epoch": 0.5936,
      "grad_norm": 36.88819122314453,
      "learning_rate": 0.00018021333333333335,
      "loss": -116.3457,
      "step": 7420
    },
    {
      "epoch": 0.5944,
      "grad_norm": 41.39708709716797,
      "learning_rate": 0.00018018666666666668,
      "loss": -117.3655,
      "step": 7430
    },
    {
      "epoch": 0.5952,
      "grad_norm": 49.122520446777344,
      "learning_rate": 0.00018016,
      "loss": -117.8375,
      "step": 7440
    },
    {
      "epoch": 0.596,
      "grad_norm": 78.77855682373047,
      "learning_rate": 0.00018013333333333334,
      "loss": -118.1002,
      "step": 7450
    },
    {
      "epoch": 0.5968,
      "grad_norm": 66.4359359741211,
      "learning_rate": 0.00018010666666666667,
      "loss": -117.0915,
      "step": 7460
    },
    {
      "epoch": 0.5976,
      "grad_norm": 36.20977783203125,
      "learning_rate": 0.00018008,
      "loss": -117.064,
      "step": 7470
    },
    {
      "epoch": 0.5984,
      "grad_norm": 36.05269241333008,
      "learning_rate": 0.00018005333333333335,
      "loss": -116.2517,
      "step": 7480
    },
    {
      "epoch": 0.5992,
      "grad_norm": 39.3780632019043,
      "learning_rate": 0.00018002666666666668,
      "loss": -116.5394,
      "step": 7490
    },
    {
      "epoch": 0.6,
      "grad_norm": 57.44963836669922,
      "learning_rate": 0.00018,
      "loss": -116.4889,
      "step": 7500
    },
    {
      "epoch": 0.6008,
      "grad_norm": 34.02549743652344,
      "learning_rate": 0.00017997333333333334,
      "loss": -116.8412,
      "step": 7510
    },
    {
      "epoch": 0.6016,
      "grad_norm": 48.246761322021484,
      "learning_rate": 0.0001799466666666667,
      "loss": -116.5046,
      "step": 7520
    },
    {
      "epoch": 0.6024,
      "grad_norm": 47.6602783203125,
      "learning_rate": 0.00017992,
      "loss": -117.1374,
      "step": 7530
    },
    {
      "epoch": 0.6032,
      "grad_norm": 46.4791145324707,
      "learning_rate": 0.00017989333333333333,
      "loss": -116.2337,
      "step": 7540
    },
    {
      "epoch": 0.604,
      "grad_norm": 46.95732498168945,
      "learning_rate": 0.00017986666666666668,
      "loss": -116.3045,
      "step": 7550
    },
    {
      "epoch": 0.6048,
      "grad_norm": 41.426795959472656,
      "learning_rate": 0.00017984,
      "loss": -116.9569,
      "step": 7560
    },
    {
      "epoch": 0.6056,
      "grad_norm": 51.146751403808594,
      "learning_rate": 0.00017981333333333334,
      "loss": -116.2387,
      "step": 7570
    },
    {
      "epoch": 0.6064,
      "grad_norm": 44.228153228759766,
      "learning_rate": 0.00017978666666666667,
      "loss": -117.5371,
      "step": 7580
    },
    {
      "epoch": 0.6072,
      "grad_norm": 48.93086624145508,
      "learning_rate": 0.00017976000000000003,
      "loss": -117.1457,
      "step": 7590
    },
    {
      "epoch": 0.608,
      "grad_norm": 33.70541763305664,
      "learning_rate": 0.00017973333333333333,
      "loss": -116.5699,
      "step": 7600
    },
    {
      "epoch": 0.6088,
      "grad_norm": 44.03140640258789,
      "learning_rate": 0.00017970666666666666,
      "loss": -117.7898,
      "step": 7610
    },
    {
      "epoch": 0.6096,
      "grad_norm": 43.485652923583984,
      "learning_rate": 0.00017968000000000001,
      "loss": -116.5696,
      "step": 7620
    },
    {
      "epoch": 0.6104,
      "grad_norm": 47.08292007446289,
      "learning_rate": 0.00017965333333333334,
      "loss": -116.7182,
      "step": 7630
    },
    {
      "epoch": 0.6112,
      "grad_norm": 35.51005172729492,
      "learning_rate": 0.00017962666666666667,
      "loss": -117.1939,
      "step": 7640
    },
    {
      "epoch": 0.612,
      "grad_norm": 34.9428596496582,
      "learning_rate": 0.0001796,
      "loss": -116.2897,
      "step": 7650
    },
    {
      "epoch": 0.6128,
      "grad_norm": 36.63413619995117,
      "learning_rate": 0.00017957333333333336,
      "loss": -116.8844,
      "step": 7660
    },
    {
      "epoch": 0.6136,
      "grad_norm": 42.40555191040039,
      "learning_rate": 0.0001795466666666667,
      "loss": -117.2214,
      "step": 7670
    },
    {
      "epoch": 0.6144,
      "grad_norm": 44.988037109375,
      "learning_rate": 0.00017952,
      "loss": -117.7959,
      "step": 7680
    },
    {
      "epoch": 0.6152,
      "grad_norm": 52.753849029541016,
      "learning_rate": 0.00017949333333333335,
      "loss": -116.485,
      "step": 7690
    },
    {
      "epoch": 0.616,
      "grad_norm": 47.51270294189453,
      "learning_rate": 0.00017946666666666667,
      "loss": -116.3552,
      "step": 7700
    },
    {
      "epoch": 0.6168,
      "grad_norm": 42.75471115112305,
      "learning_rate": 0.00017944,
      "loss": -117.8938,
      "step": 7710
    },
    {
      "epoch": 0.6176,
      "grad_norm": 29.290790557861328,
      "learning_rate": 0.00017941333333333333,
      "loss": -117.4619,
      "step": 7720
    },
    {
      "epoch": 0.6184,
      "grad_norm": 50.05339813232422,
      "learning_rate": 0.0001793866666666667,
      "loss": -116.25,
      "step": 7730
    },
    {
      "epoch": 0.6192,
      "grad_norm": 35.6251335144043,
      "learning_rate": 0.00017936000000000002,
      "loss": -116.6741,
      "step": 7740
    },
    {
      "epoch": 0.62,
      "grad_norm": 46.092987060546875,
      "learning_rate": 0.00017933333333333332,
      "loss": -116.0879,
      "step": 7750
    },
    {
      "epoch": 0.6208,
      "grad_norm": 43.09198760986328,
      "learning_rate": 0.00017930666666666668,
      "loss": -116.9907,
      "step": 7760
    },
    {
      "epoch": 0.6216,
      "grad_norm": 45.09026336669922,
      "learning_rate": 0.00017928,
      "loss": -116.6262,
      "step": 7770
    },
    {
      "epoch": 0.6224,
      "grad_norm": 48.55646896362305,
      "learning_rate": 0.00017925333333333333,
      "loss": -117.7338,
      "step": 7780
    },
    {
      "epoch": 0.6232,
      "grad_norm": 37.72053527832031,
      "learning_rate": 0.00017922666666666666,
      "loss": -115.9088,
      "step": 7790
    },
    {
      "epoch": 0.624,
      "grad_norm": 46.23629379272461,
      "learning_rate": 0.00017920000000000002,
      "loss": -117.9448,
      "step": 7800
    },
    {
      "epoch": 0.6248,
      "grad_norm": 51.24875259399414,
      "learning_rate": 0.00017917333333333335,
      "loss": -116.4868,
      "step": 7810
    },
    {
      "epoch": 0.6256,
      "grad_norm": 45.128257751464844,
      "learning_rate": 0.00017914666666666668,
      "loss": -116.6464,
      "step": 7820
    },
    {
      "epoch": 0.6264,
      "grad_norm": 76.88504028320312,
      "learning_rate": 0.00017912,
      "loss": -116.5213,
      "step": 7830
    },
    {
      "epoch": 0.6272,
      "grad_norm": 37.46649169921875,
      "learning_rate": 0.00017909333333333334,
      "loss": -116.4843,
      "step": 7840
    },
    {
      "epoch": 0.628,
      "grad_norm": 43.57767868041992,
      "learning_rate": 0.00017906666666666666,
      "loss": -116.5198,
      "step": 7850
    },
    {
      "epoch": 0.6288,
      "grad_norm": 44.009864807128906,
      "learning_rate": 0.00017904000000000002,
      "loss": -117.3353,
      "step": 7860
    },
    {
      "epoch": 0.6296,
      "grad_norm": 35.634647369384766,
      "learning_rate": 0.00017901333333333335,
      "loss": -117.6926,
      "step": 7870
    },
    {
      "epoch": 0.6304,
      "grad_norm": 34.402427673339844,
      "learning_rate": 0.00017898666666666668,
      "loss": -116.2287,
      "step": 7880
    },
    {
      "epoch": 0.6312,
      "grad_norm": 53.60330581665039,
      "learning_rate": 0.00017896,
      "loss": -116.9334,
      "step": 7890
    },
    {
      "epoch": 0.632,
      "grad_norm": 44.69499969482422,
      "learning_rate": 0.00017893333333333336,
      "loss": -116.3592,
      "step": 7900
    },
    {
      "epoch": 0.6328,
      "grad_norm": 64.42022705078125,
      "learning_rate": 0.00017890666666666667,
      "loss": -116.9894,
      "step": 7910
    },
    {
      "epoch": 0.6336,
      "grad_norm": 43.38853073120117,
      "learning_rate": 0.00017888,
      "loss": -115.3559,
      "step": 7920
    },
    {
      "epoch": 0.6344,
      "grad_norm": 33.260589599609375,
      "learning_rate": 0.00017885333333333335,
      "loss": -117.6581,
      "step": 7930
    },
    {
      "epoch": 0.6352,
      "grad_norm": 40.554141998291016,
      "learning_rate": 0.00017882666666666668,
      "loss": -117.6118,
      "step": 7940
    },
    {
      "epoch": 0.636,
      "grad_norm": 51.8065185546875,
      "learning_rate": 0.0001788,
      "loss": -115.2736,
      "step": 7950
    },
    {
      "epoch": 0.6368,
      "grad_norm": 47.713802337646484,
      "learning_rate": 0.00017877333333333334,
      "loss": -116.9253,
      "step": 7960
    },
    {
      "epoch": 0.6376,
      "grad_norm": 50.183555603027344,
      "learning_rate": 0.0001787466666666667,
      "loss": -118.1005,
      "step": 7970
    },
    {
      "epoch": 0.6384,
      "grad_norm": 37.070091247558594,
      "learning_rate": 0.00017872,
      "loss": -116.5021,
      "step": 7980
    },
    {
      "epoch": 0.6392,
      "grad_norm": 54.4720573425293,
      "learning_rate": 0.00017869333333333333,
      "loss": -118.0732,
      "step": 7990
    },
    {
      "epoch": 0.64,
      "grad_norm": 38.36760330200195,
      "learning_rate": 0.00017866666666666668,
      "loss": -116.0683,
      "step": 8000
    },
    {
      "epoch": 0.6408,
      "grad_norm": 46.75566101074219,
      "learning_rate": 0.00017864,
      "loss": -117.4946,
      "step": 8010
    },
    {
      "epoch": 0.6416,
      "grad_norm": 46.86385726928711,
      "learning_rate": 0.00017861333333333334,
      "loss": -116.7693,
      "step": 8020
    },
    {
      "epoch": 0.6424,
      "grad_norm": 39.511802673339844,
      "learning_rate": 0.00017858666666666667,
      "loss": -116.5529,
      "step": 8030
    },
    {
      "epoch": 0.6432,
      "grad_norm": 38.33841323852539,
      "learning_rate": 0.00017856000000000003,
      "loss": -115.3659,
      "step": 8040
    },
    {
      "epoch": 0.644,
      "grad_norm": 56.69696807861328,
      "learning_rate": 0.00017853333333333335,
      "loss": -117.1656,
      "step": 8050
    },
    {
      "epoch": 0.6448,
      "grad_norm": 41.945037841796875,
      "learning_rate": 0.00017850666666666666,
      "loss": -114.8363,
      "step": 8060
    },
    {
      "epoch": 0.6456,
      "grad_norm": 53.45170593261719,
      "learning_rate": 0.00017848,
      "loss": -115.7865,
      "step": 8070
    },
    {
      "epoch": 0.6464,
      "grad_norm": 59.19235610961914,
      "learning_rate": 0.00017845333333333334,
      "loss": -117.3003,
      "step": 8080
    },
    {
      "epoch": 0.6472,
      "grad_norm": 51.13331604003906,
      "learning_rate": 0.00017842666666666667,
      "loss": -115.6961,
      "step": 8090
    },
    {
      "epoch": 0.648,
      "grad_norm": 49.789833068847656,
      "learning_rate": 0.0001784,
      "loss": -116.2824,
      "step": 8100
    },
    {
      "epoch": 0.6488,
      "grad_norm": 45.11083221435547,
      "learning_rate": 0.00017837333333333336,
      "loss": -117.308,
      "step": 8110
    },
    {
      "epoch": 0.6496,
      "grad_norm": 44.691226959228516,
      "learning_rate": 0.00017834666666666668,
      "loss": -115.8684,
      "step": 8120
    },
    {
      "epoch": 0.6504,
      "grad_norm": 41.87520980834961,
      "learning_rate": 0.00017832,
      "loss": -117.5663,
      "step": 8130
    },
    {
      "epoch": 0.6512,
      "grad_norm": 32.527652740478516,
      "learning_rate": 0.00017829333333333334,
      "loss": -116.7983,
      "step": 8140
    },
    {
      "epoch": 0.652,
      "grad_norm": 38.96406936645508,
      "learning_rate": 0.00017826666666666667,
      "loss": -117.0484,
      "step": 8150
    },
    {
      "epoch": 0.6528,
      "grad_norm": 32.98625946044922,
      "learning_rate": 0.00017824,
      "loss": -117.2295,
      "step": 8160
    },
    {
      "epoch": 0.6536,
      "grad_norm": 56.24735641479492,
      "learning_rate": 0.00017821333333333333,
      "loss": -116.7363,
      "step": 8170
    },
    {
      "epoch": 0.6544,
      "grad_norm": 61.56997299194336,
      "learning_rate": 0.00017818666666666669,
      "loss": -117.373,
      "step": 8180
    },
    {
      "epoch": 0.6552,
      "grad_norm": 55.31660079956055,
      "learning_rate": 0.00017816000000000002,
      "loss": -116.2382,
      "step": 8190
    },
    {
      "epoch": 0.656,
      "grad_norm": 50.214683532714844,
      "learning_rate": 0.00017813333333333334,
      "loss": -116.5147,
      "step": 8200
    },
    {
      "epoch": 0.6568,
      "grad_norm": 27.964555740356445,
      "learning_rate": 0.00017810666666666667,
      "loss": -116.3624,
      "step": 8210
    },
    {
      "epoch": 0.6576,
      "grad_norm": 44.756526947021484,
      "learning_rate": 0.00017808,
      "loss": -116.5015,
      "step": 8220
    },
    {
      "epoch": 0.6584,
      "grad_norm": 44.36581802368164,
      "learning_rate": 0.00017805333333333333,
      "loss": -115.8949,
      "step": 8230
    },
    {
      "epoch": 0.6592,
      "grad_norm": 35.15604019165039,
      "learning_rate": 0.0001780266666666667,
      "loss": -116.1405,
      "step": 8240
    },
    {
      "epoch": 0.66,
      "grad_norm": 51.24054718017578,
      "learning_rate": 0.00017800000000000002,
      "loss": -116.5141,
      "step": 8250
    },
    {
      "epoch": 0.6608,
      "grad_norm": 34.18670654296875,
      "learning_rate": 0.00017797333333333335,
      "loss": -115.8745,
      "step": 8260
    },
    {
      "epoch": 0.6616,
      "grad_norm": 46.126312255859375,
      "learning_rate": 0.00017794666666666668,
      "loss": -115.6763,
      "step": 8270
    },
    {
      "epoch": 0.6624,
      "grad_norm": 34.94189453125,
      "learning_rate": 0.00017792,
      "loss": -118.0447,
      "step": 8280
    },
    {
      "epoch": 0.6632,
      "grad_norm": 31.322267532348633,
      "learning_rate": 0.00017789333333333333,
      "loss": -116.9949,
      "step": 8290
    },
    {
      "epoch": 0.664,
      "grad_norm": 35.45851135253906,
      "learning_rate": 0.00017786666666666666,
      "loss": -117.3494,
      "step": 8300
    },
    {
      "epoch": 0.6648,
      "grad_norm": 35.05217742919922,
      "learning_rate": 0.00017784000000000002,
      "loss": -117.5676,
      "step": 8310
    },
    {
      "epoch": 0.6656,
      "grad_norm": 48.82609558105469,
      "learning_rate": 0.00017781333333333335,
      "loss": -116.975,
      "step": 8320
    },
    {
      "epoch": 0.6664,
      "grad_norm": 40.97316360473633,
      "learning_rate": 0.00017778666666666668,
      "loss": -116.6808,
      "step": 8330
    },
    {
      "epoch": 0.6672,
      "grad_norm": 37.475433349609375,
      "learning_rate": 0.00017776,
      "loss": -116.7773,
      "step": 8340
    },
    {
      "epoch": 0.668,
      "grad_norm": 32.0035285949707,
      "learning_rate": 0.00017773333333333336,
      "loss": -116.5331,
      "step": 8350
    },
    {
      "epoch": 0.6688,
      "grad_norm": 45.81812286376953,
      "learning_rate": 0.00017770666666666666,
      "loss": -116.0437,
      "step": 8360
    },
    {
      "epoch": 0.6696,
      "grad_norm": 32.23917007446289,
      "learning_rate": 0.00017768,
      "loss": -118.425,
      "step": 8370
    },
    {
      "epoch": 0.6704,
      "grad_norm": 34.261878967285156,
      "learning_rate": 0.00017765333333333335,
      "loss": -116.5344,
      "step": 8380
    },
    {
      "epoch": 0.6712,
      "grad_norm": 49.778499603271484,
      "learning_rate": 0.00017762666666666668,
      "loss": -116.2209,
      "step": 8390
    },
    {
      "epoch": 0.672,
      "grad_norm": 44.060821533203125,
      "learning_rate": 0.0001776,
      "loss": -115.2077,
      "step": 8400
    },
    {
      "epoch": 0.6728,
      "grad_norm": 39.106258392333984,
      "learning_rate": 0.00017757333333333334,
      "loss": -117.6046,
      "step": 8410
    },
    {
      "epoch": 0.6736,
      "grad_norm": 35.700721740722656,
      "learning_rate": 0.0001775466666666667,
      "loss": -116.6779,
      "step": 8420
    },
    {
      "epoch": 0.6744,
      "grad_norm": 38.49739456176758,
      "learning_rate": 0.00017752,
      "loss": -116.8539,
      "step": 8430
    },
    {
      "epoch": 0.6752,
      "grad_norm": 36.63945388793945,
      "learning_rate": 0.00017749333333333332,
      "loss": -117.2094,
      "step": 8440
    },
    {
      "epoch": 0.676,
      "grad_norm": 179.92340087890625,
      "learning_rate": 0.00017746666666666668,
      "loss": -117.8736,
      "step": 8450
    },
    {
      "epoch": 0.6768,
      "grad_norm": 43.007362365722656,
      "learning_rate": 0.00017744,
      "loss": -116.5265,
      "step": 8460
    },
    {
      "epoch": 0.6776,
      "grad_norm": 37.76871109008789,
      "learning_rate": 0.00017741333333333334,
      "loss": -116.0239,
      "step": 8470
    },
    {
      "epoch": 0.6784,
      "grad_norm": 54.19415283203125,
      "learning_rate": 0.00017738666666666667,
      "loss": -117.2823,
      "step": 8480
    },
    {
      "epoch": 0.6792,
      "grad_norm": 45.628604888916016,
      "learning_rate": 0.00017736000000000002,
      "loss": -117.2208,
      "step": 8490
    },
    {
      "epoch": 0.68,
      "grad_norm": 37.583499908447266,
      "learning_rate": 0.00017733333333333335,
      "loss": -116.9004,
      "step": 8500
    },
    {
      "epoch": 0.6808,
      "grad_norm": 47.783626556396484,
      "learning_rate": 0.00017730666666666665,
      "loss": -117.8704,
      "step": 8510
    },
    {
      "epoch": 0.6816,
      "grad_norm": 37.981529235839844,
      "learning_rate": 0.00017728,
      "loss": -117.1079,
      "step": 8520
    },
    {
      "epoch": 0.6824,
      "grad_norm": 41.569759368896484,
      "learning_rate": 0.00017725333333333334,
      "loss": -117.124,
      "step": 8530
    },
    {
      "epoch": 0.6832,
      "grad_norm": 44.38874816894531,
      "learning_rate": 0.00017722666666666667,
      "loss": -116.388,
      "step": 8540
    },
    {
      "epoch": 0.684,
      "grad_norm": 30.202728271484375,
      "learning_rate": 0.0001772,
      "loss": -117.1989,
      "step": 8550
    },
    {
      "epoch": 0.6848,
      "grad_norm": 35.20930862426758,
      "learning_rate": 0.00017717333333333335,
      "loss": -115.8835,
      "step": 8560
    },
    {
      "epoch": 0.6856,
      "grad_norm": 36.52688217163086,
      "learning_rate": 0.00017714666666666668,
      "loss": -118.0904,
      "step": 8570
    },
    {
      "epoch": 0.6864,
      "grad_norm": 34.76478958129883,
      "learning_rate": 0.00017712,
      "loss": -117.7977,
      "step": 8580
    },
    {
      "epoch": 0.6872,
      "grad_norm": 40.544227600097656,
      "learning_rate": 0.00017709333333333334,
      "loss": -115.5086,
      "step": 8590
    },
    {
      "epoch": 0.688,
      "grad_norm": 38.520450592041016,
      "learning_rate": 0.00017706666666666667,
      "loss": -117.872,
      "step": 8600
    },
    {
      "epoch": 0.6888,
      "grad_norm": 130.09129333496094,
      "learning_rate": 0.00017704,
      "loss": -118.113,
      "step": 8610
    },
    {
      "epoch": 0.6896,
      "grad_norm": 43.78343200683594,
      "learning_rate": 0.00017701333333333336,
      "loss": -116.0582,
      "step": 8620
    },
    {
      "epoch": 0.6904,
      "grad_norm": 40.78398513793945,
      "learning_rate": 0.00017698666666666668,
      "loss": -116.8421,
      "step": 8630
    },
    {
      "epoch": 0.6912,
      "grad_norm": 33.13228225708008,
      "learning_rate": 0.00017696,
      "loss": -114.556,
      "step": 8640
    },
    {
      "epoch": 0.692,
      "grad_norm": 39.38179016113281,
      "learning_rate": 0.00017693333333333334,
      "loss": -116.4203,
      "step": 8650
    },
    {
      "epoch": 0.6928,
      "grad_norm": 54.1925163269043,
      "learning_rate": 0.00017690666666666667,
      "loss": -116.6408,
      "step": 8660
    },
    {
      "epoch": 0.6936,
      "grad_norm": 35.46259307861328,
      "learning_rate": 0.00017688,
      "loss": -117.6413,
      "step": 8670
    },
    {
      "epoch": 0.6944,
      "grad_norm": 40.612125396728516,
      "learning_rate": 0.00017685333333333333,
      "loss": -117.3251,
      "step": 8680
    },
    {
      "epoch": 0.6952,
      "grad_norm": 32.79493713378906,
      "learning_rate": 0.00017682666666666669,
      "loss": -116.3192,
      "step": 8690
    },
    {
      "epoch": 0.696,
      "grad_norm": 34.6302490234375,
      "learning_rate": 0.00017680000000000001,
      "loss": -117.6763,
      "step": 8700
    },
    {
      "epoch": 0.6968,
      "grad_norm": 37.50491714477539,
      "learning_rate": 0.00017677333333333334,
      "loss": -116.9201,
      "step": 8710
    },
    {
      "epoch": 0.6976,
      "grad_norm": 36.83292770385742,
      "learning_rate": 0.00017674666666666667,
      "loss": -117.813,
      "step": 8720
    },
    {
      "epoch": 0.6984,
      "grad_norm": 42.22932052612305,
      "learning_rate": 0.00017672000000000003,
      "loss": -116.1193,
      "step": 8730
    },
    {
      "epoch": 0.6992,
      "grad_norm": 36.96061325073242,
      "learning_rate": 0.00017669333333333333,
      "loss": -116.2352,
      "step": 8740
    },
    {
      "epoch": 0.7,
      "grad_norm": 36.22679901123047,
      "learning_rate": 0.00017666666666666666,
      "loss": -115.5243,
      "step": 8750
    },
    {
      "epoch": 0.7008,
      "grad_norm": 40.591957092285156,
      "learning_rate": 0.00017664000000000002,
      "loss": -117.5236,
      "step": 8760
    },
    {
      "epoch": 0.7016,
      "grad_norm": 51.056861877441406,
      "learning_rate": 0.00017661333333333335,
      "loss": -116.7633,
      "step": 8770
    },
    {
      "epoch": 0.7024,
      "grad_norm": 36.088687896728516,
      "learning_rate": 0.00017658666666666667,
      "loss": -116.8765,
      "step": 8780
    },
    {
      "epoch": 0.7032,
      "grad_norm": 49.614891052246094,
      "learning_rate": 0.00017656,
      "loss": -118.246,
      "step": 8790
    },
    {
      "epoch": 0.704,
      "grad_norm": 32.98260498046875,
      "learning_rate": 0.00017653333333333336,
      "loss": -116.8395,
      "step": 8800
    },
    {
      "epoch": 0.7048,
      "grad_norm": 37.58303451538086,
      "learning_rate": 0.00017650666666666666,
      "loss": -116.2576,
      "step": 8810
    },
    {
      "epoch": 0.7056,
      "grad_norm": 61.086238861083984,
      "learning_rate": 0.00017648,
      "loss": -116.5773,
      "step": 8820
    },
    {
      "epoch": 0.7064,
      "grad_norm": 31.73227310180664,
      "learning_rate": 0.00017645333333333335,
      "loss": -117.3061,
      "step": 8830
    },
    {
      "epoch": 0.7072,
      "grad_norm": 27.5671329498291,
      "learning_rate": 0.00017642666666666668,
      "loss": -117.2918,
      "step": 8840
    },
    {
      "epoch": 0.708,
      "grad_norm": 156.36019897460938,
      "learning_rate": 0.0001764,
      "loss": -117.2204,
      "step": 8850
    },
    {
      "epoch": 0.7088,
      "grad_norm": 48.73027801513672,
      "learning_rate": 0.00017637333333333333,
      "loss": -116.2298,
      "step": 8860
    },
    {
      "epoch": 0.7096,
      "grad_norm": 33.24005126953125,
      "learning_rate": 0.0001763466666666667,
      "loss": -117.226,
      "step": 8870
    },
    {
      "epoch": 0.7104,
      "grad_norm": 36.19174575805664,
      "learning_rate": 0.00017632000000000002,
      "loss": -115.1355,
      "step": 8880
    },
    {
      "epoch": 0.7112,
      "grad_norm": 32.27180862426758,
      "learning_rate": 0.00017629333333333332,
      "loss": -117.7675,
      "step": 8890
    },
    {
      "epoch": 0.712,
      "grad_norm": 42.77313995361328,
      "learning_rate": 0.00017626666666666668,
      "loss": -117.2102,
      "step": 8900
    },
    {
      "epoch": 0.7128,
      "grad_norm": 45.13800048828125,
      "learning_rate": 0.00017624,
      "loss": -116.2951,
      "step": 8910
    },
    {
      "epoch": 0.7136,
      "grad_norm": 62.660648345947266,
      "learning_rate": 0.00017621333333333334,
      "loss": -116.3383,
      "step": 8920
    },
    {
      "epoch": 0.7144,
      "grad_norm": 47.011653900146484,
      "learning_rate": 0.00017618666666666666,
      "loss": -117.1264,
      "step": 8930
    },
    {
      "epoch": 0.7152,
      "grad_norm": 33.322303771972656,
      "learning_rate": 0.00017616000000000002,
      "loss": -117.647,
      "step": 8940
    },
    {
      "epoch": 0.716,
      "grad_norm": 41.159523010253906,
      "learning_rate": 0.00017613333333333335,
      "loss": -117.6452,
      "step": 8950
    },
    {
      "epoch": 0.7168,
      "grad_norm": 41.147586822509766,
      "learning_rate": 0.00017610666666666665,
      "loss": -116.6981,
      "step": 8960
    },
    {
      "epoch": 0.7176,
      "grad_norm": 47.418766021728516,
      "learning_rate": 0.00017608,
      "loss": -116.6269,
      "step": 8970
    },
    {
      "epoch": 0.7184,
      "grad_norm": 41.500022888183594,
      "learning_rate": 0.00017605333333333334,
      "loss": -116.465,
      "step": 8980
    },
    {
      "epoch": 0.7192,
      "grad_norm": 28.2304744720459,
      "learning_rate": 0.00017602666666666667,
      "loss": -115.6281,
      "step": 8990
    },
    {
      "epoch": 0.72,
      "grad_norm": 37.49712371826172,
      "learning_rate": 0.00017600000000000002,
      "loss": -116.0471,
      "step": 9000
    },
    {
      "epoch": 0.7208,
      "grad_norm": 32.3600959777832,
      "learning_rate": 0.00017597333333333335,
      "loss": -117.6102,
      "step": 9010
    },
    {
      "epoch": 0.7216,
      "grad_norm": 32.07460403442383,
      "learning_rate": 0.00017594666666666668,
      "loss": -116.0979,
      "step": 9020
    },
    {
      "epoch": 0.7224,
      "grad_norm": 42.53641891479492,
      "learning_rate": 0.00017592,
      "loss": -116.4295,
      "step": 9030
    },
    {
      "epoch": 0.7232,
      "grad_norm": 43.53851318359375,
      "learning_rate": 0.00017589333333333334,
      "loss": -117.6577,
      "step": 9040
    },
    {
      "epoch": 0.724,
      "grad_norm": 45.58750915527344,
      "learning_rate": 0.00017586666666666667,
      "loss": -116.8985,
      "step": 9050
    },
    {
      "epoch": 0.7248,
      "grad_norm": 36.50088119506836,
      "learning_rate": 0.00017584,
      "loss": -117.1226,
      "step": 9060
    },
    {
      "epoch": 0.7256,
      "grad_norm": 35.25590896606445,
      "learning_rate": 0.00017581333333333335,
      "loss": -117.053,
      "step": 9070
    },
    {
      "epoch": 0.7264,
      "grad_norm": 46.611083984375,
      "learning_rate": 0.00017578666666666668,
      "loss": -117.6817,
      "step": 9080
    },
    {
      "epoch": 0.7272,
      "grad_norm": 37.277313232421875,
      "learning_rate": 0.00017576,
      "loss": -118.4638,
      "step": 9090
    },
    {
      "epoch": 0.728,
      "grad_norm": 38.4487419128418,
      "learning_rate": 0.00017573333333333334,
      "loss": -117.8905,
      "step": 9100
    },
    {
      "epoch": 0.7288,
      "grad_norm": 42.57809066772461,
      "learning_rate": 0.0001757066666666667,
      "loss": -117.9848,
      "step": 9110
    },
    {
      "epoch": 0.7296,
      "grad_norm": 38.56827163696289,
      "learning_rate": 0.00017568,
      "loss": -116.8407,
      "step": 9120
    },
    {
      "epoch": 0.7304,
      "grad_norm": 26.96900177001953,
      "learning_rate": 0.00017565333333333333,
      "loss": -117.8851,
      "step": 9130
    },
    {
      "epoch": 0.7312,
      "grad_norm": 36.88459396362305,
      "learning_rate": 0.00017562666666666668,
      "loss": -115.8911,
      "step": 9140
    },
    {
      "epoch": 0.732,
      "grad_norm": 34.27349090576172,
      "learning_rate": 0.0001756,
      "loss": -117.159,
      "step": 9150
    },
    {
      "epoch": 0.7328,
      "grad_norm": 34.07572937011719,
      "learning_rate": 0.00017557333333333334,
      "loss": -116.1964,
      "step": 9160
    },
    {
      "epoch": 0.7336,
      "grad_norm": 46.15312194824219,
      "learning_rate": 0.00017554666666666667,
      "loss": -117.7465,
      "step": 9170
    },
    {
      "epoch": 0.7344,
      "grad_norm": 36.199012756347656,
      "learning_rate": 0.00017552000000000003,
      "loss": -116.5112,
      "step": 9180
    },
    {
      "epoch": 0.7352,
      "grad_norm": 28.545446395874023,
      "learning_rate": 0.00017549333333333333,
      "loss": -118.1452,
      "step": 9190
    },
    {
      "epoch": 0.736,
      "grad_norm": 32.911617279052734,
      "learning_rate": 0.00017546666666666666,
      "loss": -116.1325,
      "step": 9200
    },
    {
      "epoch": 0.7368,
      "grad_norm": 37.589996337890625,
      "learning_rate": 0.00017544000000000001,
      "loss": -117.8555,
      "step": 9210
    },
    {
      "epoch": 0.7376,
      "grad_norm": 43.62628173828125,
      "learning_rate": 0.00017541333333333334,
      "loss": -116.2054,
      "step": 9220
    },
    {
      "epoch": 0.7384,
      "grad_norm": 40.8306999206543,
      "learning_rate": 0.00017538666666666667,
      "loss": -116.0067,
      "step": 9230
    },
    {
      "epoch": 0.7392,
      "grad_norm": 39.630210876464844,
      "learning_rate": 0.00017536,
      "loss": -116.7189,
      "step": 9240
    },
    {
      "epoch": 0.74,
      "grad_norm": 37.813682556152344,
      "learning_rate": 0.00017533333333333336,
      "loss": -118.126,
      "step": 9250
    },
    {
      "epoch": 0.7408,
      "grad_norm": 29.68982696533203,
      "learning_rate": 0.0001753066666666667,
      "loss": -117.0832,
      "step": 9260
    },
    {
      "epoch": 0.7416,
      "grad_norm": 34.11018753051758,
      "learning_rate": 0.00017528,
      "loss": -114.9562,
      "step": 9270
    },
    {
      "epoch": 0.7424,
      "grad_norm": 38.50886154174805,
      "learning_rate": 0.00017525333333333334,
      "loss": -115.7195,
      "step": 9280
    },
    {
      "epoch": 0.7432,
      "grad_norm": 43.83024215698242,
      "learning_rate": 0.00017522666666666667,
      "loss": -117.0039,
      "step": 9290
    },
    {
      "epoch": 0.744,
      "grad_norm": 44.01445007324219,
      "learning_rate": 0.0001752,
      "loss": -117.5827,
      "step": 9300
    },
    {
      "epoch": 0.7448,
      "grad_norm": 42.317962646484375,
      "learning_rate": 0.00017517333333333333,
      "loss": -117.1248,
      "step": 9310
    },
    {
      "epoch": 0.7456,
      "grad_norm": 41.37525939941406,
      "learning_rate": 0.0001751466666666667,
      "loss": -117.1991,
      "step": 9320
    },
    {
      "epoch": 0.7464,
      "grad_norm": 35.179283142089844,
      "learning_rate": 0.00017512000000000002,
      "loss": -116.7185,
      "step": 9330
    },
    {
      "epoch": 0.7472,
      "grad_norm": 29.872228622436523,
      "learning_rate": 0.00017509333333333332,
      "loss": -117.3814,
      "step": 9340
    },
    {
      "epoch": 0.748,
      "grad_norm": 36.73237609863281,
      "learning_rate": 0.00017506666666666668,
      "loss": -117.6131,
      "step": 9350
    },
    {
      "epoch": 0.7488,
      "grad_norm": 39.22553253173828,
      "learning_rate": 0.00017504,
      "loss": -116.827,
      "step": 9360
    },
    {
      "epoch": 0.7496,
      "grad_norm": 35.09416580200195,
      "learning_rate": 0.00017501333333333333,
      "loss": -116.2173,
      "step": 9370
    },
    {
      "epoch": 0.7504,
      "grad_norm": 39.46403121948242,
      "learning_rate": 0.0001749866666666667,
      "loss": -116.7392,
      "step": 9380
    },
    {
      "epoch": 0.7512,
      "grad_norm": 51.719024658203125,
      "learning_rate": 0.00017496000000000002,
      "loss": -117.0319,
      "step": 9390
    },
    {
      "epoch": 0.752,
      "grad_norm": 32.57200622558594,
      "learning_rate": 0.00017493333333333335,
      "loss": -115.1694,
      "step": 9400
    },
    {
      "epoch": 0.7528,
      "grad_norm": 45.162742614746094,
      "learning_rate": 0.00017490666666666668,
      "loss": -115.714,
      "step": 9410
    },
    {
      "epoch": 0.7536,
      "grad_norm": 37.24314880371094,
      "learning_rate": 0.00017488,
      "loss": -117.3681,
      "step": 9420
    },
    {
      "epoch": 0.7544,
      "grad_norm": 40.98931121826172,
      "learning_rate": 0.00017485333333333334,
      "loss": -117.7077,
      "step": 9430
    },
    {
      "epoch": 0.7552,
      "grad_norm": 37.85551834106445,
      "learning_rate": 0.00017482666666666666,
      "loss": -116.4321,
      "step": 9440
    },
    {
      "epoch": 0.756,
      "grad_norm": 56.13038635253906,
      "learning_rate": 0.00017480000000000002,
      "loss": -118.009,
      "step": 9450
    },
    {
      "epoch": 0.7568,
      "grad_norm": 40.8016357421875,
      "learning_rate": 0.00017477333333333335,
      "loss": -119.1305,
      "step": 9460
    },
    {
      "epoch": 0.7576,
      "grad_norm": 30.534414291381836,
      "learning_rate": 0.00017474666666666668,
      "loss": -117.2079,
      "step": 9470
    },
    {
      "epoch": 0.7584,
      "grad_norm": 37.898746490478516,
      "learning_rate": 0.00017472,
      "loss": -117.0391,
      "step": 9480
    },
    {
      "epoch": 0.7592,
      "grad_norm": 42.81519317626953,
      "learning_rate": 0.00017469333333333334,
      "loss": -116.5269,
      "step": 9490
    },
    {
      "epoch": 0.76,
      "grad_norm": 35.926368713378906,
      "learning_rate": 0.00017466666666666667,
      "loss": -116.5483,
      "step": 9500
    },
    {
      "epoch": 0.7608,
      "grad_norm": 35.566001892089844,
      "learning_rate": 0.00017464,
      "loss": -117.7541,
      "step": 9510
    },
    {
      "epoch": 0.7616,
      "grad_norm": 29.93995475769043,
      "learning_rate": 0.00017461333333333335,
      "loss": -117.3806,
      "step": 9520
    },
    {
      "epoch": 0.7624,
      "grad_norm": 28.539628982543945,
      "learning_rate": 0.00017458666666666668,
      "loss": -116.825,
      "step": 9530
    },
    {
      "epoch": 0.7632,
      "grad_norm": 50.70758056640625,
      "learning_rate": 0.00017456,
      "loss": -117.6176,
      "step": 9540
    },
    {
      "epoch": 0.764,
      "grad_norm": 36.3371696472168,
      "learning_rate": 0.00017453333333333334,
      "loss": -117.0015,
      "step": 9550
    },
    {
      "epoch": 0.7648,
      "grad_norm": 32.858863830566406,
      "learning_rate": 0.0001745066666666667,
      "loss": -117.6041,
      "step": 9560
    },
    {
      "epoch": 0.7656,
      "grad_norm": 42.86309051513672,
      "learning_rate": 0.00017448,
      "loss": -116.3444,
      "step": 9570
    },
    {
      "epoch": 0.7664,
      "grad_norm": 53.366912841796875,
      "learning_rate": 0.00017445333333333333,
      "loss": -116.296,
      "step": 9580
    },
    {
      "epoch": 0.7672,
      "grad_norm": 37.78923797607422,
      "learning_rate": 0.00017442666666666668,
      "loss": -117.2076,
      "step": 9590
    },
    {
      "epoch": 0.768,
      "grad_norm": 42.18913269042969,
      "learning_rate": 0.0001744,
      "loss": -117.1174,
      "step": 9600
    },
    {
      "epoch": 0.7688,
      "grad_norm": 42.92565155029297,
      "learning_rate": 0.00017437333333333334,
      "loss": -116.2135,
      "step": 9610
    },
    {
      "epoch": 0.7696,
      "grad_norm": 48.124366760253906,
      "learning_rate": 0.00017434666666666667,
      "loss": -115.8326,
      "step": 9620
    },
    {
      "epoch": 0.7704,
      "grad_norm": 36.04734420776367,
      "learning_rate": 0.00017432000000000003,
      "loss": -117.1097,
      "step": 9630
    },
    {
      "epoch": 0.7712,
      "grad_norm": 37.95924758911133,
      "learning_rate": 0.00017429333333333333,
      "loss": -117.987,
      "step": 9640
    },
    {
      "epoch": 0.772,
      "grad_norm": 37.06597137451172,
      "learning_rate": 0.00017426666666666666,
      "loss": -117.3225,
      "step": 9650
    },
    {
      "epoch": 0.7728,
      "grad_norm": 33.998260498046875,
      "learning_rate": 0.00017424,
      "loss": -117.0352,
      "step": 9660
    },
    {
      "epoch": 0.7736,
      "grad_norm": 32.9652099609375,
      "learning_rate": 0.00017421333333333334,
      "loss": -118.2414,
      "step": 9670
    },
    {
      "epoch": 0.7744,
      "grad_norm": 36.97661590576172,
      "learning_rate": 0.00017418666666666667,
      "loss": -115.7202,
      "step": 9680
    },
    {
      "epoch": 0.7752,
      "grad_norm": 39.901695251464844,
      "learning_rate": 0.00017416,
      "loss": -117.2575,
      "step": 9690
    },
    {
      "epoch": 0.776,
      "grad_norm": 57.448856353759766,
      "learning_rate": 0.00017413333333333336,
      "loss": -117.2167,
      "step": 9700
    },
    {
      "epoch": 0.7768,
      "grad_norm": 45.93378829956055,
      "learning_rate": 0.00017410666666666668,
      "loss": -118.1347,
      "step": 9710
    },
    {
      "epoch": 0.7776,
      "grad_norm": 51.501529693603516,
      "learning_rate": 0.00017408,
      "loss": -115.721,
      "step": 9720
    },
    {
      "epoch": 0.7784,
      "grad_norm": 36.28866958618164,
      "learning_rate": 0.00017405333333333334,
      "loss": -118.1411,
      "step": 9730
    },
    {
      "epoch": 0.7792,
      "grad_norm": 36.06340789794922,
      "learning_rate": 0.00017402666666666667,
      "loss": -118.0621,
      "step": 9740
    },
    {
      "epoch": 0.78,
      "grad_norm": 30.802003860473633,
      "learning_rate": 0.000174,
      "loss": -117.9844,
      "step": 9750
    },
    {
      "epoch": 0.7808,
      "grad_norm": 33.037010192871094,
      "learning_rate": 0.00017397333333333336,
      "loss": -117.3135,
      "step": 9760
    },
    {
      "epoch": 0.7816,
      "grad_norm": 50.03852844238281,
      "learning_rate": 0.00017394666666666669,
      "loss": -117.3165,
      "step": 9770
    },
    {
      "epoch": 0.7824,
      "grad_norm": 50.68558120727539,
      "learning_rate": 0.00017392000000000002,
      "loss": -117.2146,
      "step": 9780
    },
    {
      "epoch": 0.7832,
      "grad_norm": 38.66961669921875,
      "learning_rate": 0.00017389333333333334,
      "loss": -116.7565,
      "step": 9790
    },
    {
      "epoch": 0.784,
      "grad_norm": 37.93056869506836,
      "learning_rate": 0.00017386666666666667,
      "loss": -115.32,
      "step": 9800
    },
    {
      "epoch": 0.7848,
      "grad_norm": 31.89476776123047,
      "learning_rate": 0.00017384,
      "loss": -117.7201,
      "step": 9810
    },
    {
      "epoch": 0.7856,
      "grad_norm": 37.77830505371094,
      "learning_rate": 0.00017381333333333333,
      "loss": -116.4914,
      "step": 9820
    },
    {
      "epoch": 0.7864,
      "grad_norm": 33.47239685058594,
      "learning_rate": 0.0001737866666666667,
      "loss": -117.0523,
      "step": 9830
    },
    {
      "epoch": 0.7872,
      "grad_norm": 47.43251419067383,
      "learning_rate": 0.00017376000000000002,
      "loss": -117.3681,
      "step": 9840
    },
    {
      "epoch": 0.788,
      "grad_norm": 37.15602111816406,
      "learning_rate": 0.00017373333333333335,
      "loss": -117.6162,
      "step": 9850
    },
    {
      "epoch": 0.7888,
      "grad_norm": 31.92695426940918,
      "learning_rate": 0.00017370666666666668,
      "loss": -117.0347,
      "step": 9860
    },
    {
      "epoch": 0.7896,
      "grad_norm": 34.538578033447266,
      "learning_rate": 0.00017368,
      "loss": -116.8584,
      "step": 9870
    },
    {
      "epoch": 0.7904,
      "grad_norm": 44.177120208740234,
      "learning_rate": 0.00017365333333333333,
      "loss": -117.6602,
      "step": 9880
    },
    {
      "epoch": 0.7912,
      "grad_norm": 41.836727142333984,
      "learning_rate": 0.00017362666666666666,
      "loss": -117.2714,
      "step": 9890
    },
    {
      "epoch": 0.792,
      "grad_norm": 39.481483459472656,
      "learning_rate": 0.00017360000000000002,
      "loss": -118.0631,
      "step": 9900
    },
    {
      "epoch": 0.7928,
      "grad_norm": 39.051300048828125,
      "learning_rate": 0.00017357333333333335,
      "loss": -116.6002,
      "step": 9910
    },
    {
      "epoch": 0.7936,
      "grad_norm": 30.939611434936523,
      "learning_rate": 0.00017354666666666668,
      "loss": -116.9089,
      "step": 9920
    },
    {
      "epoch": 0.7944,
      "grad_norm": 41.90079116821289,
      "learning_rate": 0.00017352,
      "loss": -117.4229,
      "step": 9930
    },
    {
      "epoch": 0.7952,
      "grad_norm": 45.26663589477539,
      "learning_rate": 0.00017349333333333336,
      "loss": -117.4943,
      "step": 9940
    },
    {
      "epoch": 0.796,
      "grad_norm": 37.87739944458008,
      "learning_rate": 0.00017346666666666666,
      "loss": -117.2341,
      "step": 9950
    },
    {
      "epoch": 0.7968,
      "grad_norm": 33.9995002746582,
      "learning_rate": 0.00017344,
      "loss": -117.9946,
      "step": 9960
    },
    {
      "epoch": 0.7976,
      "grad_norm": 32.17803955078125,
      "learning_rate": 0.00017341333333333335,
      "loss": -118.1242,
      "step": 9970
    },
    {
      "epoch": 0.7984,
      "grad_norm": 33.92988967895508,
      "learning_rate": 0.00017338666666666668,
      "loss": -117.4049,
      "step": 9980
    },
    {
      "epoch": 0.7992,
      "grad_norm": 51.09221649169922,
      "learning_rate": 0.00017336,
      "loss": -116.9355,
      "step": 9990
    },
    {
      "epoch": 0.8,
      "grad_norm": 37.503170013427734,
      "learning_rate": 0.00017333333333333334,
      "loss": -117.2671,
      "step": 10000
    },
    {
      "epoch": 0.8008,
      "grad_norm": 37.772159576416016,
      "learning_rate": 0.0001733066666666667,
      "loss": -116.6958,
      "step": 10010
    },
    {
      "epoch": 0.8016,
      "grad_norm": 36.61260223388672,
      "learning_rate": 0.00017328,
      "loss": -116.3866,
      "step": 10020
    },
    {
      "epoch": 0.8024,
      "grad_norm": 39.40378189086914,
      "learning_rate": 0.00017325333333333332,
      "loss": -117.2326,
      "step": 10030
    },
    {
      "epoch": 0.8032,
      "grad_norm": 47.54624557495117,
      "learning_rate": 0.00017322666666666668,
      "loss": -117.5456,
      "step": 10040
    },
    {
      "epoch": 0.804,
      "grad_norm": 41.16878128051758,
      "learning_rate": 0.0001732,
      "loss": -117.0015,
      "step": 10050
    },
    {
      "epoch": 0.8048,
      "grad_norm": 39.90586471557617,
      "learning_rate": 0.00017317333333333334,
      "loss": -117.7439,
      "step": 10060
    },
    {
      "epoch": 0.8056,
      "grad_norm": 45.470333099365234,
      "learning_rate": 0.00017314666666666667,
      "loss": -117.4301,
      "step": 10070
    },
    {
      "epoch": 0.8064,
      "grad_norm": 43.886932373046875,
      "learning_rate": 0.00017312000000000002,
      "loss": -117.3564,
      "step": 10080
    },
    {
      "epoch": 0.8072,
      "grad_norm": 49.464515686035156,
      "learning_rate": 0.00017309333333333335,
      "loss": -117.2263,
      "step": 10090
    },
    {
      "epoch": 0.808,
      "grad_norm": 40.31156921386719,
      "learning_rate": 0.00017306666666666665,
      "loss": -117.438,
      "step": 10100
    },
    {
      "epoch": 0.8088,
      "grad_norm": 36.24325942993164,
      "learning_rate": 0.00017304,
      "loss": -116.7521,
      "step": 10110
    },
    {
      "epoch": 0.8096,
      "grad_norm": 45.666664123535156,
      "learning_rate": 0.00017301333333333334,
      "loss": -116.3852,
      "step": 10120
    },
    {
      "epoch": 0.8104,
      "grad_norm": 36.255680084228516,
      "learning_rate": 0.00017298666666666667,
      "loss": -117.3309,
      "step": 10130
    },
    {
      "epoch": 0.8112,
      "grad_norm": 42.89284133911133,
      "learning_rate": 0.00017296,
      "loss": -117.3657,
      "step": 10140
    },
    {
      "epoch": 0.812,
      "grad_norm": 40.64197540283203,
      "learning_rate": 0.00017293333333333335,
      "loss": -116.6611,
      "step": 10150
    },
    {
      "epoch": 0.8128,
      "grad_norm": 41.116615295410156,
      "learning_rate": 0.00017290666666666668,
      "loss": -116.1314,
      "step": 10160
    },
    {
      "epoch": 0.8136,
      "grad_norm": 43.88105392456055,
      "learning_rate": 0.00017287999999999998,
      "loss": -115.3332,
      "step": 10170
    },
    {
      "epoch": 0.8144,
      "grad_norm": 37.77107620239258,
      "learning_rate": 0.00017285333333333334,
      "loss": -117.3244,
      "step": 10180
    },
    {
      "epoch": 0.8152,
      "grad_norm": 42.688419342041016,
      "learning_rate": 0.00017282666666666667,
      "loss": -117.143,
      "step": 10190
    },
    {
      "epoch": 0.816,
      "grad_norm": 42.0047607421875,
      "learning_rate": 0.0001728,
      "loss": -116.3286,
      "step": 10200
    },
    {
      "epoch": 0.8168,
      "grad_norm": 35.11088180541992,
      "learning_rate": 0.00017277333333333336,
      "loss": -117.6834,
      "step": 10210
    },
    {
      "epoch": 0.8176,
      "grad_norm": 46.41966247558594,
      "learning_rate": 0.00017274666666666668,
      "loss": -117.7795,
      "step": 10220
    },
    {
      "epoch": 0.8184,
      "grad_norm": 39.221378326416016,
      "learning_rate": 0.00017272,
      "loss": -116.1563,
      "step": 10230
    },
    {
      "epoch": 0.8192,
      "grad_norm": 58.53653335571289,
      "learning_rate": 0.00017269333333333334,
      "loss": -116.3387,
      "step": 10240
    },
    {
      "epoch": 0.82,
      "grad_norm": 87.43763732910156,
      "learning_rate": 0.00017266666666666667,
      "loss": -117.8699,
      "step": 10250
    },
    {
      "epoch": 0.8208,
      "grad_norm": 38.228267669677734,
      "learning_rate": 0.00017264,
      "loss": -117.0538,
      "step": 10260
    },
    {
      "epoch": 0.8216,
      "grad_norm": 42.80567169189453,
      "learning_rate": 0.00017261333333333333,
      "loss": -117.4334,
      "step": 10270
    },
    {
      "epoch": 0.8224,
      "grad_norm": 39.02827072143555,
      "learning_rate": 0.00017258666666666669,
      "loss": -118.1673,
      "step": 10280
    },
    {
      "epoch": 0.8232,
      "grad_norm": 60.59226989746094,
      "learning_rate": 0.00017256000000000001,
      "loss": -116.5187,
      "step": 10290
    },
    {
      "epoch": 0.824,
      "grad_norm": 41.76185607910156,
      "learning_rate": 0.00017253333333333334,
      "loss": -116.8979,
      "step": 10300
    },
    {
      "epoch": 0.8248,
      "grad_norm": 37.19527816772461,
      "learning_rate": 0.00017250666666666667,
      "loss": -115.4639,
      "step": 10310
    },
    {
      "epoch": 0.8256,
      "grad_norm": 45.12327575683594,
      "learning_rate": 0.00017248000000000003,
      "loss": -116.9445,
      "step": 10320
    },
    {
      "epoch": 0.8264,
      "grad_norm": 52.68502426147461,
      "learning_rate": 0.00017245333333333333,
      "loss": -117.5002,
      "step": 10330
    },
    {
      "epoch": 0.8272,
      "grad_norm": 44.341888427734375,
      "learning_rate": 0.00017242666666666666,
      "loss": -116.784,
      "step": 10340
    },
    {
      "epoch": 0.828,
      "grad_norm": 38.46736145019531,
      "learning_rate": 0.00017240000000000002,
      "loss": -116.4557,
      "step": 10350
    },
    {
      "epoch": 0.8288,
      "grad_norm": 40.9367561340332,
      "learning_rate": 0.00017237333333333335,
      "loss": -116.2437,
      "step": 10360
    },
    {
      "epoch": 0.8296,
      "grad_norm": 37.091243743896484,
      "learning_rate": 0.00017234666666666667,
      "loss": -117.0054,
      "step": 10370
    },
    {
      "epoch": 0.8304,
      "grad_norm": 42.10951232910156,
      "learning_rate": 0.00017232,
      "loss": -117.3552,
      "step": 10380
    },
    {
      "epoch": 0.8312,
      "grad_norm": 44.67382049560547,
      "learning_rate": 0.00017229333333333336,
      "loss": -116.5488,
      "step": 10390
    },
    {
      "epoch": 0.832,
      "grad_norm": 46.90599060058594,
      "learning_rate": 0.00017226666666666666,
      "loss": -116.997,
      "step": 10400
    },
    {
      "epoch": 0.8328,
      "grad_norm": 43.2051887512207,
      "learning_rate": 0.00017224,
      "loss": -117.8617,
      "step": 10410
    },
    {
      "epoch": 0.8336,
      "grad_norm": 49.99418258666992,
      "learning_rate": 0.00017221333333333335,
      "loss": -117.4969,
      "step": 10420
    },
    {
      "epoch": 0.8344,
      "grad_norm": 47.406593322753906,
      "learning_rate": 0.00017218666666666668,
      "loss": -118.1681,
      "step": 10430
    },
    {
      "epoch": 0.8352,
      "grad_norm": 49.753013610839844,
      "learning_rate": 0.00017216,
      "loss": -116.1992,
      "step": 10440
    },
    {
      "epoch": 0.836,
      "grad_norm": 49.02772521972656,
      "learning_rate": 0.00017213333333333333,
      "loss": -116.1705,
      "step": 10450
    },
    {
      "epoch": 0.8368,
      "grad_norm": 43.43007278442383,
      "learning_rate": 0.0001721066666666667,
      "loss": -116.5815,
      "step": 10460
    },
    {
      "epoch": 0.8376,
      "grad_norm": 31.58871078491211,
      "learning_rate": 0.00017208000000000002,
      "loss": -117.5287,
      "step": 10470
    },
    {
      "epoch": 0.8384,
      "grad_norm": 49.23271942138672,
      "learning_rate": 0.00017205333333333332,
      "loss": -117.0649,
      "step": 10480
    },
    {
      "epoch": 0.8392,
      "grad_norm": 53.49134826660156,
      "learning_rate": 0.00017202666666666668,
      "loss": -116.4555,
      "step": 10490
    },
    {
      "epoch": 0.84,
      "grad_norm": 49.910499572753906,
      "learning_rate": 0.000172,
      "loss": -116.1321,
      "step": 10500
    },
    {
      "epoch": 0.8408,
      "grad_norm": 44.502891540527344,
      "learning_rate": 0.00017197333333333334,
      "loss": -117.3347,
      "step": 10510
    },
    {
      "epoch": 0.8416,
      "grad_norm": 34.5551872253418,
      "learning_rate": 0.00017194666666666666,
      "loss": -117.7789,
      "step": 10520
    },
    {
      "epoch": 0.8424,
      "grad_norm": 53.7381477355957,
      "learning_rate": 0.00017192000000000002,
      "loss": -116.8215,
      "step": 10530
    },
    {
      "epoch": 0.8432,
      "grad_norm": 41.34278106689453,
      "learning_rate": 0.00017189333333333335,
      "loss": -117.2384,
      "step": 10540
    },
    {
      "epoch": 0.844,
      "grad_norm": 43.545230865478516,
      "learning_rate": 0.00017186666666666665,
      "loss": -117.1495,
      "step": 10550
    },
    {
      "epoch": 0.8448,
      "grad_norm": 41.03671646118164,
      "learning_rate": 0.00017184,
      "loss": -117.6649,
      "step": 10560
    },
    {
      "epoch": 0.8456,
      "grad_norm": 49.4794807434082,
      "learning_rate": 0.00017181333333333334,
      "loss": -116.336,
      "step": 10570
    },
    {
      "epoch": 0.8464,
      "grad_norm": 50.667442321777344,
      "learning_rate": 0.00017178666666666667,
      "loss": -116.4479,
      "step": 10580
    },
    {
      "epoch": 0.8472,
      "grad_norm": 38.7114143371582,
      "learning_rate": 0.00017176000000000002,
      "loss": -116.2674,
      "step": 10590
    },
    {
      "epoch": 0.848,
      "grad_norm": 37.19160842895508,
      "learning_rate": 0.00017173333333333335,
      "loss": -116.617,
      "step": 10600
    },
    {
      "epoch": 0.8488,
      "grad_norm": 44.786376953125,
      "learning_rate": 0.00017170666666666668,
      "loss": -117.2163,
      "step": 10610
    },
    {
      "epoch": 0.8496,
      "grad_norm": 34.55177688598633,
      "learning_rate": 0.00017168,
      "loss": -118.1401,
      "step": 10620
    },
    {
      "epoch": 0.8504,
      "grad_norm": 40.394126892089844,
      "learning_rate": 0.00017165333333333334,
      "loss": -117.2233,
      "step": 10630
    },
    {
      "epoch": 0.8512,
      "grad_norm": 40.517032623291016,
      "learning_rate": 0.00017162666666666667,
      "loss": -117.2076,
      "step": 10640
    },
    {
      "epoch": 0.852,
      "grad_norm": 47.97421646118164,
      "learning_rate": 0.0001716,
      "loss": -116.997,
      "step": 10650
    },
    {
      "epoch": 0.8528,
      "grad_norm": 76.55226135253906,
      "learning_rate": 0.00017157333333333335,
      "loss": -115.8859,
      "step": 10660
    },
    {
      "epoch": 0.8536,
      "grad_norm": 43.8157844543457,
      "learning_rate": 0.00017154666666666668,
      "loss": -116.2482,
      "step": 10670
    },
    {
      "epoch": 0.8544,
      "grad_norm": 45.0828857421875,
      "learning_rate": 0.00017152,
      "loss": -117.9285,
      "step": 10680
    },
    {
      "epoch": 0.8552,
      "grad_norm": 55.63522720336914,
      "learning_rate": 0.00017149333333333334,
      "loss": -117.4379,
      "step": 10690
    },
    {
      "epoch": 0.856,
      "grad_norm": 39.49064254760742,
      "learning_rate": 0.00017146666666666667,
      "loss": -117.9574,
      "step": 10700
    },
    {
      "epoch": 0.8568,
      "grad_norm": 50.40845489501953,
      "learning_rate": 0.00017144,
      "loss": -117.0945,
      "step": 10710
    },
    {
      "epoch": 0.8576,
      "grad_norm": 40.5634765625,
      "learning_rate": 0.00017141333333333333,
      "loss": -116.5181,
      "step": 10720
    },
    {
      "epoch": 0.8584,
      "grad_norm": 39.477516174316406,
      "learning_rate": 0.00017138666666666668,
      "loss": -116.8594,
      "step": 10730
    },
    {
      "epoch": 0.8592,
      "grad_norm": 36.81827926635742,
      "learning_rate": 0.00017136,
      "loss": -117.475,
      "step": 10740
    },
    {
      "epoch": 0.86,
      "grad_norm": 44.508724212646484,
      "learning_rate": 0.00017133333333333334,
      "loss": -116.8335,
      "step": 10750
    },
    {
      "epoch": 0.8608,
      "grad_norm": 62.364986419677734,
      "learning_rate": 0.00017130666666666667,
      "loss": -117.4392,
      "step": 10760
    },
    {
      "epoch": 0.8616,
      "grad_norm": 43.73081588745117,
      "learning_rate": 0.00017128000000000003,
      "loss": -117.0599,
      "step": 10770
    },
    {
      "epoch": 0.8624,
      "grad_norm": 66.94947052001953,
      "learning_rate": 0.00017125333333333333,
      "loss": -116.784,
      "step": 10780
    },
    {
      "epoch": 0.8632,
      "grad_norm": 52.402645111083984,
      "learning_rate": 0.00017122666666666666,
      "loss": -117.4685,
      "step": 10790
    },
    {
      "epoch": 0.864,
      "grad_norm": 54.24318313598633,
      "learning_rate": 0.00017120000000000001,
      "loss": -116.8503,
      "step": 10800
    },
    {
      "epoch": 0.8648,
      "grad_norm": 32.41240310668945,
      "learning_rate": 0.00017117333333333334,
      "loss": -115.5198,
      "step": 10810
    },
    {
      "epoch": 0.8656,
      "grad_norm": 60.55221939086914,
      "learning_rate": 0.00017114666666666667,
      "loss": -117.6249,
      "step": 10820
    },
    {
      "epoch": 0.8664,
      "grad_norm": 45.93354415893555,
      "learning_rate": 0.00017112,
      "loss": -118.1,
      "step": 10830
    },
    {
      "epoch": 0.8672,
      "grad_norm": 32.65840148925781,
      "learning_rate": 0.00017109333333333336,
      "loss": -118.149,
      "step": 10840
    },
    {
      "epoch": 0.868,
      "grad_norm": 46.73919677734375,
      "learning_rate": 0.00017106666666666666,
      "loss": -117.4324,
      "step": 10850
    },
    {
      "epoch": 0.8688,
      "grad_norm": 68.0079116821289,
      "learning_rate": 0.00017104,
      "loss": -116.2782,
      "step": 10860
    },
    {
      "epoch": 0.8696,
      "grad_norm": 40.47300720214844,
      "learning_rate": 0.00017101333333333334,
      "loss": -116.9724,
      "step": 10870
    },
    {
      "epoch": 0.8704,
      "grad_norm": 45.03017044067383,
      "learning_rate": 0.00017098666666666667,
      "loss": -118.548,
      "step": 10880
    },
    {
      "epoch": 0.8712,
      "grad_norm": 54.10895919799805,
      "learning_rate": 0.00017096,
      "loss": -117.3672,
      "step": 10890
    },
    {
      "epoch": 0.872,
      "grad_norm": 52.7243537902832,
      "learning_rate": 0.00017093333333333333,
      "loss": -116.5961,
      "step": 10900
    },
    {
      "epoch": 0.8728,
      "grad_norm": 57.79999923706055,
      "learning_rate": 0.0001709066666666667,
      "loss": -116.2746,
      "step": 10910
    },
    {
      "epoch": 0.8736,
      "grad_norm": 40.860260009765625,
      "learning_rate": 0.00017088000000000002,
      "loss": -116.9371,
      "step": 10920
    },
    {
      "epoch": 0.8744,
      "grad_norm": 46.69642639160156,
      "learning_rate": 0.00017085333333333332,
      "loss": -116.1149,
      "step": 10930
    },
    {
      "epoch": 0.8752,
      "grad_norm": 53.12354278564453,
      "learning_rate": 0.00017082666666666668,
      "loss": -116.8472,
      "step": 10940
    },
    {
      "epoch": 0.876,
      "grad_norm": 47.51912307739258,
      "learning_rate": 0.0001708,
      "loss": -117.569,
      "step": 10950
    },
    {
      "epoch": 0.8768,
      "grad_norm": 42.66727066040039,
      "learning_rate": 0.00017077333333333333,
      "loss": -116.5379,
      "step": 10960
    },
    {
      "epoch": 0.8776,
      "grad_norm": 78.94515228271484,
      "learning_rate": 0.0001707466666666667,
      "loss": -115.5433,
      "step": 10970
    },
    {
      "epoch": 0.8784,
      "grad_norm": 79.12973022460938,
      "learning_rate": 0.00017072000000000002,
      "loss": -116.6542,
      "step": 10980
    },
    {
      "epoch": 0.8792,
      "grad_norm": 66.27777862548828,
      "learning_rate": 0.00017069333333333335,
      "loss": -116.4489,
      "step": 10990
    },
    {
      "epoch": 0.88,
      "grad_norm": 38.99708938598633,
      "learning_rate": 0.00017066666666666668,
      "loss": -117.8698,
      "step": 11000
    },
    {
      "epoch": 0.8808,
      "grad_norm": 58.28094482421875,
      "learning_rate": 0.00017064,
      "loss": -118.0992,
      "step": 11010
    },
    {
      "epoch": 0.8816,
      "grad_norm": 33.74074172973633,
      "learning_rate": 0.00017061333333333334,
      "loss": -118.4569,
      "step": 11020
    },
    {
      "epoch": 0.8824,
      "grad_norm": 56.2979621887207,
      "learning_rate": 0.00017058666666666666,
      "loss": -117.1643,
      "step": 11030
    },
    {
      "epoch": 0.8832,
      "grad_norm": 73.16983032226562,
      "learning_rate": 0.00017056000000000002,
      "loss": -117.9697,
      "step": 11040
    },
    {
      "epoch": 0.884,
      "grad_norm": 2345.68310546875,
      "learning_rate": 0.00017053333333333335,
      "loss": -117.2909,
      "step": 11050
    },
    {
      "epoch": 0.8848,
      "grad_norm": 51.043251037597656,
      "learning_rate": 0.00017050666666666668,
      "loss": -116.749,
      "step": 11060
    },
    {
      "epoch": 0.8856,
      "grad_norm": 49.97084426879883,
      "learning_rate": 0.00017048,
      "loss": -116.8081,
      "step": 11070
    },
    {
      "epoch": 0.8864,
      "grad_norm": 84.06050109863281,
      "learning_rate": 0.00017045333333333334,
      "loss": -115.6229,
      "step": 11080
    },
    {
      "epoch": 0.8872,
      "grad_norm": 64.27342987060547,
      "learning_rate": 0.00017042666666666667,
      "loss": -117.589,
      "step": 11090
    },
    {
      "epoch": 0.888,
      "grad_norm": 43.46214294433594,
      "learning_rate": 0.0001704,
      "loss": -116.3023,
      "step": 11100
    },
    {
      "epoch": 0.8888,
      "grad_norm": 49.09113693237305,
      "learning_rate": 0.00017037333333333335,
      "loss": -116.7576,
      "step": 11110
    },
    {
      "epoch": 0.8896,
      "grad_norm": 44.83989715576172,
      "learning_rate": 0.00017034666666666668,
      "loss": -117.6646,
      "step": 11120
    },
    {
      "epoch": 0.8904,
      "grad_norm": 36.398983001708984,
      "learning_rate": 0.00017032,
      "loss": -117.3703,
      "step": 11130
    },
    {
      "epoch": 0.8912,
      "grad_norm": 64.61691284179688,
      "learning_rate": 0.00017029333333333334,
      "loss": -116.1187,
      "step": 11140
    },
    {
      "epoch": 0.892,
      "grad_norm": 52.2093391418457,
      "learning_rate": 0.0001702666666666667,
      "loss": -116.9979,
      "step": 11150
    },
    {
      "epoch": 0.8928,
      "grad_norm": 95.13056182861328,
      "learning_rate": 0.00017024,
      "loss": -116.2487,
      "step": 11160
    },
    {
      "epoch": 0.8936,
      "grad_norm": 106.93550872802734,
      "learning_rate": 0.00017021333333333333,
      "loss": -117.1441,
      "step": 11170
    },
    {
      "epoch": 0.8944,
      "grad_norm": 53.875919342041016,
      "learning_rate": 0.00017018666666666668,
      "loss": -117.663,
      "step": 11180
    },
    {
      "epoch": 0.8952,
      "grad_norm": 59.16569900512695,
      "learning_rate": 0.00017016,
      "loss": -116.8862,
      "step": 11190
    },
    {
      "epoch": 0.896,
      "grad_norm": 35.7396125793457,
      "learning_rate": 0.00017013333333333334,
      "loss": -118.0181,
      "step": 11200
    },
    {
      "epoch": 0.8968,
      "grad_norm": 60.68769073486328,
      "learning_rate": 0.00017010666666666667,
      "loss": -117.0843,
      "step": 11210
    },
    {
      "epoch": 0.8976,
      "grad_norm": 69.48706817626953,
      "learning_rate": 0.00017008000000000002,
      "loss": -116.4234,
      "step": 11220
    },
    {
      "epoch": 0.8984,
      "grad_norm": 53.78166198730469,
      "learning_rate": 0.00017005333333333333,
      "loss": -116.5045,
      "step": 11230
    },
    {
      "epoch": 0.8992,
      "grad_norm": 75.88245391845703,
      "learning_rate": 0.00017002666666666666,
      "loss": -117.0709,
      "step": 11240
    },
    {
      "epoch": 0.9,
      "grad_norm": 53.00850296020508,
      "learning_rate": 0.00017,
      "loss": -117.6511,
      "step": 11250
    },
    {
      "epoch": 0.9008,
      "grad_norm": 129.80845642089844,
      "learning_rate": 0.00016997333333333334,
      "loss": -117.225,
      "step": 11260
    },
    {
      "epoch": 0.9016,
      "grad_norm": 66.79264068603516,
      "learning_rate": 0.00016994666666666667,
      "loss": -116.9115,
      "step": 11270
    },
    {
      "epoch": 0.9024,
      "grad_norm": 50.26638412475586,
      "learning_rate": 0.00016992,
      "loss": -115.9179,
      "step": 11280
    },
    {
      "epoch": 0.9032,
      "grad_norm": 44.79767608642578,
      "learning_rate": 0.00016989333333333336,
      "loss": -117.7906,
      "step": 11290
    },
    {
      "epoch": 0.904,
      "grad_norm": 98.13970947265625,
      "learning_rate": 0.00016986666666666668,
      "loss": -117.3611,
      "step": 11300
    },
    {
      "epoch": 0.9048,
      "grad_norm": 50.60435485839844,
      "learning_rate": 0.00016984,
      "loss": -117.1874,
      "step": 11310
    },
    {
      "epoch": 0.9056,
      "grad_norm": 44.26810836791992,
      "learning_rate": 0.00016981333333333334,
      "loss": -117.956,
      "step": 11320
    },
    {
      "epoch": 0.9064,
      "grad_norm": 78.46216583251953,
      "learning_rate": 0.00016978666666666667,
      "loss": -116.0973,
      "step": 11330
    },
    {
      "epoch": 0.9072,
      "grad_norm": 31.51936149597168,
      "learning_rate": 0.00016976,
      "loss": -117.7792,
      "step": 11340
    },
    {
      "epoch": 0.908,
      "grad_norm": 65.59077453613281,
      "learning_rate": 0.00016973333333333336,
      "loss": -117.0417,
      "step": 11350
    },
    {
      "epoch": 0.9088,
      "grad_norm": 92.92515563964844,
      "learning_rate": 0.00016970666666666669,
      "loss": -117.3844,
      "step": 11360
    },
    {
      "epoch": 0.9096,
      "grad_norm": 93.76172637939453,
      "learning_rate": 0.00016968000000000002,
      "loss": -116.4693,
      "step": 11370
    },
    {
      "epoch": 0.9104,
      "grad_norm": 57.56781005859375,
      "learning_rate": 0.00016965333333333332,
      "loss": -117.3151,
      "step": 11380
    },
    {
      "epoch": 0.9112,
      "grad_norm": 88.88276672363281,
      "learning_rate": 0.00016962666666666667,
      "loss": -117.8213,
      "step": 11390
    },
    {
      "epoch": 0.912,
      "grad_norm": 41.182071685791016,
      "learning_rate": 0.0001696,
      "loss": -117.2842,
      "step": 11400
    },
    {
      "epoch": 0.9128,
      "grad_norm": 89.75202941894531,
      "learning_rate": 0.00016957333333333333,
      "loss": -117.8226,
      "step": 11410
    },
    {
      "epoch": 0.9136,
      "grad_norm": 79.86038208007812,
      "learning_rate": 0.0001695466666666667,
      "loss": -116.5803,
      "step": 11420
    },
    {
      "epoch": 0.9144,
      "grad_norm": 38.13020706176758,
      "learning_rate": 0.00016952000000000002,
      "loss": -117.9943,
      "step": 11430
    },
    {
      "epoch": 0.9152,
      "grad_norm": 47.87346649169922,
      "learning_rate": 0.00016949333333333335,
      "loss": -117.9595,
      "step": 11440
    },
    {
      "epoch": 0.916,
      "grad_norm": 53.99599838256836,
      "learning_rate": 0.00016946666666666667,
      "loss": -117.7559,
      "step": 11450
    },
    {
      "epoch": 0.9168,
      "grad_norm": 58.622257232666016,
      "learning_rate": 0.00016944,
      "loss": -115.8508,
      "step": 11460
    },
    {
      "epoch": 0.9176,
      "grad_norm": 47.04182052612305,
      "learning_rate": 0.00016941333333333333,
      "loss": -115.8703,
      "step": 11470
    },
    {
      "epoch": 0.9184,
      "grad_norm": 39.412559509277344,
      "learning_rate": 0.00016938666666666666,
      "loss": -116.5477,
      "step": 11480
    },
    {
      "epoch": 0.9192,
      "grad_norm": 68.67630767822266,
      "learning_rate": 0.00016936000000000002,
      "loss": -116.9294,
      "step": 11490
    },
    {
      "epoch": 0.92,
      "grad_norm": 87.48612976074219,
      "learning_rate": 0.00016933333333333335,
      "loss": -117.8866,
      "step": 11500
    },
    {
      "epoch": 0.9208,
      "grad_norm": 65.42521667480469,
      "learning_rate": 0.00016930666666666668,
      "loss": -117.9344,
      "step": 11510
    },
    {
      "epoch": 0.9216,
      "grad_norm": 94.50769805908203,
      "learning_rate": 0.00016928,
      "loss": -115.4215,
      "step": 11520
    },
    {
      "epoch": 0.9224,
      "grad_norm": 57.769710540771484,
      "learning_rate": 0.00016925333333333333,
      "loss": -116.5389,
      "step": 11530
    },
    {
      "epoch": 0.9232,
      "grad_norm": 67.38834381103516,
      "learning_rate": 0.00016922666666666666,
      "loss": -115.7775,
      "step": 11540
    },
    {
      "epoch": 0.924,
      "grad_norm": 96.54400634765625,
      "learning_rate": 0.0001692,
      "loss": -116.6125,
      "step": 11550
    },
    {
      "epoch": 0.9248,
      "grad_norm": 55.41862106323242,
      "learning_rate": 0.00016917333333333335,
      "loss": -118.0456,
      "step": 11560
    },
    {
      "epoch": 0.9256,
      "grad_norm": 62.771636962890625,
      "learning_rate": 0.00016914666666666668,
      "loss": -117.1667,
      "step": 11570
    },
    {
      "epoch": 0.9264,
      "grad_norm": 50.08879852294922,
      "learning_rate": 0.00016912,
      "loss": -118.1164,
      "step": 11580
    },
    {
      "epoch": 0.9272,
      "grad_norm": 65.00941467285156,
      "learning_rate": 0.00016909333333333334,
      "loss": -118.5503,
      "step": 11590
    },
    {
      "epoch": 0.928,
      "grad_norm": 85.84394073486328,
      "learning_rate": 0.0001690666666666667,
      "loss": -117.5815,
      "step": 11600
    },
    {
      "epoch": 0.9288,
      "grad_norm": 53.24177551269531,
      "learning_rate": 0.00016904,
      "loss": -117.4457,
      "step": 11610
    },
    {
      "epoch": 0.9296,
      "grad_norm": 61.08743667602539,
      "learning_rate": 0.00016901333333333332,
      "loss": -117.5816,
      "step": 11620
    },
    {
      "epoch": 0.9304,
      "grad_norm": 81.92003631591797,
      "learning_rate": 0.00016898666666666668,
      "loss": -116.8873,
      "step": 11630
    },
    {
      "epoch": 0.9312,
      "grad_norm": 90.92742919921875,
      "learning_rate": 0.00016896,
      "loss": -116.7108,
      "step": 11640
    },
    {
      "epoch": 0.932,
      "grad_norm": 48.70656967163086,
      "learning_rate": 0.00016893333333333334,
      "loss": -117.1813,
      "step": 11650
    },
    {
      "epoch": 0.9328,
      "grad_norm": 42.55426025390625,
      "learning_rate": 0.00016890666666666667,
      "loss": -116.9715,
      "step": 11660
    },
    {
      "epoch": 0.9336,
      "grad_norm": 60.878662109375,
      "learning_rate": 0.00016888000000000002,
      "loss": -116.6686,
      "step": 11670
    },
    {
      "epoch": 0.9344,
      "grad_norm": 60.07355880737305,
      "learning_rate": 0.00016885333333333335,
      "loss": -117.0254,
      "step": 11680
    },
    {
      "epoch": 0.9352,
      "grad_norm": 89.84535217285156,
      "learning_rate": 0.00016882666666666665,
      "loss": -116.4059,
      "step": 11690
    },
    {
      "epoch": 0.936,
      "grad_norm": 62.63066482543945,
      "learning_rate": 0.0001688,
      "loss": -117.2381,
      "step": 11700
    },
    {
      "epoch": 0.9368,
      "grad_norm": 68.66329956054688,
      "learning_rate": 0.00016877333333333334,
      "loss": -117.5925,
      "step": 11710
    },
    {
      "epoch": 0.9376,
      "grad_norm": 100.86043548583984,
      "learning_rate": 0.00016874666666666667,
      "loss": -116.4201,
      "step": 11720
    },
    {
      "epoch": 0.9384,
      "grad_norm": 97.3386001586914,
      "learning_rate": 0.00016872000000000002,
      "loss": -116.3398,
      "step": 11730
    },
    {
      "epoch": 0.9392,
      "grad_norm": 91.05880737304688,
      "learning_rate": 0.00016869333333333335,
      "loss": -117.6229,
      "step": 11740
    },
    {
      "epoch": 0.94,
      "grad_norm": 103.01835632324219,
      "learning_rate": 0.00016866666666666668,
      "loss": -116.9775,
      "step": 11750
    },
    {
      "epoch": 0.9408,
      "grad_norm": 48.371612548828125,
      "learning_rate": 0.00016863999999999998,
      "loss": -116.3242,
      "step": 11760
    },
    {
      "epoch": 0.9416,
      "grad_norm": 126.0789794921875,
      "learning_rate": 0.00016861333333333334,
      "loss": -116.5924,
      "step": 11770
    },
    {
      "epoch": 0.9424,
      "grad_norm": 95.73176574707031,
      "learning_rate": 0.00016858666666666667,
      "loss": -116.8206,
      "step": 11780
    },
    {
      "epoch": 0.9432,
      "grad_norm": 92.86315155029297,
      "learning_rate": 0.00016856,
      "loss": -116.7017,
      "step": 11790
    },
    {
      "epoch": 0.944,
      "grad_norm": 150.93795776367188,
      "learning_rate": 0.00016853333333333336,
      "loss": -116.8603,
      "step": 11800
    },
    {
      "epoch": 0.9448,
      "grad_norm": 67.49483489990234,
      "learning_rate": 0.00016850666666666668,
      "loss": -117.1851,
      "step": 11810
    },
    {
      "epoch": 0.9456,
      "grad_norm": 59.44088363647461,
      "learning_rate": 0.00016848,
      "loss": -117.1036,
      "step": 11820
    },
    {
      "epoch": 0.9464,
      "grad_norm": 70.77653503417969,
      "learning_rate": 0.00016845333333333334,
      "loss": -116.9822,
      "step": 11830
    },
    {
      "epoch": 0.9472,
      "grad_norm": 78.12226867675781,
      "learning_rate": 0.00016842666666666667,
      "loss": -116.9894,
      "step": 11840
    },
    {
      "epoch": 0.948,
      "grad_norm": 58.28801727294922,
      "learning_rate": 0.0001684,
      "loss": -117.5537,
      "step": 11850
    },
    {
      "epoch": 0.9488,
      "grad_norm": 57.58671569824219,
      "learning_rate": 0.00016837333333333333,
      "loss": -116.5528,
      "step": 11860
    },
    {
      "epoch": 0.9496,
      "grad_norm": 59.427703857421875,
      "learning_rate": 0.00016834666666666669,
      "loss": -116.8795,
      "step": 11870
    },
    {
      "epoch": 0.9504,
      "grad_norm": 53.4565544128418,
      "learning_rate": 0.00016832000000000001,
      "loss": -117.6356,
      "step": 11880
    },
    {
      "epoch": 0.9512,
      "grad_norm": 60.467281341552734,
      "learning_rate": 0.00016829333333333334,
      "loss": -115.928,
      "step": 11890
    },
    {
      "epoch": 0.952,
      "grad_norm": 67.98981475830078,
      "learning_rate": 0.00016826666666666667,
      "loss": -115.659,
      "step": 11900
    },
    {
      "epoch": 0.9528,
      "grad_norm": 94.52641296386719,
      "learning_rate": 0.00016824,
      "loss": -116.5728,
      "step": 11910
    },
    {
      "epoch": 0.9536,
      "grad_norm": 98.77193450927734,
      "learning_rate": 0.00016821333333333333,
      "loss": -116.9253,
      "step": 11920
    },
    {
      "epoch": 0.9544,
      "grad_norm": 75.30245208740234,
      "learning_rate": 0.00016818666666666666,
      "loss": -115.8313,
      "step": 11930
    },
    {
      "epoch": 0.9552,
      "grad_norm": 102.65462493896484,
      "learning_rate": 0.00016816000000000002,
      "loss": -116.5553,
      "step": 11940
    },
    {
      "epoch": 0.956,
      "grad_norm": 56.96399688720703,
      "learning_rate": 0.00016813333333333335,
      "loss": -117.7242,
      "step": 11950
    },
    {
      "epoch": 0.9568,
      "grad_norm": 174.66162109375,
      "learning_rate": 0.00016810666666666667,
      "loss": -115.0269,
      "step": 11960
    },
    {
      "epoch": 0.9576,
      "grad_norm": 282.7590026855469,
      "learning_rate": 0.00016808,
      "loss": -116.7868,
      "step": 11970
    },
    {
      "epoch": 0.9584,
      "grad_norm": 109.38117218017578,
      "learning_rate": 0.00016805333333333336,
      "loss": -116.1076,
      "step": 11980
    },
    {
      "epoch": 0.9592,
      "grad_norm": 93.59733581542969,
      "learning_rate": 0.00016802666666666666,
      "loss": -117.6869,
      "step": 11990
    },
    {
      "epoch": 0.96,
      "grad_norm": 123.62594604492188,
      "learning_rate": 0.000168,
      "loss": -116.7614,
      "step": 12000
    },
    {
      "epoch": 0.9608,
      "grad_norm": 130.962158203125,
      "learning_rate": 0.00016797333333333335,
      "loss": -116.4338,
      "step": 12010
    },
    {
      "epoch": 0.9616,
      "grad_norm": 86.5330581665039,
      "learning_rate": 0.00016794666666666668,
      "loss": -115.407,
      "step": 12020
    },
    {
      "epoch": 0.9624,
      "grad_norm": 78.27218627929688,
      "learning_rate": 0.00016792,
      "loss": -118.2721,
      "step": 12030
    },
    {
      "epoch": 0.9632,
      "grad_norm": 55.548583984375,
      "learning_rate": 0.00016789333333333333,
      "loss": -116.7706,
      "step": 12040
    },
    {
      "epoch": 0.964,
      "grad_norm": 118.6967544555664,
      "learning_rate": 0.0001678666666666667,
      "loss": -115.772,
      "step": 12050
    },
    {
      "epoch": 0.9648,
      "grad_norm": 73.47734069824219,
      "learning_rate": 0.00016784,
      "loss": -116.682,
      "step": 12060
    },
    {
      "epoch": 0.9656,
      "grad_norm": 313.8199462890625,
      "learning_rate": 0.00016781333333333332,
      "loss": -116.7422,
      "step": 12070
    },
    {
      "epoch": 0.9664,
      "grad_norm": 73.9266357421875,
      "learning_rate": 0.00016778666666666668,
      "loss": -116.4822,
      "step": 12080
    },
    {
      "epoch": 0.9672,
      "grad_norm": 55.15097427368164,
      "learning_rate": 0.00016776,
      "loss": -116.2024,
      "step": 12090
    },
    {
      "epoch": 0.968,
      "grad_norm": 67.08422088623047,
      "learning_rate": 0.00016773333333333334,
      "loss": -116.0222,
      "step": 12100
    },
    {
      "epoch": 0.9688,
      "grad_norm": 55.75651168823242,
      "learning_rate": 0.0001677066666666667,
      "loss": -116.8702,
      "step": 12110
    },
    {
      "epoch": 0.9696,
      "grad_norm": 67.22200775146484,
      "learning_rate": 0.00016768000000000002,
      "loss": -117.4585,
      "step": 12120
    },
    {
      "epoch": 0.9704,
      "grad_norm": 54.228614807128906,
      "learning_rate": 0.00016765333333333335,
      "loss": -116.8407,
      "step": 12130
    },
    {
      "epoch": 0.9712,
      "grad_norm": 73.72709655761719,
      "learning_rate": 0.00016762666666666665,
      "loss": -117.1802,
      "step": 12140
    },
    {
      "epoch": 0.972,
      "grad_norm": 111.2944564819336,
      "learning_rate": 0.0001676,
      "loss": -117.1577,
      "step": 12150
    },
    {
      "epoch": 0.9728,
      "grad_norm": 102.09548950195312,
      "learning_rate": 0.00016757333333333334,
      "loss": -116.9526,
      "step": 12160
    },
    {
      "epoch": 0.9736,
      "grad_norm": 54.8115348815918,
      "learning_rate": 0.00016754666666666667,
      "loss": -117.2896,
      "step": 12170
    },
    {
      "epoch": 0.9744,
      "grad_norm": 63.34724807739258,
      "learning_rate": 0.00016752000000000002,
      "loss": -116.6205,
      "step": 12180
    },
    {
      "epoch": 0.9752,
      "grad_norm": 102.85255432128906,
      "learning_rate": 0.00016749333333333335,
      "loss": -115.8241,
      "step": 12190
    },
    {
      "epoch": 0.976,
      "grad_norm": 153.78504943847656,
      "learning_rate": 0.00016746666666666668,
      "loss": -116.9328,
      "step": 12200
    },
    {
      "epoch": 0.9768,
      "grad_norm": 96.98687744140625,
      "learning_rate": 0.00016744,
      "loss": -117.1988,
      "step": 12210
    },
    {
      "epoch": 0.9776,
      "grad_norm": 53.04060745239258,
      "learning_rate": 0.00016741333333333334,
      "loss": -117.3927,
      "step": 12220
    },
    {
      "epoch": 0.9784,
      "grad_norm": 120.947265625,
      "learning_rate": 0.00016738666666666667,
      "loss": -118.79,
      "step": 12230
    },
    {
      "epoch": 0.9792,
      "grad_norm": 63.88637924194336,
      "learning_rate": 0.00016736,
      "loss": -118.1137,
      "step": 12240
    },
    {
      "epoch": 0.98,
      "grad_norm": 79.90894317626953,
      "learning_rate": 0.00016733333333333335,
      "loss": -117.0547,
      "step": 12250
    },
    {
      "epoch": 0.9808,
      "grad_norm": 89.6541748046875,
      "learning_rate": 0.00016730666666666668,
      "loss": -117.8245,
      "step": 12260
    },
    {
      "epoch": 0.9816,
      "grad_norm": 90.37093353271484,
      "learning_rate": 0.00016728,
      "loss": -117.3316,
      "step": 12270
    },
    {
      "epoch": 0.9824,
      "grad_norm": 109.24824523925781,
      "learning_rate": 0.00016725333333333334,
      "loss": -116.7387,
      "step": 12280
    },
    {
      "epoch": 0.9832,
      "grad_norm": 109.61309814453125,
      "learning_rate": 0.00016722666666666667,
      "loss": -115.8086,
      "step": 12290
    },
    {
      "epoch": 0.984,
      "grad_norm": 94.31756591796875,
      "learning_rate": 0.0001672,
      "loss": -117.0687,
      "step": 12300
    },
    {
      "epoch": 0.9848,
      "grad_norm": 54.325374603271484,
      "learning_rate": 0.00016717333333333333,
      "loss": -117.6413,
      "step": 12310
    },
    {
      "epoch": 0.9856,
      "grad_norm": 61.49561309814453,
      "learning_rate": 0.00016714666666666668,
      "loss": -116.8855,
      "step": 12320
    },
    {
      "epoch": 0.9864,
      "grad_norm": 75.02899169921875,
      "learning_rate": 0.00016712,
      "loss": -116.7139,
      "step": 12330
    },
    {
      "epoch": 0.9872,
      "grad_norm": 77.19142150878906,
      "learning_rate": 0.00016709333333333334,
      "loss": -115.8201,
      "step": 12340
    },
    {
      "epoch": 0.988,
      "grad_norm": 69.97565460205078,
      "learning_rate": 0.00016706666666666667,
      "loss": -116.5596,
      "step": 12350
    },
    {
      "epoch": 0.9888,
      "grad_norm": 275.8172607421875,
      "learning_rate": 0.00016704000000000003,
      "loss": -111.323,
      "step": 12360
    },
    {
      "epoch": 0.9896,
      "grad_norm": 235.21502685546875,
      "learning_rate": 0.00016701333333333333,
      "loss": -114.5424,
      "step": 12370
    },
    {
      "epoch": 0.9904,
      "grad_norm": 87.40684509277344,
      "learning_rate": 0.00016698666666666666,
      "loss": -114.9749,
      "step": 12380
    },
    {
      "epoch": 0.9912,
      "grad_norm": 123.42211151123047,
      "learning_rate": 0.00016696000000000001,
      "loss": -115.4471,
      "step": 12390
    },
    {
      "epoch": 0.992,
      "grad_norm": 76.83316040039062,
      "learning_rate": 0.00016693333333333334,
      "loss": -115.2606,
      "step": 12400
    },
    {
      "epoch": 0.9928,
      "grad_norm": 63.09186553955078,
      "learning_rate": 0.00016690666666666667,
      "loss": -115.8853,
      "step": 12410
    },
    {
      "epoch": 0.9936,
      "grad_norm": 60.785831451416016,
      "learning_rate": 0.00016688,
      "loss": -115.6949,
      "step": 12420
    },
    {
      "epoch": 0.9944,
      "grad_norm": 97.13720703125,
      "learning_rate": 0.00016685333333333336,
      "loss": -115.1071,
      "step": 12430
    },
    {
      "epoch": 0.9952,
      "grad_norm": 67.03722381591797,
      "learning_rate": 0.00016682666666666666,
      "loss": -117.1334,
      "step": 12440
    },
    {
      "epoch": 0.996,
      "grad_norm": 54.17840576171875,
      "learning_rate": 0.0001668,
      "loss": -116.3746,
      "step": 12450
    },
    {
      "epoch": 0.9968,
      "grad_norm": 43.18458557128906,
      "learning_rate": 0.00016677333333333334,
      "loss": -116.0513,
      "step": 12460
    },
    {
      "epoch": 0.9976,
      "grad_norm": 49.142059326171875,
      "learning_rate": 0.00016674666666666667,
      "loss": -116.3116,
      "step": 12470
    },
    {
      "epoch": 0.9984,
      "grad_norm": 41.380279541015625,
      "learning_rate": 0.00016672,
      "loss": -114.7757,
      "step": 12480
    },
    {
      "epoch": 0.9992,
      "grad_norm": 61.18212127685547,
      "learning_rate": 0.00016669333333333336,
      "loss": -116.3841,
      "step": 12490
    },
    {
      "epoch": 1.0,
      "grad_norm": 69.9844970703125,
      "learning_rate": 0.0001666666666666667,
      "loss": -115.4973,
      "step": 12500
    },
    {
      "epoch": 1.0008,
      "grad_norm": 57.444488525390625,
      "learning_rate": 0.00016664000000000002,
      "loss": -116.3631,
      "step": 12510
    },
    {
      "epoch": 1.0016,
      "grad_norm": 70.29529571533203,
      "learning_rate": 0.00016661333333333332,
      "loss": -115.0418,
      "step": 12520
    },
    {
      "epoch": 1.0024,
      "grad_norm": 106.41748046875,
      "learning_rate": 0.00016658666666666668,
      "loss": -114.8772,
      "step": 12530
    },
    {
      "epoch": 1.0032,
      "grad_norm": 124.3656234741211,
      "learning_rate": 0.00016656,
      "loss": -114.6076,
      "step": 12540
    },
    {
      "epoch": 1.004,
      "grad_norm": 125.31057739257812,
      "learning_rate": 0.00016653333333333333,
      "loss": -114.167,
      "step": 12550
    },
    {
      "epoch": 1.0048,
      "grad_norm": 66.61286926269531,
      "learning_rate": 0.0001665066666666667,
      "loss": -115.1909,
      "step": 12560
    },
    {
      "epoch": 1.0056,
      "grad_norm": 79.3866958618164,
      "learning_rate": 0.00016648000000000002,
      "loss": -114.314,
      "step": 12570
    },
    {
      "epoch": 1.0064,
      "grad_norm": 59.62410354614258,
      "learning_rate": 0.00016645333333333335,
      "loss": -115.7286,
      "step": 12580
    },
    {
      "epoch": 1.0072,
      "grad_norm": 155.0753173828125,
      "learning_rate": 0.00016642666666666668,
      "loss": -115.4274,
      "step": 12590
    },
    {
      "epoch": 1.008,
      "grad_norm": 1971.216552734375,
      "learning_rate": 0.0001664,
      "loss": -115.3993,
      "step": 12600
    },
    {
      "epoch": 1.0088,
      "grad_norm": 177.792724609375,
      "learning_rate": 0.00016637333333333334,
      "loss": -115.0372,
      "step": 12610
    },
    {
      "epoch": 1.0096,
      "grad_norm": 74.5004653930664,
      "learning_rate": 0.00016634666666666666,
      "loss": -114.2532,
      "step": 12620
    },
    {
      "epoch": 1.0104,
      "grad_norm": 122.3790512084961,
      "learning_rate": 0.00016632000000000002,
      "loss": -114.0865,
      "step": 12630
    },
    {
      "epoch": 1.0112,
      "grad_norm": 65.56031036376953,
      "learning_rate": 0.00016629333333333335,
      "loss": -115.3709,
      "step": 12640
    },
    {
      "epoch": 1.012,
      "grad_norm": 68.56060791015625,
      "learning_rate": 0.00016626666666666668,
      "loss": -114.9609,
      "step": 12650
    },
    {
      "epoch": 1.0128,
      "grad_norm": 51.92560958862305,
      "learning_rate": 0.00016624,
      "loss": -114.6326,
      "step": 12660
    },
    {
      "epoch": 1.0136,
      "grad_norm": 47.39127731323242,
      "learning_rate": 0.00016621333333333334,
      "loss": -115.8666,
      "step": 12670
    },
    {
      "epoch": 1.0144,
      "grad_norm": 66.92921447753906,
      "learning_rate": 0.00016618666666666667,
      "loss": -114.5521,
      "step": 12680
    },
    {
      "epoch": 1.0152,
      "grad_norm": 62.572410583496094,
      "learning_rate": 0.00016616,
      "loss": -114.6716,
      "step": 12690
    },
    {
      "epoch": 1.016,
      "grad_norm": 69.18968200683594,
      "learning_rate": 0.00016613333333333335,
      "loss": -115.4283,
      "step": 12700
    },
    {
      "epoch": 1.0168,
      "grad_norm": 60.560245513916016,
      "learning_rate": 0.00016610666666666668,
      "loss": -115.2442,
      "step": 12710
    },
    {
      "epoch": 1.0176,
      "grad_norm": 63.34351348876953,
      "learning_rate": 0.00016608,
      "loss": -115.1461,
      "step": 12720
    },
    {
      "epoch": 1.0184,
      "grad_norm": 169.16542053222656,
      "learning_rate": 0.00016605333333333334,
      "loss": -114.9029,
      "step": 12730
    },
    {
      "epoch": 1.0192,
      "grad_norm": 93.17167663574219,
      "learning_rate": 0.00016602666666666667,
      "loss": -114.4474,
      "step": 12740
    },
    {
      "epoch": 1.02,
      "grad_norm": 66.56460571289062,
      "learning_rate": 0.000166,
      "loss": -116.1753,
      "step": 12750
    },
    {
      "epoch": 1.0208,
      "grad_norm": 130.25669860839844,
      "learning_rate": 0.00016597333333333333,
      "loss": -113.217,
      "step": 12760
    },
    {
      "epoch": 1.0216,
      "grad_norm": 104.13505554199219,
      "learning_rate": 0.00016594666666666668,
      "loss": -114.637,
      "step": 12770
    },
    {
      "epoch": 1.0224,
      "grad_norm": 124.0998306274414,
      "learning_rate": 0.00016592,
      "loss": -115.4509,
      "step": 12780
    },
    {
      "epoch": 1.0232,
      "grad_norm": 59.520263671875,
      "learning_rate": 0.00016589333333333334,
      "loss": -115.5241,
      "step": 12790
    },
    {
      "epoch": 1.024,
      "grad_norm": 57.271488189697266,
      "learning_rate": 0.00016586666666666667,
      "loss": -114.9571,
      "step": 12800
    },
    {
      "epoch": 1.0248,
      "grad_norm": 68.08418273925781,
      "learning_rate": 0.00016584000000000002,
      "loss": -115.9352,
      "step": 12810
    },
    {
      "epoch": 1.0256,
      "grad_norm": 69.32470703125,
      "learning_rate": 0.00016581333333333333,
      "loss": -116.2047,
      "step": 12820
    },
    {
      "epoch": 1.0264,
      "grad_norm": 55.850921630859375,
      "learning_rate": 0.00016578666666666666,
      "loss": -115.0182,
      "step": 12830
    },
    {
      "epoch": 1.0272,
      "grad_norm": 57.49272918701172,
      "learning_rate": 0.00016576,
      "loss": -115.9211,
      "step": 12840
    },
    {
      "epoch": 1.028,
      "grad_norm": 149.4462127685547,
      "learning_rate": 0.00016573333333333334,
      "loss": -114.6556,
      "step": 12850
    },
    {
      "epoch": 1.0288,
      "grad_norm": 101.10395050048828,
      "learning_rate": 0.00016570666666666667,
      "loss": -114.4656,
      "step": 12860
    },
    {
      "epoch": 1.0296,
      "grad_norm": 83.4496841430664,
      "learning_rate": 0.00016568000000000003,
      "loss": -115.4949,
      "step": 12870
    },
    {
      "epoch": 1.0304,
      "grad_norm": 79.38800048828125,
      "learning_rate": 0.00016565333333333336,
      "loss": -115.314,
      "step": 12880
    },
    {
      "epoch": 1.0312,
      "grad_norm": 47.0601692199707,
      "learning_rate": 0.00016562666666666668,
      "loss": -115.7994,
      "step": 12890
    },
    {
      "epoch": 1.032,
      "grad_norm": 78.96002197265625,
      "learning_rate": 0.0001656,
      "loss": -115.3958,
      "step": 12900
    },
    {
      "epoch": 1.0328,
      "grad_norm": 53.725425720214844,
      "learning_rate": 0.00016557333333333334,
      "loss": -115.6682,
      "step": 12910
    },
    {
      "epoch": 1.0336,
      "grad_norm": 105.03076171875,
      "learning_rate": 0.00016554666666666667,
      "loss": -114.5087,
      "step": 12920
    },
    {
      "epoch": 1.0344,
      "grad_norm": 69.72941589355469,
      "learning_rate": 0.00016552,
      "loss": -115.4678,
      "step": 12930
    },
    {
      "epoch": 1.0352,
      "grad_norm": 79.69670104980469,
      "learning_rate": 0.00016549333333333336,
      "loss": -116.223,
      "step": 12940
    },
    {
      "epoch": 1.036,
      "grad_norm": 51.0986442565918,
      "learning_rate": 0.00016546666666666669,
      "loss": -115.3467,
      "step": 12950
    },
    {
      "epoch": 1.0368,
      "grad_norm": 55.8572998046875,
      "learning_rate": 0.00016544000000000002,
      "loss": -115.5392,
      "step": 12960
    },
    {
      "epoch": 1.0376,
      "grad_norm": 45.91038513183594,
      "learning_rate": 0.00016541333333333334,
      "loss": -116.2,
      "step": 12970
    },
    {
      "epoch": 1.0384,
      "grad_norm": 68.55473327636719,
      "learning_rate": 0.00016538666666666667,
      "loss": -114.7337,
      "step": 12980
    },
    {
      "epoch": 1.0392,
      "grad_norm": 41.000667572021484,
      "learning_rate": 0.00016536,
      "loss": -115.0427,
      "step": 12990
    },
    {
      "epoch": 1.04,
      "grad_norm": 45.46289825439453,
      "learning_rate": 0.00016533333333333333,
      "loss": -115.4222,
      "step": 13000
    },
    {
      "epoch": 1.0408,
      "grad_norm": 48.935611724853516,
      "learning_rate": 0.0001653066666666667,
      "loss": -114.8559,
      "step": 13010
    },
    {
      "epoch": 1.0416,
      "grad_norm": 80.27906036376953,
      "learning_rate": 0.00016528000000000002,
      "loss": -114.1986,
      "step": 13020
    },
    {
      "epoch": 1.0424,
      "grad_norm": 80.3494644165039,
      "learning_rate": 0.00016525333333333335,
      "loss": -114.6985,
      "step": 13030
    },
    {
      "epoch": 1.0432,
      "grad_norm": 73.14787292480469,
      "learning_rate": 0.00016522666666666667,
      "loss": -114.595,
      "step": 13040
    },
    {
      "epoch": 1.044,
      "grad_norm": 52.88898468017578,
      "learning_rate": 0.0001652,
      "loss": -114.7232,
      "step": 13050
    },
    {
      "epoch": 1.0448,
      "grad_norm": 44.176761627197266,
      "learning_rate": 0.00016517333333333333,
      "loss": -113.4088,
      "step": 13060
    },
    {
      "epoch": 1.0456,
      "grad_norm": 54.76133728027344,
      "learning_rate": 0.00016514666666666666,
      "loss": -115.4364,
      "step": 13070
    },
    {
      "epoch": 1.0464,
      "grad_norm": 57.8114013671875,
      "learning_rate": 0.00016512000000000002,
      "loss": -116.1593,
      "step": 13080
    },
    {
      "epoch": 1.0472,
      "grad_norm": 85.52544403076172,
      "learning_rate": 0.00016509333333333335,
      "loss": -114.4524,
      "step": 13090
    },
    {
      "epoch": 1.048,
      "grad_norm": 107.3317642211914,
      "learning_rate": 0.00016506666666666668,
      "loss": -113.6253,
      "step": 13100
    },
    {
      "epoch": 1.0488,
      "grad_norm": 76.39970397949219,
      "learning_rate": 0.00016504,
      "loss": -116.4055,
      "step": 13110
    },
    {
      "epoch": 1.0496,
      "grad_norm": 43.01819610595703,
      "learning_rate": 0.00016501333333333333,
      "loss": -114.5133,
      "step": 13120
    },
    {
      "epoch": 1.0504,
      "grad_norm": 236.675048828125,
      "learning_rate": 0.00016498666666666666,
      "loss": -115.1052,
      "step": 13130
    },
    {
      "epoch": 1.0512,
      "grad_norm": 71.80680847167969,
      "learning_rate": 0.00016496,
      "loss": -113.8224,
      "step": 13140
    },
    {
      "epoch": 1.052,
      "grad_norm": 83.22933959960938,
      "learning_rate": 0.00016493333333333335,
      "loss": -114.9469,
      "step": 13150
    },
    {
      "epoch": 1.0528,
      "grad_norm": 83.58198547363281,
      "learning_rate": 0.00016490666666666668,
      "loss": -114.98,
      "step": 13160
    },
    {
      "epoch": 1.0536,
      "grad_norm": 66.27971649169922,
      "learning_rate": 0.00016488,
      "loss": -115.8482,
      "step": 13170
    },
    {
      "epoch": 1.0544,
      "grad_norm": 77.94251251220703,
      "learning_rate": 0.00016485333333333334,
      "loss": -114.6467,
      "step": 13180
    },
    {
      "epoch": 1.0552,
      "grad_norm": 121.12091827392578,
      "learning_rate": 0.0001648266666666667,
      "loss": -115.3359,
      "step": 13190
    },
    {
      "epoch": 1.056,
      "grad_norm": 112.35478973388672,
      "learning_rate": 0.0001648,
      "loss": -114.4378,
      "step": 13200
    },
    {
      "epoch": 1.0568,
      "grad_norm": 59.90471649169922,
      "learning_rate": 0.00016477333333333332,
      "loss": -115.5614,
      "step": 13210
    },
    {
      "epoch": 1.0576,
      "grad_norm": 63.89481735229492,
      "learning_rate": 0.00016474666666666668,
      "loss": -114.9456,
      "step": 13220
    },
    {
      "epoch": 1.0584,
      "grad_norm": 60.22328186035156,
      "learning_rate": 0.00016472,
      "loss": -115.2475,
      "step": 13230
    },
    {
      "epoch": 1.0592,
      "grad_norm": 59.807647705078125,
      "learning_rate": 0.00016469333333333334,
      "loss": -115.4971,
      "step": 13240
    },
    {
      "epoch": 1.06,
      "grad_norm": 44.87676239013672,
      "learning_rate": 0.00016466666666666667,
      "loss": -115.1662,
      "step": 13250
    },
    {
      "epoch": 1.0608,
      "grad_norm": 60.21931838989258,
      "learning_rate": 0.00016464000000000002,
      "loss": -115.484,
      "step": 13260
    },
    {
      "epoch": 1.0616,
      "grad_norm": 43.59270095825195,
      "learning_rate": 0.00016461333333333332,
      "loss": -115.4914,
      "step": 13270
    },
    {
      "epoch": 1.0624,
      "grad_norm": 57.739593505859375,
      "learning_rate": 0.00016458666666666665,
      "loss": -114.4952,
      "step": 13280
    },
    {
      "epoch": 1.0632,
      "grad_norm": 97.23734283447266,
      "learning_rate": 0.00016456,
      "loss": -114.477,
      "step": 13290
    },
    {
      "epoch": 1.064,
      "grad_norm": 78.90298461914062,
      "learning_rate": 0.00016453333333333334,
      "loss": -115.3649,
      "step": 13300
    },
    {
      "epoch": 1.0648,
      "grad_norm": 49.840721130371094,
      "learning_rate": 0.00016450666666666667,
      "loss": -114.1658,
      "step": 13310
    },
    {
      "epoch": 1.0656,
      "grad_norm": 52.0182991027832,
      "learning_rate": 0.00016448000000000002,
      "loss": -115.0638,
      "step": 13320
    },
    {
      "epoch": 1.0664,
      "grad_norm": 60.04191970825195,
      "learning_rate": 0.00016445333333333335,
      "loss": -114.7278,
      "step": 13330
    },
    {
      "epoch": 1.0672,
      "grad_norm": 75.60108947753906,
      "learning_rate": 0.00016442666666666668,
      "loss": -114.8332,
      "step": 13340
    },
    {
      "epoch": 1.068,
      "grad_norm": 43.453426361083984,
      "learning_rate": 0.0001644,
      "loss": -115.6211,
      "step": 13350
    },
    {
      "epoch": 1.0688,
      "grad_norm": 65.8123550415039,
      "learning_rate": 0.00016437333333333334,
      "loss": -114.4558,
      "step": 13360
    },
    {
      "epoch": 1.0695999999999999,
      "grad_norm": 92.98384094238281,
      "learning_rate": 0.00016434666666666667,
      "loss": -114.4373,
      "step": 13370
    },
    {
      "epoch": 1.0704,
      "grad_norm": 106.88571166992188,
      "learning_rate": 0.00016432,
      "loss": -114.1034,
      "step": 13380
    },
    {
      "epoch": 1.0712,
      "grad_norm": 122.26738739013672,
      "learning_rate": 0.00016429333333333336,
      "loss": -114.2246,
      "step": 13390
    },
    {
      "epoch": 1.072,
      "grad_norm": 300.93017578125,
      "learning_rate": 0.00016426666666666668,
      "loss": -112.2308,
      "step": 13400
    },
    {
      "epoch": 1.0728,
      "grad_norm": 84.00848388671875,
      "learning_rate": 0.00016424,
      "loss": -115.2729,
      "step": 13410
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 274.774169921875,
      "learning_rate": 0.00016421333333333334,
      "loss": -115.4674,
      "step": 13420
    },
    {
      "epoch": 1.0744,
      "grad_norm": 190.33518981933594,
      "learning_rate": 0.00016418666666666667,
      "loss": -114.7966,
      "step": 13430
    },
    {
      "epoch": 1.0752,
      "grad_norm": 57.54215621948242,
      "learning_rate": 0.00016416,
      "loss": -113.664,
      "step": 13440
    },
    {
      "epoch": 1.076,
      "grad_norm": 113.93291473388672,
      "learning_rate": 0.00016413333333333333,
      "loss": -113.4538,
      "step": 13450
    },
    {
      "epoch": 1.0768,
      "grad_norm": 47.90643310546875,
      "learning_rate": 0.00016410666666666669,
      "loss": -114.4839,
      "step": 13460
    },
    {
      "epoch": 1.0776,
      "grad_norm": 80.8706283569336,
      "learning_rate": 0.00016408000000000001,
      "loss": -114.9885,
      "step": 13470
    },
    {
      "epoch": 1.0784,
      "grad_norm": 214.82144165039062,
      "learning_rate": 0.00016405333333333334,
      "loss": -114.8993,
      "step": 13480
    },
    {
      "epoch": 1.0792,
      "grad_norm": 92.50384521484375,
      "learning_rate": 0.00016402666666666667,
      "loss": -114.3464,
      "step": 13490
    },
    {
      "epoch": 1.08,
      "grad_norm": 71.07521057128906,
      "learning_rate": 0.000164,
      "loss": -115.8328,
      "step": 13500
    },
    {
      "epoch": 1.0808,
      "grad_norm": 146.72665405273438,
      "learning_rate": 0.00016397333333333333,
      "loss": -115.2961,
      "step": 13510
    },
    {
      "epoch": 1.0816,
      "grad_norm": 108.21526336669922,
      "learning_rate": 0.00016394666666666666,
      "loss": -114.0761,
      "step": 13520
    },
    {
      "epoch": 1.0824,
      "grad_norm": 67.55166625976562,
      "learning_rate": 0.00016392000000000002,
      "loss": -112.8897,
      "step": 13530
    },
    {
      "epoch": 1.0832,
      "grad_norm": 228.00396728515625,
      "learning_rate": 0.00016389333333333335,
      "loss": -113.6284,
      "step": 13540
    },
    {
      "epoch": 1.084,
      "grad_norm": 64.09646606445312,
      "learning_rate": 0.00016386666666666667,
      "loss": -114.9048,
      "step": 13550
    },
    {
      "epoch": 1.0848,
      "grad_norm": 170.2908477783203,
      "learning_rate": 0.00016384,
      "loss": -114.2193,
      "step": 13560
    },
    {
      "epoch": 1.0856,
      "grad_norm": 61.023773193359375,
      "learning_rate": 0.00016381333333333336,
      "loss": -116.1081,
      "step": 13570
    },
    {
      "epoch": 1.0864,
      "grad_norm": 78.98086547851562,
      "learning_rate": 0.00016378666666666666,
      "loss": -115.2262,
      "step": 13580
    },
    {
      "epoch": 1.0872,
      "grad_norm": 108.76072692871094,
      "learning_rate": 0.00016376,
      "loss": -113.9149,
      "step": 13590
    },
    {
      "epoch": 1.088,
      "grad_norm": 87.88572692871094,
      "learning_rate": 0.00016373333333333335,
      "loss": -110.9109,
      "step": 13600
    },
    {
      "epoch": 1.0888,
      "grad_norm": 336.71142578125,
      "learning_rate": 0.00016370666666666668,
      "loss": -113.0002,
      "step": 13610
    },
    {
      "epoch": 1.0896,
      "grad_norm": 117.6514892578125,
      "learning_rate": 0.00016368,
      "loss": -113.9593,
      "step": 13620
    },
    {
      "epoch": 1.0904,
      "grad_norm": 143.6277618408203,
      "learning_rate": 0.00016365333333333333,
      "loss": -115.0592,
      "step": 13630
    },
    {
      "epoch": 1.0912,
      "grad_norm": 381.3492126464844,
      "learning_rate": 0.0001636266666666667,
      "loss": -113.1135,
      "step": 13640
    },
    {
      "epoch": 1.092,
      "grad_norm": 96.65679168701172,
      "learning_rate": 0.0001636,
      "loss": -114.185,
      "step": 13650
    },
    {
      "epoch": 1.0928,
      "grad_norm": 124.36814880371094,
      "learning_rate": 0.00016357333333333332,
      "loss": -112.9836,
      "step": 13660
    },
    {
      "epoch": 1.0936,
      "grad_norm": 127.62847137451172,
      "learning_rate": 0.00016354666666666668,
      "loss": -112.6027,
      "step": 13670
    },
    {
      "epoch": 1.0944,
      "grad_norm": 3490.801025390625,
      "learning_rate": 0.00016352,
      "loss": -110.4045,
      "step": 13680
    },
    {
      "epoch": 1.0952,
      "grad_norm": 87.25005340576172,
      "learning_rate": 0.00016349333333333334,
      "loss": -112.2021,
      "step": 13690
    },
    {
      "epoch": 1.096,
      "grad_norm": 54.09878158569336,
      "learning_rate": 0.0001634666666666667,
      "loss": -113.7252,
      "step": 13700
    },
    {
      "epoch": 1.0968,
      "grad_norm": 61.89280700683594,
      "learning_rate": 0.00016344000000000002,
      "loss": -115.0124,
      "step": 13710
    },
    {
      "epoch": 1.0976,
      "grad_norm": 89.01599884033203,
      "learning_rate": 0.00016341333333333335,
      "loss": -113.4949,
      "step": 13720
    },
    {
      "epoch": 1.0984,
      "grad_norm": 62.37044143676758,
      "learning_rate": 0.00016338666666666668,
      "loss": -112.539,
      "step": 13730
    },
    {
      "epoch": 1.0992,
      "grad_norm": 111.0685043334961,
      "learning_rate": 0.00016336,
      "loss": -114.5915,
      "step": 13740
    },
    {
      "epoch": 1.1,
      "grad_norm": 111.9040298461914,
      "learning_rate": 0.00016333333333333334,
      "loss": -114.2936,
      "step": 13750
    },
    {
      "epoch": 1.1008,
      "grad_norm": 66.1371841430664,
      "learning_rate": 0.00016330666666666667,
      "loss": -112.9309,
      "step": 13760
    },
    {
      "epoch": 1.1016,
      "grad_norm": 69.6031265258789,
      "learning_rate": 0.00016328000000000002,
      "loss": -115.0554,
      "step": 13770
    },
    {
      "epoch": 1.1024,
      "grad_norm": 87.27265167236328,
      "learning_rate": 0.00016325333333333335,
      "loss": -114.1071,
      "step": 13780
    },
    {
      "epoch": 1.1032,
      "grad_norm": 99.17670440673828,
      "learning_rate": 0.00016322666666666668,
      "loss": -113.8675,
      "step": 13790
    },
    {
      "epoch": 1.104,
      "grad_norm": 69.7165298461914,
      "learning_rate": 0.0001632,
      "loss": -114.4355,
      "step": 13800
    },
    {
      "epoch": 1.1048,
      "grad_norm": 3172.23681640625,
      "learning_rate": 0.00016317333333333334,
      "loss": -111.902,
      "step": 13810
    },
    {
      "epoch": 1.1056,
      "grad_norm": 97.14134979248047,
      "learning_rate": 0.00016314666666666667,
      "loss": -112.8422,
      "step": 13820
    },
    {
      "epoch": 1.1064,
      "grad_norm": 115.14803314208984,
      "learning_rate": 0.00016312,
      "loss": -114.1157,
      "step": 13830
    },
    {
      "epoch": 1.1072,
      "grad_norm": 62.27098846435547,
      "learning_rate": 0.00016309333333333335,
      "loss": -112.7835,
      "step": 13840
    },
    {
      "epoch": 1.108,
      "grad_norm": 63.48937225341797,
      "learning_rate": 0.00016306666666666668,
      "loss": -114.9854,
      "step": 13850
    },
    {
      "epoch": 1.1088,
      "grad_norm": 162.0688934326172,
      "learning_rate": 0.00016304,
      "loss": -112.9765,
      "step": 13860
    },
    {
      "epoch": 1.1096,
      "grad_norm": 75.43901824951172,
      "learning_rate": 0.00016301333333333334,
      "loss": -114.873,
      "step": 13870
    },
    {
      "epoch": 1.1104,
      "grad_norm": 66.77526092529297,
      "learning_rate": 0.00016298666666666667,
      "loss": -113.822,
      "step": 13880
    },
    {
      "epoch": 1.1112,
      "grad_norm": 55.36674880981445,
      "learning_rate": 0.00016296,
      "loss": -113.0135,
      "step": 13890
    },
    {
      "epoch": 1.112,
      "grad_norm": 80.53665161132812,
      "learning_rate": 0.00016293333333333333,
      "loss": -113.8379,
      "step": 13900
    },
    {
      "epoch": 1.1128,
      "grad_norm": 79.58307647705078,
      "learning_rate": 0.00016290666666666668,
      "loss": -114.7705,
      "step": 13910
    },
    {
      "epoch": 1.1136,
      "grad_norm": 61.555625915527344,
      "learning_rate": 0.00016288,
      "loss": -113.9358,
      "step": 13920
    },
    {
      "epoch": 1.1144,
      "grad_norm": 59.29352951049805,
      "learning_rate": 0.00016285333333333334,
      "loss": -113.5334,
      "step": 13930
    },
    {
      "epoch": 1.1152,
      "grad_norm": 229.0404510498047,
      "learning_rate": 0.00016282666666666667,
      "loss": -113.4232,
      "step": 13940
    },
    {
      "epoch": 1.116,
      "grad_norm": 52.39222717285156,
      "learning_rate": 0.0001628,
      "loss": -112.4976,
      "step": 13950
    },
    {
      "epoch": 1.1168,
      "grad_norm": 56.3404541015625,
      "learning_rate": 0.00016277333333333333,
      "loss": -114.6696,
      "step": 13960
    },
    {
      "epoch": 1.1176,
      "grad_norm": 73.5768814086914,
      "learning_rate": 0.00016274666666666666,
      "loss": -115.1199,
      "step": 13970
    },
    {
      "epoch": 1.1184,
      "grad_norm": 77.4969253540039,
      "learning_rate": 0.00016272000000000001,
      "loss": -113.4364,
      "step": 13980
    },
    {
      "epoch": 1.1192,
      "grad_norm": 62.998355865478516,
      "learning_rate": 0.00016269333333333334,
      "loss": -113.5368,
      "step": 13990
    },
    {
      "epoch": 1.12,
      "grad_norm": 99.32018280029297,
      "learning_rate": 0.00016266666666666667,
      "loss": -115.4776,
      "step": 14000
    },
    {
      "epoch": 1.1208,
      "grad_norm": 79.05542755126953,
      "learning_rate": 0.00016264,
      "loss": -113.9626,
      "step": 14010
    },
    {
      "epoch": 1.1216,
      "grad_norm": 51.01441192626953,
      "learning_rate": 0.00016261333333333336,
      "loss": -114.6037,
      "step": 14020
    },
    {
      "epoch": 1.1224,
      "grad_norm": 116.61627197265625,
      "learning_rate": 0.00016258666666666666,
      "loss": -114.8534,
      "step": 14030
    },
    {
      "epoch": 1.1232,
      "grad_norm": 76.09244537353516,
      "learning_rate": 0.00016256,
      "loss": -115.4368,
      "step": 14040
    },
    {
      "epoch": 1.124,
      "grad_norm": 43.99741744995117,
      "learning_rate": 0.00016253333333333334,
      "loss": -114.3363,
      "step": 14050
    },
    {
      "epoch": 1.1248,
      "grad_norm": 88.3769760131836,
      "learning_rate": 0.00016250666666666667,
      "loss": -113.0061,
      "step": 14060
    },
    {
      "epoch": 1.1256,
      "grad_norm": 75.30457305908203,
      "learning_rate": 0.00016248,
      "loss": -114.5534,
      "step": 14070
    },
    {
      "epoch": 1.1264,
      "grad_norm": 54.41944122314453,
      "learning_rate": 0.00016245333333333336,
      "loss": -114.0701,
      "step": 14080
    },
    {
      "epoch": 1.1272,
      "grad_norm": 65.72260284423828,
      "learning_rate": 0.0001624266666666667,
      "loss": -114.8251,
      "step": 14090
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 46.506141662597656,
      "learning_rate": 0.00016240000000000002,
      "loss": -114.5408,
      "step": 14100
    },
    {
      "epoch": 1.1288,
      "grad_norm": 53.8522834777832,
      "learning_rate": 0.00016237333333333335,
      "loss": -114.5605,
      "step": 14110
    },
    {
      "epoch": 1.1296,
      "grad_norm": 44.7677116394043,
      "learning_rate": 0.00016234666666666668,
      "loss": -114.5812,
      "step": 14120
    },
    {
      "epoch": 1.1304,
      "grad_norm": 71.27498626708984,
      "learning_rate": 0.00016232,
      "loss": -112.4784,
      "step": 14130
    },
    {
      "epoch": 1.1312,
      "grad_norm": 57.178611755371094,
      "learning_rate": 0.00016229333333333333,
      "loss": -112.5701,
      "step": 14140
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 78.73223114013672,
      "learning_rate": 0.0001622666666666667,
      "loss": -114.1829,
      "step": 14150
    },
    {
      "epoch": 1.1328,
      "grad_norm": 111.46888732910156,
      "learning_rate": 0.00016224000000000002,
      "loss": -114.5299,
      "step": 14160
    },
    {
      "epoch": 1.1336,
      "grad_norm": 36.51978302001953,
      "learning_rate": 0.00016221333333333335,
      "loss": -114.1932,
      "step": 14170
    },
    {
      "epoch": 1.1344,
      "grad_norm": 76.62730407714844,
      "learning_rate": 0.00016218666666666668,
      "loss": -113.4347,
      "step": 14180
    },
    {
      "epoch": 1.1352,
      "grad_norm": 51.94744873046875,
      "learning_rate": 0.00016216,
      "loss": -114.3651,
      "step": 14190
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 41.84835433959961,
      "learning_rate": 0.00016213333333333334,
      "loss": -113.3516,
      "step": 14200
    },
    {
      "epoch": 1.1368,
      "grad_norm": 82.71410369873047,
      "learning_rate": 0.00016210666666666666,
      "loss": -115.4211,
      "step": 14210
    },
    {
      "epoch": 1.1376,
      "grad_norm": 53.75401306152344,
      "learning_rate": 0.00016208000000000002,
      "loss": -114.5128,
      "step": 14220
    },
    {
      "epoch": 1.1384,
      "grad_norm": 39.13063049316406,
      "learning_rate": 0.00016205333333333335,
      "loss": -114.5284,
      "step": 14230
    },
    {
      "epoch": 1.1392,
      "grad_norm": 53.30813217163086,
      "learning_rate": 0.00016202666666666668,
      "loss": -114.7168,
      "step": 14240
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 48.50184631347656,
      "learning_rate": 0.000162,
      "loss": -115.8384,
      "step": 14250
    },
    {
      "epoch": 1.1408,
      "grad_norm": 47.92903137207031,
      "learning_rate": 0.00016197333333333334,
      "loss": -115.1587,
      "step": 14260
    },
    {
      "epoch": 1.1416,
      "grad_norm": 678.4876098632812,
      "learning_rate": 0.00016194666666666667,
      "loss": -114.4846,
      "step": 14270
    },
    {
      "epoch": 1.1424,
      "grad_norm": 67.0184555053711,
      "learning_rate": 0.00016192,
      "loss": -114.8807,
      "step": 14280
    },
    {
      "epoch": 1.1432,
      "grad_norm": 223.6226043701172,
      "learning_rate": 0.00016189333333333335,
      "loss": -113.6384,
      "step": 14290
    },
    {
      "epoch": 1.144,
      "grad_norm": 78.51970672607422,
      "learning_rate": 0.00016186666666666668,
      "loss": -114.5492,
      "step": 14300
    },
    {
      "epoch": 1.1448,
      "grad_norm": 65.01700592041016,
      "learning_rate": 0.00016184,
      "loss": -113.7324,
      "step": 14310
    },
    {
      "epoch": 1.1456,
      "grad_norm": 78.21405792236328,
      "learning_rate": 0.00016181333333333334,
      "loss": -113.4601,
      "step": 14320
    },
    {
      "epoch": 1.1464,
      "grad_norm": 49.31260299682617,
      "learning_rate": 0.00016178666666666667,
      "loss": -115.9972,
      "step": 14330
    },
    {
      "epoch": 1.1472,
      "grad_norm": 66.46142578125,
      "learning_rate": 0.00016176,
      "loss": -114.4609,
      "step": 14340
    },
    {
      "epoch": 1.148,
      "grad_norm": 39.69892501831055,
      "learning_rate": 0.00016173333333333333,
      "loss": -114.0408,
      "step": 14350
    },
    {
      "epoch": 1.1488,
      "grad_norm": 62.14849090576172,
      "learning_rate": 0.00016170666666666668,
      "loss": -114.0391,
      "step": 14360
    },
    {
      "epoch": 1.1496,
      "grad_norm": 71.3232421875,
      "learning_rate": 0.00016168,
      "loss": -114.4198,
      "step": 14370
    },
    {
      "epoch": 1.1504,
      "grad_norm": 64.00019836425781,
      "learning_rate": 0.00016165333333333334,
      "loss": -113.6421,
      "step": 14380
    },
    {
      "epoch": 1.1512,
      "grad_norm": 42.968788146972656,
      "learning_rate": 0.00016162666666666667,
      "loss": -115.1817,
      "step": 14390
    },
    {
      "epoch": 1.152,
      "grad_norm": 59.17898178100586,
      "learning_rate": 0.00016160000000000002,
      "loss": -114.5833,
      "step": 14400
    },
    {
      "epoch": 1.1528,
      "grad_norm": 63.233612060546875,
      "learning_rate": 0.00016157333333333333,
      "loss": -115.1844,
      "step": 14410
    },
    {
      "epoch": 1.1536,
      "grad_norm": 200.05108642578125,
      "learning_rate": 0.00016154666666666666,
      "loss": -114.2285,
      "step": 14420
    },
    {
      "epoch": 1.1544,
      "grad_norm": 80.76761627197266,
      "learning_rate": 0.00016152,
      "loss": -113.1399,
      "step": 14430
    },
    {
      "epoch": 1.1552,
      "grad_norm": 35.49745178222656,
      "learning_rate": 0.00016149333333333334,
      "loss": -115.0271,
      "step": 14440
    },
    {
      "epoch": 1.156,
      "grad_norm": 54.9648323059082,
      "learning_rate": 0.00016146666666666667,
      "loss": -114.4705,
      "step": 14450
    },
    {
      "epoch": 1.1568,
      "grad_norm": 65.74812316894531,
      "learning_rate": 0.00016144000000000003,
      "loss": -115.3363,
      "step": 14460
    },
    {
      "epoch": 1.1576,
      "grad_norm": 146.1957550048828,
      "learning_rate": 0.00016141333333333336,
      "loss": -114.1013,
      "step": 14470
    },
    {
      "epoch": 1.1584,
      "grad_norm": 59.42613983154297,
      "learning_rate": 0.00016138666666666666,
      "loss": -115.4057,
      "step": 14480
    },
    {
      "epoch": 1.1592,
      "grad_norm": 82.45623016357422,
      "learning_rate": 0.00016136,
      "loss": -114.184,
      "step": 14490
    },
    {
      "epoch": 1.16,
      "grad_norm": 182.42431640625,
      "learning_rate": 0.00016133333333333334,
      "loss": -113.7675,
      "step": 14500
    },
    {
      "epoch": 1.1608,
      "grad_norm": 49.045623779296875,
      "learning_rate": 0.00016130666666666667,
      "loss": -113.3182,
      "step": 14510
    },
    {
      "epoch": 1.1616,
      "grad_norm": 55.115596771240234,
      "learning_rate": 0.00016128,
      "loss": -112.1674,
      "step": 14520
    },
    {
      "epoch": 1.1623999999999999,
      "grad_norm": 73.9544448852539,
      "learning_rate": 0.00016125333333333336,
      "loss": -114.3374,
      "step": 14530
    },
    {
      "epoch": 1.1632,
      "grad_norm": 68.32061004638672,
      "learning_rate": 0.00016122666666666669,
      "loss": -112.855,
      "step": 14540
    },
    {
      "epoch": 1.164,
      "grad_norm": 125.96096801757812,
      "learning_rate": 0.00016120000000000002,
      "loss": -114.8678,
      "step": 14550
    },
    {
      "epoch": 1.1648,
      "grad_norm": 82.39248657226562,
      "learning_rate": 0.00016117333333333334,
      "loss": -111.9602,
      "step": 14560
    },
    {
      "epoch": 1.1656,
      "grad_norm": 91.87837982177734,
      "learning_rate": 0.00016114666666666667,
      "loss": -113.1756,
      "step": 14570
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 96.7498550415039,
      "learning_rate": 0.00016112,
      "loss": -112.8114,
      "step": 14580
    },
    {
      "epoch": 1.1672,
      "grad_norm": 78.33724212646484,
      "learning_rate": 0.00016109333333333333,
      "loss": -112.8242,
      "step": 14590
    },
    {
      "epoch": 1.168,
      "grad_norm": 54.88640594482422,
      "learning_rate": 0.0001610666666666667,
      "loss": -114.5931,
      "step": 14600
    },
    {
      "epoch": 1.1688,
      "grad_norm": 66.12509155273438,
      "learning_rate": 0.00016104000000000002,
      "loss": -112.4942,
      "step": 14610
    },
    {
      "epoch": 1.1696,
      "grad_norm": 54.728946685791016,
      "learning_rate": 0.00016101333333333335,
      "loss": -114.531,
      "step": 14620
    },
    {
      "epoch": 1.1703999999999999,
      "grad_norm": 920.3743896484375,
      "learning_rate": 0.00016098666666666667,
      "loss": -113.6337,
      "step": 14630
    },
    {
      "epoch": 1.1712,
      "grad_norm": 61.214996337890625,
      "learning_rate": 0.00016096,
      "loss": -112.6744,
      "step": 14640
    },
    {
      "epoch": 1.172,
      "grad_norm": 39.9565544128418,
      "learning_rate": 0.00016093333333333333,
      "loss": -114.5083,
      "step": 14650
    },
    {
      "epoch": 1.1728,
      "grad_norm": 86.42827606201172,
      "learning_rate": 0.00016090666666666666,
      "loss": -114.8449,
      "step": 14660
    },
    {
      "epoch": 1.1736,
      "grad_norm": 52.86133575439453,
      "learning_rate": 0.00016088000000000002,
      "loss": -113.7501,
      "step": 14670
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 117.95782470703125,
      "learning_rate": 0.00016085333333333335,
      "loss": -111.3892,
      "step": 14680
    },
    {
      "epoch": 1.1752,
      "grad_norm": 62.53562545776367,
      "learning_rate": 0.00016082666666666668,
      "loss": -112.4903,
      "step": 14690
    },
    {
      "epoch": 1.176,
      "grad_norm": 70.35801696777344,
      "learning_rate": 0.0001608,
      "loss": -112.9849,
      "step": 14700
    },
    {
      "epoch": 1.1768,
      "grad_norm": 128.37867736816406,
      "learning_rate": 0.00016077333333333333,
      "loss": -112.8498,
      "step": 14710
    },
    {
      "epoch": 1.1776,
      "grad_norm": 58.061744689941406,
      "learning_rate": 0.00016074666666666666,
      "loss": -114.2851,
      "step": 14720
    },
    {
      "epoch": 1.1784,
      "grad_norm": 60.228797912597656,
      "learning_rate": 0.00016072,
      "loss": -113.598,
      "step": 14730
    },
    {
      "epoch": 1.1792,
      "grad_norm": 70.79666900634766,
      "learning_rate": 0.00016069333333333335,
      "loss": -114.1338,
      "step": 14740
    },
    {
      "epoch": 1.18,
      "grad_norm": 84.44005584716797,
      "learning_rate": 0.00016066666666666668,
      "loss": -114.9061,
      "step": 14750
    },
    {
      "epoch": 1.1808,
      "grad_norm": 45.47882080078125,
      "learning_rate": 0.00016064,
      "loss": -114.2431,
      "step": 14760
    },
    {
      "epoch": 1.1816,
      "grad_norm": 44.39668655395508,
      "learning_rate": 0.00016061333333333334,
      "loss": -114.6838,
      "step": 14770
    },
    {
      "epoch": 1.1824,
      "grad_norm": 37.47141647338867,
      "learning_rate": 0.0001605866666666667,
      "loss": -114.734,
      "step": 14780
    },
    {
      "epoch": 1.1832,
      "grad_norm": 53.53913116455078,
      "learning_rate": 0.00016056,
      "loss": -114.8611,
      "step": 14790
    },
    {
      "epoch": 1.184,
      "grad_norm": 88.38733673095703,
      "learning_rate": 0.00016053333333333332,
      "loss": -114.0229,
      "step": 14800
    },
    {
      "epoch": 1.1848,
      "grad_norm": 47.562984466552734,
      "learning_rate": 0.00016050666666666668,
      "loss": -115.1125,
      "step": 14810
    },
    {
      "epoch": 1.1856,
      "grad_norm": 84.6954574584961,
      "learning_rate": 0.00016048,
      "loss": -112.843,
      "step": 14820
    },
    {
      "epoch": 1.1864,
      "grad_norm": 48.650264739990234,
      "learning_rate": 0.00016045333333333334,
      "loss": -115.6033,
      "step": 14830
    },
    {
      "epoch": 1.1872,
      "grad_norm": 51.40550231933594,
      "learning_rate": 0.0001604266666666667,
      "loss": -114.4726,
      "step": 14840
    },
    {
      "epoch": 1.188,
      "grad_norm": 138.5155487060547,
      "learning_rate": 0.00016040000000000002,
      "loss": -114.5705,
      "step": 14850
    },
    {
      "epoch": 1.1888,
      "grad_norm": 58.411346435546875,
      "learning_rate": 0.00016037333333333332,
      "loss": -114.5865,
      "step": 14860
    },
    {
      "epoch": 1.1896,
      "grad_norm": 127.46846008300781,
      "learning_rate": 0.00016034666666666665,
      "loss": -114.3462,
      "step": 14870
    },
    {
      "epoch": 1.1904,
      "grad_norm": 41.253753662109375,
      "learning_rate": 0.00016032,
      "loss": -112.7809,
      "step": 14880
    },
    {
      "epoch": 1.1912,
      "grad_norm": 65.10868835449219,
      "learning_rate": 0.00016029333333333334,
      "loss": -114.5055,
      "step": 14890
    },
    {
      "epoch": 1.192,
      "grad_norm": 111.02740478515625,
      "learning_rate": 0.00016026666666666667,
      "loss": -114.1921,
      "step": 14900
    },
    {
      "epoch": 1.1928,
      "grad_norm": 115.30587005615234,
      "learning_rate": 0.00016024000000000002,
      "loss": -114.1629,
      "step": 14910
    },
    {
      "epoch": 1.1936,
      "grad_norm": 305.91650390625,
      "learning_rate": 0.00016021333333333335,
      "loss": -113.5781,
      "step": 14920
    },
    {
      "epoch": 1.1944,
      "grad_norm": 57.49977493286133,
      "learning_rate": 0.00016018666666666668,
      "loss": -113.7346,
      "step": 14930
    },
    {
      "epoch": 1.1952,
      "grad_norm": 207.99302673339844,
      "learning_rate": 0.00016016,
      "loss": -115.2193,
      "step": 14940
    },
    {
      "epoch": 1.196,
      "grad_norm": 63.13298416137695,
      "learning_rate": 0.00016013333333333334,
      "loss": -114.3124,
      "step": 14950
    },
    {
      "epoch": 1.1968,
      "grad_norm": 48.55502700805664,
      "learning_rate": 0.00016010666666666667,
      "loss": -115.607,
      "step": 14960
    },
    {
      "epoch": 1.1976,
      "grad_norm": 73.01734161376953,
      "learning_rate": 0.00016008,
      "loss": -113.9201,
      "step": 14970
    },
    {
      "epoch": 1.1984,
      "grad_norm": 62.96584701538086,
      "learning_rate": 0.00016005333333333335,
      "loss": -115.7387,
      "step": 14980
    },
    {
      "epoch": 1.1992,
      "grad_norm": 54.21083068847656,
      "learning_rate": 0.00016002666666666668,
      "loss": -114.4531,
      "step": 14990
    },
    {
      "epoch": 1.2,
      "grad_norm": 49.96942138671875,
      "learning_rate": 0.00016,
      "loss": -115.4693,
      "step": 15000
    },
    {
      "epoch": 1.2008,
      "grad_norm": 51.35708236694336,
      "learning_rate": 0.00015997333333333334,
      "loss": -113.9798,
      "step": 15010
    },
    {
      "epoch": 1.2016,
      "grad_norm": 79.10597229003906,
      "learning_rate": 0.00015994666666666667,
      "loss": -113.8279,
      "step": 15020
    },
    {
      "epoch": 1.2024,
      "grad_norm": 66.75945281982422,
      "learning_rate": 0.00015992,
      "loss": -114.734,
      "step": 15030
    },
    {
      "epoch": 1.2032,
      "grad_norm": 43.77897644042969,
      "learning_rate": 0.00015989333333333333,
      "loss": -114.5037,
      "step": 15040
    },
    {
      "epoch": 1.204,
      "grad_norm": 43.64409637451172,
      "learning_rate": 0.00015986666666666669,
      "loss": -115.7891,
      "step": 15050
    },
    {
      "epoch": 1.2048,
      "grad_norm": 63.16431427001953,
      "learning_rate": 0.00015984000000000001,
      "loss": -113.8928,
      "step": 15060
    },
    {
      "epoch": 1.2056,
      "grad_norm": 75.33705139160156,
      "learning_rate": 0.00015981333333333334,
      "loss": -114.8421,
      "step": 15070
    },
    {
      "epoch": 1.2064,
      "grad_norm": 94.25210571289062,
      "learning_rate": 0.00015978666666666667,
      "loss": -114.8192,
      "step": 15080
    },
    {
      "epoch": 1.2072,
      "grad_norm": 55.10553741455078,
      "learning_rate": 0.00015976,
      "loss": -115.3056,
      "step": 15090
    },
    {
      "epoch": 1.208,
      "grad_norm": 42.45444869995117,
      "learning_rate": 0.00015973333333333333,
      "loss": -115.0364,
      "step": 15100
    },
    {
      "epoch": 1.2088,
      "grad_norm": 79.39749145507812,
      "learning_rate": 0.00015970666666666666,
      "loss": -114.3921,
      "step": 15110
    },
    {
      "epoch": 1.2096,
      "grad_norm": 55.45378875732422,
      "learning_rate": 0.00015968000000000002,
      "loss": -115.2707,
      "step": 15120
    },
    {
      "epoch": 1.2104,
      "grad_norm": 49.8899040222168,
      "learning_rate": 0.00015965333333333335,
      "loss": -115.0761,
      "step": 15130
    },
    {
      "epoch": 1.2112,
      "grad_norm": 57.1456413269043,
      "learning_rate": 0.00015962666666666667,
      "loss": -115.2233,
      "step": 15140
    },
    {
      "epoch": 1.212,
      "grad_norm": 52.02346420288086,
      "learning_rate": 0.0001596,
      "loss": -116.7128,
      "step": 15150
    },
    {
      "epoch": 1.2128,
      "grad_norm": 41.59880828857422,
      "learning_rate": 0.00015957333333333333,
      "loss": -113.9786,
      "step": 15160
    },
    {
      "epoch": 1.2136,
      "grad_norm": 52.671104431152344,
      "learning_rate": 0.00015954666666666666,
      "loss": -112.2436,
      "step": 15170
    },
    {
      "epoch": 1.2144,
      "grad_norm": 85.41568756103516,
      "learning_rate": 0.00015952,
      "loss": -114.5375,
      "step": 15180
    },
    {
      "epoch": 1.2152,
      "grad_norm": 55.96591567993164,
      "learning_rate": 0.00015949333333333335,
      "loss": -115.0811,
      "step": 15190
    },
    {
      "epoch": 1.216,
      "grad_norm": 48.3001594543457,
      "learning_rate": 0.00015946666666666668,
      "loss": -115.7755,
      "step": 15200
    },
    {
      "epoch": 1.2168,
      "grad_norm": 87.61382293701172,
      "learning_rate": 0.00015944,
      "loss": -114.9143,
      "step": 15210
    },
    {
      "epoch": 1.2176,
      "grad_norm": 70.22651672363281,
      "learning_rate": 0.00015941333333333336,
      "loss": -111.7849,
      "step": 15220
    },
    {
      "epoch": 1.2184,
      "grad_norm": 63.556907653808594,
      "learning_rate": 0.0001593866666666667,
      "loss": -114.6172,
      "step": 15230
    },
    {
      "epoch": 1.2192,
      "grad_norm": 57.69001007080078,
      "learning_rate": 0.00015936,
      "loss": -114.4575,
      "step": 15240
    },
    {
      "epoch": 1.22,
      "grad_norm": 60.874691009521484,
      "learning_rate": 0.00015933333333333332,
      "loss": -114.2617,
      "step": 15250
    },
    {
      "epoch": 1.2208,
      "grad_norm": 55.63650131225586,
      "learning_rate": 0.00015930666666666668,
      "loss": -115.4134,
      "step": 15260
    },
    {
      "epoch": 1.2216,
      "grad_norm": 57.570247650146484,
      "learning_rate": 0.00015928,
      "loss": -116.7804,
      "step": 15270
    },
    {
      "epoch": 1.2224,
      "grad_norm": 62.49252700805664,
      "learning_rate": 0.00015925333333333334,
      "loss": -115.6547,
      "step": 15280
    },
    {
      "epoch": 1.2232,
      "grad_norm": 60.40922164916992,
      "learning_rate": 0.0001592266666666667,
      "loss": -114.8436,
      "step": 15290
    },
    {
      "epoch": 1.224,
      "grad_norm": 58.417476654052734,
      "learning_rate": 0.00015920000000000002,
      "loss": -114.1861,
      "step": 15300
    },
    {
      "epoch": 1.2248,
      "grad_norm": 57.683162689208984,
      "learning_rate": 0.00015917333333333332,
      "loss": -114.7695,
      "step": 15310
    },
    {
      "epoch": 1.2256,
      "grad_norm": 83.02511596679688,
      "learning_rate": 0.00015914666666666668,
      "loss": -114.7505,
      "step": 15320
    },
    {
      "epoch": 1.2264,
      "grad_norm": 60.70705795288086,
      "learning_rate": 0.00015912,
      "loss": -115.1629,
      "step": 15330
    },
    {
      "epoch": 1.2272,
      "grad_norm": 43.713134765625,
      "learning_rate": 0.00015909333333333334,
      "loss": -114.1898,
      "step": 15340
    },
    {
      "epoch": 1.228,
      "grad_norm": 76.6175765991211,
      "learning_rate": 0.00015906666666666667,
      "loss": -116.3165,
      "step": 15350
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 87.33978271484375,
      "learning_rate": 0.00015904000000000002,
      "loss": -114.2578,
      "step": 15360
    },
    {
      "epoch": 1.2296,
      "grad_norm": 80.86941528320312,
      "learning_rate": 0.00015901333333333335,
      "loss": -114.4771,
      "step": 15370
    },
    {
      "epoch": 1.2304,
      "grad_norm": 55.61539077758789,
      "learning_rate": 0.00015898666666666668,
      "loss": -114.7596,
      "step": 15380
    },
    {
      "epoch": 1.2312,
      "grad_norm": 70.02622985839844,
      "learning_rate": 0.00015896,
      "loss": -114.8417,
      "step": 15390
    },
    {
      "epoch": 1.232,
      "grad_norm": 75.94625091552734,
      "learning_rate": 0.00015893333333333334,
      "loss": -114.9993,
      "step": 15400
    },
    {
      "epoch": 1.2328000000000001,
      "grad_norm": 76.9031982421875,
      "learning_rate": 0.00015890666666666667,
      "loss": -115.8023,
      "step": 15410
    },
    {
      "epoch": 1.2336,
      "grad_norm": 55.90003204345703,
      "learning_rate": 0.00015888,
      "loss": -114.6129,
      "step": 15420
    },
    {
      "epoch": 1.2344,
      "grad_norm": 48.79921340942383,
      "learning_rate": 0.00015885333333333335,
      "loss": -114.3365,
      "step": 15430
    },
    {
      "epoch": 1.2352,
      "grad_norm": 61.12717056274414,
      "learning_rate": 0.00015882666666666668,
      "loss": -114.0477,
      "step": 15440
    },
    {
      "epoch": 1.236,
      "grad_norm": 65.34709167480469,
      "learning_rate": 0.0001588,
      "loss": -114.1374,
      "step": 15450
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 75.36463165283203,
      "learning_rate": 0.00015877333333333334,
      "loss": -114.9193,
      "step": 15460
    },
    {
      "epoch": 1.2376,
      "grad_norm": 78.38164520263672,
      "learning_rate": 0.00015874666666666667,
      "loss": -113.4824,
      "step": 15470
    },
    {
      "epoch": 1.2384,
      "grad_norm": 84.18028259277344,
      "learning_rate": 0.00015872,
      "loss": -113.7796,
      "step": 15480
    },
    {
      "epoch": 1.2392,
      "grad_norm": 54.51068115234375,
      "learning_rate": 0.00015869333333333333,
      "loss": -114.6371,
      "step": 15490
    },
    {
      "epoch": 1.24,
      "grad_norm": 62.44926071166992,
      "learning_rate": 0.00015866666666666668,
      "loss": -113.5,
      "step": 15500
    },
    {
      "epoch": 1.2408,
      "grad_norm": 53.988346099853516,
      "learning_rate": 0.00015864,
      "loss": -114.3766,
      "step": 15510
    },
    {
      "epoch": 1.2416,
      "grad_norm": 68.44823455810547,
      "learning_rate": 0.00015861333333333334,
      "loss": -115.545,
      "step": 15520
    },
    {
      "epoch": 1.2424,
      "grad_norm": 52.19215774536133,
      "learning_rate": 0.00015858666666666667,
      "loss": -116.308,
      "step": 15530
    },
    {
      "epoch": 1.2432,
      "grad_norm": 67.90331268310547,
      "learning_rate": 0.00015856,
      "loss": -114.6244,
      "step": 15540
    },
    {
      "epoch": 1.244,
      "grad_norm": 69.59051513671875,
      "learning_rate": 0.00015853333333333333,
      "loss": -114.462,
      "step": 15550
    },
    {
      "epoch": 1.2448,
      "grad_norm": 114.30504608154297,
      "learning_rate": 0.00015850666666666666,
      "loss": -114.6299,
      "step": 15560
    },
    {
      "epoch": 1.2456,
      "grad_norm": 49.429080963134766,
      "learning_rate": 0.00015848000000000001,
      "loss": -114.2859,
      "step": 15570
    },
    {
      "epoch": 1.2464,
      "grad_norm": 66.91151428222656,
      "learning_rate": 0.00015845333333333334,
      "loss": -115.6816,
      "step": 15580
    },
    {
      "epoch": 1.2472,
      "grad_norm": 67.74938201904297,
      "learning_rate": 0.00015842666666666667,
      "loss": -115.43,
      "step": 15590
    },
    {
      "epoch": 1.248,
      "grad_norm": 63.28197479248047,
      "learning_rate": 0.00015840000000000003,
      "loss": -114.7126,
      "step": 15600
    },
    {
      "epoch": 1.2488,
      "grad_norm": 64.1523208618164,
      "learning_rate": 0.00015837333333333336,
      "loss": -115.2617,
      "step": 15610
    },
    {
      "epoch": 1.2496,
      "grad_norm": 61.21489715576172,
      "learning_rate": 0.00015834666666666666,
      "loss": -115.4385,
      "step": 15620
    },
    {
      "epoch": 1.2504,
      "grad_norm": 61.0362548828125,
      "learning_rate": 0.00015832,
      "loss": -113.9302,
      "step": 15630
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 52.52396011352539,
      "learning_rate": 0.00015829333333333334,
      "loss": -116.1879,
      "step": 15640
    },
    {
      "epoch": 1.252,
      "grad_norm": 71.1827163696289,
      "learning_rate": 0.00015826666666666667,
      "loss": -115.6405,
      "step": 15650
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 49.57060241699219,
      "learning_rate": 0.00015824,
      "loss": -114.704,
      "step": 15660
    },
    {
      "epoch": 1.2536,
      "grad_norm": 51.013240814208984,
      "learning_rate": 0.00015821333333333336,
      "loss": -114.7543,
      "step": 15670
    },
    {
      "epoch": 1.2544,
      "grad_norm": 78.68618774414062,
      "learning_rate": 0.0001581866666666667,
      "loss": -115.1937,
      "step": 15680
    },
    {
      "epoch": 1.2551999999999999,
      "grad_norm": 120.26351165771484,
      "learning_rate": 0.00015816,
      "loss": -114.2287,
      "step": 15690
    },
    {
      "epoch": 1.256,
      "grad_norm": 59.99007797241211,
      "learning_rate": 0.00015813333333333335,
      "loss": -115.7215,
      "step": 15700
    },
    {
      "epoch": 1.2568,
      "grad_norm": 61.28348922729492,
      "learning_rate": 0.00015810666666666668,
      "loss": -114.7988,
      "step": 15710
    },
    {
      "epoch": 1.2576,
      "grad_norm": 69.2947006225586,
      "learning_rate": 0.00015808,
      "loss": -114.9241,
      "step": 15720
    },
    {
      "epoch": 1.2584,
      "grad_norm": 64.64073944091797,
      "learning_rate": 0.00015805333333333333,
      "loss": -115.0072,
      "step": 15730
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 63.909542083740234,
      "learning_rate": 0.0001580266666666667,
      "loss": -114.6502,
      "step": 15740
    },
    {
      "epoch": 1.26,
      "grad_norm": 65.77407836914062,
      "learning_rate": 0.00015800000000000002,
      "loss": -115.1723,
      "step": 15750
    },
    {
      "epoch": 1.2608,
      "grad_norm": 58.566192626953125,
      "learning_rate": 0.00015797333333333335,
      "loss": -115.0826,
      "step": 15760
    },
    {
      "epoch": 1.2616,
      "grad_norm": 76.12036895751953,
      "learning_rate": 0.00015794666666666668,
      "loss": -113.6202,
      "step": 15770
    },
    {
      "epoch": 1.2624,
      "grad_norm": 71.55289459228516,
      "learning_rate": 0.00015792,
      "loss": -114.3429,
      "step": 15780
    },
    {
      "epoch": 1.2631999999999999,
      "grad_norm": 66.14604949951172,
      "learning_rate": 0.00015789333333333333,
      "loss": -115.4382,
      "step": 15790
    },
    {
      "epoch": 1.264,
      "grad_norm": 56.33871841430664,
      "learning_rate": 0.00015786666666666666,
      "loss": -114.8968,
      "step": 15800
    },
    {
      "epoch": 1.2648,
      "grad_norm": 54.69660186767578,
      "learning_rate": 0.00015784000000000002,
      "loss": -115.8587,
      "step": 15810
    },
    {
      "epoch": 1.2656,
      "grad_norm": 65.52230834960938,
      "learning_rate": 0.00015781333333333335,
      "loss": -115.7509,
      "step": 15820
    },
    {
      "epoch": 1.2664,
      "grad_norm": 79.84103393554688,
      "learning_rate": 0.00015778666666666668,
      "loss": -113.9721,
      "step": 15830
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 91.3614501953125,
      "learning_rate": 0.00015776,
      "loss": -113.1522,
      "step": 15840
    },
    {
      "epoch": 1.268,
      "grad_norm": 69.69999694824219,
      "learning_rate": 0.00015773333333333334,
      "loss": -114.5475,
      "step": 15850
    },
    {
      "epoch": 1.2688,
      "grad_norm": 70.65373992919922,
      "learning_rate": 0.00015770666666666667,
      "loss": -115.2246,
      "step": 15860
    },
    {
      "epoch": 1.2696,
      "grad_norm": 99.45852661132812,
      "learning_rate": 0.00015768,
      "loss": -114.7085,
      "step": 15870
    },
    {
      "epoch": 1.2704,
      "grad_norm": 77.83953094482422,
      "learning_rate": 0.00015765333333333335,
      "loss": -115.6257,
      "step": 15880
    },
    {
      "epoch": 1.2711999999999999,
      "grad_norm": 92.62419128417969,
      "learning_rate": 0.00015762666666666668,
      "loss": -114.9272,
      "step": 15890
    },
    {
      "epoch": 1.272,
      "grad_norm": 64.55335998535156,
      "learning_rate": 0.0001576,
      "loss": -113.7511,
      "step": 15900
    },
    {
      "epoch": 1.2728,
      "grad_norm": 81.64805603027344,
      "learning_rate": 0.00015757333333333334,
      "loss": -114.8882,
      "step": 15910
    },
    {
      "epoch": 1.2736,
      "grad_norm": 63.119537353515625,
      "learning_rate": 0.00015754666666666667,
      "loss": -114.7927,
      "step": 15920
    },
    {
      "epoch": 1.2744,
      "grad_norm": 65.25743103027344,
      "learning_rate": 0.00015752,
      "loss": -115.8652,
      "step": 15930
    },
    {
      "epoch": 1.2752,
      "grad_norm": 81.48893737792969,
      "learning_rate": 0.00015749333333333333,
      "loss": -115.8974,
      "step": 15940
    },
    {
      "epoch": 1.276,
      "grad_norm": 86.8906478881836,
      "learning_rate": 0.00015746666666666668,
      "loss": -115.0604,
      "step": 15950
    },
    {
      "epoch": 1.2768,
      "grad_norm": 62.310176849365234,
      "learning_rate": 0.00015744,
      "loss": -114.3477,
      "step": 15960
    },
    {
      "epoch": 1.2776,
      "grad_norm": 73.21205139160156,
      "learning_rate": 0.00015741333333333334,
      "loss": -114.7913,
      "step": 15970
    },
    {
      "epoch": 1.2784,
      "grad_norm": 71.38441467285156,
      "learning_rate": 0.0001573866666666667,
      "loss": -115.1027,
      "step": 15980
    },
    {
      "epoch": 1.2792,
      "grad_norm": 61.98588943481445,
      "learning_rate": 0.00015736000000000002,
      "loss": -114.764,
      "step": 15990
    },
    {
      "epoch": 1.28,
      "grad_norm": 78.85823059082031,
      "learning_rate": 0.00015733333333333333,
      "loss": -115.3353,
      "step": 16000
    },
    {
      "epoch": 1.2808,
      "grad_norm": 53.80997848510742,
      "learning_rate": 0.00015730666666666666,
      "loss": -116.2641,
      "step": 16010
    },
    {
      "epoch": 1.2816,
      "grad_norm": 65.71346282958984,
      "learning_rate": 0.00015728,
      "loss": -113.5889,
      "step": 16020
    },
    {
      "epoch": 1.2824,
      "grad_norm": 66.46076965332031,
      "learning_rate": 0.00015725333333333334,
      "loss": -115.1969,
      "step": 16030
    },
    {
      "epoch": 1.2832,
      "grad_norm": 70.2730941772461,
      "learning_rate": 0.00015722666666666667,
      "loss": -115.5045,
      "step": 16040
    },
    {
      "epoch": 1.284,
      "grad_norm": 74.98191833496094,
      "learning_rate": 0.00015720000000000003,
      "loss": -115.2227,
      "step": 16050
    },
    {
      "epoch": 1.2848,
      "grad_norm": 74.65802764892578,
      "learning_rate": 0.00015717333333333336,
      "loss": -115.2632,
      "step": 16060
    },
    {
      "epoch": 1.2856,
      "grad_norm": 51.1591911315918,
      "learning_rate": 0.00015714666666666666,
      "loss": -113.7338,
      "step": 16070
    },
    {
      "epoch": 1.2864,
      "grad_norm": 78.10047912597656,
      "learning_rate": 0.00015712000000000001,
      "loss": -115.5451,
      "step": 16080
    },
    {
      "epoch": 1.2872,
      "grad_norm": 82.2795639038086,
      "learning_rate": 0.00015709333333333334,
      "loss": -116.4968,
      "step": 16090
    },
    {
      "epoch": 1.288,
      "grad_norm": 83.01210021972656,
      "learning_rate": 0.00015706666666666667,
      "loss": -114.9422,
      "step": 16100
    },
    {
      "epoch": 1.2888,
      "grad_norm": 67.21568298339844,
      "learning_rate": 0.00015704,
      "loss": -115.3036,
      "step": 16110
    },
    {
      "epoch": 1.2896,
      "grad_norm": 63.897979736328125,
      "learning_rate": 0.00015701333333333336,
      "loss": -114.5853,
      "step": 16120
    },
    {
      "epoch": 1.2904,
      "grad_norm": 62.15467071533203,
      "learning_rate": 0.00015698666666666669,
      "loss": -115.3002,
      "step": 16130
    },
    {
      "epoch": 1.2912,
      "grad_norm": 115.12538146972656,
      "learning_rate": 0.00015696000000000002,
      "loss": -115.285,
      "step": 16140
    },
    {
      "epoch": 1.292,
      "grad_norm": 87.23673248291016,
      "learning_rate": 0.00015693333333333334,
      "loss": -116.0282,
      "step": 16150
    },
    {
      "epoch": 1.2928,
      "grad_norm": 67.70500946044922,
      "learning_rate": 0.00015690666666666667,
      "loss": -116.0644,
      "step": 16160
    },
    {
      "epoch": 1.2936,
      "grad_norm": 74.4842300415039,
      "learning_rate": 0.00015688,
      "loss": -114.3819,
      "step": 16170
    },
    {
      "epoch": 1.2944,
      "grad_norm": 122.40225219726562,
      "learning_rate": 0.00015685333333333333,
      "loss": -114.6418,
      "step": 16180
    },
    {
      "epoch": 1.2952,
      "grad_norm": 58.49176025390625,
      "learning_rate": 0.0001568266666666667,
      "loss": -115.1177,
      "step": 16190
    },
    {
      "epoch": 1.296,
      "grad_norm": 69.60902404785156,
      "learning_rate": 0.00015680000000000002,
      "loss": -114.4257,
      "step": 16200
    },
    {
      "epoch": 1.2968,
      "grad_norm": 772.8843994140625,
      "learning_rate": 0.00015677333333333335,
      "loss": -112.9261,
      "step": 16210
    },
    {
      "epoch": 1.2976,
      "grad_norm": 107.17015075683594,
      "learning_rate": 0.00015674666666666667,
      "loss": -111.7904,
      "step": 16220
    },
    {
      "epoch": 1.2984,
      "grad_norm": 78.95710754394531,
      "learning_rate": 0.00015672,
      "loss": -115.6779,
      "step": 16230
    },
    {
      "epoch": 1.2992,
      "grad_norm": 64.15848541259766,
      "learning_rate": 0.00015669333333333333,
      "loss": -113.9931,
      "step": 16240
    },
    {
      "epoch": 1.3,
      "grad_norm": 87.58484649658203,
      "learning_rate": 0.00015666666666666666,
      "loss": -114.8104,
      "step": 16250
    },
    {
      "epoch": 1.3008,
      "grad_norm": 58.95500564575195,
      "learning_rate": 0.00015664000000000002,
      "loss": -114.619,
      "step": 16260
    },
    {
      "epoch": 1.3016,
      "grad_norm": 84.90796661376953,
      "learning_rate": 0.00015661333333333335,
      "loss": -115.0985,
      "step": 16270
    },
    {
      "epoch": 1.3024,
      "grad_norm": 120.64408874511719,
      "learning_rate": 0.00015658666666666668,
      "loss": -115.5895,
      "step": 16280
    },
    {
      "epoch": 1.3032,
      "grad_norm": 93.01589965820312,
      "learning_rate": 0.00015656,
      "loss": -114.5221,
      "step": 16290
    },
    {
      "epoch": 1.304,
      "grad_norm": 67.82109069824219,
      "learning_rate": 0.00015653333333333333,
      "loss": -114.7282,
      "step": 16300
    },
    {
      "epoch": 1.3048,
      "grad_norm": 80.30514526367188,
      "learning_rate": 0.00015650666666666666,
      "loss": -114.4577,
      "step": 16310
    },
    {
      "epoch": 1.3056,
      "grad_norm": 60.241153717041016,
      "learning_rate": 0.00015648,
      "loss": -113.21,
      "step": 16320
    },
    {
      "epoch": 1.3064,
      "grad_norm": 75.52767181396484,
      "learning_rate": 0.00015645333333333335,
      "loss": -115.509,
      "step": 16330
    },
    {
      "epoch": 1.3072,
      "grad_norm": 106.75074005126953,
      "learning_rate": 0.00015642666666666668,
      "loss": -114.4855,
      "step": 16340
    },
    {
      "epoch": 1.308,
      "grad_norm": 81.57096862792969,
      "learning_rate": 0.0001564,
      "loss": -114.3223,
      "step": 16350
    },
    {
      "epoch": 1.3088,
      "grad_norm": 81.22389221191406,
      "learning_rate": 0.00015637333333333334,
      "loss": -114.4565,
      "step": 16360
    },
    {
      "epoch": 1.3096,
      "grad_norm": 163.5589599609375,
      "learning_rate": 0.00015634666666666667,
      "loss": -115.011,
      "step": 16370
    },
    {
      "epoch": 1.3104,
      "grad_norm": 58.090240478515625,
      "learning_rate": 0.00015632,
      "loss": -115.582,
      "step": 16380
    },
    {
      "epoch": 1.3112,
      "grad_norm": 98.97676849365234,
      "learning_rate": 0.00015629333333333332,
      "loss": -113.687,
      "step": 16390
    },
    {
      "epoch": 1.312,
      "grad_norm": 73.71510314941406,
      "learning_rate": 0.00015626666666666668,
      "loss": -114.7824,
      "step": 16400
    },
    {
      "epoch": 1.3128,
      "grad_norm": 90.36725616455078,
      "learning_rate": 0.00015624,
      "loss": -114.1302,
      "step": 16410
    },
    {
      "epoch": 1.3136,
      "grad_norm": 64.13126373291016,
      "learning_rate": 0.00015621333333333334,
      "loss": -114.1679,
      "step": 16420
    },
    {
      "epoch": 1.3144,
      "grad_norm": 82.82881164550781,
      "learning_rate": 0.0001561866666666667,
      "loss": -114.0715,
      "step": 16430
    },
    {
      "epoch": 1.3152,
      "grad_norm": 126.65470886230469,
      "learning_rate": 0.00015616000000000002,
      "loss": -113.7823,
      "step": 16440
    },
    {
      "epoch": 1.316,
      "grad_norm": 95.8511962890625,
      "learning_rate": 0.00015613333333333332,
      "loss": -115.2966,
      "step": 16450
    },
    {
      "epoch": 1.3168,
      "grad_norm": 62.27412414550781,
      "learning_rate": 0.00015610666666666668,
      "loss": -113.6475,
      "step": 16460
    },
    {
      "epoch": 1.3176,
      "grad_norm": 70.88752746582031,
      "learning_rate": 0.00015608,
      "loss": -114.2519,
      "step": 16470
    },
    {
      "epoch": 1.3184,
      "grad_norm": 71.811279296875,
      "learning_rate": 0.00015605333333333334,
      "loss": -113.5948,
      "step": 16480
    },
    {
      "epoch": 1.3192,
      "grad_norm": 75.82632446289062,
      "learning_rate": 0.00015602666666666667,
      "loss": -115.2355,
      "step": 16490
    },
    {
      "epoch": 1.32,
      "grad_norm": 57.94560241699219,
      "learning_rate": 0.00015600000000000002,
      "loss": -115.1534,
      "step": 16500
    },
    {
      "epoch": 1.3208,
      "grad_norm": 58.40652847290039,
      "learning_rate": 0.00015597333333333335,
      "loss": -113.7371,
      "step": 16510
    },
    {
      "epoch": 1.3216,
      "grad_norm": 64.99263000488281,
      "learning_rate": 0.00015594666666666666,
      "loss": -115.069,
      "step": 16520
    },
    {
      "epoch": 1.3224,
      "grad_norm": 163.9884490966797,
      "learning_rate": 0.00015592,
      "loss": -116.0522,
      "step": 16530
    },
    {
      "epoch": 1.3232,
      "grad_norm": 65.04570770263672,
      "learning_rate": 0.00015589333333333334,
      "loss": -114.1659,
      "step": 16540
    },
    {
      "epoch": 1.324,
      "grad_norm": 110.0140609741211,
      "learning_rate": 0.00015586666666666667,
      "loss": -116.5296,
      "step": 16550
    },
    {
      "epoch": 1.3248,
      "grad_norm": 115.91842651367188,
      "learning_rate": 0.00015584,
      "loss": -113.9111,
      "step": 16560
    },
    {
      "epoch": 1.3256000000000001,
      "grad_norm": 78.11107635498047,
      "learning_rate": 0.00015581333333333335,
      "loss": -114.7037,
      "step": 16570
    },
    {
      "epoch": 1.3264,
      "grad_norm": 59.704978942871094,
      "learning_rate": 0.00015578666666666668,
      "loss": -114.5512,
      "step": 16580
    },
    {
      "epoch": 1.3272,
      "grad_norm": 69.39225006103516,
      "learning_rate": 0.00015576,
      "loss": -114.8215,
      "step": 16590
    },
    {
      "epoch": 1.328,
      "grad_norm": 69.8138198852539,
      "learning_rate": 0.00015573333333333334,
      "loss": -115.8022,
      "step": 16600
    },
    {
      "epoch": 1.3288,
      "grad_norm": 44.7988166809082,
      "learning_rate": 0.00015570666666666667,
      "loss": -114.7751,
      "step": 16610
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 94.6048355102539,
      "learning_rate": 0.00015568,
      "loss": -114.4283,
      "step": 16620
    },
    {
      "epoch": 1.3304,
      "grad_norm": 64.53520965576172,
      "learning_rate": 0.00015565333333333333,
      "loss": -115.2046,
      "step": 16630
    },
    {
      "epoch": 1.3312,
      "grad_norm": 77.26148223876953,
      "learning_rate": 0.00015562666666666669,
      "loss": -114.7066,
      "step": 16640
    },
    {
      "epoch": 1.332,
      "grad_norm": 57.10806655883789,
      "learning_rate": 0.00015560000000000001,
      "loss": -114.4143,
      "step": 16650
    },
    {
      "epoch": 1.3328,
      "grad_norm": 126.77438354492188,
      "learning_rate": 0.00015557333333333334,
      "loss": -114.3425,
      "step": 16660
    },
    {
      "epoch": 1.3336000000000001,
      "grad_norm": 75.69691467285156,
      "learning_rate": 0.00015554666666666667,
      "loss": -113.7493,
      "step": 16670
    },
    {
      "epoch": 1.3344,
      "grad_norm": 86.56230163574219,
      "learning_rate": 0.00015552,
      "loss": -114.7643,
      "step": 16680
    },
    {
      "epoch": 1.3352,
      "grad_norm": 51.08270263671875,
      "learning_rate": 0.00015549333333333333,
      "loss": -113.9479,
      "step": 16690
    },
    {
      "epoch": 1.336,
      "grad_norm": 96.54212951660156,
      "learning_rate": 0.00015546666666666666,
      "loss": -115.8298,
      "step": 16700
    },
    {
      "epoch": 1.3368,
      "grad_norm": 97.56843566894531,
      "learning_rate": 0.00015544000000000002,
      "loss": -115.459,
      "step": 16710
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 68.85564422607422,
      "learning_rate": 0.00015541333333333335,
      "loss": -113.6072,
      "step": 16720
    },
    {
      "epoch": 1.3384,
      "grad_norm": 77.0892105102539,
      "learning_rate": 0.00015538666666666667,
      "loss": -114.7768,
      "step": 16730
    },
    {
      "epoch": 1.3392,
      "grad_norm": 97.71812438964844,
      "learning_rate": 0.00015536,
      "loss": -114.2472,
      "step": 16740
    },
    {
      "epoch": 1.34,
      "grad_norm": 85.23298645019531,
      "learning_rate": 0.00015533333333333333,
      "loss": -115.3711,
      "step": 16750
    },
    {
      "epoch": 1.3408,
      "grad_norm": 106.13204193115234,
      "learning_rate": 0.00015530666666666666,
      "loss": -115.0921,
      "step": 16760
    },
    {
      "epoch": 1.3416000000000001,
      "grad_norm": 78.591796875,
      "learning_rate": 0.00015528,
      "loss": -115.8826,
      "step": 16770
    },
    {
      "epoch": 1.3424,
      "grad_norm": 64.33605194091797,
      "learning_rate": 0.00015525333333333335,
      "loss": -114.0855,
      "step": 16780
    },
    {
      "epoch": 1.3432,
      "grad_norm": 101.95280456542969,
      "learning_rate": 0.00015522666666666668,
      "loss": -114.7928,
      "step": 16790
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 76.78368377685547,
      "learning_rate": 0.0001552,
      "loss": -115.7594,
      "step": 16800
    },
    {
      "epoch": 1.3448,
      "grad_norm": 88.67174530029297,
      "learning_rate": 0.00015517333333333336,
      "loss": -113.4053,
      "step": 16810
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 60.25768280029297,
      "learning_rate": 0.0001551466666666667,
      "loss": -114.7951,
      "step": 16820
    },
    {
      "epoch": 1.3464,
      "grad_norm": 48.45520782470703,
      "learning_rate": 0.00015512,
      "loss": -115.5058,
      "step": 16830
    },
    {
      "epoch": 1.3472,
      "grad_norm": 91.3466567993164,
      "learning_rate": 0.00015509333333333335,
      "loss": -114.3789,
      "step": 16840
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 97.52607727050781,
      "learning_rate": 0.00015506666666666668,
      "loss": -114.9315,
      "step": 16850
    },
    {
      "epoch": 1.3488,
      "grad_norm": 78.79407501220703,
      "learning_rate": 0.00015504,
      "loss": -116.1952,
      "step": 16860
    },
    {
      "epoch": 1.3496000000000001,
      "grad_norm": 105.06755065917969,
      "learning_rate": 0.00015501333333333334,
      "loss": -115.9658,
      "step": 16870
    },
    {
      "epoch": 1.3504,
      "grad_norm": 101.53050231933594,
      "learning_rate": 0.0001549866666666667,
      "loss": -115.4141,
      "step": 16880
    },
    {
      "epoch": 1.3512,
      "grad_norm": 102.50237274169922,
      "learning_rate": 0.00015496000000000002,
      "loss": -114.3934,
      "step": 16890
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 80.12030792236328,
      "learning_rate": 0.00015493333333333332,
      "loss": -116.6766,
      "step": 16900
    },
    {
      "epoch": 1.3528,
      "grad_norm": 69.00224304199219,
      "learning_rate": 0.00015490666666666668,
      "loss": -115.0785,
      "step": 16910
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 111.02576446533203,
      "learning_rate": 0.00015488,
      "loss": -114.8326,
      "step": 16920
    },
    {
      "epoch": 1.3544,
      "grad_norm": 62.76764678955078,
      "learning_rate": 0.00015485333333333334,
      "loss": -115.6438,
      "step": 16930
    },
    {
      "epoch": 1.3552,
      "grad_norm": 171.39315795898438,
      "learning_rate": 0.00015482666666666667,
      "loss": -115.6589,
      "step": 16940
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 79.42778015136719,
      "learning_rate": 0.00015480000000000002,
      "loss": -114.4614,
      "step": 16950
    },
    {
      "epoch": 1.3568,
      "grad_norm": 80.10526275634766,
      "learning_rate": 0.00015477333333333335,
      "loss": -115.2829,
      "step": 16960
    },
    {
      "epoch": 1.3576,
      "grad_norm": 107.54898834228516,
      "learning_rate": 0.00015474666666666668,
      "loss": -114.9441,
      "step": 16970
    },
    {
      "epoch": 1.3584,
      "grad_norm": 90.0042724609375,
      "learning_rate": 0.00015472,
      "loss": -114.2897,
      "step": 16980
    },
    {
      "epoch": 1.3592,
      "grad_norm": 95.03338623046875,
      "learning_rate": 0.00015469333333333334,
      "loss": -115.3061,
      "step": 16990
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 103.2951431274414,
      "learning_rate": 0.00015466666666666667,
      "loss": -116.4417,
      "step": 17000
    },
    {
      "epoch": 1.3608,
      "grad_norm": 128.1478729248047,
      "learning_rate": 0.00015464,
      "loss": -114.8642,
      "step": 17010
    },
    {
      "epoch": 1.3616,
      "grad_norm": 83.5162582397461,
      "learning_rate": 0.00015461333333333335,
      "loss": -115.6854,
      "step": 17020
    },
    {
      "epoch": 1.3624,
      "grad_norm": 96.93244171142578,
      "learning_rate": 0.00015458666666666668,
      "loss": -114.787,
      "step": 17030
    },
    {
      "epoch": 1.3632,
      "grad_norm": 107.56535339355469,
      "learning_rate": 0.00015456,
      "loss": -114.3117,
      "step": 17040
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 126.6244888305664,
      "learning_rate": 0.00015453333333333334,
      "loss": -113.9875,
      "step": 17050
    },
    {
      "epoch": 1.3648,
      "grad_norm": 91.03846740722656,
      "learning_rate": 0.00015450666666666667,
      "loss": -114.5103,
      "step": 17060
    },
    {
      "epoch": 1.3656,
      "grad_norm": 62.16288757324219,
      "learning_rate": 0.00015448,
      "loss": -114.7584,
      "step": 17070
    },
    {
      "epoch": 1.3664,
      "grad_norm": 212.02532958984375,
      "learning_rate": 0.00015445333333333333,
      "loss": -114.1839,
      "step": 17080
    },
    {
      "epoch": 1.3672,
      "grad_norm": 151.69691467285156,
      "learning_rate": 0.00015442666666666668,
      "loss": -113.764,
      "step": 17090
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 99.244873046875,
      "learning_rate": 0.0001544,
      "loss": -113.894,
      "step": 17100
    },
    {
      "epoch": 1.3688,
      "grad_norm": 146.77548217773438,
      "learning_rate": 0.00015437333333333334,
      "loss": -113.6039,
      "step": 17110
    },
    {
      "epoch": 1.3696,
      "grad_norm": 89.54518127441406,
      "learning_rate": 0.00015434666666666667,
      "loss": -114.9148,
      "step": 17120
    },
    {
      "epoch": 1.3704,
      "grad_norm": 57.71653366088867,
      "learning_rate": 0.00015432,
      "loss": -116.0776,
      "step": 17130
    },
    {
      "epoch": 1.3712,
      "grad_norm": 87.0379867553711,
      "learning_rate": 0.00015429333333333333,
      "loss": -115.7216,
      "step": 17140
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 84.24197387695312,
      "learning_rate": 0.00015426666666666666,
      "loss": -114.6993,
      "step": 17150
    },
    {
      "epoch": 1.3728,
      "grad_norm": 123.99501037597656,
      "learning_rate": 0.00015424000000000001,
      "loss": -113.2966,
      "step": 17160
    },
    {
      "epoch": 1.3736,
      "grad_norm": 105.42259216308594,
      "learning_rate": 0.00015421333333333334,
      "loss": -116.18,
      "step": 17170
    },
    {
      "epoch": 1.3744,
      "grad_norm": 78.63032531738281,
      "learning_rate": 0.00015418666666666667,
      "loss": -114.9193,
      "step": 17180
    },
    {
      "epoch": 1.3752,
      "grad_norm": 138.77560424804688,
      "learning_rate": 0.00015416000000000003,
      "loss": -115.0201,
      "step": 17190
    },
    {
      "epoch": 1.376,
      "grad_norm": 98.47191619873047,
      "learning_rate": 0.00015413333333333336,
      "loss": -115.1145,
      "step": 17200
    },
    {
      "epoch": 1.3768,
      "grad_norm": 118.61788940429688,
      "learning_rate": 0.00015410666666666666,
      "loss": -114.1625,
      "step": 17210
    },
    {
      "epoch": 1.3776,
      "grad_norm": 129.3617706298828,
      "learning_rate": 0.00015408000000000002,
      "loss": -114.9613,
      "step": 17220
    },
    {
      "epoch": 1.3784,
      "grad_norm": 72.84434509277344,
      "learning_rate": 0.00015405333333333334,
      "loss": -115.0775,
      "step": 17230
    },
    {
      "epoch": 1.3792,
      "grad_norm": 107.966064453125,
      "learning_rate": 0.00015402666666666667,
      "loss": -115.7508,
      "step": 17240
    },
    {
      "epoch": 1.38,
      "grad_norm": 101.05348205566406,
      "learning_rate": 0.000154,
      "loss": -115.761,
      "step": 17250
    },
    {
      "epoch": 1.3808,
      "grad_norm": 114.09282684326172,
      "learning_rate": 0.00015397333333333336,
      "loss": -115.2056,
      "step": 17260
    },
    {
      "epoch": 1.3816,
      "grad_norm": 102.27743530273438,
      "learning_rate": 0.0001539466666666667,
      "loss": -115.9483,
      "step": 17270
    },
    {
      "epoch": 1.3824,
      "grad_norm": 176.86581420898438,
      "learning_rate": 0.00015392,
      "loss": -116.0894,
      "step": 17280
    },
    {
      "epoch": 1.3832,
      "grad_norm": 113.4091796875,
      "learning_rate": 0.00015389333333333335,
      "loss": -113.1431,
      "step": 17290
    },
    {
      "epoch": 1.384,
      "grad_norm": 132.01315307617188,
      "learning_rate": 0.00015386666666666668,
      "loss": -114.5842,
      "step": 17300
    },
    {
      "epoch": 1.3848,
      "grad_norm": 176.1749725341797,
      "learning_rate": 0.00015384,
      "loss": -115.4279,
      "step": 17310
    },
    {
      "epoch": 1.3856,
      "grad_norm": 86.22509765625,
      "learning_rate": 0.00015381333333333333,
      "loss": -115.0501,
      "step": 17320
    },
    {
      "epoch": 1.3864,
      "grad_norm": 97.61686706542969,
      "learning_rate": 0.0001537866666666667,
      "loss": -114.7695,
      "step": 17330
    },
    {
      "epoch": 1.3872,
      "grad_norm": 111.5750503540039,
      "learning_rate": 0.00015376000000000002,
      "loss": -114.9692,
      "step": 17340
    },
    {
      "epoch": 1.388,
      "grad_norm": 87.89727020263672,
      "learning_rate": 0.00015373333333333335,
      "loss": -116.2915,
      "step": 17350
    },
    {
      "epoch": 1.3888,
      "grad_norm": 73.56796264648438,
      "learning_rate": 0.00015370666666666668,
      "loss": -115.4429,
      "step": 17360
    },
    {
      "epoch": 1.3896,
      "grad_norm": 87.98904418945312,
      "learning_rate": 0.00015368,
      "loss": -114.2506,
      "step": 17370
    },
    {
      "epoch": 1.3904,
      "grad_norm": 131.310791015625,
      "learning_rate": 0.00015365333333333333,
      "loss": -114.5834,
      "step": 17380
    },
    {
      "epoch": 1.3912,
      "grad_norm": 104.18254852294922,
      "learning_rate": 0.00015362666666666666,
      "loss": -115.1767,
      "step": 17390
    },
    {
      "epoch": 1.392,
      "grad_norm": 61.713905334472656,
      "learning_rate": 0.00015360000000000002,
      "loss": -115.6226,
      "step": 17400
    },
    {
      "epoch": 1.3928,
      "grad_norm": 117.40975952148438,
      "learning_rate": 0.00015357333333333335,
      "loss": -115.1487,
      "step": 17410
    },
    {
      "epoch": 1.3936,
      "grad_norm": 75.73595428466797,
      "learning_rate": 0.00015354666666666668,
      "loss": -113.9623,
      "step": 17420
    },
    {
      "epoch": 1.3944,
      "grad_norm": 122.25657653808594,
      "learning_rate": 0.00015352,
      "loss": -115.3132,
      "step": 17430
    },
    {
      "epoch": 1.3952,
      "grad_norm": 101.19903564453125,
      "learning_rate": 0.00015349333333333334,
      "loss": -116.3418,
      "step": 17440
    },
    {
      "epoch": 1.396,
      "grad_norm": 96.31082916259766,
      "learning_rate": 0.00015346666666666667,
      "loss": -115.7541,
      "step": 17450
    },
    {
      "epoch": 1.3968,
      "grad_norm": 94.24244689941406,
      "learning_rate": 0.00015344,
      "loss": -113.7735,
      "step": 17460
    },
    {
      "epoch": 1.3976,
      "grad_norm": 75.62796020507812,
      "learning_rate": 0.00015341333333333335,
      "loss": -115.0794,
      "step": 17470
    },
    {
      "epoch": 1.3984,
      "grad_norm": 161.42845153808594,
      "learning_rate": 0.00015338666666666668,
      "loss": -115.2572,
      "step": 17480
    },
    {
      "epoch": 1.3992,
      "grad_norm": 94.28527069091797,
      "learning_rate": 0.00015336,
      "loss": -116.0811,
      "step": 17490
    },
    {
      "epoch": 1.4,
      "grad_norm": 73.40411376953125,
      "learning_rate": 0.00015333333333333334,
      "loss": -116.3788,
      "step": 17500
    },
    {
      "epoch": 1.4008,
      "grad_norm": 106.11282348632812,
      "learning_rate": 0.00015330666666666667,
      "loss": -116.1084,
      "step": 17510
    },
    {
      "epoch": 1.4016,
      "grad_norm": 189.94876098632812,
      "learning_rate": 0.00015328,
      "loss": -114.7419,
      "step": 17520
    },
    {
      "epoch": 1.4024,
      "grad_norm": 149.2256622314453,
      "learning_rate": 0.00015325333333333333,
      "loss": -115.6957,
      "step": 17530
    },
    {
      "epoch": 1.4032,
      "grad_norm": 66.15582275390625,
      "learning_rate": 0.00015322666666666668,
      "loss": -114.199,
      "step": 17540
    },
    {
      "epoch": 1.404,
      "grad_norm": 49.109466552734375,
      "learning_rate": 0.0001532,
      "loss": -115.4166,
      "step": 17550
    },
    {
      "epoch": 1.4048,
      "grad_norm": 93.0813980102539,
      "learning_rate": 0.00015317333333333334,
      "loss": -115.1899,
      "step": 17560
    },
    {
      "epoch": 1.4056,
      "grad_norm": 91.90589141845703,
      "learning_rate": 0.0001531466666666667,
      "loss": -115.6583,
      "step": 17570
    },
    {
      "epoch": 1.4064,
      "grad_norm": 147.1775665283203,
      "learning_rate": 0.00015312,
      "loss": -116.6126,
      "step": 17580
    },
    {
      "epoch": 1.4072,
      "grad_norm": 87.8643798828125,
      "learning_rate": 0.00015309333333333333,
      "loss": -115.8774,
      "step": 17590
    },
    {
      "epoch": 1.408,
      "grad_norm": 142.28460693359375,
      "learning_rate": 0.00015306666666666666,
      "loss": -114.9736,
      "step": 17600
    },
    {
      "epoch": 1.4088,
      "grad_norm": 131.9186553955078,
      "learning_rate": 0.00015304,
      "loss": -116.0493,
      "step": 17610
    },
    {
      "epoch": 1.4096,
      "grad_norm": 140.47332763671875,
      "learning_rate": 0.00015301333333333334,
      "loss": -114.7704,
      "step": 17620
    },
    {
      "epoch": 1.4104,
      "grad_norm": 69.4457015991211,
      "learning_rate": 0.00015298666666666667,
      "loss": -115.6843,
      "step": 17630
    },
    {
      "epoch": 1.4112,
      "grad_norm": 165.82275390625,
      "learning_rate": 0.00015296000000000003,
      "loss": -115.0891,
      "step": 17640
    },
    {
      "epoch": 1.412,
      "grad_norm": 97.14949035644531,
      "learning_rate": 0.00015293333333333336,
      "loss": -116.2839,
      "step": 17650
    },
    {
      "epoch": 1.4128,
      "grad_norm": 63.30683135986328,
      "learning_rate": 0.00015290666666666666,
      "loss": -115.1248,
      "step": 17660
    },
    {
      "epoch": 1.4136,
      "grad_norm": 92.84410095214844,
      "learning_rate": 0.00015288,
      "loss": -116.0479,
      "step": 17670
    },
    {
      "epoch": 1.4144,
      "grad_norm": 137.94024658203125,
      "learning_rate": 0.00015285333333333334,
      "loss": -114.7177,
      "step": 17680
    },
    {
      "epoch": 1.4152,
      "grad_norm": 98.75066375732422,
      "learning_rate": 0.00015282666666666667,
      "loss": -114.3561,
      "step": 17690
    },
    {
      "epoch": 1.416,
      "grad_norm": 88.15868377685547,
      "learning_rate": 0.0001528,
      "loss": -116.4391,
      "step": 17700
    },
    {
      "epoch": 1.4168,
      "grad_norm": 89.73106384277344,
      "learning_rate": 0.00015277333333333336,
      "loss": -115.8323,
      "step": 17710
    },
    {
      "epoch": 1.4176,
      "grad_norm": 110.55674743652344,
      "learning_rate": 0.00015274666666666669,
      "loss": -116.3034,
      "step": 17720
    },
    {
      "epoch": 1.4184,
      "grad_norm": 112.49617004394531,
      "learning_rate": 0.00015272,
      "loss": -116.0716,
      "step": 17730
    },
    {
      "epoch": 1.4192,
      "grad_norm": 161.62319946289062,
      "learning_rate": 0.00015269333333333334,
      "loss": -115.2642,
      "step": 17740
    },
    {
      "epoch": 1.42,
      "grad_norm": 76.63613891601562,
      "learning_rate": 0.00015266666666666667,
      "loss": -116.1433,
      "step": 17750
    },
    {
      "epoch": 1.4208,
      "grad_norm": 121.79415130615234,
      "learning_rate": 0.00015264,
      "loss": -114.882,
      "step": 17760
    },
    {
      "epoch": 1.4216,
      "grad_norm": 104.71410369873047,
      "learning_rate": 0.00015261333333333333,
      "loss": -115.8487,
      "step": 17770
    },
    {
      "epoch": 1.4224,
      "grad_norm": 93.88151550292969,
      "learning_rate": 0.0001525866666666667,
      "loss": -113.944,
      "step": 17780
    },
    {
      "epoch": 1.4232,
      "grad_norm": 109.43047332763672,
      "learning_rate": 0.00015256000000000002,
      "loss": -114.9016,
      "step": 17790
    },
    {
      "epoch": 1.424,
      "grad_norm": 187.8695831298828,
      "learning_rate": 0.00015253333333333335,
      "loss": -115.9902,
      "step": 17800
    },
    {
      "epoch": 1.4248,
      "grad_norm": 66.53206634521484,
      "learning_rate": 0.00015250666666666667,
      "loss": -115.319,
      "step": 17810
    },
    {
      "epoch": 1.4256,
      "grad_norm": 71.1916732788086,
      "learning_rate": 0.00015248,
      "loss": -116.2813,
      "step": 17820
    },
    {
      "epoch": 1.4264000000000001,
      "grad_norm": 92.41678619384766,
      "learning_rate": 0.00015245333333333333,
      "loss": -115.0892,
      "step": 17830
    },
    {
      "epoch": 1.4272,
      "grad_norm": 72.15184783935547,
      "learning_rate": 0.00015242666666666666,
      "loss": -115.6256,
      "step": 17840
    },
    {
      "epoch": 1.428,
      "grad_norm": 114.06668090820312,
      "learning_rate": 0.00015240000000000002,
      "loss": -115.6842,
      "step": 17850
    },
    {
      "epoch": 1.4288,
      "grad_norm": 250.27066040039062,
      "learning_rate": 0.00015237333333333335,
      "loss": -115.0798,
      "step": 17860
    },
    {
      "epoch": 1.4296,
      "grad_norm": 141.8844757080078,
      "learning_rate": 0.00015234666666666668,
      "loss": -114.4561,
      "step": 17870
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 79.8505630493164,
      "learning_rate": 0.00015232,
      "loss": -117.3087,
      "step": 17880
    },
    {
      "epoch": 1.4312,
      "grad_norm": 154.81089782714844,
      "learning_rate": 0.00015229333333333333,
      "loss": -115.0128,
      "step": 17890
    },
    {
      "epoch": 1.432,
      "grad_norm": 113.76787567138672,
      "learning_rate": 0.00015226666666666666,
      "loss": -115.2107,
      "step": 17900
    },
    {
      "epoch": 1.4328,
      "grad_norm": 99.85737609863281,
      "learning_rate": 0.00015224,
      "loss": -116.0187,
      "step": 17910
    },
    {
      "epoch": 1.4336,
      "grad_norm": 171.55677795410156,
      "learning_rate": 0.00015221333333333335,
      "loss": -114.2756,
      "step": 17920
    },
    {
      "epoch": 1.4344000000000001,
      "grad_norm": 163.2982940673828,
      "learning_rate": 0.00015218666666666668,
      "loss": -115.774,
      "step": 17930
    },
    {
      "epoch": 1.4352,
      "grad_norm": 149.0499725341797,
      "learning_rate": 0.00015216,
      "loss": -113.5721,
      "step": 17940
    },
    {
      "epoch": 1.436,
      "grad_norm": 146.47386169433594,
      "learning_rate": 0.00015213333333333336,
      "loss": -115.4023,
      "step": 17950
    },
    {
      "epoch": 1.4368,
      "grad_norm": 195.08944702148438,
      "learning_rate": 0.00015210666666666666,
      "loss": -115.3525,
      "step": 17960
    },
    {
      "epoch": 1.4376,
      "grad_norm": 140.802001953125,
      "learning_rate": 0.00015208,
      "loss": -115.2593,
      "step": 17970
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 107.49595642089844,
      "learning_rate": 0.00015205333333333332,
      "loss": -114.9901,
      "step": 17980
    },
    {
      "epoch": 1.4392,
      "grad_norm": 80.19282531738281,
      "learning_rate": 0.00015202666666666668,
      "loss": -116.5206,
      "step": 17990
    },
    {
      "epoch": 1.44,
      "grad_norm": 184.98959350585938,
      "learning_rate": 0.000152,
      "loss": -114.5135,
      "step": 18000
    },
    {
      "epoch": 1.4408,
      "grad_norm": 142.6279296875,
      "learning_rate": 0.00015197333333333334,
      "loss": -114.8644,
      "step": 18010
    },
    {
      "epoch": 1.4416,
      "grad_norm": 232.3841094970703,
      "learning_rate": 0.0001519466666666667,
      "loss": -115.6808,
      "step": 18020
    },
    {
      "epoch": 1.4424000000000001,
      "grad_norm": 168.32005310058594,
      "learning_rate": 0.00015192000000000002,
      "loss": -114.5913,
      "step": 18030
    },
    {
      "epoch": 1.4432,
      "grad_norm": 128.15982055664062,
      "learning_rate": 0.00015189333333333332,
      "loss": -115.5277,
      "step": 18040
    },
    {
      "epoch": 1.444,
      "grad_norm": 152.1522674560547,
      "learning_rate": 0.00015186666666666668,
      "loss": -114.645,
      "step": 18050
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 225.24452209472656,
      "learning_rate": 0.00015184,
      "loss": -114.9344,
      "step": 18060
    },
    {
      "epoch": 1.4456,
      "grad_norm": 227.25100708007812,
      "learning_rate": 0.00015181333333333334,
      "loss": -114.4414,
      "step": 18070
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 149.58572387695312,
      "learning_rate": 0.00015178666666666667,
      "loss": -115.7105,
      "step": 18080
    },
    {
      "epoch": 1.4472,
      "grad_norm": 97.9288101196289,
      "learning_rate": 0.00015176000000000002,
      "loss": -116.3823,
      "step": 18090
    },
    {
      "epoch": 1.448,
      "grad_norm": 142.1650390625,
      "learning_rate": 0.00015173333333333335,
      "loss": -116.2564,
      "step": 18100
    },
    {
      "epoch": 1.4487999999999999,
      "grad_norm": 96.28116607666016,
      "learning_rate": 0.00015170666666666666,
      "loss": -115.9265,
      "step": 18110
    },
    {
      "epoch": 1.4496,
      "grad_norm": 106.31333923339844,
      "learning_rate": 0.00015168,
      "loss": -116.2889,
      "step": 18120
    },
    {
      "epoch": 1.4504000000000001,
      "grad_norm": 129.47879028320312,
      "learning_rate": 0.00015165333333333334,
      "loss": -115.4424,
      "step": 18130
    },
    {
      "epoch": 1.4512,
      "grad_norm": 90.49705505371094,
      "learning_rate": 0.00015162666666666667,
      "loss": -114.9517,
      "step": 18140
    },
    {
      "epoch": 1.452,
      "grad_norm": 138.20303344726562,
      "learning_rate": 0.0001516,
      "loss": -115.0532,
      "step": 18150
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 191.59596252441406,
      "learning_rate": 0.00015157333333333335,
      "loss": -115.6228,
      "step": 18160
    },
    {
      "epoch": 1.4536,
      "grad_norm": 124.96900939941406,
      "learning_rate": 0.00015154666666666668,
      "loss": -114.1716,
      "step": 18170
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 169.7991180419922,
      "learning_rate": 0.00015152,
      "loss": -116.0929,
      "step": 18180
    },
    {
      "epoch": 1.4552,
      "grad_norm": 184.28074645996094,
      "learning_rate": 0.00015149333333333334,
      "loss": -115.3819,
      "step": 18190
    },
    {
      "epoch": 1.456,
      "grad_norm": 105.40851593017578,
      "learning_rate": 0.00015146666666666667,
      "loss": -115.881,
      "step": 18200
    },
    {
      "epoch": 1.4567999999999999,
      "grad_norm": 118.74126434326172,
      "learning_rate": 0.00015144,
      "loss": -115.3833,
      "step": 18210
    },
    {
      "epoch": 1.4576,
      "grad_norm": 110.56692504882812,
      "learning_rate": 0.00015141333333333333,
      "loss": -115.6113,
      "step": 18220
    },
    {
      "epoch": 1.4584,
      "grad_norm": 105.51863098144531,
      "learning_rate": 0.00015138666666666669,
      "loss": -114.6124,
      "step": 18230
    },
    {
      "epoch": 1.4592,
      "grad_norm": 76.89215087890625,
      "learning_rate": 0.00015136000000000001,
      "loss": -116.3684,
      "step": 18240
    },
    {
      "epoch": 1.46,
      "grad_norm": 97.41831970214844,
      "learning_rate": 0.00015133333333333334,
      "loss": -114.6146,
      "step": 18250
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 172.13441467285156,
      "learning_rate": 0.00015130666666666667,
      "loss": -115.2355,
      "step": 18260
    },
    {
      "epoch": 1.4616,
      "grad_norm": 111.99754333496094,
      "learning_rate": 0.00015128,
      "loss": -115.4821,
      "step": 18270
    },
    {
      "epoch": 1.4624,
      "grad_norm": 131.30337524414062,
      "learning_rate": 0.00015125333333333333,
      "loss": -115.6157,
      "step": 18280
    },
    {
      "epoch": 1.4632,
      "grad_norm": 125.83778381347656,
      "learning_rate": 0.00015122666666666666,
      "loss": -115.9215,
      "step": 18290
    },
    {
      "epoch": 1.464,
      "grad_norm": 106.89964294433594,
      "learning_rate": 0.00015120000000000002,
      "loss": -116.9854,
      "step": 18300
    },
    {
      "epoch": 1.4647999999999999,
      "grad_norm": 133.2691650390625,
      "learning_rate": 0.00015117333333333335,
      "loss": -115.6006,
      "step": 18310
    },
    {
      "epoch": 1.4656,
      "grad_norm": 86.31963348388672,
      "learning_rate": 0.00015114666666666667,
      "loss": -116.2542,
      "step": 18320
    },
    {
      "epoch": 1.4664,
      "grad_norm": 159.8810272216797,
      "learning_rate": 0.00015112000000000003,
      "loss": -115.1782,
      "step": 18330
    },
    {
      "epoch": 1.4672,
      "grad_norm": 205.4381103515625,
      "learning_rate": 0.00015109333333333333,
      "loss": -114.9158,
      "step": 18340
    },
    {
      "epoch": 1.468,
      "grad_norm": 185.31114196777344,
      "learning_rate": 0.00015106666666666666,
      "loss": -114.8556,
      "step": 18350
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 178.12501525878906,
      "learning_rate": 0.00015104,
      "loss": -116.0519,
      "step": 18360
    },
    {
      "epoch": 1.4696,
      "grad_norm": 185.0387725830078,
      "learning_rate": 0.00015101333333333335,
      "loss": -115.2125,
      "step": 18370
    },
    {
      "epoch": 1.4704,
      "grad_norm": 104.61895751953125,
      "learning_rate": 0.00015098666666666668,
      "loss": -115.3439,
      "step": 18380
    },
    {
      "epoch": 1.4712,
      "grad_norm": 124.14167022705078,
      "learning_rate": 0.00015096,
      "loss": -115.6739,
      "step": 18390
    },
    {
      "epoch": 1.472,
      "grad_norm": 87.70626831054688,
      "learning_rate": 0.00015093333333333336,
      "loss": -115.7477,
      "step": 18400
    },
    {
      "epoch": 1.4727999999999999,
      "grad_norm": 110.68903350830078,
      "learning_rate": 0.00015090666666666666,
      "loss": -114.5295,
      "step": 18410
    },
    {
      "epoch": 1.4736,
      "grad_norm": 120.46629333496094,
      "learning_rate": 0.00015088,
      "loss": -114.7688,
      "step": 18420
    },
    {
      "epoch": 1.4744,
      "grad_norm": 156.96359252929688,
      "learning_rate": 0.00015085333333333335,
      "loss": -115.0222,
      "step": 18430
    },
    {
      "epoch": 1.4752,
      "grad_norm": 166.93624877929688,
      "learning_rate": 0.00015082666666666668,
      "loss": -116.2763,
      "step": 18440
    },
    {
      "epoch": 1.476,
      "grad_norm": 140.16505432128906,
      "learning_rate": 0.0001508,
      "loss": -115.983,
      "step": 18450
    },
    {
      "epoch": 1.4768,
      "grad_norm": 112.8608169555664,
      "learning_rate": 0.00015077333333333334,
      "loss": -114.7219,
      "step": 18460
    },
    {
      "epoch": 1.4776,
      "grad_norm": 222.93637084960938,
      "learning_rate": 0.0001507466666666667,
      "loss": -115.226,
      "step": 18470
    },
    {
      "epoch": 1.4784,
      "grad_norm": 173.019287109375,
      "learning_rate": 0.00015072000000000002,
      "loss": -115.8579,
      "step": 18480
    },
    {
      "epoch": 1.4792,
      "grad_norm": 126.49242401123047,
      "learning_rate": 0.00015069333333333332,
      "loss": -115.9774,
      "step": 18490
    },
    {
      "epoch": 1.48,
      "grad_norm": 199.53759765625,
      "learning_rate": 0.00015066666666666668,
      "loss": -115.6807,
      "step": 18500
    },
    {
      "epoch": 1.4808,
      "grad_norm": 95.5223388671875,
      "learning_rate": 0.00015064,
      "loss": -115.8208,
      "step": 18510
    },
    {
      "epoch": 1.4816,
      "grad_norm": 167.42669677734375,
      "learning_rate": 0.00015061333333333334,
      "loss": -116.7128,
      "step": 18520
    },
    {
      "epoch": 1.4824,
      "grad_norm": 117.57063293457031,
      "learning_rate": 0.00015058666666666667,
      "loss": -115.8243,
      "step": 18530
    },
    {
      "epoch": 1.4832,
      "grad_norm": 155.7910919189453,
      "learning_rate": 0.00015056000000000002,
      "loss": -115.8633,
      "step": 18540
    },
    {
      "epoch": 1.484,
      "grad_norm": 131.78286743164062,
      "learning_rate": 0.00015053333333333335,
      "loss": -116.1946,
      "step": 18550
    },
    {
      "epoch": 1.4848,
      "grad_norm": 113.91564178466797,
      "learning_rate": 0.00015050666666666668,
      "loss": -115.6914,
      "step": 18560
    },
    {
      "epoch": 1.4856,
      "grad_norm": 130.55397033691406,
      "learning_rate": 0.00015048,
      "loss": -115.9993,
      "step": 18570
    },
    {
      "epoch": 1.4864,
      "grad_norm": 113.47891235351562,
      "learning_rate": 0.00015045333333333334,
      "loss": -115.1526,
      "step": 18580
    },
    {
      "epoch": 1.4872,
      "grad_norm": 108.70222473144531,
      "learning_rate": 0.00015042666666666667,
      "loss": -115.6815,
      "step": 18590
    },
    {
      "epoch": 1.488,
      "grad_norm": 88.6073989868164,
      "learning_rate": 0.0001504,
      "loss": -116.8552,
      "step": 18600
    },
    {
      "epoch": 1.4888,
      "grad_norm": 125.49088287353516,
      "learning_rate": 0.00015037333333333335,
      "loss": -117.0074,
      "step": 18610
    },
    {
      "epoch": 1.4896,
      "grad_norm": 104.73514556884766,
      "learning_rate": 0.00015034666666666668,
      "loss": -116.7605,
      "step": 18620
    },
    {
      "epoch": 1.4904,
      "grad_norm": 154.51980590820312,
      "learning_rate": 0.00015032,
      "loss": -116.7597,
      "step": 18630
    },
    {
      "epoch": 1.4912,
      "grad_norm": 127.53409576416016,
      "learning_rate": 0.00015029333333333334,
      "loss": -115.6439,
      "step": 18640
    },
    {
      "epoch": 1.492,
      "grad_norm": 151.90438842773438,
      "learning_rate": 0.00015026666666666667,
      "loss": -116.9286,
      "step": 18650
    },
    {
      "epoch": 1.4928,
      "grad_norm": 104.15673828125,
      "learning_rate": 0.00015024,
      "loss": -115.9152,
      "step": 18660
    },
    {
      "epoch": 1.4936,
      "grad_norm": 141.10568237304688,
      "learning_rate": 0.00015021333333333333,
      "loss": -114.8121,
      "step": 18670
    },
    {
      "epoch": 1.4944,
      "grad_norm": 136.17318725585938,
      "learning_rate": 0.00015018666666666668,
      "loss": -116.292,
      "step": 18680
    },
    {
      "epoch": 1.4952,
      "grad_norm": 209.73873901367188,
      "learning_rate": 0.00015016,
      "loss": -114.8259,
      "step": 18690
    },
    {
      "epoch": 1.496,
      "grad_norm": 200.49478149414062,
      "learning_rate": 0.00015013333333333334,
      "loss": -114.8994,
      "step": 18700
    },
    {
      "epoch": 1.4968,
      "grad_norm": 140.9697265625,
      "learning_rate": 0.0001501066666666667,
      "loss": -115.1965,
      "step": 18710
    },
    {
      "epoch": 1.4976,
      "grad_norm": 151.14404296875,
      "learning_rate": 0.00015008,
      "loss": -116.9369,
      "step": 18720
    },
    {
      "epoch": 1.4984,
      "grad_norm": 123.34342193603516,
      "learning_rate": 0.00015005333333333333,
      "loss": -115.7607,
      "step": 18730
    },
    {
      "epoch": 1.4992,
      "grad_norm": 94.84044647216797,
      "learning_rate": 0.00015002666666666666,
      "loss": -116.9432,
      "step": 18740
    },
    {
      "epoch": 1.5,
      "grad_norm": 100.33491516113281,
      "learning_rate": 0.00015000000000000001,
      "loss": -116.7705,
      "step": 18750
    },
    {
      "epoch": 1.5008,
      "grad_norm": 127.5469970703125,
      "learning_rate": 0.00014997333333333334,
      "loss": -116.3576,
      "step": 18760
    },
    {
      "epoch": 1.5016,
      "grad_norm": 144.01626586914062,
      "learning_rate": 0.00014994666666666667,
      "loss": -116.1518,
      "step": 18770
    },
    {
      "epoch": 1.5024,
      "grad_norm": 81.61959075927734,
      "learning_rate": 0.00014992000000000003,
      "loss": -115.5265,
      "step": 18780
    },
    {
      "epoch": 1.5032,
      "grad_norm": 98.15473175048828,
      "learning_rate": 0.00014989333333333333,
      "loss": -116.0967,
      "step": 18790
    },
    {
      "epoch": 1.504,
      "grad_norm": 106.14464569091797,
      "learning_rate": 0.00014986666666666666,
      "loss": -115.5247,
      "step": 18800
    },
    {
      "epoch": 1.5048,
      "grad_norm": 145.24951171875,
      "learning_rate": 0.00014984000000000002,
      "loss": -117.3233,
      "step": 18810
    },
    {
      "epoch": 1.5056,
      "grad_norm": 183.17974853515625,
      "learning_rate": 0.00014981333333333334,
      "loss": -115.0235,
      "step": 18820
    },
    {
      "epoch": 1.5064,
      "grad_norm": 90.98548889160156,
      "learning_rate": 0.00014978666666666667,
      "loss": -116.6618,
      "step": 18830
    },
    {
      "epoch": 1.5072,
      "grad_norm": 122.4155044555664,
      "learning_rate": 0.00014976,
      "loss": -115.9697,
      "step": 18840
    },
    {
      "epoch": 1.508,
      "grad_norm": 113.578125,
      "learning_rate": 0.00014973333333333336,
      "loss": -115.859,
      "step": 18850
    },
    {
      "epoch": 1.5088,
      "grad_norm": 91.15010833740234,
      "learning_rate": 0.0001497066666666667,
      "loss": -116.6,
      "step": 18860
    },
    {
      "epoch": 1.5096,
      "grad_norm": 142.23367309570312,
      "learning_rate": 0.00014968,
      "loss": -115.8733,
      "step": 18870
    },
    {
      "epoch": 1.5104,
      "grad_norm": 133.78623962402344,
      "learning_rate": 0.00014965333333333335,
      "loss": -115.9124,
      "step": 18880
    },
    {
      "epoch": 1.5112,
      "grad_norm": 91.92915344238281,
      "learning_rate": 0.00014962666666666668,
      "loss": -115.0981,
      "step": 18890
    },
    {
      "epoch": 1.512,
      "grad_norm": 134.7578582763672,
      "learning_rate": 0.0001496,
      "loss": -116.3391,
      "step": 18900
    },
    {
      "epoch": 1.5128,
      "grad_norm": 83.88250732421875,
      "learning_rate": 0.00014957333333333333,
      "loss": -116.9993,
      "step": 18910
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 254.49420166015625,
      "learning_rate": 0.0001495466666666667,
      "loss": -117.2141,
      "step": 18920
    },
    {
      "epoch": 1.5144,
      "grad_norm": 102.33089447021484,
      "learning_rate": 0.00014952000000000002,
      "loss": -115.7563,
      "step": 18930
    },
    {
      "epoch": 1.5152,
      "grad_norm": 129.2823944091797,
      "learning_rate": 0.00014949333333333332,
      "loss": -115.1999,
      "step": 18940
    },
    {
      "epoch": 1.516,
      "grad_norm": 128.7246856689453,
      "learning_rate": 0.00014946666666666668,
      "loss": -116.217,
      "step": 18950
    },
    {
      "epoch": 1.5168,
      "grad_norm": 149.83409118652344,
      "learning_rate": 0.00014944,
      "loss": -116.8016,
      "step": 18960
    },
    {
      "epoch": 1.5175999999999998,
      "grad_norm": 136.04066467285156,
      "learning_rate": 0.00014941333333333333,
      "loss": -115.747,
      "step": 18970
    },
    {
      "epoch": 1.5184,
      "grad_norm": 94.2752914428711,
      "learning_rate": 0.00014938666666666666,
      "loss": -117.4533,
      "step": 18980
    },
    {
      "epoch": 1.5192,
      "grad_norm": 102.44189453125,
      "learning_rate": 0.00014936000000000002,
      "loss": -116.3212,
      "step": 18990
    },
    {
      "epoch": 1.52,
      "grad_norm": 74.66812133789062,
      "learning_rate": 0.00014933333333333335,
      "loss": -116.59,
      "step": 19000
    },
    {
      "epoch": 1.5208,
      "grad_norm": 120.82369232177734,
      "learning_rate": 0.00014930666666666668,
      "loss": -116.2878,
      "step": 19010
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 71.06863403320312,
      "learning_rate": 0.00014928,
      "loss": -115.4726,
      "step": 19020
    },
    {
      "epoch": 1.5224,
      "grad_norm": 128.14987182617188,
      "learning_rate": 0.00014925333333333334,
      "loss": -117.4018,
      "step": 19030
    },
    {
      "epoch": 1.5232,
      "grad_norm": 144.6573486328125,
      "learning_rate": 0.00014922666666666667,
      "loss": -116.1538,
      "step": 19040
    },
    {
      "epoch": 1.524,
      "grad_norm": 138.1166534423828,
      "learning_rate": 0.0001492,
      "loss": -116.5576,
      "step": 19050
    },
    {
      "epoch": 1.5248,
      "grad_norm": 124.34595489501953,
      "learning_rate": 0.00014917333333333335,
      "loss": -116.4695,
      "step": 19060
    },
    {
      "epoch": 1.5255999999999998,
      "grad_norm": 111.53895568847656,
      "learning_rate": 0.00014914666666666668,
      "loss": -116.6372,
      "step": 19070
    },
    {
      "epoch": 1.5264,
      "grad_norm": 116.14805603027344,
      "learning_rate": 0.00014912,
      "loss": -114.6015,
      "step": 19080
    },
    {
      "epoch": 1.5272000000000001,
      "grad_norm": 114.05790710449219,
      "learning_rate": 0.00014909333333333337,
      "loss": -115.3978,
      "step": 19090
    },
    {
      "epoch": 1.528,
      "grad_norm": 126.66028594970703,
      "learning_rate": 0.00014906666666666667,
      "loss": -116.1419,
      "step": 19100
    },
    {
      "epoch": 1.5288,
      "grad_norm": 103.25312805175781,
      "learning_rate": 0.00014904,
      "loss": -115.9657,
      "step": 19110
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 114.5788803100586,
      "learning_rate": 0.00014901333333333333,
      "loss": -115.0715,
      "step": 19120
    },
    {
      "epoch": 1.5304,
      "grad_norm": 104.76988220214844,
      "learning_rate": 0.00014898666666666668,
      "loss": -116.6733,
      "step": 19130
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 73.1800308227539,
      "learning_rate": 0.00014896,
      "loss": -115.5177,
      "step": 19140
    },
    {
      "epoch": 1.532,
      "grad_norm": 120.61203002929688,
      "learning_rate": 0.00014893333333333334,
      "loss": -116.8138,
      "step": 19150
    },
    {
      "epoch": 1.5328,
      "grad_norm": 121.82381439208984,
      "learning_rate": 0.0001489066666666667,
      "loss": -116.9085,
      "step": 19160
    },
    {
      "epoch": 1.5335999999999999,
      "grad_norm": 123.35436248779297,
      "learning_rate": 0.00014888,
      "loss": -115.9249,
      "step": 19170
    },
    {
      "epoch": 1.5344,
      "grad_norm": 138.04934692382812,
      "learning_rate": 0.00014885333333333333,
      "loss": -115.2447,
      "step": 19180
    },
    {
      "epoch": 1.5352000000000001,
      "grad_norm": 135.4754638671875,
      "learning_rate": 0.00014882666666666668,
      "loss": -115.6018,
      "step": 19190
    },
    {
      "epoch": 1.536,
      "grad_norm": 100.87679290771484,
      "learning_rate": 0.0001488,
      "loss": -115.5707,
      "step": 19200
    },
    {
      "epoch": 1.5368,
      "grad_norm": 172.39833068847656,
      "learning_rate": 0.00014877333333333334,
      "loss": -117.1926,
      "step": 19210
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 177.44622802734375,
      "learning_rate": 0.00014874666666666667,
      "loss": -115.1973,
      "step": 19220
    },
    {
      "epoch": 1.5384,
      "grad_norm": 127.31195831298828,
      "learning_rate": 0.00014872000000000003,
      "loss": -115.3427,
      "step": 19230
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 132.6955108642578,
      "learning_rate": 0.00014869333333333336,
      "loss": -116.5287,
      "step": 19240
    },
    {
      "epoch": 1.54,
      "grad_norm": 80.42927551269531,
      "learning_rate": 0.00014866666666666666,
      "loss": -116.4717,
      "step": 19250
    },
    {
      "epoch": 1.5408,
      "grad_norm": 89.02778625488281,
      "learning_rate": 0.00014864,
      "loss": -116.6381,
      "step": 19260
    },
    {
      "epoch": 1.5415999999999999,
      "grad_norm": 90.66706848144531,
      "learning_rate": 0.00014861333333333334,
      "loss": -115.588,
      "step": 19270
    },
    {
      "epoch": 1.5424,
      "grad_norm": 109.04704284667969,
      "learning_rate": 0.00014858666666666667,
      "loss": -116.842,
      "step": 19280
    },
    {
      "epoch": 1.5432000000000001,
      "grad_norm": 110.11137390136719,
      "learning_rate": 0.00014856,
      "loss": -116.7763,
      "step": 19290
    },
    {
      "epoch": 1.544,
      "grad_norm": 104.01205444335938,
      "learning_rate": 0.00014853333333333336,
      "loss": -116.7504,
      "step": 19300
    },
    {
      "epoch": 1.5448,
      "grad_norm": 97.30353546142578,
      "learning_rate": 0.00014850666666666669,
      "loss": -117.0224,
      "step": 19310
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 116.41276550292969,
      "learning_rate": 0.00014848,
      "loss": -116.2044,
      "step": 19320
    },
    {
      "epoch": 1.5464,
      "grad_norm": 133.76397705078125,
      "learning_rate": 0.00014845333333333334,
      "loss": -116.2004,
      "step": 19330
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 249.94387817382812,
      "learning_rate": 0.00014842666666666667,
      "loss": -116.1458,
      "step": 19340
    },
    {
      "epoch": 1.548,
      "grad_norm": 125.9734878540039,
      "learning_rate": 0.0001484,
      "loss": -114.2951,
      "step": 19350
    },
    {
      "epoch": 1.5488,
      "grad_norm": 200.18612670898438,
      "learning_rate": 0.00014837333333333333,
      "loss": -114.7211,
      "step": 19360
    },
    {
      "epoch": 1.5495999999999999,
      "grad_norm": 160.85667419433594,
      "learning_rate": 0.0001483466666666667,
      "loss": -116.9289,
      "step": 19370
    },
    {
      "epoch": 1.5504,
      "grad_norm": 111.0391845703125,
      "learning_rate": 0.00014832000000000002,
      "loss": -117.3408,
      "step": 19380
    },
    {
      "epoch": 1.5512000000000001,
      "grad_norm": 134.70606994628906,
      "learning_rate": 0.00014829333333333335,
      "loss": -117.3403,
      "step": 19390
    },
    {
      "epoch": 1.552,
      "grad_norm": 107.76385498046875,
      "learning_rate": 0.00014826666666666667,
      "loss": -117.3867,
      "step": 19400
    },
    {
      "epoch": 1.5528,
      "grad_norm": 76.1854476928711,
      "learning_rate": 0.00014824,
      "loss": -116.4979,
      "step": 19410
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 119.35245513916016,
      "learning_rate": 0.00014821333333333333,
      "loss": -116.3877,
      "step": 19420
    },
    {
      "epoch": 1.5544,
      "grad_norm": 79.49254608154297,
      "learning_rate": 0.00014818666666666666,
      "loss": -116.1273,
      "step": 19430
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 161.76995849609375,
      "learning_rate": 0.00014816000000000002,
      "loss": -115.7292,
      "step": 19440
    },
    {
      "epoch": 1.556,
      "grad_norm": 122.37934875488281,
      "learning_rate": 0.00014813333333333335,
      "loss": -116.0222,
      "step": 19450
    },
    {
      "epoch": 1.5568,
      "grad_norm": 87.25267028808594,
      "learning_rate": 0.00014810666666666668,
      "loss": -116.8125,
      "step": 19460
    },
    {
      "epoch": 1.5575999999999999,
      "grad_norm": 189.1361083984375,
      "learning_rate": 0.00014808,
      "loss": -116.9809,
      "step": 19470
    },
    {
      "epoch": 1.5584,
      "grad_norm": 88.21812438964844,
      "learning_rate": 0.00014805333333333333,
      "loss": -116.1219,
      "step": 19480
    },
    {
      "epoch": 1.5592000000000001,
      "grad_norm": 83.27996826171875,
      "learning_rate": 0.00014802666666666666,
      "loss": -115.8315,
      "step": 19490
    },
    {
      "epoch": 1.56,
      "grad_norm": 159.9205780029297,
      "learning_rate": 0.000148,
      "loss": -116.6735,
      "step": 19500
    },
    {
      "epoch": 1.5608,
      "grad_norm": 95.56216430664062,
      "learning_rate": 0.00014797333333333335,
      "loss": -116.7024,
      "step": 19510
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 143.1485595703125,
      "learning_rate": 0.00014794666666666668,
      "loss": -116.4601,
      "step": 19520
    },
    {
      "epoch": 1.5624,
      "grad_norm": 88.66869354248047,
      "learning_rate": 0.00014792,
      "loss": -115.7877,
      "step": 19530
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 164.39608764648438,
      "learning_rate": 0.00014789333333333336,
      "loss": -117.3889,
      "step": 19540
    },
    {
      "epoch": 1.564,
      "grad_norm": 193.72377014160156,
      "learning_rate": 0.00014786666666666666,
      "loss": -117.4002,
      "step": 19550
    },
    {
      "epoch": 1.5648,
      "grad_norm": 129.1273651123047,
      "learning_rate": 0.00014784,
      "loss": -115.2125,
      "step": 19560
    },
    {
      "epoch": 1.5655999999999999,
      "grad_norm": 134.1826629638672,
      "learning_rate": 0.00014781333333333335,
      "loss": -116.4155,
      "step": 19570
    },
    {
      "epoch": 1.5664,
      "grad_norm": 87.6408462524414,
      "learning_rate": 0.00014778666666666668,
      "loss": -116.5539,
      "step": 19580
    },
    {
      "epoch": 1.5672000000000001,
      "grad_norm": 163.57167053222656,
      "learning_rate": 0.00014776,
      "loss": -115.9918,
      "step": 19590
    },
    {
      "epoch": 1.568,
      "grad_norm": 135.43563842773438,
      "learning_rate": 0.00014773333333333334,
      "loss": -117.3572,
      "step": 19600
    },
    {
      "epoch": 1.5688,
      "grad_norm": 109.62943267822266,
      "learning_rate": 0.0001477066666666667,
      "loss": -117.4992,
      "step": 19610
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 106.80635070800781,
      "learning_rate": 0.00014768,
      "loss": -116.3359,
      "step": 19620
    },
    {
      "epoch": 1.5704,
      "grad_norm": 95.21373748779297,
      "learning_rate": 0.00014765333333333332,
      "loss": -117.0256,
      "step": 19630
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 82.431884765625,
      "learning_rate": 0.00014762666666666668,
      "loss": -116.0853,
      "step": 19640
    },
    {
      "epoch": 1.572,
      "grad_norm": 129.8419952392578,
      "learning_rate": 0.0001476,
      "loss": -117.0482,
      "step": 19650
    },
    {
      "epoch": 1.5728,
      "grad_norm": 103.40672302246094,
      "learning_rate": 0.00014757333333333334,
      "loss": -116.2959,
      "step": 19660
    },
    {
      "epoch": 1.5735999999999999,
      "grad_norm": 112.77284240722656,
      "learning_rate": 0.00014754666666666667,
      "loss": -116.4051,
      "step": 19670
    },
    {
      "epoch": 1.5744,
      "grad_norm": 236.16851806640625,
      "learning_rate": 0.00014752000000000002,
      "loss": -116.8925,
      "step": 19680
    },
    {
      "epoch": 1.5752000000000002,
      "grad_norm": 132.54627990722656,
      "learning_rate": 0.00014749333333333335,
      "loss": -115.48,
      "step": 19690
    },
    {
      "epoch": 1.576,
      "grad_norm": 127.16676330566406,
      "learning_rate": 0.00014746666666666666,
      "loss": -116.5439,
      "step": 19700
    },
    {
      "epoch": 1.5768,
      "grad_norm": 113.45777130126953,
      "learning_rate": 0.00014744,
      "loss": -116.8272,
      "step": 19710
    },
    {
      "epoch": 1.5776,
      "grad_norm": 86.49960327148438,
      "learning_rate": 0.00014741333333333334,
      "loss": -116.8372,
      "step": 19720
    },
    {
      "epoch": 1.5784,
      "grad_norm": 149.4482421875,
      "learning_rate": 0.00014738666666666667,
      "loss": -115.9916,
      "step": 19730
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 103.72601318359375,
      "learning_rate": 0.00014736,
      "loss": -116.3009,
      "step": 19740
    },
    {
      "epoch": 1.58,
      "grad_norm": 164.57545471191406,
      "learning_rate": 0.00014733333333333335,
      "loss": -118.3756,
      "step": 19750
    },
    {
      "epoch": 1.5808,
      "grad_norm": 133.71591186523438,
      "learning_rate": 0.00014730666666666668,
      "loss": -116.5399,
      "step": 19760
    },
    {
      "epoch": 1.5816,
      "grad_norm": 132.42906188964844,
      "learning_rate": 0.00014728,
      "loss": -116.4902,
      "step": 19770
    },
    {
      "epoch": 1.5824,
      "grad_norm": 123.28034210205078,
      "learning_rate": 0.00014725333333333334,
      "loss": -117.2663,
      "step": 19780
    },
    {
      "epoch": 1.5832000000000002,
      "grad_norm": 243.39816284179688,
      "learning_rate": 0.00014722666666666667,
      "loss": -116.1664,
      "step": 19790
    },
    {
      "epoch": 1.584,
      "grad_norm": 107.01947784423828,
      "learning_rate": 0.0001472,
      "loss": -116.4382,
      "step": 19800
    },
    {
      "epoch": 1.5848,
      "grad_norm": 102.12876892089844,
      "learning_rate": 0.00014717333333333333,
      "loss": -116.0692,
      "step": 19810
    },
    {
      "epoch": 1.5856,
      "grad_norm": 85.3353500366211,
      "learning_rate": 0.00014714666666666669,
      "loss": -116.8512,
      "step": 19820
    },
    {
      "epoch": 1.5864,
      "grad_norm": 97.76821899414062,
      "learning_rate": 0.00014712000000000001,
      "loss": -117.2457,
      "step": 19830
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 147.8069610595703,
      "learning_rate": 0.00014709333333333334,
      "loss": -116.9917,
      "step": 19840
    },
    {
      "epoch": 1.588,
      "grad_norm": 82.14156341552734,
      "learning_rate": 0.00014706666666666667,
      "loss": -116.8784,
      "step": 19850
    },
    {
      "epoch": 1.5888,
      "grad_norm": 92.46713256835938,
      "learning_rate": 0.00014704,
      "loss": -116.2898,
      "step": 19860
    },
    {
      "epoch": 1.5896,
      "grad_norm": 123.14788055419922,
      "learning_rate": 0.00014701333333333333,
      "loss": -117.365,
      "step": 19870
    },
    {
      "epoch": 1.5904,
      "grad_norm": 129.58099365234375,
      "learning_rate": 0.00014698666666666666,
      "loss": -117.1368,
      "step": 19880
    },
    {
      "epoch": 1.5912,
      "grad_norm": 119.22148895263672,
      "learning_rate": 0.00014696000000000002,
      "loss": -116.9669,
      "step": 19890
    },
    {
      "epoch": 1.592,
      "grad_norm": 117.46598815917969,
      "learning_rate": 0.00014693333333333335,
      "loss": -116.991,
      "step": 19900
    },
    {
      "epoch": 1.5928,
      "grad_norm": 108.69014739990234,
      "learning_rate": 0.00014690666666666667,
      "loss": -115.573,
      "step": 19910
    },
    {
      "epoch": 1.5936,
      "grad_norm": 523.8753662109375,
      "learning_rate": 0.00014688000000000003,
      "loss": -117.5982,
      "step": 19920
    },
    {
      "epoch": 1.5944,
      "grad_norm": 77.24771118164062,
      "learning_rate": 0.00014685333333333333,
      "loss": -116.814,
      "step": 19930
    },
    {
      "epoch": 1.5952,
      "grad_norm": 132.02964782714844,
      "learning_rate": 0.00014682666666666666,
      "loss": -116.5155,
      "step": 19940
    },
    {
      "epoch": 1.596,
      "grad_norm": 114.06372833251953,
      "learning_rate": 0.00014680000000000002,
      "loss": -116.8122,
      "step": 19950
    },
    {
      "epoch": 1.5968,
      "grad_norm": 129.67605590820312,
      "learning_rate": 0.00014677333333333335,
      "loss": -114.9593,
      "step": 19960
    },
    {
      "epoch": 1.5976,
      "grad_norm": 142.0677490234375,
      "learning_rate": 0.00014674666666666668,
      "loss": -117.7615,
      "step": 19970
    },
    {
      "epoch": 1.5984,
      "grad_norm": 137.57656860351562,
      "learning_rate": 0.00014672,
      "loss": -116.8631,
      "step": 19980
    },
    {
      "epoch": 1.5992,
      "grad_norm": 81.97193145751953,
      "learning_rate": 0.00014669333333333336,
      "loss": -118.1873,
      "step": 19990
    },
    {
      "epoch": 1.6,
      "grad_norm": 82.2662124633789,
      "learning_rate": 0.00014666666666666666,
      "loss": -116.6136,
      "step": 20000
    },
    {
      "epoch": 1.6008,
      "grad_norm": 100.04751586914062,
      "learning_rate": 0.00014664,
      "loss": -117.1446,
      "step": 20010
    },
    {
      "epoch": 1.6016,
      "grad_norm": 109.58934020996094,
      "learning_rate": 0.00014661333333333335,
      "loss": -117.1743,
      "step": 20020
    },
    {
      "epoch": 1.6024,
      "grad_norm": 139.27162170410156,
      "learning_rate": 0.00014658666666666668,
      "loss": -116.5539,
      "step": 20030
    },
    {
      "epoch": 1.6032,
      "grad_norm": 122.33140563964844,
      "learning_rate": 0.00014656,
      "loss": -116.8536,
      "step": 20040
    },
    {
      "epoch": 1.604,
      "grad_norm": 108.24520111083984,
      "learning_rate": 0.00014653333333333334,
      "loss": -117.2151,
      "step": 20050
    },
    {
      "epoch": 1.6048,
      "grad_norm": 104.11248016357422,
      "learning_rate": 0.0001465066666666667,
      "loss": -115.5828,
      "step": 20060
    },
    {
      "epoch": 1.6056,
      "grad_norm": 147.59666442871094,
      "learning_rate": 0.00014648000000000002,
      "loss": -116.3601,
      "step": 20070
    },
    {
      "epoch": 1.6064,
      "grad_norm": 129.79592895507812,
      "learning_rate": 0.00014645333333333332,
      "loss": -116.5945,
      "step": 20080
    },
    {
      "epoch": 1.6072,
      "grad_norm": 169.98928833007812,
      "learning_rate": 0.00014642666666666668,
      "loss": -116.8972,
      "step": 20090
    },
    {
      "epoch": 1.608,
      "grad_norm": 107.95149230957031,
      "learning_rate": 0.0001464,
      "loss": -116.0396,
      "step": 20100
    },
    {
      "epoch": 1.6088,
      "grad_norm": 76.93455505371094,
      "learning_rate": 0.00014637333333333334,
      "loss": -117.445,
      "step": 20110
    },
    {
      "epoch": 1.6096,
      "grad_norm": 139.754150390625,
      "learning_rate": 0.00014634666666666667,
      "loss": -115.5976,
      "step": 20120
    },
    {
      "epoch": 1.6104,
      "grad_norm": 94.96983337402344,
      "learning_rate": 0.00014632000000000002,
      "loss": -117.3311,
      "step": 20130
    },
    {
      "epoch": 1.6112,
      "grad_norm": 102.95079040527344,
      "learning_rate": 0.00014629333333333335,
      "loss": -117.4734,
      "step": 20140
    },
    {
      "epoch": 1.612,
      "grad_norm": 95.1776123046875,
      "learning_rate": 0.00014626666666666665,
      "loss": -116.553,
      "step": 20150
    },
    {
      "epoch": 1.6128,
      "grad_norm": 140.70599365234375,
      "learning_rate": 0.00014624,
      "loss": -115.4532,
      "step": 20160
    },
    {
      "epoch": 1.6136,
      "grad_norm": 108.72329711914062,
      "learning_rate": 0.00014621333333333334,
      "loss": -117.431,
      "step": 20170
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 101.01323699951172,
      "learning_rate": 0.00014618666666666667,
      "loss": -116.5575,
      "step": 20180
    },
    {
      "epoch": 1.6152,
      "grad_norm": 225.7964324951172,
      "learning_rate": 0.00014616,
      "loss": -116.5914,
      "step": 20190
    },
    {
      "epoch": 1.616,
      "grad_norm": 146.5361328125,
      "learning_rate": 0.00014613333333333335,
      "loss": -117.8876,
      "step": 20200
    },
    {
      "epoch": 1.6168,
      "grad_norm": 169.22344970703125,
      "learning_rate": 0.00014610666666666668,
      "loss": -117.2978,
      "step": 20210
    },
    {
      "epoch": 1.6176,
      "grad_norm": 76.30571746826172,
      "learning_rate": 0.00014608,
      "loss": -116.4054,
      "step": 20220
    },
    {
      "epoch": 1.6183999999999998,
      "grad_norm": 141.74363708496094,
      "learning_rate": 0.00014605333333333334,
      "loss": -116.4151,
      "step": 20230
    },
    {
      "epoch": 1.6192,
      "grad_norm": 127.22698974609375,
      "learning_rate": 0.00014602666666666667,
      "loss": -117.0326,
      "step": 20240
    },
    {
      "epoch": 1.62,
      "grad_norm": 204.96083068847656,
      "learning_rate": 0.000146,
      "loss": -117.3873,
      "step": 20250
    },
    {
      "epoch": 1.6208,
      "grad_norm": 239.80471801757812,
      "learning_rate": 0.00014597333333333333,
      "loss": -116.4289,
      "step": 20260
    },
    {
      "epoch": 1.6216,
      "grad_norm": 130.60830688476562,
      "learning_rate": 0.00014594666666666668,
      "loss": -116.7635,
      "step": 20270
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 100.23526000976562,
      "learning_rate": 0.00014592,
      "loss": -117.3628,
      "step": 20280
    },
    {
      "epoch": 1.6232,
      "grad_norm": 115.65084838867188,
      "learning_rate": 0.00014589333333333334,
      "loss": -117.4152,
      "step": 20290
    },
    {
      "epoch": 1.624,
      "grad_norm": 104.68840026855469,
      "learning_rate": 0.00014586666666666667,
      "loss": -117.1534,
      "step": 20300
    },
    {
      "epoch": 1.6248,
      "grad_norm": 120.30870056152344,
      "learning_rate": 0.00014584,
      "loss": -117.5583,
      "step": 20310
    },
    {
      "epoch": 1.6256,
      "grad_norm": 97.23719787597656,
      "learning_rate": 0.00014581333333333333,
      "loss": -117.402,
      "step": 20320
    },
    {
      "epoch": 1.6263999999999998,
      "grad_norm": 94.8646011352539,
      "learning_rate": 0.00014578666666666668,
      "loss": -116.5829,
      "step": 20330
    },
    {
      "epoch": 1.6272,
      "grad_norm": 85.47941589355469,
      "learning_rate": 0.00014576000000000001,
      "loss": -117.6972,
      "step": 20340
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 145.8193817138672,
      "learning_rate": 0.00014573333333333334,
      "loss": -116.5192,
      "step": 20350
    },
    {
      "epoch": 1.6288,
      "grad_norm": 150.4110565185547,
      "learning_rate": 0.00014570666666666667,
      "loss": -116.8682,
      "step": 20360
    },
    {
      "epoch": 1.6296,
      "grad_norm": 144.62277221679688,
      "learning_rate": 0.00014568000000000003,
      "loss": -116.71,
      "step": 20370
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 78.92864990234375,
      "learning_rate": 0.00014565333333333333,
      "loss": -118.0216,
      "step": 20380
    },
    {
      "epoch": 1.6312,
      "grad_norm": 135.98143005371094,
      "learning_rate": 0.00014562666666666666,
      "loss": -117.3766,
      "step": 20390
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 138.99961853027344,
      "learning_rate": 0.00014560000000000002,
      "loss": -117.7505,
      "step": 20400
    },
    {
      "epoch": 1.6328,
      "grad_norm": 128.9060516357422,
      "learning_rate": 0.00014557333333333334,
      "loss": -117.8081,
      "step": 20410
    },
    {
      "epoch": 1.6336,
      "grad_norm": 222.5364227294922,
      "learning_rate": 0.00014554666666666667,
      "loss": -117.3155,
      "step": 20420
    },
    {
      "epoch": 1.6343999999999999,
      "grad_norm": 87.83069610595703,
      "learning_rate": 0.00014552,
      "loss": -117.3535,
      "step": 20430
    },
    {
      "epoch": 1.6352,
      "grad_norm": 158.40176391601562,
      "learning_rate": 0.00014549333333333336,
      "loss": -117.0712,
      "step": 20440
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 137.69898986816406,
      "learning_rate": 0.0001454666666666667,
      "loss": -118.1904,
      "step": 20450
    },
    {
      "epoch": 1.6368,
      "grad_norm": 77.1274642944336,
      "learning_rate": 0.00014544,
      "loss": -117.6644,
      "step": 20460
    },
    {
      "epoch": 1.6376,
      "grad_norm": 158.26608276367188,
      "learning_rate": 0.00014541333333333335,
      "loss": -115.9137,
      "step": 20470
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 64.8165054321289,
      "learning_rate": 0.00014538666666666668,
      "loss": -116.8513,
      "step": 20480
    },
    {
      "epoch": 1.6392,
      "grad_norm": 60.57124328613281,
      "learning_rate": 0.00014536,
      "loss": -117.6331,
      "step": 20490
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 99.06470489501953,
      "learning_rate": 0.00014533333333333333,
      "loss": -116.5075,
      "step": 20500
    },
    {
      "epoch": 1.6408,
      "grad_norm": 118.67098236083984,
      "learning_rate": 0.0001453066666666667,
      "loss": -117.0322,
      "step": 20510
    },
    {
      "epoch": 1.6416,
      "grad_norm": 110.51368713378906,
      "learning_rate": 0.00014528000000000002,
      "loss": -116.71,
      "step": 20520
    },
    {
      "epoch": 1.6423999999999999,
      "grad_norm": 171.12509155273438,
      "learning_rate": 0.00014525333333333332,
      "loss": -117.6782,
      "step": 20530
    },
    {
      "epoch": 1.6432,
      "grad_norm": 114.42481231689453,
      "learning_rate": 0.00014522666666666668,
      "loss": -116.9454,
      "step": 20540
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 212.899169921875,
      "learning_rate": 0.0001452,
      "loss": -117.2481,
      "step": 20550
    },
    {
      "epoch": 1.6448,
      "grad_norm": 86.80517578125,
      "learning_rate": 0.00014517333333333333,
      "loss": -117.0163,
      "step": 20560
    },
    {
      "epoch": 1.6456,
      "grad_norm": 99.5357894897461,
      "learning_rate": 0.00014514666666666666,
      "loss": -116.7017,
      "step": 20570
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 126.91100311279297,
      "learning_rate": 0.00014512000000000002,
      "loss": -117.2318,
      "step": 20580
    },
    {
      "epoch": 1.6472,
      "grad_norm": 96.62116241455078,
      "learning_rate": 0.00014509333333333335,
      "loss": -116.1376,
      "step": 20590
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 100.1251449584961,
      "learning_rate": 0.00014506666666666668,
      "loss": -116.8892,
      "step": 20600
    },
    {
      "epoch": 1.6488,
      "grad_norm": 120.7854995727539,
      "learning_rate": 0.00014504,
      "loss": -116.8177,
      "step": 20610
    },
    {
      "epoch": 1.6496,
      "grad_norm": 132.33042907714844,
      "learning_rate": 0.00014501333333333334,
      "loss": -117.7601,
      "step": 20620
    },
    {
      "epoch": 1.6503999999999999,
      "grad_norm": 137.3877410888672,
      "learning_rate": 0.00014498666666666667,
      "loss": -116.7357,
      "step": 20630
    },
    {
      "epoch": 1.6512,
      "grad_norm": 68.55204010009766,
      "learning_rate": 0.00014496,
      "loss": -116.7355,
      "step": 20640
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 143.84986877441406,
      "learning_rate": 0.00014493333333333335,
      "loss": -118.0148,
      "step": 20650
    },
    {
      "epoch": 1.6528,
      "grad_norm": 93.58677673339844,
      "learning_rate": 0.00014490666666666668,
      "loss": -117.4891,
      "step": 20660
    },
    {
      "epoch": 1.6536,
      "grad_norm": 78.91841125488281,
      "learning_rate": 0.00014488,
      "loss": -117.8729,
      "step": 20670
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 101.32467651367188,
      "learning_rate": 0.00014485333333333334,
      "loss": -117.1577,
      "step": 20680
    },
    {
      "epoch": 1.6552,
      "grad_norm": 159.2547149658203,
      "learning_rate": 0.00014482666666666667,
      "loss": -117.6146,
      "step": 20690
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 104.71642303466797,
      "learning_rate": 0.0001448,
      "loss": -116.7355,
      "step": 20700
    },
    {
      "epoch": 1.6568,
      "grad_norm": 140.05426025390625,
      "learning_rate": 0.00014477333333333333,
      "loss": -116.9588,
      "step": 20710
    },
    {
      "epoch": 1.6576,
      "grad_norm": 116.21290588378906,
      "learning_rate": 0.00014474666666666668,
      "loss": -118.1058,
      "step": 20720
    },
    {
      "epoch": 1.6583999999999999,
      "grad_norm": 151.5200958251953,
      "learning_rate": 0.00014472,
      "loss": -116.9791,
      "step": 20730
    },
    {
      "epoch": 1.6592,
      "grad_norm": 110.93804931640625,
      "learning_rate": 0.00014469333333333334,
      "loss": -117.5216,
      "step": 20740
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 115.55313873291016,
      "learning_rate": 0.0001446666666666667,
      "loss": -116.07,
      "step": 20750
    },
    {
      "epoch": 1.6608,
      "grad_norm": 136.283935546875,
      "learning_rate": 0.00014464,
      "loss": -117.8956,
      "step": 20760
    },
    {
      "epoch": 1.6616,
      "grad_norm": 95.75353240966797,
      "learning_rate": 0.00014461333333333333,
      "loss": -117.4582,
      "step": 20770
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 143.55455017089844,
      "learning_rate": 0.00014458666666666668,
      "loss": -117.4676,
      "step": 20780
    },
    {
      "epoch": 1.6632,
      "grad_norm": 168.59898376464844,
      "learning_rate": 0.00014456,
      "loss": -117.7208,
      "step": 20790
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 64.42981719970703,
      "learning_rate": 0.00014453333333333334,
      "loss": -117.0733,
      "step": 20800
    },
    {
      "epoch": 1.6648,
      "grad_norm": 133.5110626220703,
      "learning_rate": 0.00014450666666666667,
      "loss": -118.055,
      "step": 20810
    },
    {
      "epoch": 1.6656,
      "grad_norm": 194.1951141357422,
      "learning_rate": 0.00014448000000000003,
      "loss": -116.9653,
      "step": 20820
    },
    {
      "epoch": 1.6663999999999999,
      "grad_norm": 119.00901794433594,
      "learning_rate": 0.00014445333333333333,
      "loss": -118.025,
      "step": 20830
    },
    {
      "epoch": 1.6672,
      "grad_norm": 116.98336029052734,
      "learning_rate": 0.00014442666666666666,
      "loss": -118.5379,
      "step": 20840
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 70.08898162841797,
      "learning_rate": 0.0001444,
      "loss": -117.0747,
      "step": 20850
    },
    {
      "epoch": 1.6688,
      "grad_norm": 108.08590698242188,
      "learning_rate": 0.00014437333333333334,
      "loss": -116.1376,
      "step": 20860
    },
    {
      "epoch": 1.6696,
      "grad_norm": 135.81239318847656,
      "learning_rate": 0.00014434666666666667,
      "loss": -118.8292,
      "step": 20870
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 163.4408416748047,
      "learning_rate": 0.00014432,
      "loss": -117.0197,
      "step": 20880
    },
    {
      "epoch": 1.6712,
      "grad_norm": 103.78763580322266,
      "learning_rate": 0.00014429333333333336,
      "loss": -117.334,
      "step": 20890
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 96.69371795654297,
      "learning_rate": 0.00014426666666666669,
      "loss": -117.8336,
      "step": 20900
    },
    {
      "epoch": 1.6728,
      "grad_norm": 141.59535217285156,
      "learning_rate": 0.00014424,
      "loss": -116.6907,
      "step": 20910
    },
    {
      "epoch": 1.6736,
      "grad_norm": 178.39256286621094,
      "learning_rate": 0.00014421333333333334,
      "loss": -116.3869,
      "step": 20920
    },
    {
      "epoch": 1.6743999999999999,
      "grad_norm": 102.0096206665039,
      "learning_rate": 0.00014418666666666667,
      "loss": -116.5785,
      "step": 20930
    },
    {
      "epoch": 1.6752,
      "grad_norm": 78.376220703125,
      "learning_rate": 0.00014416,
      "loss": -117.874,
      "step": 20940
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 117.75306701660156,
      "learning_rate": 0.00014413333333333333,
      "loss": -117.1584,
      "step": 20950
    },
    {
      "epoch": 1.6768,
      "grad_norm": 57.8436279296875,
      "learning_rate": 0.0001441066666666667,
      "loss": -116.6185,
      "step": 20960
    },
    {
      "epoch": 1.6776,
      "grad_norm": 68.39032745361328,
      "learning_rate": 0.00014408000000000002,
      "loss": -115.9156,
      "step": 20970
    },
    {
      "epoch": 1.6784,
      "grad_norm": 105.41043090820312,
      "learning_rate": 0.00014405333333333335,
      "loss": -117.9359,
      "step": 20980
    },
    {
      "epoch": 1.6792,
      "grad_norm": 149.011474609375,
      "learning_rate": 0.00014402666666666667,
      "loss": -117.9975,
      "step": 20990
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 82.45010375976562,
      "learning_rate": 0.000144,
      "loss": -116.5887,
      "step": 21000
    },
    {
      "epoch": 1.6808,
      "grad_norm": 128.9319610595703,
      "learning_rate": 0.00014397333333333333,
      "loss": -116.6453,
      "step": 21010
    },
    {
      "epoch": 1.6816,
      "grad_norm": 145.59107971191406,
      "learning_rate": 0.00014394666666666666,
      "loss": -117.5801,
      "step": 21020
    },
    {
      "epoch": 1.6824,
      "grad_norm": 96.08916473388672,
      "learning_rate": 0.00014392000000000002,
      "loss": -117.4557,
      "step": 21030
    },
    {
      "epoch": 1.6832,
      "grad_norm": 75.65036010742188,
      "learning_rate": 0.00014389333333333335,
      "loss": -116.7863,
      "step": 21040
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 94.98621368408203,
      "learning_rate": 0.00014386666666666668,
      "loss": -116.407,
      "step": 21050
    },
    {
      "epoch": 1.6848,
      "grad_norm": 186.9213104248047,
      "learning_rate": 0.00014384,
      "loss": -117.6309,
      "step": 21060
    },
    {
      "epoch": 1.6856,
      "grad_norm": 67.68608093261719,
      "learning_rate": 0.00014381333333333333,
      "loss": -117.3867,
      "step": 21070
    },
    {
      "epoch": 1.6864,
      "grad_norm": 135.38690185546875,
      "learning_rate": 0.00014378666666666666,
      "loss": -117.7927,
      "step": 21080
    },
    {
      "epoch": 1.6872,
      "grad_norm": 82.42550659179688,
      "learning_rate": 0.00014376,
      "loss": -117.0962,
      "step": 21090
    },
    {
      "epoch": 1.688,
      "grad_norm": 182.0130157470703,
      "learning_rate": 0.00014373333333333335,
      "loss": -116.7124,
      "step": 21100
    },
    {
      "epoch": 1.6888,
      "grad_norm": 79.94859313964844,
      "learning_rate": 0.00014370666666666668,
      "loss": -116.9261,
      "step": 21110
    },
    {
      "epoch": 1.6896,
      "grad_norm": 117.9031982421875,
      "learning_rate": 0.00014368,
      "loss": -117.3008,
      "step": 21120
    },
    {
      "epoch": 1.6904,
      "grad_norm": 154.15667724609375,
      "learning_rate": 0.00014365333333333336,
      "loss": -117.1978,
      "step": 21130
    },
    {
      "epoch": 1.6912,
      "grad_norm": 139.2431640625,
      "learning_rate": 0.00014362666666666666,
      "loss": -117.1602,
      "step": 21140
    },
    {
      "epoch": 1.692,
      "grad_norm": 176.96499633789062,
      "learning_rate": 0.0001436,
      "loss": -118.2246,
      "step": 21150
    },
    {
      "epoch": 1.6928,
      "grad_norm": 156.42396545410156,
      "learning_rate": 0.00014357333333333335,
      "loss": -117.3423,
      "step": 21160
    },
    {
      "epoch": 1.6936,
      "grad_norm": 76.20317840576172,
      "learning_rate": 0.00014354666666666668,
      "loss": -116.7079,
      "step": 21170
    },
    {
      "epoch": 1.6944,
      "grad_norm": 78.37528991699219,
      "learning_rate": 0.00014352,
      "loss": -117.6718,
      "step": 21180
    },
    {
      "epoch": 1.6952,
      "grad_norm": 112.67237854003906,
      "learning_rate": 0.00014349333333333334,
      "loss": -116.7933,
      "step": 21190
    },
    {
      "epoch": 1.696,
      "grad_norm": 133.49400329589844,
      "learning_rate": 0.0001434666666666667,
      "loss": -118.2381,
      "step": 21200
    },
    {
      "epoch": 1.6968,
      "grad_norm": 71.3167953491211,
      "learning_rate": 0.00014344,
      "loss": -117.8704,
      "step": 21210
    },
    {
      "epoch": 1.6976,
      "grad_norm": 169.15859985351562,
      "learning_rate": 0.00014341333333333332,
      "loss": -115.2648,
      "step": 21220
    },
    {
      "epoch": 1.6984,
      "grad_norm": 110.77522277832031,
      "learning_rate": 0.00014338666666666668,
      "loss": -116.8731,
      "step": 21230
    },
    {
      "epoch": 1.6992,
      "grad_norm": 105.10784912109375,
      "learning_rate": 0.00014336,
      "loss": -118.0252,
      "step": 21240
    },
    {
      "epoch": 1.7,
      "grad_norm": 86.47347259521484,
      "learning_rate": 0.00014333333333333334,
      "loss": -117.5296,
      "step": 21250
    },
    {
      "epoch": 1.7008,
      "grad_norm": 119.92674255371094,
      "learning_rate": 0.00014330666666666667,
      "loss": -118.9337,
      "step": 21260
    },
    {
      "epoch": 1.7016,
      "grad_norm": 163.46817016601562,
      "learning_rate": 0.00014328000000000002,
      "loss": -117.781,
      "step": 21270
    },
    {
      "epoch": 1.7024,
      "grad_norm": 86.09630584716797,
      "learning_rate": 0.00014325333333333335,
      "loss": -117.9716,
      "step": 21280
    },
    {
      "epoch": 1.7032,
      "grad_norm": 128.40658569335938,
      "learning_rate": 0.00014322666666666666,
      "loss": -118.187,
      "step": 21290
    },
    {
      "epoch": 1.704,
      "grad_norm": 172.5924072265625,
      "learning_rate": 0.0001432,
      "loss": -117.594,
      "step": 21300
    },
    {
      "epoch": 1.7048,
      "grad_norm": 266.437255859375,
      "learning_rate": 0.00014317333333333334,
      "loss": -117.7533,
      "step": 21310
    },
    {
      "epoch": 1.7056,
      "grad_norm": 99.87174987792969,
      "learning_rate": 0.00014314666666666667,
      "loss": -116.9479,
      "step": 21320
    },
    {
      "epoch": 1.7064,
      "grad_norm": 91.60237884521484,
      "learning_rate": 0.00014312,
      "loss": -117.6554,
      "step": 21330
    },
    {
      "epoch": 1.7072,
      "grad_norm": 110.40779113769531,
      "learning_rate": 0.00014309333333333335,
      "loss": -118.479,
      "step": 21340
    },
    {
      "epoch": 1.708,
      "grad_norm": 107.05506896972656,
      "learning_rate": 0.00014306666666666668,
      "loss": -117.7435,
      "step": 21350
    },
    {
      "epoch": 1.7088,
      "grad_norm": 128.846923828125,
      "learning_rate": 0.00014303999999999999,
      "loss": -117.2186,
      "step": 21360
    },
    {
      "epoch": 1.7096,
      "grad_norm": 113.69147491455078,
      "learning_rate": 0.00014301333333333334,
      "loss": -116.916,
      "step": 21370
    },
    {
      "epoch": 1.7104,
      "grad_norm": 109.09063720703125,
      "learning_rate": 0.00014298666666666667,
      "loss": -117.5523,
      "step": 21380
    },
    {
      "epoch": 1.7112,
      "grad_norm": 98.37505340576172,
      "learning_rate": 0.00014296,
      "loss": -117.7286,
      "step": 21390
    },
    {
      "epoch": 1.712,
      "grad_norm": 108.70370483398438,
      "learning_rate": 0.00014293333333333333,
      "loss": -117.9707,
      "step": 21400
    },
    {
      "epoch": 1.7128,
      "grad_norm": 199.9634246826172,
      "learning_rate": 0.00014290666666666669,
      "loss": -118.9822,
      "step": 21410
    },
    {
      "epoch": 1.7136,
      "grad_norm": 90.33207702636719,
      "learning_rate": 0.00014288000000000001,
      "loss": -116.9101,
      "step": 21420
    },
    {
      "epoch": 1.7144,
      "grad_norm": 146.22491455078125,
      "learning_rate": 0.00014285333333333334,
      "loss": -117.9447,
      "step": 21430
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 126.9264144897461,
      "learning_rate": 0.00014282666666666667,
      "loss": -117.9642,
      "step": 21440
    },
    {
      "epoch": 1.716,
      "grad_norm": 205.84548950195312,
      "learning_rate": 0.0001428,
      "loss": -117.9556,
      "step": 21450
    },
    {
      "epoch": 1.7168,
      "grad_norm": 111.46984100341797,
      "learning_rate": 0.00014277333333333333,
      "loss": -116.7583,
      "step": 21460
    },
    {
      "epoch": 1.7176,
      "grad_norm": 66.47722625732422,
      "learning_rate": 0.00014274666666666666,
      "loss": -116.9939,
      "step": 21470
    },
    {
      "epoch": 1.7184,
      "grad_norm": 109.26993560791016,
      "learning_rate": 0.00014272000000000002,
      "loss": -117.4525,
      "step": 21480
    },
    {
      "epoch": 1.7191999999999998,
      "grad_norm": 162.09786987304688,
      "learning_rate": 0.00014269333333333334,
      "loss": -116.3334,
      "step": 21490
    },
    {
      "epoch": 1.72,
      "grad_norm": 134.69924926757812,
      "learning_rate": 0.00014266666666666667,
      "loss": -117.3991,
      "step": 21500
    },
    {
      "epoch": 1.7208,
      "grad_norm": 134.82286071777344,
      "learning_rate": 0.00014264,
      "loss": -116.4077,
      "step": 21510
    },
    {
      "epoch": 1.7216,
      "grad_norm": 110.79271697998047,
      "learning_rate": 0.00014261333333333333,
      "loss": -117.2146,
      "step": 21520
    },
    {
      "epoch": 1.7224,
      "grad_norm": 119.01075744628906,
      "learning_rate": 0.00014258666666666666,
      "loss": -117.1385,
      "step": 21530
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 94.17277526855469,
      "learning_rate": 0.00014256000000000002,
      "loss": -117.7593,
      "step": 21540
    },
    {
      "epoch": 1.724,
      "grad_norm": 139.79212951660156,
      "learning_rate": 0.00014253333333333335,
      "loss": -118.3575,
      "step": 21550
    },
    {
      "epoch": 1.7248,
      "grad_norm": 125.60498046875,
      "learning_rate": 0.00014250666666666668,
      "loss": -117.3433,
      "step": 21560
    },
    {
      "epoch": 1.7256,
      "grad_norm": 174.0134735107422,
      "learning_rate": 0.00014248,
      "loss": -118.7572,
      "step": 21570
    },
    {
      "epoch": 1.7264,
      "grad_norm": 118.44242095947266,
      "learning_rate": 0.00014245333333333336,
      "loss": -118.3135,
      "step": 21580
    },
    {
      "epoch": 1.7271999999999998,
      "grad_norm": 94.2713851928711,
      "learning_rate": 0.00014242666666666666,
      "loss": -117.2534,
      "step": 21590
    },
    {
      "epoch": 1.728,
      "grad_norm": 77.73735046386719,
      "learning_rate": 0.0001424,
      "loss": -118.4665,
      "step": 21600
    },
    {
      "epoch": 1.7288000000000001,
      "grad_norm": 133.96902465820312,
      "learning_rate": 0.00014237333333333335,
      "loss": -117.2149,
      "step": 21610
    },
    {
      "epoch": 1.7296,
      "grad_norm": 153.99501037597656,
      "learning_rate": 0.00014234666666666668,
      "loss": -116.6213,
      "step": 21620
    },
    {
      "epoch": 1.7304,
      "grad_norm": 95.37873840332031,
      "learning_rate": 0.00014232,
      "loss": -116.6331,
      "step": 21630
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 95.88835144042969,
      "learning_rate": 0.00014229333333333334,
      "loss": -115.4532,
      "step": 21640
    },
    {
      "epoch": 1.732,
      "grad_norm": 73.80635070800781,
      "learning_rate": 0.0001422666666666667,
      "loss": -117.5006,
      "step": 21650
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 126.00849151611328,
      "learning_rate": 0.00014224000000000002,
      "loss": -116.5698,
      "step": 21660
    },
    {
      "epoch": 1.7336,
      "grad_norm": 111.24874114990234,
      "learning_rate": 0.00014221333333333332,
      "loss": -118.5657,
      "step": 21670
    },
    {
      "epoch": 1.7344,
      "grad_norm": 136.89901733398438,
      "learning_rate": 0.00014218666666666668,
      "loss": -117.4946,
      "step": 21680
    },
    {
      "epoch": 1.7351999999999999,
      "grad_norm": 109.22914123535156,
      "learning_rate": 0.00014216,
      "loss": -118.8437,
      "step": 21690
    },
    {
      "epoch": 1.736,
      "grad_norm": 90.77896118164062,
      "learning_rate": 0.00014213333333333334,
      "loss": -117.291,
      "step": 21700
    },
    {
      "epoch": 1.7368000000000001,
      "grad_norm": 144.29148864746094,
      "learning_rate": 0.00014210666666666667,
      "loss": -117.8109,
      "step": 21710
    },
    {
      "epoch": 1.7376,
      "grad_norm": 110.87276458740234,
      "learning_rate": 0.00014208000000000002,
      "loss": -116.5786,
      "step": 21720
    },
    {
      "epoch": 1.7384,
      "grad_norm": 156.88389587402344,
      "learning_rate": 0.00014205333333333335,
      "loss": -118.3864,
      "step": 21730
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 179.0191192626953,
      "learning_rate": 0.00014202666666666665,
      "loss": -118.4033,
      "step": 21740
    },
    {
      "epoch": 1.74,
      "grad_norm": 107.75253295898438,
      "learning_rate": 0.000142,
      "loss": -117.6997,
      "step": 21750
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 84.03477478027344,
      "learning_rate": 0.00014197333333333334,
      "loss": -117.9153,
      "step": 21760
    },
    {
      "epoch": 1.7416,
      "grad_norm": 105.59805297851562,
      "learning_rate": 0.00014194666666666667,
      "loss": -118.4249,
      "step": 21770
    },
    {
      "epoch": 1.7424,
      "grad_norm": 87.26025390625,
      "learning_rate": 0.00014192,
      "loss": -116.3435,
      "step": 21780
    },
    {
      "epoch": 1.7431999999999999,
      "grad_norm": 93.32868194580078,
      "learning_rate": 0.00014189333333333335,
      "loss": -118.5289,
      "step": 21790
    },
    {
      "epoch": 1.744,
      "grad_norm": 93.11421966552734,
      "learning_rate": 0.00014186666666666668,
      "loss": -117.1647,
      "step": 21800
    },
    {
      "epoch": 1.7448000000000001,
      "grad_norm": 120.74813079833984,
      "learning_rate": 0.00014184,
      "loss": -117.4331,
      "step": 21810
    },
    {
      "epoch": 1.7456,
      "grad_norm": 105.77117156982422,
      "learning_rate": 0.00014181333333333334,
      "loss": -117.1856,
      "step": 21820
    },
    {
      "epoch": 1.7464,
      "grad_norm": 101.01219940185547,
      "learning_rate": 0.00014178666666666667,
      "loss": -117.5192,
      "step": 21830
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 111.84796142578125,
      "learning_rate": 0.00014176,
      "loss": -117.976,
      "step": 21840
    },
    {
      "epoch": 1.748,
      "grad_norm": 121.39177703857422,
      "learning_rate": 0.00014173333333333333,
      "loss": -116.5378,
      "step": 21850
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 117.37361907958984,
      "learning_rate": 0.00014170666666666668,
      "loss": -117.6641,
      "step": 21860
    },
    {
      "epoch": 1.7496,
      "grad_norm": 90.0982666015625,
      "learning_rate": 0.00014168,
      "loss": -117.0952,
      "step": 21870
    },
    {
      "epoch": 1.7504,
      "grad_norm": 102.59356689453125,
      "learning_rate": 0.00014165333333333334,
      "loss": -117.4425,
      "step": 21880
    },
    {
      "epoch": 1.7511999999999999,
      "grad_norm": 96.99845123291016,
      "learning_rate": 0.00014162666666666667,
      "loss": -118.8403,
      "step": 21890
    },
    {
      "epoch": 1.752,
      "grad_norm": 111.16230010986328,
      "learning_rate": 0.0001416,
      "loss": -117.5652,
      "step": 21900
    },
    {
      "epoch": 1.7528000000000001,
      "grad_norm": 92.16417694091797,
      "learning_rate": 0.00014157333333333333,
      "loss": -118.3428,
      "step": 21910
    },
    {
      "epoch": 1.7536,
      "grad_norm": 277.38934326171875,
      "learning_rate": 0.00014154666666666668,
      "loss": -117.7863,
      "step": 21920
    },
    {
      "epoch": 1.7544,
      "grad_norm": 157.0863494873047,
      "learning_rate": 0.00014152000000000001,
      "loss": -117.6325,
      "step": 21930
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 113.96224975585938,
      "learning_rate": 0.00014149333333333334,
      "loss": -117.8901,
      "step": 21940
    },
    {
      "epoch": 1.756,
      "grad_norm": 173.2769317626953,
      "learning_rate": 0.00014146666666666667,
      "loss": -117.1311,
      "step": 21950
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 110.23790740966797,
      "learning_rate": 0.00014144000000000003,
      "loss": -116.9325,
      "step": 21960
    },
    {
      "epoch": 1.7576,
      "grad_norm": 223.2334747314453,
      "learning_rate": 0.00014141333333333333,
      "loss": -117.4446,
      "step": 21970
    },
    {
      "epoch": 1.7584,
      "grad_norm": 136.60142517089844,
      "learning_rate": 0.00014138666666666666,
      "loss": -117.7462,
      "step": 21980
    },
    {
      "epoch": 1.7591999999999999,
      "grad_norm": 92.75164031982422,
      "learning_rate": 0.00014136000000000002,
      "loss": -117.2288,
      "step": 21990
    },
    {
      "epoch": 1.76,
      "grad_norm": 78.33927154541016,
      "learning_rate": 0.00014133333333333334,
      "loss": -117.285,
      "step": 22000
    },
    {
      "epoch": 1.7608000000000001,
      "grad_norm": 87.61392974853516,
      "learning_rate": 0.00014130666666666667,
      "loss": -118.1772,
      "step": 22010
    },
    {
      "epoch": 1.7616,
      "grad_norm": 458.6822509765625,
      "learning_rate": 0.00014128,
      "loss": -119.1212,
      "step": 22020
    },
    {
      "epoch": 1.7624,
      "grad_norm": 102.13831329345703,
      "learning_rate": 0.00014125333333333336,
      "loss": -118.7052,
      "step": 22030
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 108.65095520019531,
      "learning_rate": 0.00014122666666666666,
      "loss": -118.0583,
      "step": 22040
    },
    {
      "epoch": 1.764,
      "grad_norm": 193.93666076660156,
      "learning_rate": 0.0001412,
      "loss": -117.7729,
      "step": 22050
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 210.4442138671875,
      "learning_rate": 0.00014117333333333335,
      "loss": -118.4098,
      "step": 22060
    },
    {
      "epoch": 1.7656,
      "grad_norm": 76.15414428710938,
      "learning_rate": 0.00014114666666666668,
      "loss": -118.6252,
      "step": 22070
    },
    {
      "epoch": 1.7664,
      "grad_norm": 102.72509765625,
      "learning_rate": 0.00014112,
      "loss": -116.9027,
      "step": 22080
    },
    {
      "epoch": 1.7671999999999999,
      "grad_norm": 95.5727310180664,
      "learning_rate": 0.00014109333333333333,
      "loss": -119.3713,
      "step": 22090
    },
    {
      "epoch": 1.768,
      "grad_norm": 128.1208038330078,
      "learning_rate": 0.0001410666666666667,
      "loss": -117.4732,
      "step": 22100
    },
    {
      "epoch": 1.7688000000000001,
      "grad_norm": 139.46409606933594,
      "learning_rate": 0.00014104000000000002,
      "loss": -118.3504,
      "step": 22110
    },
    {
      "epoch": 1.7696,
      "grad_norm": 91.26404571533203,
      "learning_rate": 0.00014101333333333332,
      "loss": -117.5862,
      "step": 22120
    },
    {
      "epoch": 1.7704,
      "grad_norm": 117.76810455322266,
      "learning_rate": 0.00014098666666666668,
      "loss": -117.5891,
      "step": 22130
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 134.4468994140625,
      "learning_rate": 0.00014096,
      "loss": -117.7722,
      "step": 22140
    },
    {
      "epoch": 1.772,
      "grad_norm": 112.76036071777344,
      "learning_rate": 0.00014093333333333333,
      "loss": -117.5274,
      "step": 22150
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 142.8599090576172,
      "learning_rate": 0.00014090666666666666,
      "loss": -116.9533,
      "step": 22160
    },
    {
      "epoch": 1.7736,
      "grad_norm": 78.87012481689453,
      "learning_rate": 0.00014088000000000002,
      "loss": -115.7129,
      "step": 22170
    },
    {
      "epoch": 1.7744,
      "grad_norm": 97.26058959960938,
      "learning_rate": 0.00014085333333333335,
      "loss": -118.4466,
      "step": 22180
    },
    {
      "epoch": 1.7752,
      "grad_norm": 131.3668975830078,
      "learning_rate": 0.00014082666666666668,
      "loss": -117.809,
      "step": 22190
    },
    {
      "epoch": 1.776,
      "grad_norm": 234.4195098876953,
      "learning_rate": 0.0001408,
      "loss": -117.9194,
      "step": 22200
    },
    {
      "epoch": 1.7768000000000002,
      "grad_norm": 104.40890502929688,
      "learning_rate": 0.00014077333333333334,
      "loss": -115.8264,
      "step": 22210
    },
    {
      "epoch": 1.7776,
      "grad_norm": 129.59832763671875,
      "learning_rate": 0.00014074666666666667,
      "loss": -116.2948,
      "step": 22220
    },
    {
      "epoch": 1.7784,
      "grad_norm": 87.51205444335938,
      "learning_rate": 0.00014072,
      "loss": -117.5055,
      "step": 22230
    },
    {
      "epoch": 1.7792,
      "grad_norm": 79.8331069946289,
      "learning_rate": 0.00014069333333333335,
      "loss": -117.8691,
      "step": 22240
    },
    {
      "epoch": 1.78,
      "grad_norm": 100.36366271972656,
      "learning_rate": 0.00014066666666666668,
      "loss": -116.6076,
      "step": 22250
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 123.96247863769531,
      "learning_rate": 0.00014064,
      "loss": -117.7918,
      "step": 22260
    },
    {
      "epoch": 1.7816,
      "grad_norm": 78.99567413330078,
      "learning_rate": 0.00014061333333333334,
      "loss": -117.5938,
      "step": 22270
    },
    {
      "epoch": 1.7824,
      "grad_norm": 115.14379119873047,
      "learning_rate": 0.00014058666666666667,
      "loss": -117.4703,
      "step": 22280
    },
    {
      "epoch": 1.7832,
      "grad_norm": 86.95055389404297,
      "learning_rate": 0.00014056,
      "loss": -117.0588,
      "step": 22290
    },
    {
      "epoch": 1.784,
      "grad_norm": 109.979736328125,
      "learning_rate": 0.00014053333333333335,
      "loss": -117.5578,
      "step": 22300
    },
    {
      "epoch": 1.7848000000000002,
      "grad_norm": 113.82459259033203,
      "learning_rate": 0.00014050666666666668,
      "loss": -118.4687,
      "step": 22310
    },
    {
      "epoch": 1.7856,
      "grad_norm": 151.75267028808594,
      "learning_rate": 0.00014048,
      "loss": -117.1651,
      "step": 22320
    },
    {
      "epoch": 1.7864,
      "grad_norm": 118.48372650146484,
      "learning_rate": 0.00014045333333333334,
      "loss": -116.8661,
      "step": 22330
    },
    {
      "epoch": 1.7872,
      "grad_norm": 84.06302642822266,
      "learning_rate": 0.0001404266666666667,
      "loss": -119.1597,
      "step": 22340
    },
    {
      "epoch": 1.788,
      "grad_norm": 85.18440246582031,
      "learning_rate": 0.0001404,
      "loss": -117.1403,
      "step": 22350
    },
    {
      "epoch": 1.7888,
      "grad_norm": 133.21377563476562,
      "learning_rate": 0.00014037333333333333,
      "loss": -118.651,
      "step": 22360
    },
    {
      "epoch": 1.7896,
      "grad_norm": 110.9341049194336,
      "learning_rate": 0.00014034666666666668,
      "loss": -118.0374,
      "step": 22370
    },
    {
      "epoch": 1.7904,
      "grad_norm": 129.9039764404297,
      "learning_rate": 0.00014032,
      "loss": -117.0326,
      "step": 22380
    },
    {
      "epoch": 1.7912,
      "grad_norm": 134.5853729248047,
      "learning_rate": 0.00014029333333333334,
      "loss": -117.1393,
      "step": 22390
    },
    {
      "epoch": 1.792,
      "grad_norm": 116.19127655029297,
      "learning_rate": 0.00014026666666666667,
      "loss": -116.5736,
      "step": 22400
    },
    {
      "epoch": 1.7928,
      "grad_norm": 135.37208557128906,
      "learning_rate": 0.00014024000000000003,
      "loss": -118.2213,
      "step": 22410
    },
    {
      "epoch": 1.7936,
      "grad_norm": 115.35769653320312,
      "learning_rate": 0.00014021333333333333,
      "loss": -117.4611,
      "step": 22420
    },
    {
      "epoch": 1.7944,
      "grad_norm": 217.50794982910156,
      "learning_rate": 0.00014018666666666666,
      "loss": -117.3239,
      "step": 22430
    },
    {
      "epoch": 1.7952,
      "grad_norm": 118.56290435791016,
      "learning_rate": 0.00014016,
      "loss": -116.2234,
      "step": 22440
    },
    {
      "epoch": 1.796,
      "grad_norm": 172.16384887695312,
      "learning_rate": 0.00014013333333333334,
      "loss": -118.1972,
      "step": 22450
    },
    {
      "epoch": 1.7968,
      "grad_norm": 89.06912994384766,
      "learning_rate": 0.00014010666666666667,
      "loss": -118.0478,
      "step": 22460
    },
    {
      "epoch": 1.7976,
      "grad_norm": 87.63711547851562,
      "learning_rate": 0.00014008,
      "loss": -118.3162,
      "step": 22470
    },
    {
      "epoch": 1.7984,
      "grad_norm": 107.68477630615234,
      "learning_rate": 0.00014005333333333336,
      "loss": -118.0231,
      "step": 22480
    },
    {
      "epoch": 1.7992,
      "grad_norm": 135.2403106689453,
      "learning_rate": 0.00014002666666666669,
      "loss": -116.6444,
      "step": 22490
    },
    {
      "epoch": 1.8,
      "grad_norm": 143.03514099121094,
      "learning_rate": 0.00014,
      "loss": -117.3556,
      "step": 22500
    },
    {
      "epoch": 1.8008,
      "grad_norm": 146.66973876953125,
      "learning_rate": 0.00013997333333333334,
      "loss": -118.7819,
      "step": 22510
    },
    {
      "epoch": 1.8016,
      "grad_norm": 79.34587097167969,
      "learning_rate": 0.00013994666666666667,
      "loss": -115.4024,
      "step": 22520
    },
    {
      "epoch": 1.8024,
      "grad_norm": 188.5207061767578,
      "learning_rate": 0.00013992,
      "loss": -118.152,
      "step": 22530
    },
    {
      "epoch": 1.8032,
      "grad_norm": 127.65412902832031,
      "learning_rate": 0.00013989333333333333,
      "loss": -117.881,
      "step": 22540
    },
    {
      "epoch": 1.804,
      "grad_norm": 75.25199890136719,
      "learning_rate": 0.0001398666666666667,
      "loss": -117.4406,
      "step": 22550
    },
    {
      "epoch": 1.8048,
      "grad_norm": 103.89360046386719,
      "learning_rate": 0.00013984000000000002,
      "loss": -117.2472,
      "step": 22560
    },
    {
      "epoch": 1.8056,
      "grad_norm": 91.27590942382812,
      "learning_rate": 0.00013981333333333332,
      "loss": -117.4985,
      "step": 22570
    },
    {
      "epoch": 1.8064,
      "grad_norm": 113.98329162597656,
      "learning_rate": 0.00013978666666666667,
      "loss": -118.378,
      "step": 22580
    },
    {
      "epoch": 1.8072,
      "grad_norm": 144.74371337890625,
      "learning_rate": 0.00013976,
      "loss": -117.1222,
      "step": 22590
    },
    {
      "epoch": 1.808,
      "grad_norm": 100.48687744140625,
      "learning_rate": 0.00013973333333333333,
      "loss": -118.3463,
      "step": 22600
    },
    {
      "epoch": 1.8088,
      "grad_norm": 137.31581115722656,
      "learning_rate": 0.00013970666666666666,
      "loss": -118.6469,
      "step": 22610
    },
    {
      "epoch": 1.8096,
      "grad_norm": 84.97254180908203,
      "learning_rate": 0.00013968000000000002,
      "loss": -117.2972,
      "step": 22620
    },
    {
      "epoch": 1.8104,
      "grad_norm": 193.55813598632812,
      "learning_rate": 0.00013965333333333335,
      "loss": -118.6941,
      "step": 22630
    },
    {
      "epoch": 1.8112,
      "grad_norm": 133.1509246826172,
      "learning_rate": 0.00013962666666666668,
      "loss": -117.3693,
      "step": 22640
    },
    {
      "epoch": 1.812,
      "grad_norm": 100.29277801513672,
      "learning_rate": 0.0001396,
      "loss": -116.9814,
      "step": 22650
    },
    {
      "epoch": 1.8128,
      "grad_norm": 168.05023193359375,
      "learning_rate": 0.00013957333333333333,
      "loss": -118.1187,
      "step": 22660
    },
    {
      "epoch": 1.8136,
      "grad_norm": 72.56122589111328,
      "learning_rate": 0.00013954666666666666,
      "loss": -117.4942,
      "step": 22670
    },
    {
      "epoch": 1.8144,
      "grad_norm": 79.99974060058594,
      "learning_rate": 0.00013952000000000002,
      "loss": -118.7306,
      "step": 22680
    },
    {
      "epoch": 1.8152,
      "grad_norm": 124.31189727783203,
      "learning_rate": 0.00013949333333333335,
      "loss": -117.3871,
      "step": 22690
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 70.82140350341797,
      "learning_rate": 0.00013946666666666668,
      "loss": -117.656,
      "step": 22700
    },
    {
      "epoch": 1.8168,
      "grad_norm": 159.6553192138672,
      "learning_rate": 0.00013944,
      "loss": -117.694,
      "step": 22710
    },
    {
      "epoch": 1.8176,
      "grad_norm": 109.97676086425781,
      "learning_rate": 0.00013941333333333334,
      "loss": -118.4757,
      "step": 22720
    },
    {
      "epoch": 1.8184,
      "grad_norm": 106.34336853027344,
      "learning_rate": 0.00013938666666666666,
      "loss": -117.4182,
      "step": 22730
    },
    {
      "epoch": 1.8192,
      "grad_norm": 144.16552734375,
      "learning_rate": 0.00013936,
      "loss": -117.0958,
      "step": 22740
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 92.06669616699219,
      "learning_rate": 0.00013933333333333335,
      "loss": -117.2893,
      "step": 22750
    },
    {
      "epoch": 1.8208,
      "grad_norm": 75.39954376220703,
      "learning_rate": 0.00013930666666666668,
      "loss": -117.2444,
      "step": 22760
    },
    {
      "epoch": 1.8216,
      "grad_norm": 107.7993392944336,
      "learning_rate": 0.00013928,
      "loss": -119.3336,
      "step": 22770
    },
    {
      "epoch": 1.8224,
      "grad_norm": 88.05425262451172,
      "learning_rate": 0.00013925333333333334,
      "loss": -117.3305,
      "step": 22780
    },
    {
      "epoch": 1.8232,
      "grad_norm": 113.47895050048828,
      "learning_rate": 0.0001392266666666667,
      "loss": -116.7672,
      "step": 22790
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 107.03244018554688,
      "learning_rate": 0.0001392,
      "loss": -117.5031,
      "step": 22800
    },
    {
      "epoch": 1.8248,
      "grad_norm": 159.11866760253906,
      "learning_rate": 0.00013917333333333332,
      "loss": -116.3019,
      "step": 22810
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 126.9562759399414,
      "learning_rate": 0.00013914666666666668,
      "loss": -118.9834,
      "step": 22820
    },
    {
      "epoch": 1.8264,
      "grad_norm": 153.5963134765625,
      "learning_rate": 0.00013912,
      "loss": -117.5839,
      "step": 22830
    },
    {
      "epoch": 1.8272,
      "grad_norm": 107.4268798828125,
      "learning_rate": 0.00013909333333333334,
      "loss": -117.1437,
      "step": 22840
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 87.6954574584961,
      "learning_rate": 0.00013906666666666667,
      "loss": -116.9901,
      "step": 22850
    },
    {
      "epoch": 1.8288,
      "grad_norm": 108.67058563232422,
      "learning_rate": 0.00013904000000000002,
      "loss": -117.2976,
      "step": 22860
    },
    {
      "epoch": 1.8296000000000001,
      "grad_norm": 107.28866577148438,
      "learning_rate": 0.00013901333333333335,
      "loss": -117.4166,
      "step": 22870
    },
    {
      "epoch": 1.8304,
      "grad_norm": 87.71604919433594,
      "learning_rate": 0.00013898666666666666,
      "loss": -117.9535,
      "step": 22880
    },
    {
      "epoch": 1.8312,
      "grad_norm": 117.08271789550781,
      "learning_rate": 0.00013896,
      "loss": -117.6839,
      "step": 22890
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 114.99169158935547,
      "learning_rate": 0.00013893333333333334,
      "loss": -118.1948,
      "step": 22900
    },
    {
      "epoch": 1.8328,
      "grad_norm": 79.8559799194336,
      "learning_rate": 0.00013890666666666667,
      "loss": -116.8486,
      "step": 22910
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 80.66548919677734,
      "learning_rate": 0.00013888,
      "loss": -117.7783,
      "step": 22920
    },
    {
      "epoch": 1.8344,
      "grad_norm": 96.12264251708984,
      "learning_rate": 0.00013885333333333335,
      "loss": -116.5443,
      "step": 22930
    },
    {
      "epoch": 1.8352,
      "grad_norm": 112.78910064697266,
      "learning_rate": 0.00013882666666666668,
      "loss": -118.2843,
      "step": 22940
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 82.52011108398438,
      "learning_rate": 0.00013879999999999999,
      "loss": -117.6007,
      "step": 22950
    },
    {
      "epoch": 1.8368,
      "grad_norm": 79.79143524169922,
      "learning_rate": 0.00013877333333333334,
      "loss": -117.7729,
      "step": 22960
    },
    {
      "epoch": 1.8376000000000001,
      "grad_norm": 144.0502471923828,
      "learning_rate": 0.00013874666666666667,
      "loss": -118.2586,
      "step": 22970
    },
    {
      "epoch": 1.8384,
      "grad_norm": 51.19733428955078,
      "learning_rate": 0.00013872,
      "loss": -118.6525,
      "step": 22980
    },
    {
      "epoch": 1.8392,
      "grad_norm": 141.2048797607422,
      "learning_rate": 0.00013869333333333333,
      "loss": -118.0206,
      "step": 22990
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 98.62580108642578,
      "learning_rate": 0.00013866666666666669,
      "loss": -117.1415,
      "step": 23000
    },
    {
      "epoch": 1.8408,
      "grad_norm": 110.08068084716797,
      "learning_rate": 0.00013864000000000001,
      "loss": -118.4721,
      "step": 23010
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 104.27410888671875,
      "learning_rate": 0.00013861333333333334,
      "loss": -116.9286,
      "step": 23020
    },
    {
      "epoch": 1.8424,
      "grad_norm": 109.96876525878906,
      "learning_rate": 0.00013858666666666667,
      "loss": -117.9631,
      "step": 23030
    },
    {
      "epoch": 1.8432,
      "grad_norm": 114.36573028564453,
      "learning_rate": 0.00013856,
      "loss": -118.1153,
      "step": 23040
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 91.20442962646484,
      "learning_rate": 0.00013853333333333333,
      "loss": -117.1393,
      "step": 23050
    },
    {
      "epoch": 1.8448,
      "grad_norm": 83.44336700439453,
      "learning_rate": 0.0001385066666666667,
      "loss": -117.6536,
      "step": 23060
    },
    {
      "epoch": 1.8456000000000001,
      "grad_norm": 78.16769409179688,
      "learning_rate": 0.00013848000000000002,
      "loss": -117.3986,
      "step": 23070
    },
    {
      "epoch": 1.8464,
      "grad_norm": 114.66144561767578,
      "learning_rate": 0.00013845333333333334,
      "loss": -117.8666,
      "step": 23080
    },
    {
      "epoch": 1.8472,
      "grad_norm": 180.78048706054688,
      "learning_rate": 0.00013842666666666667,
      "loss": -117.8107,
      "step": 23090
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 320.7082824707031,
      "learning_rate": 0.0001384,
      "loss": -117.7013,
      "step": 23100
    },
    {
      "epoch": 1.8488,
      "grad_norm": 133.9211883544922,
      "learning_rate": 0.00013837333333333333,
      "loss": -117.1267,
      "step": 23110
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 105.09502410888672,
      "learning_rate": 0.00013834666666666666,
      "loss": -118.0779,
      "step": 23120
    },
    {
      "epoch": 1.8504,
      "grad_norm": 125.79664611816406,
      "learning_rate": 0.00013832000000000002,
      "loss": -118.0682,
      "step": 23130
    },
    {
      "epoch": 1.8512,
      "grad_norm": 165.64022827148438,
      "learning_rate": 0.00013829333333333335,
      "loss": -117.1855,
      "step": 23140
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 106.2031478881836,
      "learning_rate": 0.00013826666666666668,
      "loss": -118.3087,
      "step": 23150
    },
    {
      "epoch": 1.8528,
      "grad_norm": 103.11967468261719,
      "learning_rate": 0.00013824,
      "loss": -118.376,
      "step": 23160
    },
    {
      "epoch": 1.8536000000000001,
      "grad_norm": 124.67056274414062,
      "learning_rate": 0.00013821333333333336,
      "loss": -116.7671,
      "step": 23170
    },
    {
      "epoch": 1.8544,
      "grad_norm": 150.06442260742188,
      "learning_rate": 0.00013818666666666666,
      "loss": -117.1746,
      "step": 23180
    },
    {
      "epoch": 1.8552,
      "grad_norm": 77.26154327392578,
      "learning_rate": 0.00013816,
      "loss": -117.8168,
      "step": 23190
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 85.85511016845703,
      "learning_rate": 0.00013813333333333335,
      "loss": -117.4258,
      "step": 23200
    },
    {
      "epoch": 1.8568,
      "grad_norm": 98.0694580078125,
      "learning_rate": 0.00013810666666666668,
      "loss": -117.4552,
      "step": 23210
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 90.81693267822266,
      "learning_rate": 0.00013808,
      "loss": -117.7991,
      "step": 23220
    },
    {
      "epoch": 1.8584,
      "grad_norm": 77.4237060546875,
      "learning_rate": 0.00013805333333333334,
      "loss": -116.8156,
      "step": 23230
    },
    {
      "epoch": 1.8592,
      "grad_norm": 133.4761505126953,
      "learning_rate": 0.0001380266666666667,
      "loss": -117.7763,
      "step": 23240
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 131.1409454345703,
      "learning_rate": 0.000138,
      "loss": -117.721,
      "step": 23250
    },
    {
      "epoch": 1.8608,
      "grad_norm": 91.5448226928711,
      "learning_rate": 0.00013797333333333332,
      "loss": -118.1073,
      "step": 23260
    },
    {
      "epoch": 1.8616000000000001,
      "grad_norm": 79.88502502441406,
      "learning_rate": 0.00013794666666666668,
      "loss": -117.4775,
      "step": 23270
    },
    {
      "epoch": 1.8624,
      "grad_norm": 109.98638153076172,
      "learning_rate": 0.00013792,
      "loss": -117.0792,
      "step": 23280
    },
    {
      "epoch": 1.8632,
      "grad_norm": 112.39797973632812,
      "learning_rate": 0.00013789333333333334,
      "loss": -118.0066,
      "step": 23290
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 97.51570892333984,
      "learning_rate": 0.00013786666666666667,
      "loss": -118.3233,
      "step": 23300
    },
    {
      "epoch": 1.8648,
      "grad_norm": 112.46782684326172,
      "learning_rate": 0.00013784000000000002,
      "loss": -117.3813,
      "step": 23310
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 117.44682312011719,
      "learning_rate": 0.00013781333333333335,
      "loss": -118.439,
      "step": 23320
    },
    {
      "epoch": 1.8664,
      "grad_norm": 117.50173950195312,
      "learning_rate": 0.00013778666666666665,
      "loss": -117.3164,
      "step": 23330
    },
    {
      "epoch": 1.8672,
      "grad_norm": 134.14718627929688,
      "learning_rate": 0.00013776,
      "loss": -117.1567,
      "step": 23340
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 213.8859100341797,
      "learning_rate": 0.00013773333333333334,
      "loss": -118.3767,
      "step": 23350
    },
    {
      "epoch": 1.8688,
      "grad_norm": 189.86279296875,
      "learning_rate": 0.00013770666666666667,
      "loss": -117.7865,
      "step": 23360
    },
    {
      "epoch": 1.8696000000000002,
      "grad_norm": 65.90254974365234,
      "learning_rate": 0.00013768,
      "loss": -119.1926,
      "step": 23370
    },
    {
      "epoch": 1.8704,
      "grad_norm": 89.93797302246094,
      "learning_rate": 0.00013765333333333335,
      "loss": -117.3112,
      "step": 23380
    },
    {
      "epoch": 1.8712,
      "grad_norm": 72.58149719238281,
      "learning_rate": 0.00013762666666666668,
      "loss": -116.9562,
      "step": 23390
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 97.27368927001953,
      "learning_rate": 0.00013759999999999998,
      "loss": -118.9137,
      "step": 23400
    },
    {
      "epoch": 1.8728,
      "grad_norm": 114.3816909790039,
      "learning_rate": 0.00013757333333333334,
      "loss": -118.225,
      "step": 23410
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 128.3567657470703,
      "learning_rate": 0.00013754666666666667,
      "loss": -117.6671,
      "step": 23420
    },
    {
      "epoch": 1.8744,
      "grad_norm": 109.31155395507812,
      "learning_rate": 0.00013752,
      "loss": -118.3229,
      "step": 23430
    },
    {
      "epoch": 1.8752,
      "grad_norm": 128.0842742919922,
      "learning_rate": 0.00013749333333333335,
      "loss": -119.0015,
      "step": 23440
    },
    {
      "epoch": 1.876,
      "grad_norm": 133.3376007080078,
      "learning_rate": 0.00013746666666666668,
      "loss": -117.8206,
      "step": 23450
    },
    {
      "epoch": 1.8768,
      "grad_norm": 125.82426452636719,
      "learning_rate": 0.00013744,
      "loss": -115.8089,
      "step": 23460
    },
    {
      "epoch": 1.8776000000000002,
      "grad_norm": 141.4514923095703,
      "learning_rate": 0.00013741333333333334,
      "loss": -118.0845,
      "step": 23470
    },
    {
      "epoch": 1.8784,
      "grad_norm": 267.8012390136719,
      "learning_rate": 0.00013738666666666667,
      "loss": -117.377,
      "step": 23480
    },
    {
      "epoch": 1.8792,
      "grad_norm": 108.42788696289062,
      "learning_rate": 0.00013736,
      "loss": -118.7408,
      "step": 23490
    },
    {
      "epoch": 1.88,
      "grad_norm": 86.27704620361328,
      "learning_rate": 0.00013733333333333333,
      "loss": -117.4664,
      "step": 23500
    },
    {
      "epoch": 1.8808,
      "grad_norm": 99.85841369628906,
      "learning_rate": 0.00013730666666666668,
      "loss": -118.7181,
      "step": 23510
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 106.37813568115234,
      "learning_rate": 0.00013728000000000001,
      "loss": -117.4686,
      "step": 23520
    },
    {
      "epoch": 1.8824,
      "grad_norm": 93.6478042602539,
      "learning_rate": 0.00013725333333333334,
      "loss": -118.5074,
      "step": 23530
    },
    {
      "epoch": 1.8832,
      "grad_norm": 76.59298706054688,
      "learning_rate": 0.00013722666666666667,
      "loss": -117.7194,
      "step": 23540
    },
    {
      "epoch": 1.884,
      "grad_norm": 96.98573303222656,
      "learning_rate": 0.00013720000000000003,
      "loss": -118.0254,
      "step": 23550
    },
    {
      "epoch": 1.8848,
      "grad_norm": 192.13571166992188,
      "learning_rate": 0.00013717333333333333,
      "loss": -118.2709,
      "step": 23560
    },
    {
      "epoch": 1.8856000000000002,
      "grad_norm": 95.98944854736328,
      "learning_rate": 0.00013714666666666666,
      "loss": -117.4658,
      "step": 23570
    },
    {
      "epoch": 1.8864,
      "grad_norm": 117.63314056396484,
      "learning_rate": 0.00013712000000000002,
      "loss": -117.4223,
      "step": 23580
    },
    {
      "epoch": 1.8872,
      "grad_norm": 101.22889709472656,
      "learning_rate": 0.00013709333333333334,
      "loss": -117.0374,
      "step": 23590
    },
    {
      "epoch": 1.888,
      "grad_norm": 65.39649200439453,
      "learning_rate": 0.00013706666666666667,
      "loss": -118.4383,
      "step": 23600
    },
    {
      "epoch": 1.8888,
      "grad_norm": 100.10811614990234,
      "learning_rate": 0.00013704,
      "loss": -118.7022,
      "step": 23610
    },
    {
      "epoch": 1.8896,
      "grad_norm": 75.73208618164062,
      "learning_rate": 0.00013701333333333336,
      "loss": -116.4791,
      "step": 23620
    },
    {
      "epoch": 1.8904,
      "grad_norm": 91.99449920654297,
      "learning_rate": 0.00013698666666666666,
      "loss": -119.213,
      "step": 23630
    },
    {
      "epoch": 1.8912,
      "grad_norm": 123.58291625976562,
      "learning_rate": 0.00013696,
      "loss": -118.8085,
      "step": 23640
    },
    {
      "epoch": 1.892,
      "grad_norm": 88.39807891845703,
      "learning_rate": 0.00013693333333333335,
      "loss": -118.1115,
      "step": 23650
    },
    {
      "epoch": 1.8928,
      "grad_norm": 121.17621612548828,
      "learning_rate": 0.00013690666666666667,
      "loss": -118.0665,
      "step": 23660
    },
    {
      "epoch": 1.8936,
      "grad_norm": 93.3237533569336,
      "learning_rate": 0.00013688,
      "loss": -117.8672,
      "step": 23670
    },
    {
      "epoch": 1.8944,
      "grad_norm": 127.5502700805664,
      "learning_rate": 0.00013685333333333333,
      "loss": -117.0767,
      "step": 23680
    },
    {
      "epoch": 1.8952,
      "grad_norm": 100.6087646484375,
      "learning_rate": 0.0001368266666666667,
      "loss": -116.2013,
      "step": 23690
    },
    {
      "epoch": 1.896,
      "grad_norm": 114.98143005371094,
      "learning_rate": 0.00013680000000000002,
      "loss": -117.7055,
      "step": 23700
    },
    {
      "epoch": 1.8968,
      "grad_norm": 118.45972442626953,
      "learning_rate": 0.00013677333333333332,
      "loss": -118.1573,
      "step": 23710
    },
    {
      "epoch": 1.8976,
      "grad_norm": 115.96829223632812,
      "learning_rate": 0.00013674666666666668,
      "loss": -118.6273,
      "step": 23720
    },
    {
      "epoch": 1.8984,
      "grad_norm": 120.97747802734375,
      "learning_rate": 0.00013672,
      "loss": -117.0847,
      "step": 23730
    },
    {
      "epoch": 1.8992,
      "grad_norm": 113.00474548339844,
      "learning_rate": 0.00013669333333333333,
      "loss": -118.8606,
      "step": 23740
    },
    {
      "epoch": 1.9,
      "grad_norm": 100.61886596679688,
      "learning_rate": 0.00013666666666666666,
      "loss": -117.463,
      "step": 23750
    },
    {
      "epoch": 1.9008,
      "grad_norm": 99.33324432373047,
      "learning_rate": 0.00013664000000000002,
      "loss": -117.5678,
      "step": 23760
    },
    {
      "epoch": 1.9016,
      "grad_norm": 103.69805145263672,
      "learning_rate": 0.00013661333333333335,
      "loss": -117.8744,
      "step": 23770
    },
    {
      "epoch": 1.9024,
      "grad_norm": 74.7794418334961,
      "learning_rate": 0.00013658666666666665,
      "loss": -117.4783,
      "step": 23780
    },
    {
      "epoch": 1.9032,
      "grad_norm": 128.47938537597656,
      "learning_rate": 0.00013656,
      "loss": -117.8629,
      "step": 23790
    },
    {
      "epoch": 1.904,
      "grad_norm": 163.08016967773438,
      "learning_rate": 0.00013653333333333334,
      "loss": -118.8132,
      "step": 23800
    },
    {
      "epoch": 1.9048,
      "grad_norm": 140.59481811523438,
      "learning_rate": 0.00013650666666666667,
      "loss": -117.5949,
      "step": 23810
    },
    {
      "epoch": 1.9056,
      "grad_norm": 61.903663635253906,
      "learning_rate": 0.00013648,
      "loss": -117.5439,
      "step": 23820
    },
    {
      "epoch": 1.9064,
      "grad_norm": 180.4200439453125,
      "learning_rate": 0.00013645333333333335,
      "loss": -118.0506,
      "step": 23830
    },
    {
      "epoch": 1.9072,
      "grad_norm": 129.4546661376953,
      "learning_rate": 0.00013642666666666668,
      "loss": -118.4588,
      "step": 23840
    },
    {
      "epoch": 1.908,
      "grad_norm": 104.81307983398438,
      "learning_rate": 0.0001364,
      "loss": -116.7434,
      "step": 23850
    },
    {
      "epoch": 1.9088,
      "grad_norm": 130.80426025390625,
      "learning_rate": 0.00013637333333333334,
      "loss": -116.7176,
      "step": 23860
    },
    {
      "epoch": 1.9096,
      "grad_norm": 67.09895324707031,
      "learning_rate": 0.00013634666666666667,
      "loss": -119.2459,
      "step": 23870
    },
    {
      "epoch": 1.9104,
      "grad_norm": 102.35134887695312,
      "learning_rate": 0.00013632,
      "loss": -118.2811,
      "step": 23880
    },
    {
      "epoch": 1.9112,
      "grad_norm": 138.4624786376953,
      "learning_rate": 0.00013629333333333335,
      "loss": -117.49,
      "step": 23890
    },
    {
      "epoch": 1.912,
      "grad_norm": 103.8604736328125,
      "learning_rate": 0.00013626666666666668,
      "loss": -118.7029,
      "step": 23900
    },
    {
      "epoch": 1.9127999999999998,
      "grad_norm": 90.23529052734375,
      "learning_rate": 0.00013624,
      "loss": -118.8328,
      "step": 23910
    },
    {
      "epoch": 1.9136,
      "grad_norm": 95.983154296875,
      "learning_rate": 0.00013621333333333334,
      "loss": -116.5431,
      "step": 23920
    },
    {
      "epoch": 1.9144,
      "grad_norm": 155.3740234375,
      "learning_rate": 0.00013618666666666667,
      "loss": -118.8075,
      "step": 23930
    },
    {
      "epoch": 1.9152,
      "grad_norm": 76.27371978759766,
      "learning_rate": 0.00013616,
      "loss": -116.7283,
      "step": 23940
    },
    {
      "epoch": 1.916,
      "grad_norm": 113.76548767089844,
      "learning_rate": 0.00013613333333333333,
      "loss": -118.5096,
      "step": 23950
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 132.27841186523438,
      "learning_rate": 0.00013610666666666668,
      "loss": -118.0387,
      "step": 23960
    },
    {
      "epoch": 1.9176,
      "grad_norm": 80.89615631103516,
      "learning_rate": 0.00013608,
      "loss": -118.1347,
      "step": 23970
    },
    {
      "epoch": 1.9184,
      "grad_norm": 408.4048767089844,
      "learning_rate": 0.00013605333333333334,
      "loss": -117.8865,
      "step": 23980
    },
    {
      "epoch": 1.9192,
      "grad_norm": 104.59228515625,
      "learning_rate": 0.00013602666666666667,
      "loss": -116.4928,
      "step": 23990
    },
    {
      "epoch": 1.92,
      "grad_norm": 111.0843734741211,
      "learning_rate": 0.00013600000000000003,
      "loss": -117.5598,
      "step": 24000
    },
    {
      "epoch": 1.9207999999999998,
      "grad_norm": 157.7613983154297,
      "learning_rate": 0.00013597333333333333,
      "loss": -117.3271,
      "step": 24010
    },
    {
      "epoch": 1.9216,
      "grad_norm": 76.37005615234375,
      "learning_rate": 0.00013594666666666666,
      "loss": -117.7984,
      "step": 24020
    },
    {
      "epoch": 1.9224,
      "grad_norm": 107.8878173828125,
      "learning_rate": 0.00013592,
      "loss": -117.7911,
      "step": 24030
    },
    {
      "epoch": 1.9232,
      "grad_norm": 66.83638000488281,
      "learning_rate": 0.00013589333333333334,
      "loss": -117.9266,
      "step": 24040
    },
    {
      "epoch": 1.924,
      "grad_norm": 108.36610412597656,
      "learning_rate": 0.00013586666666666667,
      "loss": -118.748,
      "step": 24050
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 131.11819458007812,
      "learning_rate": 0.00013584,
      "loss": -117.1028,
      "step": 24060
    },
    {
      "epoch": 1.9256,
      "grad_norm": 105.25041198730469,
      "learning_rate": 0.00013581333333333336,
      "loss": -118.1991,
      "step": 24070
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 164.13253784179688,
      "learning_rate": 0.00013578666666666669,
      "loss": -117.3188,
      "step": 24080
    },
    {
      "epoch": 1.9272,
      "grad_norm": 121.98918914794922,
      "learning_rate": 0.00013576,
      "loss": -118.8083,
      "step": 24090
    },
    {
      "epoch": 1.928,
      "grad_norm": 105.91295623779297,
      "learning_rate": 0.00013573333333333334,
      "loss": -117.5133,
      "step": 24100
    },
    {
      "epoch": 1.9287999999999998,
      "grad_norm": 72.77693176269531,
      "learning_rate": 0.00013570666666666667,
      "loss": -118.4627,
      "step": 24110
    },
    {
      "epoch": 1.9296,
      "grad_norm": 131.10519409179688,
      "learning_rate": 0.00013568,
      "loss": -117.9389,
      "step": 24120
    },
    {
      "epoch": 1.9304000000000001,
      "grad_norm": 129.57591247558594,
      "learning_rate": 0.00013565333333333333,
      "loss": -118.0801,
      "step": 24130
    },
    {
      "epoch": 1.9312,
      "grad_norm": 111.85015106201172,
      "learning_rate": 0.0001356266666666667,
      "loss": -117.8569,
      "step": 24140
    },
    {
      "epoch": 1.932,
      "grad_norm": 77.79075622558594,
      "learning_rate": 0.00013560000000000002,
      "loss": -118.4647,
      "step": 24150
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 86.20305633544922,
      "learning_rate": 0.00013557333333333332,
      "loss": -118.4778,
      "step": 24160
    },
    {
      "epoch": 1.9336,
      "grad_norm": 99.17242431640625,
      "learning_rate": 0.00013554666666666667,
      "loss": -118.5223,
      "step": 24170
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 113.65077209472656,
      "learning_rate": 0.00013552,
      "loss": -119.5429,
      "step": 24180
    },
    {
      "epoch": 1.9352,
      "grad_norm": 134.9378662109375,
      "learning_rate": 0.00013549333333333333,
      "loss": -118.7124,
      "step": 24190
    },
    {
      "epoch": 1.936,
      "grad_norm": 126.98643493652344,
      "learning_rate": 0.00013546666666666666,
      "loss": -118.566,
      "step": 24200
    },
    {
      "epoch": 1.9367999999999999,
      "grad_norm": 81.68743896484375,
      "learning_rate": 0.00013544000000000002,
      "loss": -118.6293,
      "step": 24210
    },
    {
      "epoch": 1.9376,
      "grad_norm": 109.44458770751953,
      "learning_rate": 0.00013541333333333335,
      "loss": -117.6482,
      "step": 24220
    },
    {
      "epoch": 1.9384000000000001,
      "grad_norm": 151.802490234375,
      "learning_rate": 0.00013538666666666668,
      "loss": -117.5933,
      "step": 24230
    },
    {
      "epoch": 1.9392,
      "grad_norm": 149.4102325439453,
      "learning_rate": 0.00013536,
      "loss": -117.9963,
      "step": 24240
    },
    {
      "epoch": 1.94,
      "grad_norm": 79.47612762451172,
      "learning_rate": 0.00013533333333333333,
      "loss": -117.5358,
      "step": 24250
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 148.71603393554688,
      "learning_rate": 0.00013530666666666666,
      "loss": -118.984,
      "step": 24260
    },
    {
      "epoch": 1.9416,
      "grad_norm": 88.01901245117188,
      "learning_rate": 0.00013528000000000002,
      "loss": -117.1142,
      "step": 24270
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 116.84193420410156,
      "learning_rate": 0.00013525333333333335,
      "loss": -117.6218,
      "step": 24280
    },
    {
      "epoch": 1.9432,
      "grad_norm": 103.29841613769531,
      "learning_rate": 0.00013522666666666668,
      "loss": -118.2767,
      "step": 24290
    },
    {
      "epoch": 1.944,
      "grad_norm": 104.01677703857422,
      "learning_rate": 0.0001352,
      "loss": -118.2921,
      "step": 24300
    },
    {
      "epoch": 1.9447999999999999,
      "grad_norm": 99.08296966552734,
      "learning_rate": 0.00013517333333333334,
      "loss": -118.2624,
      "step": 24310
    },
    {
      "epoch": 1.9456,
      "grad_norm": 90.3076171875,
      "learning_rate": 0.00013514666666666666,
      "loss": -118.9037,
      "step": 24320
    },
    {
      "epoch": 1.9464000000000001,
      "grad_norm": 263.1220397949219,
      "learning_rate": 0.00013512,
      "loss": -119.0696,
      "step": 24330
    },
    {
      "epoch": 1.9472,
      "grad_norm": 165.18597412109375,
      "learning_rate": 0.00013509333333333335,
      "loss": -118.0635,
      "step": 24340
    },
    {
      "epoch": 1.948,
      "grad_norm": 119.09326171875,
      "learning_rate": 0.00013506666666666668,
      "loss": -116.7523,
      "step": 24350
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 109.76736450195312,
      "learning_rate": 0.00013504,
      "loss": -118.2885,
      "step": 24360
    },
    {
      "epoch": 1.9496,
      "grad_norm": 89.56295776367188,
      "learning_rate": 0.00013501333333333334,
      "loss": -118.783,
      "step": 24370
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 157.54937744140625,
      "learning_rate": 0.0001349866666666667,
      "loss": -116.928,
      "step": 24380
    },
    {
      "epoch": 1.9512,
      "grad_norm": 104.68958282470703,
      "learning_rate": 0.00013496,
      "loss": -118.3018,
      "step": 24390
    },
    {
      "epoch": 1.952,
      "grad_norm": 127.69114685058594,
      "learning_rate": 0.00013493333333333332,
      "loss": -118.3642,
      "step": 24400
    },
    {
      "epoch": 1.9527999999999999,
      "grad_norm": 92.7698974609375,
      "learning_rate": 0.00013490666666666668,
      "loss": -117.657,
      "step": 24410
    },
    {
      "epoch": 1.9536,
      "grad_norm": 419.3840637207031,
      "learning_rate": 0.00013488,
      "loss": -118.3699,
      "step": 24420
    },
    {
      "epoch": 1.9544000000000001,
      "grad_norm": 122.53406524658203,
      "learning_rate": 0.00013485333333333334,
      "loss": -117.4873,
      "step": 24430
    },
    {
      "epoch": 1.9552,
      "grad_norm": 96.57937622070312,
      "learning_rate": 0.00013482666666666667,
      "loss": -118.5533,
      "step": 24440
    },
    {
      "epoch": 1.956,
      "grad_norm": 94.94673919677734,
      "learning_rate": 0.00013480000000000002,
      "loss": -117.7188,
      "step": 24450
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 104.45388793945312,
      "learning_rate": 0.00013477333333333333,
      "loss": -118.5538,
      "step": 24460
    },
    {
      "epoch": 1.9576,
      "grad_norm": 69.71794128417969,
      "learning_rate": 0.00013474666666666666,
      "loss": -117.905,
      "step": 24470
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 116.8606185913086,
      "learning_rate": 0.00013472,
      "loss": -117.1936,
      "step": 24480
    },
    {
      "epoch": 1.9592,
      "grad_norm": 157.67576599121094,
      "learning_rate": 0.00013469333333333334,
      "loss": -118.555,
      "step": 24490
    },
    {
      "epoch": 1.96,
      "grad_norm": 168.65237426757812,
      "learning_rate": 0.00013466666666666667,
      "loss": -117.6457,
      "step": 24500
    },
    {
      "epoch": 1.9607999999999999,
      "grad_norm": 139.12538146972656,
      "learning_rate": 0.00013464,
      "loss": -118.4148,
      "step": 24510
    },
    {
      "epoch": 1.9616,
      "grad_norm": 90.2447280883789,
      "learning_rate": 0.00013461333333333335,
      "loss": -118.2219,
      "step": 24520
    },
    {
      "epoch": 1.9624000000000001,
      "grad_norm": 105.69734191894531,
      "learning_rate": 0.00013458666666666668,
      "loss": -117.7237,
      "step": 24530
    },
    {
      "epoch": 1.9632,
      "grad_norm": 103.47518920898438,
      "learning_rate": 0.00013455999999999999,
      "loss": -118.1298,
      "step": 24540
    },
    {
      "epoch": 1.964,
      "grad_norm": 135.30751037597656,
      "learning_rate": 0.00013453333333333334,
      "loss": -117.7829,
      "step": 24550
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 110.1793212890625,
      "learning_rate": 0.00013450666666666667,
      "loss": -116.941,
      "step": 24560
    },
    {
      "epoch": 1.9656,
      "grad_norm": 109.47084045410156,
      "learning_rate": 0.00013448,
      "loss": -119.422,
      "step": 24570
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 109.48389434814453,
      "learning_rate": 0.00013445333333333333,
      "loss": -118.3525,
      "step": 24580
    },
    {
      "epoch": 1.9672,
      "grad_norm": 73.60432434082031,
      "learning_rate": 0.00013442666666666669,
      "loss": -118.7449,
      "step": 24590
    },
    {
      "epoch": 1.968,
      "grad_norm": 144.72451782226562,
      "learning_rate": 0.00013440000000000001,
      "loss": -118.4516,
      "step": 24600
    },
    {
      "epoch": 1.9687999999999999,
      "grad_norm": 356.8720703125,
      "learning_rate": 0.00013437333333333332,
      "loss": -118.1206,
      "step": 24610
    },
    {
      "epoch": 1.9696,
      "grad_norm": 126.33008575439453,
      "learning_rate": 0.00013434666666666667,
      "loss": -119.2894,
      "step": 24620
    },
    {
      "epoch": 1.9704000000000002,
      "grad_norm": 68.49043273925781,
      "learning_rate": 0.00013432,
      "loss": -118.0494,
      "step": 24630
    },
    {
      "epoch": 1.9712,
      "grad_norm": 97.12480163574219,
      "learning_rate": 0.00013429333333333333,
      "loss": -118.0186,
      "step": 24640
    },
    {
      "epoch": 1.972,
      "grad_norm": 66.72996520996094,
      "learning_rate": 0.0001342666666666667,
      "loss": -118.2175,
      "step": 24650
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 70.84854125976562,
      "learning_rate": 0.00013424000000000002,
      "loss": -118.4499,
      "step": 24660
    },
    {
      "epoch": 1.9736,
      "grad_norm": 209.5277099609375,
      "learning_rate": 0.00013421333333333334,
      "loss": -117.6302,
      "step": 24670
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 146.6021270751953,
      "learning_rate": 0.00013418666666666667,
      "loss": -117.8227,
      "step": 24680
    },
    {
      "epoch": 1.9752,
      "grad_norm": 90.37133026123047,
      "learning_rate": 0.00013416,
      "loss": -118.8544,
      "step": 24690
    },
    {
      "epoch": 1.976,
      "grad_norm": 84.8250961303711,
      "learning_rate": 0.00013413333333333333,
      "loss": -119.4925,
      "step": 24700
    },
    {
      "epoch": 1.9768,
      "grad_norm": 114.18083190917969,
      "learning_rate": 0.00013410666666666666,
      "loss": -117.4722,
      "step": 24710
    },
    {
      "epoch": 1.9776,
      "grad_norm": 152.93983459472656,
      "learning_rate": 0.00013408000000000002,
      "loss": -117.9559,
      "step": 24720
    },
    {
      "epoch": 1.9784000000000002,
      "grad_norm": 125.05746459960938,
      "learning_rate": 0.00013405333333333335,
      "loss": -118.3213,
      "step": 24730
    },
    {
      "epoch": 1.9792,
      "grad_norm": 128.99169921875,
      "learning_rate": 0.00013402666666666668,
      "loss": -116.9065,
      "step": 24740
    },
    {
      "epoch": 1.98,
      "grad_norm": 81.2417221069336,
      "learning_rate": 0.000134,
      "loss": -118.6249,
      "step": 24750
    },
    {
      "epoch": 1.9808,
      "grad_norm": 126.28367614746094,
      "learning_rate": 0.00013397333333333336,
      "loss": -117.5727,
      "step": 24760
    },
    {
      "epoch": 1.9816,
      "grad_norm": 78.53572845458984,
      "learning_rate": 0.00013394666666666666,
      "loss": -117.0971,
      "step": 24770
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 72.16437530517578,
      "learning_rate": 0.00013392,
      "loss": -117.7882,
      "step": 24780
    },
    {
      "epoch": 1.9832,
      "grad_norm": 107.86722564697266,
      "learning_rate": 0.00013389333333333335,
      "loss": -117.6921,
      "step": 24790
    },
    {
      "epoch": 1.984,
      "grad_norm": 80.85317993164062,
      "learning_rate": 0.00013386666666666668,
      "loss": -118.4502,
      "step": 24800
    },
    {
      "epoch": 1.9848,
      "grad_norm": 96.128173828125,
      "learning_rate": 0.00013384,
      "loss": -117.393,
      "step": 24810
    },
    {
      "epoch": 1.9856,
      "grad_norm": 96.45445251464844,
      "learning_rate": 0.00013381333333333334,
      "loss": -117.7791,
      "step": 24820
    },
    {
      "epoch": 1.9864000000000002,
      "grad_norm": 118.28086853027344,
      "learning_rate": 0.0001337866666666667,
      "loss": -116.9038,
      "step": 24830
    },
    {
      "epoch": 1.9872,
      "grad_norm": 172.73236083984375,
      "learning_rate": 0.00013376,
      "loss": -118.1125,
      "step": 24840
    },
    {
      "epoch": 1.988,
      "grad_norm": 73.68335723876953,
      "learning_rate": 0.00013373333333333332,
      "loss": -118.6879,
      "step": 24850
    },
    {
      "epoch": 1.9888,
      "grad_norm": 103.2510757446289,
      "learning_rate": 0.00013370666666666668,
      "loss": -117.5662,
      "step": 24860
    },
    {
      "epoch": 1.9896,
      "grad_norm": 112.78011322021484,
      "learning_rate": 0.00013368,
      "loss": -117.1932,
      "step": 24870
    },
    {
      "epoch": 1.9904,
      "grad_norm": 98.56373596191406,
      "learning_rate": 0.00013365333333333334,
      "loss": -118.8768,
      "step": 24880
    },
    {
      "epoch": 1.9912,
      "grad_norm": 74.93357849121094,
      "learning_rate": 0.00013362666666666667,
      "loss": -117.8014,
      "step": 24890
    },
    {
      "epoch": 1.992,
      "grad_norm": 106.95183563232422,
      "learning_rate": 0.00013360000000000002,
      "loss": -117.2533,
      "step": 24900
    },
    {
      "epoch": 1.9928,
      "grad_norm": 77.56739044189453,
      "learning_rate": 0.00013357333333333335,
      "loss": -117.7859,
      "step": 24910
    },
    {
      "epoch": 1.9936,
      "grad_norm": 98.52327728271484,
      "learning_rate": 0.00013354666666666665,
      "loss": -118.7727,
      "step": 24920
    },
    {
      "epoch": 1.9944,
      "grad_norm": 116.89501953125,
      "learning_rate": 0.00013352,
      "loss": -117.4083,
      "step": 24930
    },
    {
      "epoch": 1.9952,
      "grad_norm": 96.05475616455078,
      "learning_rate": 0.00013349333333333334,
      "loss": -118.034,
      "step": 24940
    },
    {
      "epoch": 1.996,
      "grad_norm": 871.739013671875,
      "learning_rate": 0.00013346666666666667,
      "loss": -118.1128,
      "step": 24950
    },
    {
      "epoch": 1.9968,
      "grad_norm": 79.98875427246094,
      "learning_rate": 0.00013344,
      "loss": -118.152,
      "step": 24960
    },
    {
      "epoch": 1.9976,
      "grad_norm": 216.5790252685547,
      "learning_rate": 0.00013341333333333335,
      "loss": -117.9948,
      "step": 24970
    },
    {
      "epoch": 1.9984,
      "grad_norm": 99.08905029296875,
      "learning_rate": 0.00013338666666666668,
      "loss": -119.1888,
      "step": 24980
    },
    {
      "epoch": 1.9992,
      "grad_norm": 122.162841796875,
      "learning_rate": 0.00013335999999999998,
      "loss": -116.9862,
      "step": 24990
    },
    {
      "epoch": 2.0,
      "grad_norm": 117.1363525390625,
      "learning_rate": 0.00013333333333333334,
      "loss": -118.5684,
      "step": 25000
    }
  ],
  "logging_steps": 10,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.49606105088e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
