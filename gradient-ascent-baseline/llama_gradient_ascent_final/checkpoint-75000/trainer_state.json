{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 75000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0008,
      "grad_norm": 33.29362106323242,
      "learning_rate": 0.00019997333333333334,
      "loss": -11.7693,
      "step": 10
    },
    {
      "epoch": 0.0016,
      "grad_norm": 88.6811294555664,
      "learning_rate": 0.0001999466666666667,
      "loss": -19.5457,
      "step": 20
    },
    {
      "epoch": 0.0024,
      "grad_norm": 80.93682861328125,
      "learning_rate": 0.00019992000000000002,
      "loss": -32.6267,
      "step": 30
    },
    {
      "epoch": 0.0032,
      "grad_norm": 62.05029296875,
      "learning_rate": 0.00019989333333333333,
      "loss": -44.2532,
      "step": 40
    },
    {
      "epoch": 0.004,
      "grad_norm": 95.35437774658203,
      "learning_rate": 0.00019986666666666668,
      "loss": -54.6137,
      "step": 50
    },
    {
      "epoch": 0.0048,
      "grad_norm": 285.8903503417969,
      "learning_rate": 0.00019984,
      "loss": -67.3955,
      "step": 60
    },
    {
      "epoch": 0.0056,
      "grad_norm": 66.17249298095703,
      "learning_rate": 0.00019981333333333334,
      "loss": -77.2874,
      "step": 70
    },
    {
      "epoch": 0.0064,
      "grad_norm": 47.76024627685547,
      "learning_rate": 0.00019978666666666667,
      "loss": -83.9726,
      "step": 80
    },
    {
      "epoch": 0.0072,
      "grad_norm": 70.7253189086914,
      "learning_rate": 0.00019976000000000003,
      "loss": -89.4578,
      "step": 90
    },
    {
      "epoch": 0.008,
      "grad_norm": 62.08152770996094,
      "learning_rate": 0.00019973333333333335,
      "loss": -93.1687,
      "step": 100
    },
    {
      "epoch": 0.0088,
      "grad_norm": 36.339698791503906,
      "learning_rate": 0.00019970666666666668,
      "loss": -97.4488,
      "step": 110
    },
    {
      "epoch": 0.0096,
      "grad_norm": 47.71453857421875,
      "learning_rate": 0.00019968,
      "loss": -98.5919,
      "step": 120
    },
    {
      "epoch": 0.0104,
      "grad_norm": 56.312496185302734,
      "learning_rate": 0.00019965333333333334,
      "loss": -100.9339,
      "step": 130
    },
    {
      "epoch": 0.0112,
      "grad_norm": 55.961978912353516,
      "learning_rate": 0.00019962666666666667,
      "loss": -102.526,
      "step": 140
    },
    {
      "epoch": 0.012,
      "grad_norm": 50.273353576660156,
      "learning_rate": 0.0001996,
      "loss": -102.9351,
      "step": 150
    },
    {
      "epoch": 0.0128,
      "grad_norm": 63.39498519897461,
      "learning_rate": 0.00019957333333333336,
      "loss": -104.2318,
      "step": 160
    },
    {
      "epoch": 0.0136,
      "grad_norm": 80.68345642089844,
      "learning_rate": 0.00019954666666666669,
      "loss": -105.9233,
      "step": 170
    },
    {
      "epoch": 0.0144,
      "grad_norm": 101.63463592529297,
      "learning_rate": 0.00019952000000000001,
      "loss": -105.9457,
      "step": 180
    },
    {
      "epoch": 0.0152,
      "grad_norm": 99.14067840576172,
      "learning_rate": 0.00019949333333333334,
      "loss": -105.4273,
      "step": 190
    },
    {
      "epoch": 0.016,
      "grad_norm": 98.58280944824219,
      "learning_rate": 0.00019946666666666667,
      "loss": -107.6783,
      "step": 200
    },
    {
      "epoch": 0.0168,
      "grad_norm": 71.8197021484375,
      "learning_rate": 0.00019944,
      "loss": -108.0865,
      "step": 210
    },
    {
      "epoch": 0.0176,
      "grad_norm": 89.12177276611328,
      "learning_rate": 0.00019941333333333333,
      "loss": -108.4013,
      "step": 220
    },
    {
      "epoch": 0.0184,
      "grad_norm": 96.14568328857422,
      "learning_rate": 0.0001993866666666667,
      "loss": -108.9349,
      "step": 230
    },
    {
      "epoch": 0.0192,
      "grad_norm": 58.35210418701172,
      "learning_rate": 0.00019936000000000002,
      "loss": -110.3853,
      "step": 240
    },
    {
      "epoch": 0.02,
      "grad_norm": 63.979820251464844,
      "learning_rate": 0.00019933333333333334,
      "loss": -109.618,
      "step": 250
    },
    {
      "epoch": 0.0208,
      "grad_norm": 89.14130401611328,
      "learning_rate": 0.00019930666666666667,
      "loss": -110.2455,
      "step": 260
    },
    {
      "epoch": 0.0216,
      "grad_norm": 87.4437026977539,
      "learning_rate": 0.00019928,
      "loss": -111.7455,
      "step": 270
    },
    {
      "epoch": 0.0224,
      "grad_norm": 106.41680908203125,
      "learning_rate": 0.00019925333333333333,
      "loss": -110.5917,
      "step": 280
    },
    {
      "epoch": 0.0232,
      "grad_norm": 72.2378921508789,
      "learning_rate": 0.00019922666666666666,
      "loss": -110.4584,
      "step": 290
    },
    {
      "epoch": 0.024,
      "grad_norm": 90.60639953613281,
      "learning_rate": 0.00019920000000000002,
      "loss": -111.878,
      "step": 300
    },
    {
      "epoch": 0.0248,
      "grad_norm": 63.72541046142578,
      "learning_rate": 0.00019917333333333335,
      "loss": -110.814,
      "step": 310
    },
    {
      "epoch": 0.0256,
      "grad_norm": 112.77288818359375,
      "learning_rate": 0.00019914666666666668,
      "loss": -111.3397,
      "step": 320
    },
    {
      "epoch": 0.0264,
      "grad_norm": 91.1357650756836,
      "learning_rate": 0.00019912,
      "loss": -111.671,
      "step": 330
    },
    {
      "epoch": 0.0272,
      "grad_norm": 63.38945007324219,
      "learning_rate": 0.00019909333333333336,
      "loss": -112.2091,
      "step": 340
    },
    {
      "epoch": 0.028,
      "grad_norm": 59.81158447265625,
      "learning_rate": 0.00019906666666666666,
      "loss": -111.018,
      "step": 350
    },
    {
      "epoch": 0.0288,
      "grad_norm": 75.66832733154297,
      "learning_rate": 0.00019904,
      "loss": -112.6134,
      "step": 360
    },
    {
      "epoch": 0.0296,
      "grad_norm": 130.0281219482422,
      "learning_rate": 0.00019901333333333335,
      "loss": -112.6587,
      "step": 370
    },
    {
      "epoch": 0.0304,
      "grad_norm": 94.29589080810547,
      "learning_rate": 0.00019898666666666668,
      "loss": -111.5289,
      "step": 380
    },
    {
      "epoch": 0.0312,
      "grad_norm": 651.4603881835938,
      "learning_rate": 0.00019896,
      "loss": -112.362,
      "step": 390
    },
    {
      "epoch": 0.032,
      "grad_norm": 70.79876708984375,
      "learning_rate": 0.00019893333333333336,
      "loss": -112.3065,
      "step": 400
    },
    {
      "epoch": 0.0328,
      "grad_norm": 109.63276672363281,
      "learning_rate": 0.0001989066666666667,
      "loss": -112.4015,
      "step": 410
    },
    {
      "epoch": 0.0336,
      "grad_norm": 84.60466766357422,
      "learning_rate": 0.00019888,
      "loss": -112.1274,
      "step": 420
    },
    {
      "epoch": 0.0344,
      "grad_norm": 105.31868743896484,
      "learning_rate": 0.00019885333333333335,
      "loss": -113.687,
      "step": 430
    },
    {
      "epoch": 0.0352,
      "grad_norm": 59.35783767700195,
      "learning_rate": 0.00019882666666666668,
      "loss": -112.9827,
      "step": 440
    },
    {
      "epoch": 0.036,
      "grad_norm": 78.44389343261719,
      "learning_rate": 0.0001988,
      "loss": -113.8208,
      "step": 450
    },
    {
      "epoch": 0.0368,
      "grad_norm": 97.66646575927734,
      "learning_rate": 0.00019877333333333334,
      "loss": -114.2702,
      "step": 460
    },
    {
      "epoch": 0.0376,
      "grad_norm": 152.36741638183594,
      "learning_rate": 0.0001987466666666667,
      "loss": -111.9326,
      "step": 470
    },
    {
      "epoch": 0.0384,
      "grad_norm": 124.24365234375,
      "learning_rate": 0.00019872000000000002,
      "loss": -113.9961,
      "step": 480
    },
    {
      "epoch": 0.0392,
      "grad_norm": 90.356689453125,
      "learning_rate": 0.00019869333333333335,
      "loss": -114.2804,
      "step": 490
    },
    {
      "epoch": 0.04,
      "grad_norm": 77.0312271118164,
      "learning_rate": 0.00019866666666666668,
      "loss": -113.9764,
      "step": 500
    },
    {
      "epoch": 0.0408,
      "grad_norm": 69.40702056884766,
      "learning_rate": 0.00019864,
      "loss": -113.8124,
      "step": 510
    },
    {
      "epoch": 0.0416,
      "grad_norm": 87.39144134521484,
      "learning_rate": 0.00019861333333333334,
      "loss": -114.4427,
      "step": 520
    },
    {
      "epoch": 0.0424,
      "grad_norm": 62.119869232177734,
      "learning_rate": 0.00019858666666666667,
      "loss": -114.0859,
      "step": 530
    },
    {
      "epoch": 0.0432,
      "grad_norm": 77.51637268066406,
      "learning_rate": 0.00019856000000000002,
      "loss": -113.9016,
      "step": 540
    },
    {
      "epoch": 0.044,
      "grad_norm": 71.01506805419922,
      "learning_rate": 0.00019853333333333335,
      "loss": -115.0679,
      "step": 550
    },
    {
      "epoch": 0.0448,
      "grad_norm": 92.6697998046875,
      "learning_rate": 0.00019850666666666668,
      "loss": -113.6978,
      "step": 560
    },
    {
      "epoch": 0.0456,
      "grad_norm": 92.64653015136719,
      "learning_rate": 0.00019848,
      "loss": -115.451,
      "step": 570
    },
    {
      "epoch": 0.0464,
      "grad_norm": 104.21144104003906,
      "learning_rate": 0.00019845333333333334,
      "loss": -113.819,
      "step": 580
    },
    {
      "epoch": 0.0472,
      "grad_norm": 160.2574462890625,
      "learning_rate": 0.00019842666666666667,
      "loss": -114.2994,
      "step": 590
    },
    {
      "epoch": 0.048,
      "grad_norm": 77.76377868652344,
      "learning_rate": 0.0001984,
      "loss": -114.3774,
      "step": 600
    },
    {
      "epoch": 0.0488,
      "grad_norm": 80.45144653320312,
      "learning_rate": 0.00019837333333333335,
      "loss": -115.2262,
      "step": 610
    },
    {
      "epoch": 0.0496,
      "grad_norm": 127.11882019042969,
      "learning_rate": 0.00019834666666666668,
      "loss": -115.102,
      "step": 620
    },
    {
      "epoch": 0.0504,
      "grad_norm": 82.68360900878906,
      "learning_rate": 0.00019832,
      "loss": -114.2812,
      "step": 630
    },
    {
      "epoch": 0.0512,
      "grad_norm": 90.7738037109375,
      "learning_rate": 0.00019829333333333334,
      "loss": -114.5657,
      "step": 640
    },
    {
      "epoch": 0.052,
      "grad_norm": 100.32475280761719,
      "learning_rate": 0.00019826666666666667,
      "loss": -115.4703,
      "step": 650
    },
    {
      "epoch": 0.0528,
      "grad_norm": 94.75779724121094,
      "learning_rate": 0.00019824,
      "loss": -114.7218,
      "step": 660
    },
    {
      "epoch": 0.0536,
      "grad_norm": 85.77596282958984,
      "learning_rate": 0.00019821333333333333,
      "loss": -114.2901,
      "step": 670
    },
    {
      "epoch": 0.0544,
      "grad_norm": 81.40138244628906,
      "learning_rate": 0.00019818666666666668,
      "loss": -115.8699,
      "step": 680
    },
    {
      "epoch": 0.0552,
      "grad_norm": 126.37874603271484,
      "learning_rate": 0.00019816000000000001,
      "loss": -115.755,
      "step": 690
    },
    {
      "epoch": 0.056,
      "grad_norm": 146.80467224121094,
      "learning_rate": 0.00019813333333333334,
      "loss": -114.5479,
      "step": 700
    },
    {
      "epoch": 0.0568,
      "grad_norm": 90.47672271728516,
      "learning_rate": 0.00019810666666666667,
      "loss": -114.4874,
      "step": 710
    },
    {
      "epoch": 0.0576,
      "grad_norm": 178.48268127441406,
      "learning_rate": 0.00019808,
      "loss": -114.075,
      "step": 720
    },
    {
      "epoch": 0.0584,
      "grad_norm": 104.98505401611328,
      "learning_rate": 0.00019805333333333333,
      "loss": -115.8809,
      "step": 730
    },
    {
      "epoch": 0.0592,
      "grad_norm": 131.71875,
      "learning_rate": 0.00019802666666666666,
      "loss": -113.7449,
      "step": 740
    },
    {
      "epoch": 0.06,
      "grad_norm": 96.82987976074219,
      "learning_rate": 0.00019800000000000002,
      "loss": -114.128,
      "step": 750
    },
    {
      "epoch": 0.0608,
      "grad_norm": 136.55215454101562,
      "learning_rate": 0.00019797333333333334,
      "loss": -114.9512,
      "step": 760
    },
    {
      "epoch": 0.0616,
      "grad_norm": 78.0204086303711,
      "learning_rate": 0.00019794666666666667,
      "loss": -114.3488,
      "step": 770
    },
    {
      "epoch": 0.0624,
      "grad_norm": 79.91584014892578,
      "learning_rate": 0.00019792000000000003,
      "loss": -115.168,
      "step": 780
    },
    {
      "epoch": 0.0632,
      "grad_norm": 133.26724243164062,
      "learning_rate": 0.00019789333333333336,
      "loss": -115.1987,
      "step": 790
    },
    {
      "epoch": 0.064,
      "grad_norm": 118.00567626953125,
      "learning_rate": 0.00019786666666666666,
      "loss": -114.4543,
      "step": 800
    },
    {
      "epoch": 0.0648,
      "grad_norm": 162.2584991455078,
      "learning_rate": 0.00019784,
      "loss": -115.0464,
      "step": 810
    },
    {
      "epoch": 0.0656,
      "grad_norm": 154.41412353515625,
      "learning_rate": 0.00019781333333333335,
      "loss": -114.2008,
      "step": 820
    },
    {
      "epoch": 0.0664,
      "grad_norm": 93.78880310058594,
      "learning_rate": 0.00019778666666666667,
      "loss": -114.3747,
      "step": 830
    },
    {
      "epoch": 0.0672,
      "grad_norm": 185.42837524414062,
      "learning_rate": 0.00019776,
      "loss": -113.1197,
      "step": 840
    },
    {
      "epoch": 0.068,
      "grad_norm": 109.28009033203125,
      "learning_rate": 0.00019773333333333336,
      "loss": -114.077,
      "step": 850
    },
    {
      "epoch": 0.0688,
      "grad_norm": 81.7590560913086,
      "learning_rate": 0.0001977066666666667,
      "loss": -114.7538,
      "step": 860
    },
    {
      "epoch": 0.0696,
      "grad_norm": 103.29158782958984,
      "learning_rate": 0.00019768,
      "loss": -116.5384,
      "step": 870
    },
    {
      "epoch": 0.0704,
      "grad_norm": 95.2335205078125,
      "learning_rate": 0.00019765333333333335,
      "loss": -115.241,
      "step": 880
    },
    {
      "epoch": 0.0712,
      "grad_norm": 107.10286712646484,
      "learning_rate": 0.00019762666666666668,
      "loss": -114.4508,
      "step": 890
    },
    {
      "epoch": 0.072,
      "grad_norm": 95.78987121582031,
      "learning_rate": 0.0001976,
      "loss": -115.3888,
      "step": 900
    },
    {
      "epoch": 0.0728,
      "grad_norm": 101.52413940429688,
      "learning_rate": 0.00019757333333333333,
      "loss": -114.9042,
      "step": 910
    },
    {
      "epoch": 0.0736,
      "grad_norm": 93.3606948852539,
      "learning_rate": 0.0001975466666666667,
      "loss": -115.4693,
      "step": 920
    },
    {
      "epoch": 0.0744,
      "grad_norm": 77.70208740234375,
      "learning_rate": 0.00019752000000000002,
      "loss": -115.6664,
      "step": 930
    },
    {
      "epoch": 0.0752,
      "grad_norm": 129.92074584960938,
      "learning_rate": 0.00019749333333333335,
      "loss": -114.164,
      "step": 940
    },
    {
      "epoch": 0.076,
      "grad_norm": 129.43418884277344,
      "learning_rate": 0.00019746666666666668,
      "loss": -115.4093,
      "step": 950
    },
    {
      "epoch": 0.0768,
      "grad_norm": 161.49240112304688,
      "learning_rate": 0.00019744,
      "loss": -114.6987,
      "step": 960
    },
    {
      "epoch": 0.0776,
      "grad_norm": 95.78922271728516,
      "learning_rate": 0.00019741333333333334,
      "loss": -115.1326,
      "step": 970
    },
    {
      "epoch": 0.0784,
      "grad_norm": 137.501708984375,
      "learning_rate": 0.00019738666666666667,
      "loss": -117.0616,
      "step": 980
    },
    {
      "epoch": 0.0792,
      "grad_norm": 90.75619506835938,
      "learning_rate": 0.00019736000000000002,
      "loss": -114.6751,
      "step": 990
    },
    {
      "epoch": 0.08,
      "grad_norm": 99.48433685302734,
      "learning_rate": 0.00019733333333333335,
      "loss": -114.6571,
      "step": 1000
    },
    {
      "epoch": 0.0808,
      "grad_norm": 103.70498657226562,
      "learning_rate": 0.00019730666666666668,
      "loss": -114.7998,
      "step": 1010
    },
    {
      "epoch": 0.0816,
      "grad_norm": 115.91633605957031,
      "learning_rate": 0.00019728,
      "loss": -114.9305,
      "step": 1020
    },
    {
      "epoch": 0.0824,
      "grad_norm": 97.43905639648438,
      "learning_rate": 0.00019725333333333334,
      "loss": -114.9948,
      "step": 1030
    },
    {
      "epoch": 0.0832,
      "grad_norm": 98.72494506835938,
      "learning_rate": 0.00019722666666666667,
      "loss": -114.7399,
      "step": 1040
    },
    {
      "epoch": 0.084,
      "grad_norm": 100.03331756591797,
      "learning_rate": 0.0001972,
      "loss": -115.4478,
      "step": 1050
    },
    {
      "epoch": 0.0848,
      "grad_norm": 64.29741668701172,
      "learning_rate": 0.00019717333333333335,
      "loss": -114.6254,
      "step": 1060
    },
    {
      "epoch": 0.0856,
      "grad_norm": 100.44302368164062,
      "learning_rate": 0.00019714666666666668,
      "loss": -115.1388,
      "step": 1070
    },
    {
      "epoch": 0.0864,
      "grad_norm": 97.56311798095703,
      "learning_rate": 0.00019712,
      "loss": -116.1743,
      "step": 1080
    },
    {
      "epoch": 0.0872,
      "grad_norm": 112.4156265258789,
      "learning_rate": 0.00019709333333333334,
      "loss": -114.7417,
      "step": 1090
    },
    {
      "epoch": 0.088,
      "grad_norm": 78.50469970703125,
      "learning_rate": 0.00019706666666666667,
      "loss": -115.5503,
      "step": 1100
    },
    {
      "epoch": 0.0888,
      "grad_norm": 88.62971496582031,
      "learning_rate": 0.00019704,
      "loss": -116.8199,
      "step": 1110
    },
    {
      "epoch": 0.0896,
      "grad_norm": 82.18741607666016,
      "learning_rate": 0.00019701333333333333,
      "loss": -115.5558,
      "step": 1120
    },
    {
      "epoch": 0.0904,
      "grad_norm": 68.86174011230469,
      "learning_rate": 0.00019698666666666668,
      "loss": -116.2156,
      "step": 1130
    },
    {
      "epoch": 0.0912,
      "grad_norm": 78.0696029663086,
      "learning_rate": 0.00019696,
      "loss": -114.6772,
      "step": 1140
    },
    {
      "epoch": 0.092,
      "grad_norm": 124.79463958740234,
      "learning_rate": 0.00019693333333333334,
      "loss": -115.2518,
      "step": 1150
    },
    {
      "epoch": 0.0928,
      "grad_norm": 99.35363006591797,
      "learning_rate": 0.0001969066666666667,
      "loss": -117.0764,
      "step": 1160
    },
    {
      "epoch": 0.0936,
      "grad_norm": 109.3826675415039,
      "learning_rate": 0.00019688000000000003,
      "loss": -115.577,
      "step": 1170
    },
    {
      "epoch": 0.0944,
      "grad_norm": 134.3365936279297,
      "learning_rate": 0.00019685333333333333,
      "loss": -115.8222,
      "step": 1180
    },
    {
      "epoch": 0.0952,
      "grad_norm": 120.15205383300781,
      "learning_rate": 0.00019682666666666666,
      "loss": -116.0396,
      "step": 1190
    },
    {
      "epoch": 0.096,
      "grad_norm": 141.12403869628906,
      "learning_rate": 0.0001968,
      "loss": -116.0833,
      "step": 1200
    },
    {
      "epoch": 0.0968,
      "grad_norm": 115.08223724365234,
      "learning_rate": 0.00019677333333333334,
      "loss": -114.9823,
      "step": 1210
    },
    {
      "epoch": 0.0976,
      "grad_norm": 116.84838104248047,
      "learning_rate": 0.00019674666666666667,
      "loss": -114.8518,
      "step": 1220
    },
    {
      "epoch": 0.0984,
      "grad_norm": 108.42768859863281,
      "learning_rate": 0.00019672000000000003,
      "loss": -115.5387,
      "step": 1230
    },
    {
      "epoch": 0.0992,
      "grad_norm": 105.79828643798828,
      "learning_rate": 0.00019669333333333336,
      "loss": -116.0635,
      "step": 1240
    },
    {
      "epoch": 0.1,
      "grad_norm": 110.59992980957031,
      "learning_rate": 0.00019666666666666666,
      "loss": -115.2531,
      "step": 1250
    },
    {
      "epoch": 0.1008,
      "grad_norm": 154.4980926513672,
      "learning_rate": 0.00019664000000000001,
      "loss": -116.1368,
      "step": 1260
    },
    {
      "epoch": 0.1016,
      "grad_norm": 151.39822387695312,
      "learning_rate": 0.00019661333333333334,
      "loss": -115.4222,
      "step": 1270
    },
    {
      "epoch": 0.1024,
      "grad_norm": 121.9175033569336,
      "learning_rate": 0.00019658666666666667,
      "loss": -115.5649,
      "step": 1280
    },
    {
      "epoch": 0.1032,
      "grad_norm": 98.10501098632812,
      "learning_rate": 0.00019656,
      "loss": -116.006,
      "step": 1290
    },
    {
      "epoch": 0.104,
      "grad_norm": 111.61466217041016,
      "learning_rate": 0.00019653333333333336,
      "loss": -116.8658,
      "step": 1300
    },
    {
      "epoch": 0.1048,
      "grad_norm": 108.80281829833984,
      "learning_rate": 0.0001965066666666667,
      "loss": -116.3845,
      "step": 1310
    },
    {
      "epoch": 0.1056,
      "grad_norm": 99.57032775878906,
      "learning_rate": 0.00019648000000000002,
      "loss": -114.9959,
      "step": 1320
    },
    {
      "epoch": 0.1064,
      "grad_norm": 87.7214584350586,
      "learning_rate": 0.00019645333333333335,
      "loss": -116.0236,
      "step": 1330
    },
    {
      "epoch": 0.1072,
      "grad_norm": 98.60572052001953,
      "learning_rate": 0.00019642666666666667,
      "loss": -116.9043,
      "step": 1340
    },
    {
      "epoch": 0.108,
      "grad_norm": 70.14616394042969,
      "learning_rate": 0.0001964,
      "loss": -116.6236,
      "step": 1350
    },
    {
      "epoch": 0.1088,
      "grad_norm": 92.24253845214844,
      "learning_rate": 0.00019637333333333333,
      "loss": -116.6087,
      "step": 1360
    },
    {
      "epoch": 0.1096,
      "grad_norm": 71.57415771484375,
      "learning_rate": 0.0001963466666666667,
      "loss": -115.9293,
      "step": 1370
    },
    {
      "epoch": 0.1104,
      "grad_norm": 122.57061767578125,
      "learning_rate": 0.00019632000000000002,
      "loss": -116.9844,
      "step": 1380
    },
    {
      "epoch": 0.1112,
      "grad_norm": 81.31510925292969,
      "learning_rate": 0.00019629333333333335,
      "loss": -116.5014,
      "step": 1390
    },
    {
      "epoch": 0.112,
      "grad_norm": 71.13145446777344,
      "learning_rate": 0.00019626666666666668,
      "loss": -114.9878,
      "step": 1400
    },
    {
      "epoch": 0.1128,
      "grad_norm": 96.53685760498047,
      "learning_rate": 0.00019624,
      "loss": -116.9482,
      "step": 1410
    },
    {
      "epoch": 0.1136,
      "grad_norm": 88.7847900390625,
      "learning_rate": 0.00019621333333333333,
      "loss": -115.7338,
      "step": 1420
    },
    {
      "epoch": 0.1144,
      "grad_norm": 88.48904418945312,
      "learning_rate": 0.00019618666666666666,
      "loss": -116.507,
      "step": 1430
    },
    {
      "epoch": 0.1152,
      "grad_norm": 108.9176254272461,
      "learning_rate": 0.00019616000000000002,
      "loss": -116.542,
      "step": 1440
    },
    {
      "epoch": 0.116,
      "grad_norm": 94.55103302001953,
      "learning_rate": 0.00019613333333333335,
      "loss": -116.511,
      "step": 1450
    },
    {
      "epoch": 0.1168,
      "grad_norm": 117.89035034179688,
      "learning_rate": 0.00019610666666666668,
      "loss": -115.9994,
      "step": 1460
    },
    {
      "epoch": 0.1176,
      "grad_norm": 86.69703674316406,
      "learning_rate": 0.00019608,
      "loss": -116.4851,
      "step": 1470
    },
    {
      "epoch": 0.1184,
      "grad_norm": 87.14048767089844,
      "learning_rate": 0.00019605333333333334,
      "loss": -116.1789,
      "step": 1480
    },
    {
      "epoch": 0.1192,
      "grad_norm": 90.2332992553711,
      "learning_rate": 0.00019602666666666666,
      "loss": -115.4658,
      "step": 1490
    },
    {
      "epoch": 0.12,
      "grad_norm": 117.67054748535156,
      "learning_rate": 0.000196,
      "loss": -116.1913,
      "step": 1500
    },
    {
      "epoch": 0.1208,
      "grad_norm": 163.81149291992188,
      "learning_rate": 0.00019597333333333335,
      "loss": -117.0726,
      "step": 1510
    },
    {
      "epoch": 0.1216,
      "grad_norm": 134.6744842529297,
      "learning_rate": 0.00019594666666666668,
      "loss": -116.0074,
      "step": 1520
    },
    {
      "epoch": 0.1224,
      "grad_norm": 119.5706558227539,
      "learning_rate": 0.00019592,
      "loss": -115.8861,
      "step": 1530
    },
    {
      "epoch": 0.1232,
      "grad_norm": 142.79071044921875,
      "learning_rate": 0.00019589333333333336,
      "loss": -116.7827,
      "step": 1540
    },
    {
      "epoch": 0.124,
      "grad_norm": 105.36556243896484,
      "learning_rate": 0.00019586666666666667,
      "loss": -117.1157,
      "step": 1550
    },
    {
      "epoch": 0.1248,
      "grad_norm": 75.69544219970703,
      "learning_rate": 0.00019584,
      "loss": -114.6769,
      "step": 1560
    },
    {
      "epoch": 0.1256,
      "grad_norm": 95.05902862548828,
      "learning_rate": 0.00019581333333333332,
      "loss": -117.5647,
      "step": 1570
    },
    {
      "epoch": 0.1264,
      "grad_norm": 129.49606323242188,
      "learning_rate": 0.00019578666666666668,
      "loss": -116.6159,
      "step": 1580
    },
    {
      "epoch": 0.1272,
      "grad_norm": 111.02128601074219,
      "learning_rate": 0.00019576,
      "loss": -117.2219,
      "step": 1590
    },
    {
      "epoch": 0.128,
      "grad_norm": 122.70800018310547,
      "learning_rate": 0.00019573333333333334,
      "loss": -115.871,
      "step": 1600
    },
    {
      "epoch": 0.1288,
      "grad_norm": 85.11271667480469,
      "learning_rate": 0.0001957066666666667,
      "loss": -116.4287,
      "step": 1610
    },
    {
      "epoch": 0.1296,
      "grad_norm": 198.78150939941406,
      "learning_rate": 0.00019568000000000002,
      "loss": -116.5709,
      "step": 1620
    },
    {
      "epoch": 0.1304,
      "grad_norm": 117.51506042480469,
      "learning_rate": 0.00019565333333333333,
      "loss": -116.3797,
      "step": 1630
    },
    {
      "epoch": 0.1312,
      "grad_norm": 124.05500793457031,
      "learning_rate": 0.00019562666666666668,
      "loss": -115.9587,
      "step": 1640
    },
    {
      "epoch": 0.132,
      "grad_norm": 80.87190246582031,
      "learning_rate": 0.0001956,
      "loss": -115.9186,
      "step": 1650
    },
    {
      "epoch": 0.1328,
      "grad_norm": 84.8852310180664,
      "learning_rate": 0.00019557333333333334,
      "loss": -116.6733,
      "step": 1660
    },
    {
      "epoch": 0.1336,
      "grad_norm": 146.5045166015625,
      "learning_rate": 0.00019554666666666667,
      "loss": -114.6718,
      "step": 1670
    },
    {
      "epoch": 0.1344,
      "grad_norm": 68.75531005859375,
      "learning_rate": 0.00019552000000000003,
      "loss": -115.5139,
      "step": 1680
    },
    {
      "epoch": 0.1352,
      "grad_norm": 104.24674987792969,
      "learning_rate": 0.00019549333333333335,
      "loss": -115.1525,
      "step": 1690
    },
    {
      "epoch": 0.136,
      "grad_norm": 91.59434509277344,
      "learning_rate": 0.00019546666666666668,
      "loss": -116.5295,
      "step": 1700
    },
    {
      "epoch": 0.1368,
      "grad_norm": 155.1585693359375,
      "learning_rate": 0.00019544,
      "loss": -115.6815,
      "step": 1710
    },
    {
      "epoch": 0.1376,
      "grad_norm": 100.99142456054688,
      "learning_rate": 0.00019541333333333334,
      "loss": -116.4891,
      "step": 1720
    },
    {
      "epoch": 0.1384,
      "grad_norm": 120.42520141601562,
      "learning_rate": 0.00019538666666666667,
      "loss": -115.6239,
      "step": 1730
    },
    {
      "epoch": 0.1392,
      "grad_norm": 111.8515853881836,
      "learning_rate": 0.00019536,
      "loss": -116.2184,
      "step": 1740
    },
    {
      "epoch": 0.14,
      "grad_norm": 152.400390625,
      "learning_rate": 0.00019533333333333336,
      "loss": -116.7681,
      "step": 1750
    },
    {
      "epoch": 0.1408,
      "grad_norm": 92.07115936279297,
      "learning_rate": 0.00019530666666666669,
      "loss": -116.0309,
      "step": 1760
    },
    {
      "epoch": 0.1416,
      "grad_norm": 69.31975555419922,
      "learning_rate": 0.00019528000000000001,
      "loss": -115.8824,
      "step": 1770
    },
    {
      "epoch": 0.1424,
      "grad_norm": 81.69468688964844,
      "learning_rate": 0.00019525333333333334,
      "loss": -116.9399,
      "step": 1780
    },
    {
      "epoch": 0.1432,
      "grad_norm": 81.63911437988281,
      "learning_rate": 0.00019522666666666667,
      "loss": -117.6871,
      "step": 1790
    },
    {
      "epoch": 0.144,
      "grad_norm": 216.7011260986328,
      "learning_rate": 0.0001952,
      "loss": -116.508,
      "step": 1800
    },
    {
      "epoch": 0.1448,
      "grad_norm": 92.41412353515625,
      "learning_rate": 0.00019517333333333333,
      "loss": -116.3433,
      "step": 1810
    },
    {
      "epoch": 0.1456,
      "grad_norm": 100.36270904541016,
      "learning_rate": 0.0001951466666666667,
      "loss": -117.1633,
      "step": 1820
    },
    {
      "epoch": 0.1464,
      "grad_norm": 90.75164794921875,
      "learning_rate": 0.00019512000000000002,
      "loss": -116.9237,
      "step": 1830
    },
    {
      "epoch": 0.1472,
      "grad_norm": 104.2852783203125,
      "learning_rate": 0.00019509333333333334,
      "loss": -116.464,
      "step": 1840
    },
    {
      "epoch": 0.148,
      "grad_norm": 83.07512664794922,
      "learning_rate": 0.00019506666666666667,
      "loss": -115.2329,
      "step": 1850
    },
    {
      "epoch": 0.1488,
      "grad_norm": 128.6038818359375,
      "learning_rate": 0.00019504,
      "loss": -116.7028,
      "step": 1860
    },
    {
      "epoch": 0.1496,
      "grad_norm": 110.7585678100586,
      "learning_rate": 0.00019501333333333333,
      "loss": -117.946,
      "step": 1870
    },
    {
      "epoch": 0.1504,
      "grad_norm": 136.41221618652344,
      "learning_rate": 0.00019498666666666666,
      "loss": -115.3426,
      "step": 1880
    },
    {
      "epoch": 0.1512,
      "grad_norm": 79.72608184814453,
      "learning_rate": 0.00019496000000000002,
      "loss": -117.0876,
      "step": 1890
    },
    {
      "epoch": 0.152,
      "grad_norm": 122.6067123413086,
      "learning_rate": 0.00019493333333333335,
      "loss": -116.174,
      "step": 1900
    },
    {
      "epoch": 0.1528,
      "grad_norm": 111.66474151611328,
      "learning_rate": 0.00019490666666666668,
      "loss": -116.375,
      "step": 1910
    },
    {
      "epoch": 0.1536,
      "grad_norm": 123.6317367553711,
      "learning_rate": 0.00019488000000000003,
      "loss": -116.6466,
      "step": 1920
    },
    {
      "epoch": 0.1544,
      "grad_norm": 87.61900329589844,
      "learning_rate": 0.00019485333333333333,
      "loss": -117.6579,
      "step": 1930
    },
    {
      "epoch": 0.1552,
      "grad_norm": 82.45188903808594,
      "learning_rate": 0.00019482666666666666,
      "loss": -116.1819,
      "step": 1940
    },
    {
      "epoch": 0.156,
      "grad_norm": 103.1208267211914,
      "learning_rate": 0.0001948,
      "loss": -117.2946,
      "step": 1950
    },
    {
      "epoch": 0.1568,
      "grad_norm": 112.88037109375,
      "learning_rate": 0.00019477333333333335,
      "loss": -117.2511,
      "step": 1960
    },
    {
      "epoch": 0.1576,
      "grad_norm": 77.01392364501953,
      "learning_rate": 0.00019474666666666668,
      "loss": -116.9234,
      "step": 1970
    },
    {
      "epoch": 0.1584,
      "grad_norm": 74.04377746582031,
      "learning_rate": 0.00019472,
      "loss": -115.76,
      "step": 1980
    },
    {
      "epoch": 0.1592,
      "grad_norm": 73.6789321899414,
      "learning_rate": 0.00019469333333333336,
      "loss": -116.1822,
      "step": 1990
    },
    {
      "epoch": 0.16,
      "grad_norm": 94.78670501708984,
      "learning_rate": 0.0001946666666666667,
      "loss": -118.0996,
      "step": 2000
    },
    {
      "epoch": 0.1608,
      "grad_norm": 102.3202133178711,
      "learning_rate": 0.00019464,
      "loss": -116.683,
      "step": 2010
    },
    {
      "epoch": 0.1616,
      "grad_norm": 95.40208435058594,
      "learning_rate": 0.00019461333333333335,
      "loss": -115.7574,
      "step": 2020
    },
    {
      "epoch": 0.1624,
      "grad_norm": 100.09839630126953,
      "learning_rate": 0.00019458666666666668,
      "loss": -117.3755,
      "step": 2030
    },
    {
      "epoch": 0.1632,
      "grad_norm": 117.47175598144531,
      "learning_rate": 0.00019456,
      "loss": -116.6912,
      "step": 2040
    },
    {
      "epoch": 0.164,
      "grad_norm": 94.3270034790039,
      "learning_rate": 0.00019453333333333334,
      "loss": -116.4651,
      "step": 2050
    },
    {
      "epoch": 0.1648,
      "grad_norm": 103.73236846923828,
      "learning_rate": 0.0001945066666666667,
      "loss": -117.0672,
      "step": 2060
    },
    {
      "epoch": 0.1656,
      "grad_norm": 115.82574462890625,
      "learning_rate": 0.00019448000000000002,
      "loss": -117.8261,
      "step": 2070
    },
    {
      "epoch": 0.1664,
      "grad_norm": 75.38306427001953,
      "learning_rate": 0.00019445333333333332,
      "loss": -116.4981,
      "step": 2080
    },
    {
      "epoch": 0.1672,
      "grad_norm": 80.08241271972656,
      "learning_rate": 0.00019442666666666668,
      "loss": -116.2032,
      "step": 2090
    },
    {
      "epoch": 0.168,
      "grad_norm": 133.91014099121094,
      "learning_rate": 0.0001944,
      "loss": -117.7254,
      "step": 2100
    },
    {
      "epoch": 0.1688,
      "grad_norm": 95.9876480102539,
      "learning_rate": 0.00019437333333333334,
      "loss": -115.9967,
      "step": 2110
    },
    {
      "epoch": 0.1696,
      "grad_norm": 100.36714935302734,
      "learning_rate": 0.00019434666666666667,
      "loss": -117.9294,
      "step": 2120
    },
    {
      "epoch": 0.1704,
      "grad_norm": 106.18048858642578,
      "learning_rate": 0.00019432000000000002,
      "loss": -116.9587,
      "step": 2130
    },
    {
      "epoch": 0.1712,
      "grad_norm": 121.1848373413086,
      "learning_rate": 0.00019429333333333335,
      "loss": -115.8333,
      "step": 2140
    },
    {
      "epoch": 0.172,
      "grad_norm": 108.23075103759766,
      "learning_rate": 0.00019426666666666668,
      "loss": -118.0052,
      "step": 2150
    },
    {
      "epoch": 0.1728,
      "grad_norm": 95.86546325683594,
      "learning_rate": 0.00019424,
      "loss": -116.8358,
      "step": 2160
    },
    {
      "epoch": 0.1736,
      "grad_norm": 114.08641815185547,
      "learning_rate": 0.00019421333333333334,
      "loss": -116.7631,
      "step": 2170
    },
    {
      "epoch": 0.1744,
      "grad_norm": 66.86699676513672,
      "learning_rate": 0.00019418666666666667,
      "loss": -116.4661,
      "step": 2180
    },
    {
      "epoch": 0.1752,
      "grad_norm": 99.85564422607422,
      "learning_rate": 0.00019416,
      "loss": -116.3232,
      "step": 2190
    },
    {
      "epoch": 0.176,
      "grad_norm": 87.9434585571289,
      "learning_rate": 0.00019413333333333335,
      "loss": -117.6634,
      "step": 2200
    },
    {
      "epoch": 0.1768,
      "grad_norm": 110.09405517578125,
      "learning_rate": 0.00019410666666666668,
      "loss": -116.4121,
      "step": 2210
    },
    {
      "epoch": 0.1776,
      "grad_norm": 81.56073760986328,
      "learning_rate": 0.00019408,
      "loss": -116.647,
      "step": 2220
    },
    {
      "epoch": 0.1784,
      "grad_norm": 110.5086441040039,
      "learning_rate": 0.00019405333333333334,
      "loss": -116.3069,
      "step": 2230
    },
    {
      "epoch": 0.1792,
      "grad_norm": 96.45773315429688,
      "learning_rate": 0.00019402666666666667,
      "loss": -116.8556,
      "step": 2240
    },
    {
      "epoch": 0.18,
      "grad_norm": 115.26629638671875,
      "learning_rate": 0.000194,
      "loss": -115.8583,
      "step": 2250
    },
    {
      "epoch": 0.1808,
      "grad_norm": 111.51345825195312,
      "learning_rate": 0.00019397333333333333,
      "loss": -115.7597,
      "step": 2260
    },
    {
      "epoch": 0.1816,
      "grad_norm": 92.91079711914062,
      "learning_rate": 0.00019394666666666668,
      "loss": -116.8208,
      "step": 2270
    },
    {
      "epoch": 0.1824,
      "grad_norm": 81.81238555908203,
      "learning_rate": 0.00019392000000000001,
      "loss": -116.7149,
      "step": 2280
    },
    {
      "epoch": 0.1832,
      "grad_norm": 92.9188232421875,
      "learning_rate": 0.00019389333333333334,
      "loss": -116.3398,
      "step": 2290
    },
    {
      "epoch": 0.184,
      "grad_norm": 122.13356018066406,
      "learning_rate": 0.0001938666666666667,
      "loss": -115.5792,
      "step": 2300
    },
    {
      "epoch": 0.1848,
      "grad_norm": 113.79287719726562,
      "learning_rate": 0.00019384,
      "loss": -116.7411,
      "step": 2310
    },
    {
      "epoch": 0.1856,
      "grad_norm": 85.51744842529297,
      "learning_rate": 0.00019381333333333333,
      "loss": -116.8315,
      "step": 2320
    },
    {
      "epoch": 0.1864,
      "grad_norm": 110.90833282470703,
      "learning_rate": 0.00019378666666666666,
      "loss": -117.0648,
      "step": 2330
    },
    {
      "epoch": 0.1872,
      "grad_norm": 88.8200912475586,
      "learning_rate": 0.00019376000000000002,
      "loss": -116.8185,
      "step": 2340
    },
    {
      "epoch": 0.188,
      "grad_norm": 96.23922729492188,
      "learning_rate": 0.00019373333333333334,
      "loss": -116.912,
      "step": 2350
    },
    {
      "epoch": 0.1888,
      "grad_norm": 72.27816772460938,
      "learning_rate": 0.00019370666666666667,
      "loss": -117.1941,
      "step": 2360
    },
    {
      "epoch": 0.1896,
      "grad_norm": 95.32795715332031,
      "learning_rate": 0.00019368000000000003,
      "loss": -116.3282,
      "step": 2370
    },
    {
      "epoch": 0.1904,
      "grad_norm": 57.43355941772461,
      "learning_rate": 0.00019365333333333336,
      "loss": -117.7411,
      "step": 2380
    },
    {
      "epoch": 0.1912,
      "grad_norm": 112.93827056884766,
      "learning_rate": 0.00019362666666666666,
      "loss": -115.314,
      "step": 2390
    },
    {
      "epoch": 0.192,
      "grad_norm": 112.46293640136719,
      "learning_rate": 0.00019360000000000002,
      "loss": -117.9412,
      "step": 2400
    },
    {
      "epoch": 0.1928,
      "grad_norm": 117.99288177490234,
      "learning_rate": 0.00019357333333333335,
      "loss": -116.1822,
      "step": 2410
    },
    {
      "epoch": 0.1936,
      "grad_norm": 140.0335693359375,
      "learning_rate": 0.00019354666666666667,
      "loss": -116.8385,
      "step": 2420
    },
    {
      "epoch": 0.1944,
      "grad_norm": 96.07984161376953,
      "learning_rate": 0.00019352,
      "loss": -116.1797,
      "step": 2430
    },
    {
      "epoch": 0.1952,
      "grad_norm": 70.34224700927734,
      "learning_rate": 0.00019349333333333336,
      "loss": -117.2126,
      "step": 2440
    },
    {
      "epoch": 0.196,
      "grad_norm": 41.83427429199219,
      "learning_rate": 0.0001934666666666667,
      "loss": -116.769,
      "step": 2450
    },
    {
      "epoch": 0.1968,
      "grad_norm": 134.31002807617188,
      "learning_rate": 0.00019344,
      "loss": -117.0863,
      "step": 2460
    },
    {
      "epoch": 0.1976,
      "grad_norm": 73.14415740966797,
      "learning_rate": 0.00019341333333333335,
      "loss": -118.0746,
      "step": 2470
    },
    {
      "epoch": 0.1984,
      "grad_norm": 70.27688598632812,
      "learning_rate": 0.00019338666666666668,
      "loss": -116.6753,
      "step": 2480
    },
    {
      "epoch": 0.1992,
      "grad_norm": 84.46620178222656,
      "learning_rate": 0.00019336,
      "loss": -117.0398,
      "step": 2490
    },
    {
      "epoch": 0.2,
      "grad_norm": 90.57164764404297,
      "learning_rate": 0.00019333333333333333,
      "loss": -116.4089,
      "step": 2500
    },
    {
      "epoch": 0.2008,
      "grad_norm": 95.27186584472656,
      "learning_rate": 0.0001933066666666667,
      "loss": -116.9419,
      "step": 2510
    },
    {
      "epoch": 0.2016,
      "grad_norm": 105.79436492919922,
      "learning_rate": 0.00019328000000000002,
      "loss": -114.8314,
      "step": 2520
    },
    {
      "epoch": 0.2024,
      "grad_norm": 76.89646911621094,
      "learning_rate": 0.00019325333333333335,
      "loss": -116.8276,
      "step": 2530
    },
    {
      "epoch": 0.2032,
      "grad_norm": 101.41246032714844,
      "learning_rate": 0.00019322666666666668,
      "loss": -117.0191,
      "step": 2540
    },
    {
      "epoch": 0.204,
      "grad_norm": 66.12213897705078,
      "learning_rate": 0.0001932,
      "loss": -117.2247,
      "step": 2550
    },
    {
      "epoch": 0.2048,
      "grad_norm": 98.54371643066406,
      "learning_rate": 0.00019317333333333334,
      "loss": -117.6077,
      "step": 2560
    },
    {
      "epoch": 0.2056,
      "grad_norm": 107.4691390991211,
      "learning_rate": 0.00019314666666666667,
      "loss": -115.3108,
      "step": 2570
    },
    {
      "epoch": 0.2064,
      "grad_norm": 114.70890808105469,
      "learning_rate": 0.00019312000000000002,
      "loss": -116.9863,
      "step": 2580
    },
    {
      "epoch": 0.2072,
      "grad_norm": 135.0662078857422,
      "learning_rate": 0.00019309333333333335,
      "loss": -115.9663,
      "step": 2590
    },
    {
      "epoch": 0.208,
      "grad_norm": 98.53016662597656,
      "learning_rate": 0.00019306666666666668,
      "loss": -117.3134,
      "step": 2600
    },
    {
      "epoch": 0.2088,
      "grad_norm": 78.45132446289062,
      "learning_rate": 0.00019304,
      "loss": -117.065,
      "step": 2610
    },
    {
      "epoch": 0.2096,
      "grad_norm": 92.63209533691406,
      "learning_rate": 0.00019301333333333334,
      "loss": -117.3034,
      "step": 2620
    },
    {
      "epoch": 0.2104,
      "grad_norm": 79.07618713378906,
      "learning_rate": 0.00019298666666666667,
      "loss": -116.2444,
      "step": 2630
    },
    {
      "epoch": 0.2112,
      "grad_norm": 131.80841064453125,
      "learning_rate": 0.00019296,
      "loss": -116.0583,
      "step": 2640
    },
    {
      "epoch": 0.212,
      "grad_norm": 83.16053009033203,
      "learning_rate": 0.00019293333333333335,
      "loss": -116.8296,
      "step": 2650
    },
    {
      "epoch": 0.2128,
      "grad_norm": 90.58116149902344,
      "learning_rate": 0.00019290666666666668,
      "loss": -116.5885,
      "step": 2660
    },
    {
      "epoch": 0.2136,
      "grad_norm": 101.73612976074219,
      "learning_rate": 0.00019288,
      "loss": -117.2519,
      "step": 2670
    },
    {
      "epoch": 0.2144,
      "grad_norm": 63.02219772338867,
      "learning_rate": 0.00019285333333333334,
      "loss": -117.4051,
      "step": 2680
    },
    {
      "epoch": 0.2152,
      "grad_norm": 79.68089294433594,
      "learning_rate": 0.00019282666666666667,
      "loss": -116.7162,
      "step": 2690
    },
    {
      "epoch": 0.216,
      "grad_norm": 114.16490936279297,
      "learning_rate": 0.0001928,
      "loss": -116.7875,
      "step": 2700
    },
    {
      "epoch": 0.2168,
      "grad_norm": 67.18409729003906,
      "learning_rate": 0.00019277333333333333,
      "loss": -115.961,
      "step": 2710
    },
    {
      "epoch": 0.2176,
      "grad_norm": 99.48344421386719,
      "learning_rate": 0.00019274666666666668,
      "loss": -118.0206,
      "step": 2720
    },
    {
      "epoch": 0.2184,
      "grad_norm": 82.21388244628906,
      "learning_rate": 0.00019272,
      "loss": -116.6992,
      "step": 2730
    },
    {
      "epoch": 0.2192,
      "grad_norm": 90.50921630859375,
      "learning_rate": 0.00019269333333333334,
      "loss": -115.4256,
      "step": 2740
    },
    {
      "epoch": 0.22,
      "grad_norm": 91.21497344970703,
      "learning_rate": 0.0001926666666666667,
      "loss": -117.903,
      "step": 2750
    },
    {
      "epoch": 0.2208,
      "grad_norm": 82.18656921386719,
      "learning_rate": 0.00019264,
      "loss": -116.5947,
      "step": 2760
    },
    {
      "epoch": 0.2216,
      "grad_norm": 78.10005187988281,
      "learning_rate": 0.00019261333333333333,
      "loss": -117.01,
      "step": 2770
    },
    {
      "epoch": 0.2224,
      "grad_norm": 124.70529174804688,
      "learning_rate": 0.00019258666666666668,
      "loss": -117.577,
      "step": 2780
    },
    {
      "epoch": 0.2232,
      "grad_norm": 62.79027557373047,
      "learning_rate": 0.00019256,
      "loss": -117.6779,
      "step": 2790
    },
    {
      "epoch": 0.224,
      "grad_norm": 84.61380767822266,
      "learning_rate": 0.00019253333333333334,
      "loss": -117.1526,
      "step": 2800
    },
    {
      "epoch": 0.2248,
      "grad_norm": 135.67636108398438,
      "learning_rate": 0.00019250666666666667,
      "loss": -117.923,
      "step": 2810
    },
    {
      "epoch": 0.2256,
      "grad_norm": 97.17060089111328,
      "learning_rate": 0.00019248000000000003,
      "loss": -117.3728,
      "step": 2820
    },
    {
      "epoch": 0.2264,
      "grad_norm": 73.30915069580078,
      "learning_rate": 0.00019245333333333336,
      "loss": -117.3215,
      "step": 2830
    },
    {
      "epoch": 0.2272,
      "grad_norm": 64.7952880859375,
      "learning_rate": 0.00019242666666666666,
      "loss": -116.3177,
      "step": 2840
    },
    {
      "epoch": 0.228,
      "grad_norm": 103.53379821777344,
      "learning_rate": 0.00019240000000000001,
      "loss": -116.813,
      "step": 2850
    },
    {
      "epoch": 0.2288,
      "grad_norm": 86.75952911376953,
      "learning_rate": 0.00019237333333333334,
      "loss": -117.1092,
      "step": 2860
    },
    {
      "epoch": 0.2296,
      "grad_norm": 83.12065887451172,
      "learning_rate": 0.00019234666666666667,
      "loss": -115.701,
      "step": 2870
    },
    {
      "epoch": 0.2304,
      "grad_norm": 104.2449722290039,
      "learning_rate": 0.00019232,
      "loss": -117.5413,
      "step": 2880
    },
    {
      "epoch": 0.2312,
      "grad_norm": 136.86949157714844,
      "learning_rate": 0.00019229333333333336,
      "loss": -117.9015,
      "step": 2890
    },
    {
      "epoch": 0.232,
      "grad_norm": 84.23482513427734,
      "learning_rate": 0.0001922666666666667,
      "loss": -116.8577,
      "step": 2900
    },
    {
      "epoch": 0.2328,
      "grad_norm": 69.14227294921875,
      "learning_rate": 0.00019224000000000002,
      "loss": -116.9006,
      "step": 2910
    },
    {
      "epoch": 0.2336,
      "grad_norm": 118.1960220336914,
      "learning_rate": 0.00019221333333333335,
      "loss": -117.615,
      "step": 2920
    },
    {
      "epoch": 0.2344,
      "grad_norm": 93.56986236572266,
      "learning_rate": 0.00019218666666666667,
      "loss": -117.0039,
      "step": 2930
    },
    {
      "epoch": 0.2352,
      "grad_norm": 114.42630767822266,
      "learning_rate": 0.00019216,
      "loss": -116.9503,
      "step": 2940
    },
    {
      "epoch": 0.236,
      "grad_norm": 95.061279296875,
      "learning_rate": 0.00019213333333333333,
      "loss": -117.6494,
      "step": 2950
    },
    {
      "epoch": 0.2368,
      "grad_norm": 88.93921661376953,
      "learning_rate": 0.0001921066666666667,
      "loss": -117.2059,
      "step": 2960
    },
    {
      "epoch": 0.2376,
      "grad_norm": 72.67536163330078,
      "learning_rate": 0.00019208000000000002,
      "loss": -117.2899,
      "step": 2970
    },
    {
      "epoch": 0.2384,
      "grad_norm": 77.17446899414062,
      "learning_rate": 0.00019205333333333335,
      "loss": -117.7868,
      "step": 2980
    },
    {
      "epoch": 0.2392,
      "grad_norm": 82.28032684326172,
      "learning_rate": 0.00019202666666666668,
      "loss": -116.3687,
      "step": 2990
    },
    {
      "epoch": 0.24,
      "grad_norm": 114.46782684326172,
      "learning_rate": 0.000192,
      "loss": -117.6017,
      "step": 3000
    },
    {
      "epoch": 0.2408,
      "grad_norm": 77.67455291748047,
      "learning_rate": 0.00019197333333333333,
      "loss": -118.2281,
      "step": 3010
    },
    {
      "epoch": 0.2416,
      "grad_norm": 93.7262954711914,
      "learning_rate": 0.00019194666666666666,
      "loss": -116.5188,
      "step": 3020
    },
    {
      "epoch": 0.2424,
      "grad_norm": 133.79672241210938,
      "learning_rate": 0.00019192000000000002,
      "loss": -118.094,
      "step": 3030
    },
    {
      "epoch": 0.2432,
      "grad_norm": 98.30145263671875,
      "learning_rate": 0.00019189333333333335,
      "loss": -116.5864,
      "step": 3040
    },
    {
      "epoch": 0.244,
      "grad_norm": 90.44749450683594,
      "learning_rate": 0.00019186666666666668,
      "loss": -117.3334,
      "step": 3050
    },
    {
      "epoch": 0.2448,
      "grad_norm": 66.46038055419922,
      "learning_rate": 0.00019184,
      "loss": -116.9456,
      "step": 3060
    },
    {
      "epoch": 0.2456,
      "grad_norm": 83.99249267578125,
      "learning_rate": 0.00019181333333333334,
      "loss": -116.6775,
      "step": 3070
    },
    {
      "epoch": 0.2464,
      "grad_norm": 64.46127319335938,
      "learning_rate": 0.00019178666666666666,
      "loss": -117.0517,
      "step": 3080
    },
    {
      "epoch": 0.2472,
      "grad_norm": 80.6865463256836,
      "learning_rate": 0.00019176,
      "loss": -117.7743,
      "step": 3090
    },
    {
      "epoch": 0.248,
      "grad_norm": 64.85520935058594,
      "learning_rate": 0.00019173333333333335,
      "loss": -117.103,
      "step": 3100
    },
    {
      "epoch": 0.2488,
      "grad_norm": 78.61631774902344,
      "learning_rate": 0.00019170666666666668,
      "loss": -116.466,
      "step": 3110
    },
    {
      "epoch": 0.2496,
      "grad_norm": 59.374820709228516,
      "learning_rate": 0.00019168,
      "loss": -116.4758,
      "step": 3120
    },
    {
      "epoch": 0.2504,
      "grad_norm": 97.58914184570312,
      "learning_rate": 0.00019165333333333336,
      "loss": -117.4509,
      "step": 3130
    },
    {
      "epoch": 0.2512,
      "grad_norm": 135.64537048339844,
      "learning_rate": 0.00019162666666666667,
      "loss": -116.115,
      "step": 3140
    },
    {
      "epoch": 0.252,
      "grad_norm": 63.67714309692383,
      "learning_rate": 0.0001916,
      "loss": -116.8232,
      "step": 3150
    },
    {
      "epoch": 0.2528,
      "grad_norm": 49.535396575927734,
      "learning_rate": 0.00019157333333333335,
      "loss": -116.654,
      "step": 3160
    },
    {
      "epoch": 0.2536,
      "grad_norm": 96.02322387695312,
      "learning_rate": 0.00019154666666666668,
      "loss": -117.17,
      "step": 3170
    },
    {
      "epoch": 0.2544,
      "grad_norm": 89.24690246582031,
      "learning_rate": 0.00019152,
      "loss": -117.594,
      "step": 3180
    },
    {
      "epoch": 0.2552,
      "grad_norm": 84.67578125,
      "learning_rate": 0.00019149333333333334,
      "loss": -116.8177,
      "step": 3190
    },
    {
      "epoch": 0.256,
      "grad_norm": 66.39517974853516,
      "learning_rate": 0.0001914666666666667,
      "loss": -117.5244,
      "step": 3200
    },
    {
      "epoch": 0.2568,
      "grad_norm": 94.3587417602539,
      "learning_rate": 0.00019144000000000002,
      "loss": -116.7972,
      "step": 3210
    },
    {
      "epoch": 0.2576,
      "grad_norm": 104.29000091552734,
      "learning_rate": 0.00019141333333333333,
      "loss": -117.5728,
      "step": 3220
    },
    {
      "epoch": 0.2584,
      "grad_norm": 68.11003112792969,
      "learning_rate": 0.00019138666666666668,
      "loss": -116.6683,
      "step": 3230
    },
    {
      "epoch": 0.2592,
      "grad_norm": 105.76512908935547,
      "learning_rate": 0.00019136,
      "loss": -116.5012,
      "step": 3240
    },
    {
      "epoch": 0.26,
      "grad_norm": 104.28616333007812,
      "learning_rate": 0.00019133333333333334,
      "loss": -117.4536,
      "step": 3250
    },
    {
      "epoch": 0.2608,
      "grad_norm": 98.94849395751953,
      "learning_rate": 0.00019130666666666667,
      "loss": -117.4869,
      "step": 3260
    },
    {
      "epoch": 0.2616,
      "grad_norm": 107.40431213378906,
      "learning_rate": 0.00019128000000000003,
      "loss": -116.3212,
      "step": 3270
    },
    {
      "epoch": 0.2624,
      "grad_norm": 91.6134033203125,
      "learning_rate": 0.00019125333333333335,
      "loss": -118.3244,
      "step": 3280
    },
    {
      "epoch": 0.2632,
      "grad_norm": 56.32924270629883,
      "learning_rate": 0.00019122666666666666,
      "loss": -117.6362,
      "step": 3290
    },
    {
      "epoch": 0.264,
      "grad_norm": 74.73661041259766,
      "learning_rate": 0.0001912,
      "loss": -117.5714,
      "step": 3300
    },
    {
      "epoch": 0.2648,
      "grad_norm": 89.59890747070312,
      "learning_rate": 0.00019117333333333334,
      "loss": -118.0859,
      "step": 3310
    },
    {
      "epoch": 0.2656,
      "grad_norm": 58.61537551879883,
      "learning_rate": 0.00019114666666666667,
      "loss": -117.8918,
      "step": 3320
    },
    {
      "epoch": 0.2664,
      "grad_norm": 63.01579284667969,
      "learning_rate": 0.00019112,
      "loss": -117.6317,
      "step": 3330
    },
    {
      "epoch": 0.2672,
      "grad_norm": 54.17140579223633,
      "learning_rate": 0.00019109333333333336,
      "loss": -117.0268,
      "step": 3340
    },
    {
      "epoch": 0.268,
      "grad_norm": 86.9528579711914,
      "learning_rate": 0.00019106666666666668,
      "loss": -117.4514,
      "step": 3350
    },
    {
      "epoch": 0.2688,
      "grad_norm": 91.69364166259766,
      "learning_rate": 0.00019104000000000001,
      "loss": -117.0095,
      "step": 3360
    },
    {
      "epoch": 0.2696,
      "grad_norm": 141.9312286376953,
      "learning_rate": 0.00019101333333333334,
      "loss": -117.9608,
      "step": 3370
    },
    {
      "epoch": 0.2704,
      "grad_norm": 96.09416961669922,
      "learning_rate": 0.00019098666666666667,
      "loss": -117.7899,
      "step": 3380
    },
    {
      "epoch": 0.2712,
      "grad_norm": 59.9031867980957,
      "learning_rate": 0.00019096,
      "loss": -117.7108,
      "step": 3390
    },
    {
      "epoch": 0.272,
      "grad_norm": 68.49246978759766,
      "learning_rate": 0.00019093333333333333,
      "loss": -116.4381,
      "step": 3400
    },
    {
      "epoch": 0.2728,
      "grad_norm": 82.70870971679688,
      "learning_rate": 0.0001909066666666667,
      "loss": -115.9575,
      "step": 3410
    },
    {
      "epoch": 0.2736,
      "grad_norm": 84.50139617919922,
      "learning_rate": 0.00019088000000000002,
      "loss": -118.2018,
      "step": 3420
    },
    {
      "epoch": 0.2744,
      "grad_norm": 108.35196685791016,
      "learning_rate": 0.00019085333333333334,
      "loss": -117.5989,
      "step": 3430
    },
    {
      "epoch": 0.2752,
      "grad_norm": 89.82722473144531,
      "learning_rate": 0.00019082666666666667,
      "loss": -117.3292,
      "step": 3440
    },
    {
      "epoch": 0.276,
      "grad_norm": 108.0348129272461,
      "learning_rate": 0.0001908,
      "loss": -116.8116,
      "step": 3450
    },
    {
      "epoch": 0.2768,
      "grad_norm": 87.86354064941406,
      "learning_rate": 0.00019077333333333333,
      "loss": -117.4632,
      "step": 3460
    },
    {
      "epoch": 0.2776,
      "grad_norm": 82.20124053955078,
      "learning_rate": 0.00019074666666666666,
      "loss": -117.7412,
      "step": 3470
    },
    {
      "epoch": 0.2784,
      "grad_norm": 95.33695220947266,
      "learning_rate": 0.00019072000000000002,
      "loss": -116.692,
      "step": 3480
    },
    {
      "epoch": 0.2792,
      "grad_norm": 80.60599517822266,
      "learning_rate": 0.00019069333333333335,
      "loss": -117.069,
      "step": 3490
    },
    {
      "epoch": 0.28,
      "grad_norm": 101.62773132324219,
      "learning_rate": 0.00019066666666666668,
      "loss": -116.7947,
      "step": 3500
    },
    {
      "epoch": 0.2808,
      "grad_norm": 108.25862121582031,
      "learning_rate": 0.00019064000000000003,
      "loss": -116.9195,
      "step": 3510
    },
    {
      "epoch": 0.2816,
      "grad_norm": 91.21430206298828,
      "learning_rate": 0.00019061333333333333,
      "loss": -116.1421,
      "step": 3520
    },
    {
      "epoch": 0.2824,
      "grad_norm": 91.51296997070312,
      "learning_rate": 0.00019058666666666666,
      "loss": -116.8669,
      "step": 3530
    },
    {
      "epoch": 0.2832,
      "grad_norm": 90.5048828125,
      "learning_rate": 0.00019056000000000002,
      "loss": -118.335,
      "step": 3540
    },
    {
      "epoch": 0.284,
      "grad_norm": 74.15998077392578,
      "learning_rate": 0.00019053333333333335,
      "loss": -117.7986,
      "step": 3550
    },
    {
      "epoch": 0.2848,
      "grad_norm": 65.29568481445312,
      "learning_rate": 0.00019050666666666668,
      "loss": -117.5826,
      "step": 3560
    },
    {
      "epoch": 0.2856,
      "grad_norm": 86.4378433227539,
      "learning_rate": 0.00019048,
      "loss": -117.0385,
      "step": 3570
    },
    {
      "epoch": 0.2864,
      "grad_norm": 97.05860900878906,
      "learning_rate": 0.00019045333333333336,
      "loss": -118.2543,
      "step": 3580
    },
    {
      "epoch": 0.2872,
      "grad_norm": 80.80037689208984,
      "learning_rate": 0.0001904266666666667,
      "loss": -118.2416,
      "step": 3590
    },
    {
      "epoch": 0.288,
      "grad_norm": 62.77082443237305,
      "learning_rate": 0.0001904,
      "loss": -116.7474,
      "step": 3600
    },
    {
      "epoch": 0.2888,
      "grad_norm": 77.68909454345703,
      "learning_rate": 0.00019037333333333335,
      "loss": -118.114,
      "step": 3610
    },
    {
      "epoch": 0.2896,
      "grad_norm": 76.13875579833984,
      "learning_rate": 0.00019034666666666668,
      "loss": -117.0027,
      "step": 3620
    },
    {
      "epoch": 0.2904,
      "grad_norm": 81.9204330444336,
      "learning_rate": 0.00019032,
      "loss": -116.2253,
      "step": 3630
    },
    {
      "epoch": 0.2912,
      "grad_norm": 74.04994201660156,
      "learning_rate": 0.00019029333333333334,
      "loss": -117.8914,
      "step": 3640
    },
    {
      "epoch": 0.292,
      "grad_norm": 60.830421447753906,
      "learning_rate": 0.0001902666666666667,
      "loss": -116.3869,
      "step": 3650
    },
    {
      "epoch": 0.2928,
      "grad_norm": 89.9209213256836,
      "learning_rate": 0.00019024000000000002,
      "loss": -118.2393,
      "step": 3660
    },
    {
      "epoch": 0.2936,
      "grad_norm": 78.48104858398438,
      "learning_rate": 0.00019021333333333332,
      "loss": -117.9432,
      "step": 3670
    },
    {
      "epoch": 0.2944,
      "grad_norm": 91.11188507080078,
      "learning_rate": 0.00019018666666666668,
      "loss": -118.4355,
      "step": 3680
    },
    {
      "epoch": 0.2952,
      "grad_norm": 101.74385833740234,
      "learning_rate": 0.00019016,
      "loss": -116.879,
      "step": 3690
    },
    {
      "epoch": 0.296,
      "grad_norm": 59.35485076904297,
      "learning_rate": 0.00019013333333333334,
      "loss": -117.561,
      "step": 3700
    },
    {
      "epoch": 0.2968,
      "grad_norm": 102.1507797241211,
      "learning_rate": 0.00019010666666666667,
      "loss": -117.1112,
      "step": 3710
    },
    {
      "epoch": 0.2976,
      "grad_norm": 91.93021392822266,
      "learning_rate": 0.00019008000000000002,
      "loss": -117.3757,
      "step": 3720
    },
    {
      "epoch": 0.2984,
      "grad_norm": 94.06929016113281,
      "learning_rate": 0.00019005333333333335,
      "loss": -117.2906,
      "step": 3730
    },
    {
      "epoch": 0.2992,
      "grad_norm": 90.54251861572266,
      "learning_rate": 0.00019002666666666668,
      "loss": -117.5997,
      "step": 3740
    },
    {
      "epoch": 0.3,
      "grad_norm": 80.07730865478516,
      "learning_rate": 0.00019,
      "loss": -117.117,
      "step": 3750
    },
    {
      "epoch": 0.3008,
      "grad_norm": 90.19027709960938,
      "learning_rate": 0.00018997333333333334,
      "loss": -117.6849,
      "step": 3760
    },
    {
      "epoch": 0.3016,
      "grad_norm": 100.54865264892578,
      "learning_rate": 0.00018994666666666667,
      "loss": -116.2589,
      "step": 3770
    },
    {
      "epoch": 0.3024,
      "grad_norm": 76.18262481689453,
      "learning_rate": 0.00018992,
      "loss": -115.5237,
      "step": 3780
    },
    {
      "epoch": 0.3032,
      "grad_norm": 50.74720001220703,
      "learning_rate": 0.00018989333333333335,
      "loss": -117.5807,
      "step": 3790
    },
    {
      "epoch": 0.304,
      "grad_norm": 65.12234497070312,
      "learning_rate": 0.00018986666666666668,
      "loss": -118.6389,
      "step": 3800
    },
    {
      "epoch": 0.3048,
      "grad_norm": 70.130615234375,
      "learning_rate": 0.00018984,
      "loss": -117.7885,
      "step": 3810
    },
    {
      "epoch": 0.3056,
      "grad_norm": 91.68362426757812,
      "learning_rate": 0.00018981333333333334,
      "loss": -118.3332,
      "step": 3820
    },
    {
      "epoch": 0.3064,
      "grad_norm": 72.5172348022461,
      "learning_rate": 0.00018978666666666667,
      "loss": -118.2262,
      "step": 3830
    },
    {
      "epoch": 0.3072,
      "grad_norm": 90.45062255859375,
      "learning_rate": 0.00018976,
      "loss": -117.2808,
      "step": 3840
    },
    {
      "epoch": 0.308,
      "grad_norm": 78.4048080444336,
      "learning_rate": 0.00018973333333333333,
      "loss": -118.6012,
      "step": 3850
    },
    {
      "epoch": 0.3088,
      "grad_norm": 84.51206970214844,
      "learning_rate": 0.00018970666666666668,
      "loss": -117.4165,
      "step": 3860
    },
    {
      "epoch": 0.3096,
      "grad_norm": 75.72750091552734,
      "learning_rate": 0.00018968,
      "loss": -115.9967,
      "step": 3870
    },
    {
      "epoch": 0.3104,
      "grad_norm": 101.60262298583984,
      "learning_rate": 0.00018965333333333334,
      "loss": -117.82,
      "step": 3880
    },
    {
      "epoch": 0.3112,
      "grad_norm": 48.47683334350586,
      "learning_rate": 0.0001896266666666667,
      "loss": -116.8025,
      "step": 3890
    },
    {
      "epoch": 0.312,
      "grad_norm": 59.449317932128906,
      "learning_rate": 0.0001896,
      "loss": -117.2565,
      "step": 3900
    },
    {
      "epoch": 0.3128,
      "grad_norm": 57.14265060424805,
      "learning_rate": 0.00018957333333333333,
      "loss": -117.9141,
      "step": 3910
    },
    {
      "epoch": 0.3136,
      "grad_norm": 54.19742965698242,
      "learning_rate": 0.00018954666666666666,
      "loss": -115.9009,
      "step": 3920
    },
    {
      "epoch": 0.3144,
      "grad_norm": 101.91368103027344,
      "learning_rate": 0.00018952000000000002,
      "loss": -117.9766,
      "step": 3930
    },
    {
      "epoch": 0.3152,
      "grad_norm": 87.10079956054688,
      "learning_rate": 0.00018949333333333334,
      "loss": -118.4094,
      "step": 3940
    },
    {
      "epoch": 0.316,
      "grad_norm": 69.60458374023438,
      "learning_rate": 0.00018946666666666667,
      "loss": -116.8932,
      "step": 3950
    },
    {
      "epoch": 0.3168,
      "grad_norm": 77.1846923828125,
      "learning_rate": 0.00018944000000000003,
      "loss": -116.9866,
      "step": 3960
    },
    {
      "epoch": 0.3176,
      "grad_norm": 55.113372802734375,
      "learning_rate": 0.00018941333333333333,
      "loss": -117.105,
      "step": 3970
    },
    {
      "epoch": 0.3184,
      "grad_norm": 75.24275207519531,
      "learning_rate": 0.00018938666666666666,
      "loss": -117.9927,
      "step": 3980
    },
    {
      "epoch": 0.3192,
      "grad_norm": 79.59107208251953,
      "learning_rate": 0.00018936000000000002,
      "loss": -116.6788,
      "step": 3990
    },
    {
      "epoch": 0.32,
      "grad_norm": 77.52447509765625,
      "learning_rate": 0.00018933333333333335,
      "loss": -116.7331,
      "step": 4000
    },
    {
      "epoch": 0.3208,
      "grad_norm": 66.54534912109375,
      "learning_rate": 0.00018930666666666667,
      "loss": -117.8879,
      "step": 4010
    },
    {
      "epoch": 0.3216,
      "grad_norm": 61.32707214355469,
      "learning_rate": 0.00018928,
      "loss": -117.8043,
      "step": 4020
    },
    {
      "epoch": 0.3224,
      "grad_norm": 58.39420700073242,
      "learning_rate": 0.00018925333333333336,
      "loss": -117.9269,
      "step": 4030
    },
    {
      "epoch": 0.3232,
      "grad_norm": 72.73858642578125,
      "learning_rate": 0.0001892266666666667,
      "loss": -117.2198,
      "step": 4040
    },
    {
      "epoch": 0.324,
      "grad_norm": 64.34188079833984,
      "learning_rate": 0.0001892,
      "loss": -118.7229,
      "step": 4050
    },
    {
      "epoch": 0.3248,
      "grad_norm": 73.39701843261719,
      "learning_rate": 0.00018917333333333335,
      "loss": -117.2468,
      "step": 4060
    },
    {
      "epoch": 0.3256,
      "grad_norm": 58.50059509277344,
      "learning_rate": 0.00018914666666666668,
      "loss": -118.2619,
      "step": 4070
    },
    {
      "epoch": 0.3264,
      "grad_norm": 68.39834594726562,
      "learning_rate": 0.00018912,
      "loss": -116.2637,
      "step": 4080
    },
    {
      "epoch": 0.3272,
      "grad_norm": 67.35710144042969,
      "learning_rate": 0.00018909333333333333,
      "loss": -118.8408,
      "step": 4090
    },
    {
      "epoch": 0.328,
      "grad_norm": 50.09440231323242,
      "learning_rate": 0.0001890666666666667,
      "loss": -117.9853,
      "step": 4100
    },
    {
      "epoch": 0.3288,
      "grad_norm": 91.46844482421875,
      "learning_rate": 0.00018904000000000002,
      "loss": -117.205,
      "step": 4110
    },
    {
      "epoch": 0.3296,
      "grad_norm": 60.12696838378906,
      "learning_rate": 0.00018901333333333335,
      "loss": -119.073,
      "step": 4120
    },
    {
      "epoch": 0.3304,
      "grad_norm": 74.77214813232422,
      "learning_rate": 0.00018898666666666668,
      "loss": -116.5203,
      "step": 4130
    },
    {
      "epoch": 0.3312,
      "grad_norm": 114.73848724365234,
      "learning_rate": 0.00018896,
      "loss": -116.171,
      "step": 4140
    },
    {
      "epoch": 0.332,
      "grad_norm": 55.35664749145508,
      "learning_rate": 0.00018893333333333334,
      "loss": -116.291,
      "step": 4150
    },
    {
      "epoch": 0.3328,
      "grad_norm": 55.78053665161133,
      "learning_rate": 0.00018890666666666667,
      "loss": -117.0785,
      "step": 4160
    },
    {
      "epoch": 0.3336,
      "grad_norm": 53.554447174072266,
      "learning_rate": 0.00018888000000000002,
      "loss": -117.9424,
      "step": 4170
    },
    {
      "epoch": 0.3344,
      "grad_norm": 66.32056427001953,
      "learning_rate": 0.00018885333333333335,
      "loss": -116.3762,
      "step": 4180
    },
    {
      "epoch": 0.3352,
      "grad_norm": 69.76102447509766,
      "learning_rate": 0.00018882666666666668,
      "loss": -116.6085,
      "step": 4190
    },
    {
      "epoch": 0.336,
      "grad_norm": 75.03665161132812,
      "learning_rate": 0.0001888,
      "loss": -117.708,
      "step": 4200
    },
    {
      "epoch": 0.3368,
      "grad_norm": 79.42057800292969,
      "learning_rate": 0.00018877333333333334,
      "loss": -117.3296,
      "step": 4210
    },
    {
      "epoch": 0.3376,
      "grad_norm": 76.7390365600586,
      "learning_rate": 0.00018874666666666667,
      "loss": -116.4856,
      "step": 4220
    },
    {
      "epoch": 0.3384,
      "grad_norm": 66.21896362304688,
      "learning_rate": 0.00018872,
      "loss": -118.2399,
      "step": 4230
    },
    {
      "epoch": 0.3392,
      "grad_norm": 93.60064697265625,
      "learning_rate": 0.00018869333333333335,
      "loss": -116.7912,
      "step": 4240
    },
    {
      "epoch": 0.34,
      "grad_norm": 95.3961181640625,
      "learning_rate": 0.00018866666666666668,
      "loss": -117.4466,
      "step": 4250
    },
    {
      "epoch": 0.3408,
      "grad_norm": 61.86552810668945,
      "learning_rate": 0.00018864,
      "loss": -116.6594,
      "step": 4260
    },
    {
      "epoch": 0.3416,
      "grad_norm": 61.491058349609375,
      "learning_rate": 0.00018861333333333337,
      "loss": -116.4522,
      "step": 4270
    },
    {
      "epoch": 0.3424,
      "grad_norm": 66.52474975585938,
      "learning_rate": 0.00018858666666666667,
      "loss": -117.536,
      "step": 4280
    },
    {
      "epoch": 0.3432,
      "grad_norm": 82.46476745605469,
      "learning_rate": 0.00018856,
      "loss": -117.8784,
      "step": 4290
    },
    {
      "epoch": 0.344,
      "grad_norm": 91.26985931396484,
      "learning_rate": 0.00018853333333333333,
      "loss": -117.48,
      "step": 4300
    },
    {
      "epoch": 0.3448,
      "grad_norm": 73.15975952148438,
      "learning_rate": 0.00018850666666666668,
      "loss": -117.187,
      "step": 4310
    },
    {
      "epoch": 0.3456,
      "grad_norm": 58.43321990966797,
      "learning_rate": 0.00018848,
      "loss": -117.3293,
      "step": 4320
    },
    {
      "epoch": 0.3464,
      "grad_norm": 73.56160736083984,
      "learning_rate": 0.00018845333333333334,
      "loss": -116.6048,
      "step": 4330
    },
    {
      "epoch": 0.3472,
      "grad_norm": 51.17985916137695,
      "learning_rate": 0.0001884266666666667,
      "loss": -118.1112,
      "step": 4340
    },
    {
      "epoch": 0.348,
      "grad_norm": 70.79485321044922,
      "learning_rate": 0.0001884,
      "loss": -117.7149,
      "step": 4350
    },
    {
      "epoch": 0.3488,
      "grad_norm": 70.31846618652344,
      "learning_rate": 0.00018837333333333333,
      "loss": -117.7418,
      "step": 4360
    },
    {
      "epoch": 0.3496,
      "grad_norm": 61.23720932006836,
      "learning_rate": 0.00018834666666666668,
      "loss": -117.6812,
      "step": 4370
    },
    {
      "epoch": 0.3504,
      "grad_norm": 64.12623596191406,
      "learning_rate": 0.00018832,
      "loss": -118.773,
      "step": 4380
    },
    {
      "epoch": 0.3512,
      "grad_norm": 73.33262634277344,
      "learning_rate": 0.00018829333333333334,
      "loss": -118.2909,
      "step": 4390
    },
    {
      "epoch": 0.352,
      "grad_norm": 62.65521240234375,
      "learning_rate": 0.00018826666666666667,
      "loss": -119.0553,
      "step": 4400
    },
    {
      "epoch": 0.3528,
      "grad_norm": 87.64215087890625,
      "learning_rate": 0.00018824000000000003,
      "loss": -118.2292,
      "step": 4410
    },
    {
      "epoch": 0.3536,
      "grad_norm": 82.80455017089844,
      "learning_rate": 0.00018821333333333336,
      "loss": -116.9786,
      "step": 4420
    },
    {
      "epoch": 0.3544,
      "grad_norm": 57.36896896362305,
      "learning_rate": 0.00018818666666666666,
      "loss": -117.8281,
      "step": 4430
    },
    {
      "epoch": 0.3552,
      "grad_norm": 62.05329895019531,
      "learning_rate": 0.00018816000000000001,
      "loss": -116.1513,
      "step": 4440
    },
    {
      "epoch": 0.356,
      "grad_norm": 88.54759979248047,
      "learning_rate": 0.00018813333333333334,
      "loss": -118.5358,
      "step": 4450
    },
    {
      "epoch": 0.3568,
      "grad_norm": 63.52227020263672,
      "learning_rate": 0.00018810666666666667,
      "loss": -118.5384,
      "step": 4460
    },
    {
      "epoch": 0.3576,
      "grad_norm": 52.25962829589844,
      "learning_rate": 0.00018808,
      "loss": -117.7742,
      "step": 4470
    },
    {
      "epoch": 0.3584,
      "grad_norm": 96.7466049194336,
      "learning_rate": 0.00018805333333333336,
      "loss": -118.7534,
      "step": 4480
    },
    {
      "epoch": 0.3592,
      "grad_norm": 79.41165924072266,
      "learning_rate": 0.0001880266666666667,
      "loss": -116.3055,
      "step": 4490
    },
    {
      "epoch": 0.36,
      "grad_norm": 102.44923400878906,
      "learning_rate": 0.000188,
      "loss": -117.312,
      "step": 4500
    },
    {
      "epoch": 0.3608,
      "grad_norm": 68.21475982666016,
      "learning_rate": 0.00018797333333333335,
      "loss": -116.1211,
      "step": 4510
    },
    {
      "epoch": 0.3616,
      "grad_norm": 53.773834228515625,
      "learning_rate": 0.00018794666666666667,
      "loss": -117.0457,
      "step": 4520
    },
    {
      "epoch": 0.3624,
      "grad_norm": 69.13665771484375,
      "learning_rate": 0.00018792,
      "loss": -117.9965,
      "step": 4530
    },
    {
      "epoch": 0.3632,
      "grad_norm": 60.934356689453125,
      "learning_rate": 0.00018789333333333333,
      "loss": -118.7036,
      "step": 4540
    },
    {
      "epoch": 0.364,
      "grad_norm": 59.8915901184082,
      "learning_rate": 0.0001878666666666667,
      "loss": -117.4698,
      "step": 4550
    },
    {
      "epoch": 0.3648,
      "grad_norm": 65.64076232910156,
      "learning_rate": 0.00018784000000000002,
      "loss": -118.0582,
      "step": 4560
    },
    {
      "epoch": 0.3656,
      "grad_norm": 101.34564971923828,
      "learning_rate": 0.00018781333333333335,
      "loss": -117.8591,
      "step": 4570
    },
    {
      "epoch": 0.3664,
      "grad_norm": 87.6083984375,
      "learning_rate": 0.00018778666666666668,
      "loss": -118.6754,
      "step": 4580
    },
    {
      "epoch": 0.3672,
      "grad_norm": 56.80071258544922,
      "learning_rate": 0.00018776,
      "loss": -116.879,
      "step": 4590
    },
    {
      "epoch": 0.368,
      "grad_norm": 63.74310302734375,
      "learning_rate": 0.00018773333333333333,
      "loss": -116.4777,
      "step": 4600
    },
    {
      "epoch": 0.3688,
      "grad_norm": 63.60380172729492,
      "learning_rate": 0.00018770666666666666,
      "loss": -117.5199,
      "step": 4610
    },
    {
      "epoch": 0.3696,
      "grad_norm": 59.35285949707031,
      "learning_rate": 0.00018768000000000002,
      "loss": -118.585,
      "step": 4620
    },
    {
      "epoch": 0.3704,
      "grad_norm": 75.16741943359375,
      "learning_rate": 0.00018765333333333335,
      "loss": -117.9438,
      "step": 4630
    },
    {
      "epoch": 0.3712,
      "grad_norm": 63.400657653808594,
      "learning_rate": 0.00018762666666666668,
      "loss": -117.0587,
      "step": 4640
    },
    {
      "epoch": 0.372,
      "grad_norm": 51.25811767578125,
      "learning_rate": 0.0001876,
      "loss": -117.5519,
      "step": 4650
    },
    {
      "epoch": 0.3728,
      "grad_norm": 94.19110107421875,
      "learning_rate": 0.00018757333333333334,
      "loss": -117.2351,
      "step": 4660
    },
    {
      "epoch": 0.3736,
      "grad_norm": 75.27001953125,
      "learning_rate": 0.00018754666666666666,
      "loss": -117.7651,
      "step": 4670
    },
    {
      "epoch": 0.3744,
      "grad_norm": 67.87802124023438,
      "learning_rate": 0.00018752,
      "loss": -118.1247,
      "step": 4680
    },
    {
      "epoch": 0.3752,
      "grad_norm": 60.08438491821289,
      "learning_rate": 0.00018749333333333335,
      "loss": -118.38,
      "step": 4690
    },
    {
      "epoch": 0.376,
      "grad_norm": 66.25332641601562,
      "learning_rate": 0.00018746666666666668,
      "loss": -117.9075,
      "step": 4700
    },
    {
      "epoch": 0.3768,
      "grad_norm": 67.30878448486328,
      "learning_rate": 0.00018744,
      "loss": -118.4781,
      "step": 4710
    },
    {
      "epoch": 0.3776,
      "grad_norm": 65.8761215209961,
      "learning_rate": 0.00018741333333333336,
      "loss": -117.2142,
      "step": 4720
    },
    {
      "epoch": 0.3784,
      "grad_norm": 58.27385711669922,
      "learning_rate": 0.00018738666666666667,
      "loss": -118.2746,
      "step": 4730
    },
    {
      "epoch": 0.3792,
      "grad_norm": 53.382606506347656,
      "learning_rate": 0.00018736,
      "loss": -117.6923,
      "step": 4740
    },
    {
      "epoch": 0.38,
      "grad_norm": 60.920108795166016,
      "learning_rate": 0.00018733333333333335,
      "loss": -117.7123,
      "step": 4750
    },
    {
      "epoch": 0.3808,
      "grad_norm": 65.83172607421875,
      "learning_rate": 0.00018730666666666668,
      "loss": -118.494,
      "step": 4760
    },
    {
      "epoch": 0.3816,
      "grad_norm": 70.22647094726562,
      "learning_rate": 0.00018728,
      "loss": -117.3532,
      "step": 4770
    },
    {
      "epoch": 0.3824,
      "grad_norm": 58.18931579589844,
      "learning_rate": 0.00018725333333333334,
      "loss": -117.0561,
      "step": 4780
    },
    {
      "epoch": 0.3832,
      "grad_norm": 60.22052001953125,
      "learning_rate": 0.0001872266666666667,
      "loss": -117.3075,
      "step": 4790
    },
    {
      "epoch": 0.384,
      "grad_norm": 72.55534362792969,
      "learning_rate": 0.00018720000000000002,
      "loss": -118.9561,
      "step": 4800
    },
    {
      "epoch": 0.3848,
      "grad_norm": 71.63127136230469,
      "learning_rate": 0.00018717333333333333,
      "loss": -118.3494,
      "step": 4810
    },
    {
      "epoch": 0.3856,
      "grad_norm": 71.6113052368164,
      "learning_rate": 0.00018714666666666668,
      "loss": -118.5831,
      "step": 4820
    },
    {
      "epoch": 0.3864,
      "grad_norm": 71.90507507324219,
      "learning_rate": 0.00018712,
      "loss": -117.8411,
      "step": 4830
    },
    {
      "epoch": 0.3872,
      "grad_norm": 73.09596252441406,
      "learning_rate": 0.00018709333333333334,
      "loss": -118.1457,
      "step": 4840
    },
    {
      "epoch": 0.388,
      "grad_norm": 58.62297058105469,
      "learning_rate": 0.00018706666666666667,
      "loss": -117.177,
      "step": 4850
    },
    {
      "epoch": 0.3888,
      "grad_norm": 59.84435272216797,
      "learning_rate": 0.00018704000000000003,
      "loss": -117.9997,
      "step": 4860
    },
    {
      "epoch": 0.3896,
      "grad_norm": 51.79825973510742,
      "learning_rate": 0.00018701333333333335,
      "loss": -118.0983,
      "step": 4870
    },
    {
      "epoch": 0.3904,
      "grad_norm": 68.60951232910156,
      "learning_rate": 0.00018698666666666666,
      "loss": -117.2107,
      "step": 4880
    },
    {
      "epoch": 0.3912,
      "grad_norm": 58.92770767211914,
      "learning_rate": 0.00018696,
      "loss": -117.2655,
      "step": 4890
    },
    {
      "epoch": 0.392,
      "grad_norm": 69.91917419433594,
      "learning_rate": 0.00018693333333333334,
      "loss": -117.1192,
      "step": 4900
    },
    {
      "epoch": 0.3928,
      "grad_norm": 85.4964828491211,
      "learning_rate": 0.00018690666666666667,
      "loss": -118.1314,
      "step": 4910
    },
    {
      "epoch": 0.3936,
      "grad_norm": 60.05213165283203,
      "learning_rate": 0.00018688,
      "loss": -117.9704,
      "step": 4920
    },
    {
      "epoch": 0.3944,
      "grad_norm": 71.92659759521484,
      "learning_rate": 0.00018685333333333336,
      "loss": -118.7735,
      "step": 4930
    },
    {
      "epoch": 0.3952,
      "grad_norm": 66.6703109741211,
      "learning_rate": 0.00018682666666666668,
      "loss": -118.9531,
      "step": 4940
    },
    {
      "epoch": 0.396,
      "grad_norm": 85.85305786132812,
      "learning_rate": 0.00018680000000000001,
      "loss": -119.1412,
      "step": 4950
    },
    {
      "epoch": 0.3968,
      "grad_norm": 76.76334381103516,
      "learning_rate": 0.00018677333333333334,
      "loss": -117.604,
      "step": 4960
    },
    {
      "epoch": 0.3976,
      "grad_norm": 73.1579818725586,
      "learning_rate": 0.00018674666666666667,
      "loss": -118.1296,
      "step": 4970
    },
    {
      "epoch": 0.3984,
      "grad_norm": 56.71463394165039,
      "learning_rate": 0.00018672,
      "loss": -117.4059,
      "step": 4980
    },
    {
      "epoch": 0.3992,
      "grad_norm": 72.11493682861328,
      "learning_rate": 0.00018669333333333333,
      "loss": -118.1254,
      "step": 4990
    },
    {
      "epoch": 0.4,
      "grad_norm": 59.09806823730469,
      "learning_rate": 0.0001866666666666667,
      "loss": -117.6472,
      "step": 5000
    },
    {
      "epoch": 0.4008,
      "grad_norm": 81.44220733642578,
      "learning_rate": 0.00018664000000000002,
      "loss": -118.5603,
      "step": 5010
    },
    {
      "epoch": 0.4016,
      "grad_norm": 50.14027404785156,
      "learning_rate": 0.00018661333333333334,
      "loss": -117.9923,
      "step": 5020
    },
    {
      "epoch": 0.4024,
      "grad_norm": 52.13078689575195,
      "learning_rate": 0.00018658666666666667,
      "loss": -117.026,
      "step": 5030
    },
    {
      "epoch": 0.4032,
      "grad_norm": 86.62969207763672,
      "learning_rate": 0.00018656,
      "loss": -118.3571,
      "step": 5040
    },
    {
      "epoch": 0.404,
      "grad_norm": 54.009525299072266,
      "learning_rate": 0.00018653333333333333,
      "loss": -118.1159,
      "step": 5050
    },
    {
      "epoch": 0.4048,
      "grad_norm": 51.501773834228516,
      "learning_rate": 0.00018650666666666666,
      "loss": -118.2261,
      "step": 5060
    },
    {
      "epoch": 0.4056,
      "grad_norm": 78.20857238769531,
      "learning_rate": 0.00018648000000000002,
      "loss": -117.3095,
      "step": 5070
    },
    {
      "epoch": 0.4064,
      "grad_norm": 70.96256256103516,
      "learning_rate": 0.00018645333333333335,
      "loss": -117.8302,
      "step": 5080
    },
    {
      "epoch": 0.4072,
      "grad_norm": 81.35531616210938,
      "learning_rate": 0.00018642666666666668,
      "loss": -117.8703,
      "step": 5090
    },
    {
      "epoch": 0.408,
      "grad_norm": 59.57603454589844,
      "learning_rate": 0.00018640000000000003,
      "loss": -118.7605,
      "step": 5100
    },
    {
      "epoch": 0.4088,
      "grad_norm": 52.101078033447266,
      "learning_rate": 0.00018637333333333333,
      "loss": -118.234,
      "step": 5110
    },
    {
      "epoch": 0.4096,
      "grad_norm": 79.62602996826172,
      "learning_rate": 0.00018634666666666666,
      "loss": -118.0338,
      "step": 5120
    },
    {
      "epoch": 0.4104,
      "grad_norm": 57.989437103271484,
      "learning_rate": 0.00018632000000000002,
      "loss": -118.0222,
      "step": 5130
    },
    {
      "epoch": 0.4112,
      "grad_norm": 97.660888671875,
      "learning_rate": 0.00018629333333333335,
      "loss": -118.4978,
      "step": 5140
    },
    {
      "epoch": 0.412,
      "grad_norm": 64.05035400390625,
      "learning_rate": 0.00018626666666666668,
      "loss": -118.6407,
      "step": 5150
    },
    {
      "epoch": 0.4128,
      "grad_norm": 67.84925079345703,
      "learning_rate": 0.00018624,
      "loss": -117.9959,
      "step": 5160
    },
    {
      "epoch": 0.4136,
      "grad_norm": 61.76091003417969,
      "learning_rate": 0.00018621333333333336,
      "loss": -117.6654,
      "step": 5170
    },
    {
      "epoch": 0.4144,
      "grad_norm": 71.74740600585938,
      "learning_rate": 0.00018618666666666666,
      "loss": -118.2965,
      "step": 5180
    },
    {
      "epoch": 0.4152,
      "grad_norm": 58.21009063720703,
      "learning_rate": 0.00018616,
      "loss": -117.7312,
      "step": 5190
    },
    {
      "epoch": 0.416,
      "grad_norm": 53.4370231628418,
      "learning_rate": 0.00018613333333333335,
      "loss": -119.2453,
      "step": 5200
    },
    {
      "epoch": 0.4168,
      "grad_norm": 56.698829650878906,
      "learning_rate": 0.00018610666666666668,
      "loss": -116.519,
      "step": 5210
    },
    {
      "epoch": 0.4176,
      "grad_norm": 55.16939163208008,
      "learning_rate": 0.00018608,
      "loss": -118.0519,
      "step": 5220
    },
    {
      "epoch": 0.4184,
      "grad_norm": 55.251041412353516,
      "learning_rate": 0.00018605333333333334,
      "loss": -117.6699,
      "step": 5230
    },
    {
      "epoch": 0.4192,
      "grad_norm": 57.12929153442383,
      "learning_rate": 0.0001860266666666667,
      "loss": -117.4114,
      "step": 5240
    },
    {
      "epoch": 0.42,
      "grad_norm": 61.62242889404297,
      "learning_rate": 0.00018600000000000002,
      "loss": -117.9552,
      "step": 5250
    },
    {
      "epoch": 0.4208,
      "grad_norm": 68.20054626464844,
      "learning_rate": 0.00018597333333333332,
      "loss": -116.5247,
      "step": 5260
    },
    {
      "epoch": 0.4216,
      "grad_norm": 61.62418746948242,
      "learning_rate": 0.00018594666666666668,
      "loss": -117.8296,
      "step": 5270
    },
    {
      "epoch": 0.4224,
      "grad_norm": 94.54126739501953,
      "learning_rate": 0.00018592,
      "loss": -117.4134,
      "step": 5280
    },
    {
      "epoch": 0.4232,
      "grad_norm": 50.19167709350586,
      "learning_rate": 0.00018589333333333334,
      "loss": -117.1484,
      "step": 5290
    },
    {
      "epoch": 0.424,
      "grad_norm": 51.50139236450195,
      "learning_rate": 0.00018586666666666667,
      "loss": -117.636,
      "step": 5300
    },
    {
      "epoch": 0.4248,
      "grad_norm": 48.563392639160156,
      "learning_rate": 0.00018584000000000002,
      "loss": -117.1007,
      "step": 5310
    },
    {
      "epoch": 0.4256,
      "grad_norm": 61.08380126953125,
      "learning_rate": 0.00018581333333333335,
      "loss": -118.976,
      "step": 5320
    },
    {
      "epoch": 0.4264,
      "grad_norm": 73.74376678466797,
      "learning_rate": 0.00018578666666666668,
      "loss": -117.5593,
      "step": 5330
    },
    {
      "epoch": 0.4272,
      "grad_norm": 51.00743865966797,
      "learning_rate": 0.00018576,
      "loss": -117.8915,
      "step": 5340
    },
    {
      "epoch": 0.428,
      "grad_norm": 70.27986145019531,
      "learning_rate": 0.00018573333333333334,
      "loss": -118.3456,
      "step": 5350
    },
    {
      "epoch": 0.4288,
      "grad_norm": 78.17018127441406,
      "learning_rate": 0.00018570666666666667,
      "loss": -117.5726,
      "step": 5360
    },
    {
      "epoch": 0.4296,
      "grad_norm": 81.59937286376953,
      "learning_rate": 0.00018568,
      "loss": -116.6868,
      "step": 5370
    },
    {
      "epoch": 0.4304,
      "grad_norm": 68.61656951904297,
      "learning_rate": 0.00018565333333333335,
      "loss": -118.418,
      "step": 5380
    },
    {
      "epoch": 0.4312,
      "grad_norm": 63.83388900756836,
      "learning_rate": 0.00018562666666666668,
      "loss": -117.9953,
      "step": 5390
    },
    {
      "epoch": 0.432,
      "grad_norm": 70.85557556152344,
      "learning_rate": 0.0001856,
      "loss": -118.0138,
      "step": 5400
    },
    {
      "epoch": 0.4328,
      "grad_norm": 45.842628479003906,
      "learning_rate": 0.00018557333333333334,
      "loss": -117.6881,
      "step": 5410
    },
    {
      "epoch": 0.4336,
      "grad_norm": 47.01387405395508,
      "learning_rate": 0.00018554666666666667,
      "loss": -118.2196,
      "step": 5420
    },
    {
      "epoch": 0.4344,
      "grad_norm": 66.80032348632812,
      "learning_rate": 0.00018552,
      "loss": -116.383,
      "step": 5430
    },
    {
      "epoch": 0.4352,
      "grad_norm": 67.99108123779297,
      "learning_rate": 0.00018549333333333333,
      "loss": -117.7609,
      "step": 5440
    },
    {
      "epoch": 0.436,
      "grad_norm": 89.03282928466797,
      "learning_rate": 0.00018546666666666668,
      "loss": -117.2178,
      "step": 5450
    },
    {
      "epoch": 0.4368,
      "grad_norm": 56.32305908203125,
      "learning_rate": 0.00018544,
      "loss": -117.6444,
      "step": 5460
    },
    {
      "epoch": 0.4376,
      "grad_norm": 58.26311111450195,
      "learning_rate": 0.00018541333333333334,
      "loss": -118.5915,
      "step": 5470
    },
    {
      "epoch": 0.4384,
      "grad_norm": 70.79578399658203,
      "learning_rate": 0.0001853866666666667,
      "loss": -117.2974,
      "step": 5480
    },
    {
      "epoch": 0.4392,
      "grad_norm": 73.35456848144531,
      "learning_rate": 0.00018536,
      "loss": -117.9935,
      "step": 5490
    },
    {
      "epoch": 0.44,
      "grad_norm": 64.85086059570312,
      "learning_rate": 0.00018533333333333333,
      "loss": -118.0302,
      "step": 5500
    },
    {
      "epoch": 0.4408,
      "grad_norm": 45.53139877319336,
      "learning_rate": 0.00018530666666666669,
      "loss": -118.3312,
      "step": 5510
    },
    {
      "epoch": 0.4416,
      "grad_norm": 68.62301635742188,
      "learning_rate": 0.00018528000000000001,
      "loss": -117.7433,
      "step": 5520
    },
    {
      "epoch": 0.4424,
      "grad_norm": 68.18976593017578,
      "learning_rate": 0.00018525333333333334,
      "loss": -118.7045,
      "step": 5530
    },
    {
      "epoch": 0.4432,
      "grad_norm": 64.1775894165039,
      "learning_rate": 0.00018522666666666667,
      "loss": -116.8935,
      "step": 5540
    },
    {
      "epoch": 0.444,
      "grad_norm": 66.92405700683594,
      "learning_rate": 0.00018520000000000003,
      "loss": -116.6579,
      "step": 5550
    },
    {
      "epoch": 0.4448,
      "grad_norm": 62.90961837768555,
      "learning_rate": 0.00018517333333333333,
      "loss": -117.2308,
      "step": 5560
    },
    {
      "epoch": 0.4456,
      "grad_norm": 209.1767578125,
      "learning_rate": 0.00018514666666666666,
      "loss": -117.6919,
      "step": 5570
    },
    {
      "epoch": 0.4464,
      "grad_norm": 503.7726135253906,
      "learning_rate": 0.00018512000000000002,
      "loss": -117.0176,
      "step": 5580
    },
    {
      "epoch": 0.4472,
      "grad_norm": 58.65449905395508,
      "learning_rate": 0.00018509333333333335,
      "loss": -115.6009,
      "step": 5590
    },
    {
      "epoch": 0.448,
      "grad_norm": 289.18756103515625,
      "learning_rate": 0.00018506666666666667,
      "loss": -114.2238,
      "step": 5600
    },
    {
      "epoch": 0.4488,
      "grad_norm": 72.02120971679688,
      "learning_rate": 0.00018504,
      "loss": -116.4714,
      "step": 5610
    },
    {
      "epoch": 0.4496,
      "grad_norm": 331.1381530761719,
      "learning_rate": 0.00018501333333333336,
      "loss": -115.0831,
      "step": 5620
    },
    {
      "epoch": 0.4504,
      "grad_norm": 304.258056640625,
      "learning_rate": 0.0001849866666666667,
      "loss": -109.1779,
      "step": 5630
    },
    {
      "epoch": 0.4512,
      "grad_norm": 144.8413543701172,
      "learning_rate": 0.00018496,
      "loss": -109.8873,
      "step": 5640
    },
    {
      "epoch": 0.452,
      "grad_norm": 210.05958557128906,
      "learning_rate": 0.00018493333333333335,
      "loss": -113.3766,
      "step": 5650
    },
    {
      "epoch": 0.4528,
      "grad_norm": 76.48722076416016,
      "learning_rate": 0.00018490666666666668,
      "loss": -113.4679,
      "step": 5660
    },
    {
      "epoch": 0.4536,
      "grad_norm": 100.2171630859375,
      "learning_rate": 0.00018488,
      "loss": -115.0325,
      "step": 5670
    },
    {
      "epoch": 0.4544,
      "grad_norm": 88.81282806396484,
      "learning_rate": 0.00018485333333333333,
      "loss": -115.3388,
      "step": 5680
    },
    {
      "epoch": 0.4552,
      "grad_norm": 61.74827194213867,
      "learning_rate": 0.0001848266666666667,
      "loss": -114.2699,
      "step": 5690
    },
    {
      "epoch": 0.456,
      "grad_norm": 68.52152252197266,
      "learning_rate": 0.00018480000000000002,
      "loss": -115.6651,
      "step": 5700
    },
    {
      "epoch": 0.4568,
      "grad_norm": 70.29524230957031,
      "learning_rate": 0.00018477333333333332,
      "loss": -114.7497,
      "step": 5710
    },
    {
      "epoch": 0.4576,
      "grad_norm": 140.2889862060547,
      "learning_rate": 0.00018474666666666668,
      "loss": -116.7624,
      "step": 5720
    },
    {
      "epoch": 0.4584,
      "grad_norm": 58.30044937133789,
      "learning_rate": 0.00018472,
      "loss": -117.1569,
      "step": 5730
    },
    {
      "epoch": 0.4592,
      "grad_norm": 64.7415771484375,
      "learning_rate": 0.00018469333333333334,
      "loss": -114.1699,
      "step": 5740
    },
    {
      "epoch": 0.46,
      "grad_norm": 65.69821166992188,
      "learning_rate": 0.00018466666666666666,
      "loss": -116.4005,
      "step": 5750
    },
    {
      "epoch": 0.4608,
      "grad_norm": 68.19248962402344,
      "learning_rate": 0.00018464000000000002,
      "loss": -115.2504,
      "step": 5760
    },
    {
      "epoch": 0.4616,
      "grad_norm": 44.06074905395508,
      "learning_rate": 0.00018461333333333335,
      "loss": -115.5051,
      "step": 5770
    },
    {
      "epoch": 0.4624,
      "grad_norm": 52.08860397338867,
      "learning_rate": 0.00018458666666666668,
      "loss": -115.4891,
      "step": 5780
    },
    {
      "epoch": 0.4632,
      "grad_norm": 39.836063385009766,
      "learning_rate": 0.00018456,
      "loss": -116.4783,
      "step": 5790
    },
    {
      "epoch": 0.464,
      "grad_norm": 44.27580642700195,
      "learning_rate": 0.00018453333333333334,
      "loss": -116.067,
      "step": 5800
    },
    {
      "epoch": 0.4648,
      "grad_norm": 55.441402435302734,
      "learning_rate": 0.00018450666666666667,
      "loss": -116.5588,
      "step": 5810
    },
    {
      "epoch": 0.4656,
      "grad_norm": 62.58677291870117,
      "learning_rate": 0.00018448,
      "loss": -116.268,
      "step": 5820
    },
    {
      "epoch": 0.4664,
      "grad_norm": 63.867332458496094,
      "learning_rate": 0.00018445333333333335,
      "loss": -116.8775,
      "step": 5830
    },
    {
      "epoch": 0.4672,
      "grad_norm": 58.55681228637695,
      "learning_rate": 0.00018442666666666668,
      "loss": -117.2855,
      "step": 5840
    },
    {
      "epoch": 0.468,
      "grad_norm": 42.382904052734375,
      "learning_rate": 0.0001844,
      "loss": -115.5458,
      "step": 5850
    },
    {
      "epoch": 0.4688,
      "grad_norm": 68.94525909423828,
      "learning_rate": 0.00018437333333333334,
      "loss": -115.7372,
      "step": 5860
    },
    {
      "epoch": 0.4696,
      "grad_norm": 56.43513107299805,
      "learning_rate": 0.00018434666666666667,
      "loss": -114.3526,
      "step": 5870
    },
    {
      "epoch": 0.4704,
      "grad_norm": 49.93012619018555,
      "learning_rate": 0.00018432,
      "loss": -115.876,
      "step": 5880
    },
    {
      "epoch": 0.4712,
      "grad_norm": 38.751556396484375,
      "learning_rate": 0.00018429333333333335,
      "loss": -116.497,
      "step": 5890
    },
    {
      "epoch": 0.472,
      "grad_norm": 45.523067474365234,
      "learning_rate": 0.00018426666666666668,
      "loss": -116.5287,
      "step": 5900
    },
    {
      "epoch": 0.4728,
      "grad_norm": 64.8694076538086,
      "learning_rate": 0.00018424,
      "loss": -115.4963,
      "step": 5910
    },
    {
      "epoch": 0.4736,
      "grad_norm": 42.826419830322266,
      "learning_rate": 0.00018421333333333334,
      "loss": -117.4052,
      "step": 5920
    },
    {
      "epoch": 0.4744,
      "grad_norm": 52.41679000854492,
      "learning_rate": 0.0001841866666666667,
      "loss": -116.3543,
      "step": 5930
    },
    {
      "epoch": 0.4752,
      "grad_norm": 51.236148834228516,
      "learning_rate": 0.00018416,
      "loss": -115.9804,
      "step": 5940
    },
    {
      "epoch": 0.476,
      "grad_norm": 87.63955688476562,
      "learning_rate": 0.00018413333333333333,
      "loss": -116.3934,
      "step": 5950
    },
    {
      "epoch": 0.4768,
      "grad_norm": 79.54676818847656,
      "learning_rate": 0.00018410666666666668,
      "loss": -116.3705,
      "step": 5960
    },
    {
      "epoch": 0.4776,
      "grad_norm": 51.251583099365234,
      "learning_rate": 0.00018408,
      "loss": -116.9209,
      "step": 5970
    },
    {
      "epoch": 0.4784,
      "grad_norm": 50.485572814941406,
      "learning_rate": 0.00018405333333333334,
      "loss": -115.5145,
      "step": 5980
    },
    {
      "epoch": 0.4792,
      "grad_norm": 54.81180191040039,
      "learning_rate": 0.00018402666666666667,
      "loss": -116.4364,
      "step": 5990
    },
    {
      "epoch": 0.48,
      "grad_norm": 51.20444869995117,
      "learning_rate": 0.00018400000000000003,
      "loss": -115.7187,
      "step": 6000
    },
    {
      "epoch": 0.4808,
      "grad_norm": 45.4708251953125,
      "learning_rate": 0.00018397333333333336,
      "loss": -116.3517,
      "step": 6010
    },
    {
      "epoch": 0.4816,
      "grad_norm": 58.68874740600586,
      "learning_rate": 0.00018394666666666666,
      "loss": -116.9612,
      "step": 6020
    },
    {
      "epoch": 0.4824,
      "grad_norm": 50.43313217163086,
      "learning_rate": 0.00018392000000000001,
      "loss": -116.3708,
      "step": 6030
    },
    {
      "epoch": 0.4832,
      "grad_norm": 62.25627136230469,
      "learning_rate": 0.00018389333333333334,
      "loss": -115.5625,
      "step": 6040
    },
    {
      "epoch": 0.484,
      "grad_norm": 34.347625732421875,
      "learning_rate": 0.00018386666666666667,
      "loss": -117.1598,
      "step": 6050
    },
    {
      "epoch": 0.4848,
      "grad_norm": 36.23813247680664,
      "learning_rate": 0.00018384,
      "loss": -117.7682,
      "step": 6060
    },
    {
      "epoch": 0.4856,
      "grad_norm": 45.396541595458984,
      "learning_rate": 0.00018381333333333336,
      "loss": -115.8577,
      "step": 6070
    },
    {
      "epoch": 0.4864,
      "grad_norm": 65.1466064453125,
      "learning_rate": 0.0001837866666666667,
      "loss": -117.1323,
      "step": 6080
    },
    {
      "epoch": 0.4872,
      "grad_norm": 52.43524932861328,
      "learning_rate": 0.00018376,
      "loss": -116.1143,
      "step": 6090
    },
    {
      "epoch": 0.488,
      "grad_norm": 42.59843444824219,
      "learning_rate": 0.00018373333333333335,
      "loss": -116.3382,
      "step": 6100
    },
    {
      "epoch": 0.4888,
      "grad_norm": 46.32158279418945,
      "learning_rate": 0.00018370666666666667,
      "loss": -115.4008,
      "step": 6110
    },
    {
      "epoch": 0.4896,
      "grad_norm": 34.64784240722656,
      "learning_rate": 0.00018368,
      "loss": -117.1124,
      "step": 6120
    },
    {
      "epoch": 0.4904,
      "grad_norm": 44.88443374633789,
      "learning_rate": 0.00018365333333333333,
      "loss": -116.7588,
      "step": 6130
    },
    {
      "epoch": 0.4912,
      "grad_norm": 43.460670471191406,
      "learning_rate": 0.0001836266666666667,
      "loss": -116.1293,
      "step": 6140
    },
    {
      "epoch": 0.492,
      "grad_norm": 47.045555114746094,
      "learning_rate": 0.00018360000000000002,
      "loss": -116.8993,
      "step": 6150
    },
    {
      "epoch": 0.4928,
      "grad_norm": 55.406105041503906,
      "learning_rate": 0.00018357333333333335,
      "loss": -116.5655,
      "step": 6160
    },
    {
      "epoch": 0.4936,
      "grad_norm": 63.63386535644531,
      "learning_rate": 0.00018354666666666668,
      "loss": -115.8754,
      "step": 6170
    },
    {
      "epoch": 0.4944,
      "grad_norm": 43.17477798461914,
      "learning_rate": 0.00018352,
      "loss": -117.8289,
      "step": 6180
    },
    {
      "epoch": 0.4952,
      "grad_norm": 63.49277877807617,
      "learning_rate": 0.00018349333333333333,
      "loss": -117.1849,
      "step": 6190
    },
    {
      "epoch": 0.496,
      "grad_norm": 52.43095779418945,
      "learning_rate": 0.00018346666666666666,
      "loss": -114.8678,
      "step": 6200
    },
    {
      "epoch": 0.4968,
      "grad_norm": 61.856849670410156,
      "learning_rate": 0.00018344000000000002,
      "loss": -116.6945,
      "step": 6210
    },
    {
      "epoch": 0.4976,
      "grad_norm": 48.52446746826172,
      "learning_rate": 0.00018341333333333335,
      "loss": -115.6499,
      "step": 6220
    },
    {
      "epoch": 0.4984,
      "grad_norm": 65.83057403564453,
      "learning_rate": 0.00018338666666666668,
      "loss": -116.5362,
      "step": 6230
    },
    {
      "epoch": 0.4992,
      "grad_norm": 41.54655456542969,
      "learning_rate": 0.00018336,
      "loss": -116.3769,
      "step": 6240
    },
    {
      "epoch": 0.5,
      "grad_norm": 49.825599670410156,
      "learning_rate": 0.00018333333333333334,
      "loss": -117.1861,
      "step": 6250
    },
    {
      "epoch": 0.5008,
      "grad_norm": 47.166263580322266,
      "learning_rate": 0.00018330666666666666,
      "loss": -116.4852,
      "step": 6260
    },
    {
      "epoch": 0.5016,
      "grad_norm": 38.58407211303711,
      "learning_rate": 0.00018328000000000002,
      "loss": -116.2399,
      "step": 6270
    },
    {
      "epoch": 0.5024,
      "grad_norm": 46.6326904296875,
      "learning_rate": 0.00018325333333333335,
      "loss": -116.2185,
      "step": 6280
    },
    {
      "epoch": 0.5032,
      "grad_norm": 48.19611740112305,
      "learning_rate": 0.00018322666666666668,
      "loss": -114.4562,
      "step": 6290
    },
    {
      "epoch": 0.504,
      "grad_norm": 37.34022903442383,
      "learning_rate": 0.0001832,
      "loss": -116.6803,
      "step": 6300
    },
    {
      "epoch": 0.5048,
      "grad_norm": 45.492740631103516,
      "learning_rate": 0.00018317333333333336,
      "loss": -115.7884,
      "step": 6310
    },
    {
      "epoch": 0.5056,
      "grad_norm": 63.233306884765625,
      "learning_rate": 0.00018314666666666667,
      "loss": -114.8778,
      "step": 6320
    },
    {
      "epoch": 0.5064,
      "grad_norm": 69.35123443603516,
      "learning_rate": 0.00018312,
      "loss": -116.1393,
      "step": 6330
    },
    {
      "epoch": 0.5072,
      "grad_norm": 53.96925735473633,
      "learning_rate": 0.00018309333333333335,
      "loss": -116.7452,
      "step": 6340
    },
    {
      "epoch": 0.508,
      "grad_norm": 46.314266204833984,
      "learning_rate": 0.00018306666666666668,
      "loss": -116.6979,
      "step": 6350
    },
    {
      "epoch": 0.5088,
      "grad_norm": 44.10148620605469,
      "learning_rate": 0.00018304,
      "loss": -116.3622,
      "step": 6360
    },
    {
      "epoch": 0.5096,
      "grad_norm": 39.20956039428711,
      "learning_rate": 0.00018301333333333334,
      "loss": -115.929,
      "step": 6370
    },
    {
      "epoch": 0.5104,
      "grad_norm": 37.32683563232422,
      "learning_rate": 0.0001829866666666667,
      "loss": -116.7858,
      "step": 6380
    },
    {
      "epoch": 0.5112,
      "grad_norm": 55.832115173339844,
      "learning_rate": 0.00018296,
      "loss": -115.8496,
      "step": 6390
    },
    {
      "epoch": 0.512,
      "grad_norm": 48.543983459472656,
      "learning_rate": 0.00018293333333333333,
      "loss": -116.7262,
      "step": 6400
    },
    {
      "epoch": 0.5128,
      "grad_norm": 51.20538330078125,
      "learning_rate": 0.00018290666666666668,
      "loss": -117.2329,
      "step": 6410
    },
    {
      "epoch": 0.5136,
      "grad_norm": 42.97933578491211,
      "learning_rate": 0.00018288,
      "loss": -117.1459,
      "step": 6420
    },
    {
      "epoch": 0.5144,
      "grad_norm": 55.37627029418945,
      "learning_rate": 0.00018285333333333334,
      "loss": -117.7163,
      "step": 6430
    },
    {
      "epoch": 0.5152,
      "grad_norm": 64.1421127319336,
      "learning_rate": 0.00018282666666666667,
      "loss": -117.5322,
      "step": 6440
    },
    {
      "epoch": 0.516,
      "grad_norm": 45.550682067871094,
      "learning_rate": 0.00018280000000000003,
      "loss": -117.0367,
      "step": 6450
    },
    {
      "epoch": 0.5168,
      "grad_norm": 47.14752960205078,
      "learning_rate": 0.00018277333333333335,
      "loss": -116.6271,
      "step": 6460
    },
    {
      "epoch": 0.5176,
      "grad_norm": 40.93819808959961,
      "learning_rate": 0.00018274666666666666,
      "loss": -116.5648,
      "step": 6470
    },
    {
      "epoch": 0.5184,
      "grad_norm": 50.87745666503906,
      "learning_rate": 0.00018272,
      "loss": -115.6418,
      "step": 6480
    },
    {
      "epoch": 0.5192,
      "grad_norm": 50.774967193603516,
      "learning_rate": 0.00018269333333333334,
      "loss": -117.5829,
      "step": 6490
    },
    {
      "epoch": 0.52,
      "grad_norm": 45.96113967895508,
      "learning_rate": 0.00018266666666666667,
      "loss": -114.8246,
      "step": 6500
    },
    {
      "epoch": 0.5208,
      "grad_norm": 47.91546630859375,
      "learning_rate": 0.00018264,
      "loss": -117.4463,
      "step": 6510
    },
    {
      "epoch": 0.5216,
      "grad_norm": 49.19036102294922,
      "learning_rate": 0.00018261333333333336,
      "loss": -116.7511,
      "step": 6520
    },
    {
      "epoch": 0.5224,
      "grad_norm": 46.935420989990234,
      "learning_rate": 0.00018258666666666668,
      "loss": -116.4588,
      "step": 6530
    },
    {
      "epoch": 0.5232,
      "grad_norm": 42.20124053955078,
      "learning_rate": 0.00018256,
      "loss": -116.4041,
      "step": 6540
    },
    {
      "epoch": 0.524,
      "grad_norm": 48.649757385253906,
      "learning_rate": 0.00018253333333333334,
      "loss": -116.9226,
      "step": 6550
    },
    {
      "epoch": 0.5248,
      "grad_norm": 44.606258392333984,
      "learning_rate": 0.00018250666666666667,
      "loss": -116.7273,
      "step": 6560
    },
    {
      "epoch": 0.5256,
      "grad_norm": 46.428550720214844,
      "learning_rate": 0.00018248,
      "loss": -116.6197,
      "step": 6570
    },
    {
      "epoch": 0.5264,
      "grad_norm": 48.56486129760742,
      "learning_rate": 0.00018245333333333333,
      "loss": -117.0523,
      "step": 6580
    },
    {
      "epoch": 0.5272,
      "grad_norm": 46.10417556762695,
      "learning_rate": 0.00018242666666666669,
      "loss": -115.8413,
      "step": 6590
    },
    {
      "epoch": 0.528,
      "grad_norm": 53.22801971435547,
      "learning_rate": 0.00018240000000000002,
      "loss": -116.6773,
      "step": 6600
    },
    {
      "epoch": 0.5288,
      "grad_norm": 54.42750549316406,
      "learning_rate": 0.00018237333333333334,
      "loss": -116.3678,
      "step": 6610
    },
    {
      "epoch": 0.5296,
      "grad_norm": 53.80844497680664,
      "learning_rate": 0.00018234666666666667,
      "loss": -115.8328,
      "step": 6620
    },
    {
      "epoch": 0.5304,
      "grad_norm": 48.40088653564453,
      "learning_rate": 0.00018232,
      "loss": -117.2703,
      "step": 6630
    },
    {
      "epoch": 0.5312,
      "grad_norm": 40.11808776855469,
      "learning_rate": 0.00018229333333333333,
      "loss": -117.6592,
      "step": 6640
    },
    {
      "epoch": 0.532,
      "grad_norm": 61.12046432495117,
      "learning_rate": 0.0001822666666666667,
      "loss": -117.4192,
      "step": 6650
    },
    {
      "epoch": 0.5328,
      "grad_norm": 48.356170654296875,
      "learning_rate": 0.00018224000000000002,
      "loss": -117.2211,
      "step": 6660
    },
    {
      "epoch": 0.5336,
      "grad_norm": 45.098384857177734,
      "learning_rate": 0.00018221333333333335,
      "loss": -116.0394,
      "step": 6670
    },
    {
      "epoch": 0.5344,
      "grad_norm": 48.34357452392578,
      "learning_rate": 0.00018218666666666668,
      "loss": -117.1676,
      "step": 6680
    },
    {
      "epoch": 0.5352,
      "grad_norm": 53.26000213623047,
      "learning_rate": 0.00018216000000000003,
      "loss": -116.0113,
      "step": 6690
    },
    {
      "epoch": 0.536,
      "grad_norm": 55.31258010864258,
      "learning_rate": 0.00018213333333333333,
      "loss": -116.2731,
      "step": 6700
    },
    {
      "epoch": 0.5368,
      "grad_norm": 43.92825698852539,
      "learning_rate": 0.00018210666666666666,
      "loss": -116.4098,
      "step": 6710
    },
    {
      "epoch": 0.5376,
      "grad_norm": 42.69938278198242,
      "learning_rate": 0.00018208000000000002,
      "loss": -116.743,
      "step": 6720
    },
    {
      "epoch": 0.5384,
      "grad_norm": 56.44179916381836,
      "learning_rate": 0.00018205333333333335,
      "loss": -115.9815,
      "step": 6730
    },
    {
      "epoch": 0.5392,
      "grad_norm": 57.73887252807617,
      "learning_rate": 0.00018202666666666668,
      "loss": -116.2379,
      "step": 6740
    },
    {
      "epoch": 0.54,
      "grad_norm": 45.61983108520508,
      "learning_rate": 0.000182,
      "loss": -117.1034,
      "step": 6750
    },
    {
      "epoch": 0.5408,
      "grad_norm": 40.802791595458984,
      "learning_rate": 0.00018197333333333336,
      "loss": -116.9061,
      "step": 6760
    },
    {
      "epoch": 0.5416,
      "grad_norm": 46.17789840698242,
      "learning_rate": 0.00018194666666666666,
      "loss": -116.395,
      "step": 6770
    },
    {
      "epoch": 0.5424,
      "grad_norm": 66.211181640625,
      "learning_rate": 0.00018192,
      "loss": -117.0488,
      "step": 6780
    },
    {
      "epoch": 0.5432,
      "grad_norm": 45.40535354614258,
      "learning_rate": 0.00018189333333333335,
      "loss": -117.1853,
      "step": 6790
    },
    {
      "epoch": 0.544,
      "grad_norm": 55.419830322265625,
      "learning_rate": 0.00018186666666666668,
      "loss": -116.8891,
      "step": 6800
    },
    {
      "epoch": 0.5448,
      "grad_norm": 39.36368179321289,
      "learning_rate": 0.00018184,
      "loss": -116.8428,
      "step": 6810
    },
    {
      "epoch": 0.5456,
      "grad_norm": 52.30836486816406,
      "learning_rate": 0.00018181333333333334,
      "loss": -117.0873,
      "step": 6820
    },
    {
      "epoch": 0.5464,
      "grad_norm": 53.93484115600586,
      "learning_rate": 0.0001817866666666667,
      "loss": -115.1083,
      "step": 6830
    },
    {
      "epoch": 0.5472,
      "grad_norm": 56.295711517333984,
      "learning_rate": 0.00018176000000000002,
      "loss": -116.0054,
      "step": 6840
    },
    {
      "epoch": 0.548,
      "grad_norm": 48.59779739379883,
      "learning_rate": 0.00018173333333333332,
      "loss": -115.3479,
      "step": 6850
    },
    {
      "epoch": 0.5488,
      "grad_norm": 39.447425842285156,
      "learning_rate": 0.00018170666666666668,
      "loss": -116.2097,
      "step": 6860
    },
    {
      "epoch": 0.5496,
      "grad_norm": 35.570987701416016,
      "learning_rate": 0.00018168,
      "loss": -116.6985,
      "step": 6870
    },
    {
      "epoch": 0.5504,
      "grad_norm": 34.70112609863281,
      "learning_rate": 0.00018165333333333334,
      "loss": -116.0671,
      "step": 6880
    },
    {
      "epoch": 0.5512,
      "grad_norm": 41.29144287109375,
      "learning_rate": 0.00018162666666666667,
      "loss": -117.6045,
      "step": 6890
    },
    {
      "epoch": 0.552,
      "grad_norm": 59.58719253540039,
      "learning_rate": 0.00018160000000000002,
      "loss": -117.2098,
      "step": 6900
    },
    {
      "epoch": 0.5528,
      "grad_norm": 47.132362365722656,
      "learning_rate": 0.00018157333333333335,
      "loss": -117.4133,
      "step": 6910
    },
    {
      "epoch": 0.5536,
      "grad_norm": 47.70840835571289,
      "learning_rate": 0.00018154666666666665,
      "loss": -116.918,
      "step": 6920
    },
    {
      "epoch": 0.5544,
      "grad_norm": 40.254676818847656,
      "learning_rate": 0.00018152,
      "loss": -117.007,
      "step": 6930
    },
    {
      "epoch": 0.5552,
      "grad_norm": 41.702632904052734,
      "learning_rate": 0.00018149333333333334,
      "loss": -117.749,
      "step": 6940
    },
    {
      "epoch": 0.556,
      "grad_norm": 57.81248474121094,
      "learning_rate": 0.00018146666666666667,
      "loss": -116.3017,
      "step": 6950
    },
    {
      "epoch": 0.5568,
      "grad_norm": 48.701385498046875,
      "learning_rate": 0.00018144,
      "loss": -116.3376,
      "step": 6960
    },
    {
      "epoch": 0.5576,
      "grad_norm": 43.65536880493164,
      "learning_rate": 0.00018141333333333335,
      "loss": -116.826,
      "step": 6970
    },
    {
      "epoch": 0.5584,
      "grad_norm": 34.90165328979492,
      "learning_rate": 0.00018138666666666668,
      "loss": -116.9659,
      "step": 6980
    },
    {
      "epoch": 0.5592,
      "grad_norm": 32.42364501953125,
      "learning_rate": 0.00018136,
      "loss": -117.0347,
      "step": 6990
    },
    {
      "epoch": 0.56,
      "grad_norm": 32.579349517822266,
      "learning_rate": 0.00018133333333333334,
      "loss": -116.7354,
      "step": 7000
    },
    {
      "epoch": 0.5608,
      "grad_norm": 42.064395904541016,
      "learning_rate": 0.00018130666666666667,
      "loss": -114.9828,
      "step": 7010
    },
    {
      "epoch": 0.5616,
      "grad_norm": 42.948299407958984,
      "learning_rate": 0.00018128,
      "loss": -117.4066,
      "step": 7020
    },
    {
      "epoch": 0.5624,
      "grad_norm": 46.69017791748047,
      "learning_rate": 0.00018125333333333333,
      "loss": -117.4523,
      "step": 7030
    },
    {
      "epoch": 0.5632,
      "grad_norm": 37.10026550292969,
      "learning_rate": 0.00018122666666666668,
      "loss": -117.9312,
      "step": 7040
    },
    {
      "epoch": 0.564,
      "grad_norm": 46.52939224243164,
      "learning_rate": 0.0001812,
      "loss": -117.4407,
      "step": 7050
    },
    {
      "epoch": 0.5648,
      "grad_norm": 55.520957946777344,
      "learning_rate": 0.00018117333333333334,
      "loss": -116.5058,
      "step": 7060
    },
    {
      "epoch": 0.5656,
      "grad_norm": 44.18156433105469,
      "learning_rate": 0.00018114666666666667,
      "loss": -116.0443,
      "step": 7070
    },
    {
      "epoch": 0.5664,
      "grad_norm": 38.81914520263672,
      "learning_rate": 0.00018112,
      "loss": -116.2994,
      "step": 7080
    },
    {
      "epoch": 0.5672,
      "grad_norm": 44.79676818847656,
      "learning_rate": 0.00018109333333333333,
      "loss": -116.6726,
      "step": 7090
    },
    {
      "epoch": 0.568,
      "grad_norm": 60.41757583618164,
      "learning_rate": 0.00018106666666666669,
      "loss": -116.6289,
      "step": 7100
    },
    {
      "epoch": 0.5688,
      "grad_norm": 39.87009811401367,
      "learning_rate": 0.00018104000000000001,
      "loss": -116.0777,
      "step": 7110
    },
    {
      "epoch": 0.5696,
      "grad_norm": 45.78936767578125,
      "learning_rate": 0.00018101333333333334,
      "loss": -116.6606,
      "step": 7120
    },
    {
      "epoch": 0.5704,
      "grad_norm": 35.33668899536133,
      "learning_rate": 0.00018098666666666667,
      "loss": -116.3044,
      "step": 7130
    },
    {
      "epoch": 0.5712,
      "grad_norm": 48.87227249145508,
      "learning_rate": 0.00018096000000000003,
      "loss": -117.4768,
      "step": 7140
    },
    {
      "epoch": 0.572,
      "grad_norm": 41.416175842285156,
      "learning_rate": 0.00018093333333333333,
      "loss": -115.8566,
      "step": 7150
    },
    {
      "epoch": 0.5728,
      "grad_norm": 41.947967529296875,
      "learning_rate": 0.00018090666666666666,
      "loss": -116.6279,
      "step": 7160
    },
    {
      "epoch": 0.5736,
      "grad_norm": 43.00808334350586,
      "learning_rate": 0.00018088000000000002,
      "loss": -115.8292,
      "step": 7170
    },
    {
      "epoch": 0.5744,
      "grad_norm": 51.22386169433594,
      "learning_rate": 0.00018085333333333335,
      "loss": -116.9219,
      "step": 7180
    },
    {
      "epoch": 0.5752,
      "grad_norm": 39.6005859375,
      "learning_rate": 0.00018082666666666667,
      "loss": -117.1109,
      "step": 7190
    },
    {
      "epoch": 0.576,
      "grad_norm": 41.917884826660156,
      "learning_rate": 0.0001808,
      "loss": -117.3129,
      "step": 7200
    },
    {
      "epoch": 0.5768,
      "grad_norm": 36.19621658325195,
      "learning_rate": 0.00018077333333333336,
      "loss": -115.5164,
      "step": 7210
    },
    {
      "epoch": 0.5776,
      "grad_norm": 44.8084602355957,
      "learning_rate": 0.0001807466666666667,
      "loss": -115.4558,
      "step": 7220
    },
    {
      "epoch": 0.5784,
      "grad_norm": 58.22062301635742,
      "learning_rate": 0.00018072,
      "loss": -116.3725,
      "step": 7230
    },
    {
      "epoch": 0.5792,
      "grad_norm": 57.924468994140625,
      "learning_rate": 0.00018069333333333335,
      "loss": -116.0147,
      "step": 7240
    },
    {
      "epoch": 0.58,
      "grad_norm": 63.88901901245117,
      "learning_rate": 0.00018066666666666668,
      "loss": -115.3961,
      "step": 7250
    },
    {
      "epoch": 0.5808,
      "grad_norm": 32.50226974487305,
      "learning_rate": 0.00018064,
      "loss": -115.9636,
      "step": 7260
    },
    {
      "epoch": 0.5816,
      "grad_norm": 35.54066467285156,
      "learning_rate": 0.00018061333333333333,
      "loss": -117.2015,
      "step": 7270
    },
    {
      "epoch": 0.5824,
      "grad_norm": 46.701759338378906,
      "learning_rate": 0.0001805866666666667,
      "loss": -117.1544,
      "step": 7280
    },
    {
      "epoch": 0.5832,
      "grad_norm": 45.865230560302734,
      "learning_rate": 0.00018056000000000002,
      "loss": -117.0609,
      "step": 7290
    },
    {
      "epoch": 0.584,
      "grad_norm": 46.22263717651367,
      "learning_rate": 0.00018053333333333332,
      "loss": -118.0108,
      "step": 7300
    },
    {
      "epoch": 0.5848,
      "grad_norm": 43.880123138427734,
      "learning_rate": 0.00018050666666666668,
      "loss": -115.7819,
      "step": 7310
    },
    {
      "epoch": 0.5856,
      "grad_norm": 47.39733123779297,
      "learning_rate": 0.00018048,
      "loss": -117.6116,
      "step": 7320
    },
    {
      "epoch": 0.5864,
      "grad_norm": 47.9794807434082,
      "learning_rate": 0.00018045333333333334,
      "loss": -116.6503,
      "step": 7330
    },
    {
      "epoch": 0.5872,
      "grad_norm": 35.93383026123047,
      "learning_rate": 0.00018042666666666666,
      "loss": -116.8597,
      "step": 7340
    },
    {
      "epoch": 0.588,
      "grad_norm": 46.510955810546875,
      "learning_rate": 0.00018040000000000002,
      "loss": -117.6313,
      "step": 7350
    },
    {
      "epoch": 0.5888,
      "grad_norm": 28.3170108795166,
      "learning_rate": 0.00018037333333333335,
      "loss": -116.5784,
      "step": 7360
    },
    {
      "epoch": 0.5896,
      "grad_norm": 38.01980209350586,
      "learning_rate": 0.00018034666666666668,
      "loss": -116.1342,
      "step": 7370
    },
    {
      "epoch": 0.5904,
      "grad_norm": 42.82265090942383,
      "learning_rate": 0.00018032,
      "loss": -117.0767,
      "step": 7380
    },
    {
      "epoch": 0.5912,
      "grad_norm": 34.74549102783203,
      "learning_rate": 0.00018029333333333334,
      "loss": -117.8968,
      "step": 7390
    },
    {
      "epoch": 0.592,
      "grad_norm": 66.24939727783203,
      "learning_rate": 0.00018026666666666667,
      "loss": -117.5779,
      "step": 7400
    },
    {
      "epoch": 0.5928,
      "grad_norm": 39.25239562988281,
      "learning_rate": 0.00018024,
      "loss": -117.2855,
      "step": 7410
    },
    {
      "epoch": 0.5936,
      "grad_norm": 36.88819122314453,
      "learning_rate": 0.00018021333333333335,
      "loss": -116.3457,
      "step": 7420
    },
    {
      "epoch": 0.5944,
      "grad_norm": 41.39708709716797,
      "learning_rate": 0.00018018666666666668,
      "loss": -117.3655,
      "step": 7430
    },
    {
      "epoch": 0.5952,
      "grad_norm": 49.122520446777344,
      "learning_rate": 0.00018016,
      "loss": -117.8375,
      "step": 7440
    },
    {
      "epoch": 0.596,
      "grad_norm": 78.77855682373047,
      "learning_rate": 0.00018013333333333334,
      "loss": -118.1002,
      "step": 7450
    },
    {
      "epoch": 0.5968,
      "grad_norm": 66.4359359741211,
      "learning_rate": 0.00018010666666666667,
      "loss": -117.0915,
      "step": 7460
    },
    {
      "epoch": 0.5976,
      "grad_norm": 36.20977783203125,
      "learning_rate": 0.00018008,
      "loss": -117.064,
      "step": 7470
    },
    {
      "epoch": 0.5984,
      "grad_norm": 36.05269241333008,
      "learning_rate": 0.00018005333333333335,
      "loss": -116.2517,
      "step": 7480
    },
    {
      "epoch": 0.5992,
      "grad_norm": 39.3780632019043,
      "learning_rate": 0.00018002666666666668,
      "loss": -116.5394,
      "step": 7490
    },
    {
      "epoch": 0.6,
      "grad_norm": 57.44963836669922,
      "learning_rate": 0.00018,
      "loss": -116.4889,
      "step": 7500
    },
    {
      "epoch": 0.6008,
      "grad_norm": 34.02549743652344,
      "learning_rate": 0.00017997333333333334,
      "loss": -116.8412,
      "step": 7510
    },
    {
      "epoch": 0.6016,
      "grad_norm": 48.246761322021484,
      "learning_rate": 0.0001799466666666667,
      "loss": -116.5046,
      "step": 7520
    },
    {
      "epoch": 0.6024,
      "grad_norm": 47.6602783203125,
      "learning_rate": 0.00017992,
      "loss": -117.1374,
      "step": 7530
    },
    {
      "epoch": 0.6032,
      "grad_norm": 46.4791145324707,
      "learning_rate": 0.00017989333333333333,
      "loss": -116.2337,
      "step": 7540
    },
    {
      "epoch": 0.604,
      "grad_norm": 46.95732498168945,
      "learning_rate": 0.00017986666666666668,
      "loss": -116.3045,
      "step": 7550
    },
    {
      "epoch": 0.6048,
      "grad_norm": 41.426795959472656,
      "learning_rate": 0.00017984,
      "loss": -116.9569,
      "step": 7560
    },
    {
      "epoch": 0.6056,
      "grad_norm": 51.146751403808594,
      "learning_rate": 0.00017981333333333334,
      "loss": -116.2387,
      "step": 7570
    },
    {
      "epoch": 0.6064,
      "grad_norm": 44.228153228759766,
      "learning_rate": 0.00017978666666666667,
      "loss": -117.5371,
      "step": 7580
    },
    {
      "epoch": 0.6072,
      "grad_norm": 48.93086624145508,
      "learning_rate": 0.00017976000000000003,
      "loss": -117.1457,
      "step": 7590
    },
    {
      "epoch": 0.608,
      "grad_norm": 33.70541763305664,
      "learning_rate": 0.00017973333333333333,
      "loss": -116.5699,
      "step": 7600
    },
    {
      "epoch": 0.6088,
      "grad_norm": 44.03140640258789,
      "learning_rate": 0.00017970666666666666,
      "loss": -117.7898,
      "step": 7610
    },
    {
      "epoch": 0.6096,
      "grad_norm": 43.485652923583984,
      "learning_rate": 0.00017968000000000001,
      "loss": -116.5696,
      "step": 7620
    },
    {
      "epoch": 0.6104,
      "grad_norm": 47.08292007446289,
      "learning_rate": 0.00017965333333333334,
      "loss": -116.7182,
      "step": 7630
    },
    {
      "epoch": 0.6112,
      "grad_norm": 35.51005172729492,
      "learning_rate": 0.00017962666666666667,
      "loss": -117.1939,
      "step": 7640
    },
    {
      "epoch": 0.612,
      "grad_norm": 34.9428596496582,
      "learning_rate": 0.0001796,
      "loss": -116.2897,
      "step": 7650
    },
    {
      "epoch": 0.6128,
      "grad_norm": 36.63413619995117,
      "learning_rate": 0.00017957333333333336,
      "loss": -116.8844,
      "step": 7660
    },
    {
      "epoch": 0.6136,
      "grad_norm": 42.40555191040039,
      "learning_rate": 0.0001795466666666667,
      "loss": -117.2214,
      "step": 7670
    },
    {
      "epoch": 0.6144,
      "grad_norm": 44.988037109375,
      "learning_rate": 0.00017952,
      "loss": -117.7959,
      "step": 7680
    },
    {
      "epoch": 0.6152,
      "grad_norm": 52.753849029541016,
      "learning_rate": 0.00017949333333333335,
      "loss": -116.485,
      "step": 7690
    },
    {
      "epoch": 0.616,
      "grad_norm": 47.51270294189453,
      "learning_rate": 0.00017946666666666667,
      "loss": -116.3552,
      "step": 7700
    },
    {
      "epoch": 0.6168,
      "grad_norm": 42.75471115112305,
      "learning_rate": 0.00017944,
      "loss": -117.8938,
      "step": 7710
    },
    {
      "epoch": 0.6176,
      "grad_norm": 29.290790557861328,
      "learning_rate": 0.00017941333333333333,
      "loss": -117.4619,
      "step": 7720
    },
    {
      "epoch": 0.6184,
      "grad_norm": 50.05339813232422,
      "learning_rate": 0.0001793866666666667,
      "loss": -116.25,
      "step": 7730
    },
    {
      "epoch": 0.6192,
      "grad_norm": 35.6251335144043,
      "learning_rate": 0.00017936000000000002,
      "loss": -116.6741,
      "step": 7740
    },
    {
      "epoch": 0.62,
      "grad_norm": 46.092987060546875,
      "learning_rate": 0.00017933333333333332,
      "loss": -116.0879,
      "step": 7750
    },
    {
      "epoch": 0.6208,
      "grad_norm": 43.09198760986328,
      "learning_rate": 0.00017930666666666668,
      "loss": -116.9907,
      "step": 7760
    },
    {
      "epoch": 0.6216,
      "grad_norm": 45.09026336669922,
      "learning_rate": 0.00017928,
      "loss": -116.6262,
      "step": 7770
    },
    {
      "epoch": 0.6224,
      "grad_norm": 48.55646896362305,
      "learning_rate": 0.00017925333333333333,
      "loss": -117.7338,
      "step": 7780
    },
    {
      "epoch": 0.6232,
      "grad_norm": 37.72053527832031,
      "learning_rate": 0.00017922666666666666,
      "loss": -115.9088,
      "step": 7790
    },
    {
      "epoch": 0.624,
      "grad_norm": 46.23629379272461,
      "learning_rate": 0.00017920000000000002,
      "loss": -117.9448,
      "step": 7800
    },
    {
      "epoch": 0.6248,
      "grad_norm": 51.24875259399414,
      "learning_rate": 0.00017917333333333335,
      "loss": -116.4868,
      "step": 7810
    },
    {
      "epoch": 0.6256,
      "grad_norm": 45.128257751464844,
      "learning_rate": 0.00017914666666666668,
      "loss": -116.6464,
      "step": 7820
    },
    {
      "epoch": 0.6264,
      "grad_norm": 76.88504028320312,
      "learning_rate": 0.00017912,
      "loss": -116.5213,
      "step": 7830
    },
    {
      "epoch": 0.6272,
      "grad_norm": 37.46649169921875,
      "learning_rate": 0.00017909333333333334,
      "loss": -116.4843,
      "step": 7840
    },
    {
      "epoch": 0.628,
      "grad_norm": 43.57767868041992,
      "learning_rate": 0.00017906666666666666,
      "loss": -116.5198,
      "step": 7850
    },
    {
      "epoch": 0.6288,
      "grad_norm": 44.009864807128906,
      "learning_rate": 0.00017904000000000002,
      "loss": -117.3353,
      "step": 7860
    },
    {
      "epoch": 0.6296,
      "grad_norm": 35.634647369384766,
      "learning_rate": 0.00017901333333333335,
      "loss": -117.6926,
      "step": 7870
    },
    {
      "epoch": 0.6304,
      "grad_norm": 34.402427673339844,
      "learning_rate": 0.00017898666666666668,
      "loss": -116.2287,
      "step": 7880
    },
    {
      "epoch": 0.6312,
      "grad_norm": 53.60330581665039,
      "learning_rate": 0.00017896,
      "loss": -116.9334,
      "step": 7890
    },
    {
      "epoch": 0.632,
      "grad_norm": 44.69499969482422,
      "learning_rate": 0.00017893333333333336,
      "loss": -116.3592,
      "step": 7900
    },
    {
      "epoch": 0.6328,
      "grad_norm": 64.42022705078125,
      "learning_rate": 0.00017890666666666667,
      "loss": -116.9894,
      "step": 7910
    },
    {
      "epoch": 0.6336,
      "grad_norm": 43.38853073120117,
      "learning_rate": 0.00017888,
      "loss": -115.3559,
      "step": 7920
    },
    {
      "epoch": 0.6344,
      "grad_norm": 33.260589599609375,
      "learning_rate": 0.00017885333333333335,
      "loss": -117.6581,
      "step": 7930
    },
    {
      "epoch": 0.6352,
      "grad_norm": 40.554141998291016,
      "learning_rate": 0.00017882666666666668,
      "loss": -117.6118,
      "step": 7940
    },
    {
      "epoch": 0.636,
      "grad_norm": 51.8065185546875,
      "learning_rate": 0.0001788,
      "loss": -115.2736,
      "step": 7950
    },
    {
      "epoch": 0.6368,
      "grad_norm": 47.713802337646484,
      "learning_rate": 0.00017877333333333334,
      "loss": -116.9253,
      "step": 7960
    },
    {
      "epoch": 0.6376,
      "grad_norm": 50.183555603027344,
      "learning_rate": 0.0001787466666666667,
      "loss": -118.1005,
      "step": 7970
    },
    {
      "epoch": 0.6384,
      "grad_norm": 37.070091247558594,
      "learning_rate": 0.00017872,
      "loss": -116.5021,
      "step": 7980
    },
    {
      "epoch": 0.6392,
      "grad_norm": 54.4720573425293,
      "learning_rate": 0.00017869333333333333,
      "loss": -118.0732,
      "step": 7990
    },
    {
      "epoch": 0.64,
      "grad_norm": 38.36760330200195,
      "learning_rate": 0.00017866666666666668,
      "loss": -116.0683,
      "step": 8000
    },
    {
      "epoch": 0.6408,
      "grad_norm": 46.75566101074219,
      "learning_rate": 0.00017864,
      "loss": -117.4946,
      "step": 8010
    },
    {
      "epoch": 0.6416,
      "grad_norm": 46.86385726928711,
      "learning_rate": 0.00017861333333333334,
      "loss": -116.7693,
      "step": 8020
    },
    {
      "epoch": 0.6424,
      "grad_norm": 39.511802673339844,
      "learning_rate": 0.00017858666666666667,
      "loss": -116.5529,
      "step": 8030
    },
    {
      "epoch": 0.6432,
      "grad_norm": 38.33841323852539,
      "learning_rate": 0.00017856000000000003,
      "loss": -115.3659,
      "step": 8040
    },
    {
      "epoch": 0.644,
      "grad_norm": 56.69696807861328,
      "learning_rate": 0.00017853333333333335,
      "loss": -117.1656,
      "step": 8050
    },
    {
      "epoch": 0.6448,
      "grad_norm": 41.945037841796875,
      "learning_rate": 0.00017850666666666666,
      "loss": -114.8363,
      "step": 8060
    },
    {
      "epoch": 0.6456,
      "grad_norm": 53.45170593261719,
      "learning_rate": 0.00017848,
      "loss": -115.7865,
      "step": 8070
    },
    {
      "epoch": 0.6464,
      "grad_norm": 59.19235610961914,
      "learning_rate": 0.00017845333333333334,
      "loss": -117.3003,
      "step": 8080
    },
    {
      "epoch": 0.6472,
      "grad_norm": 51.13331604003906,
      "learning_rate": 0.00017842666666666667,
      "loss": -115.6961,
      "step": 8090
    },
    {
      "epoch": 0.648,
      "grad_norm": 49.789833068847656,
      "learning_rate": 0.0001784,
      "loss": -116.2824,
      "step": 8100
    },
    {
      "epoch": 0.6488,
      "grad_norm": 45.11083221435547,
      "learning_rate": 0.00017837333333333336,
      "loss": -117.308,
      "step": 8110
    },
    {
      "epoch": 0.6496,
      "grad_norm": 44.691226959228516,
      "learning_rate": 0.00017834666666666668,
      "loss": -115.8684,
      "step": 8120
    },
    {
      "epoch": 0.6504,
      "grad_norm": 41.87520980834961,
      "learning_rate": 0.00017832,
      "loss": -117.5663,
      "step": 8130
    },
    {
      "epoch": 0.6512,
      "grad_norm": 32.527652740478516,
      "learning_rate": 0.00017829333333333334,
      "loss": -116.7983,
      "step": 8140
    },
    {
      "epoch": 0.652,
      "grad_norm": 38.96406936645508,
      "learning_rate": 0.00017826666666666667,
      "loss": -117.0484,
      "step": 8150
    },
    {
      "epoch": 0.6528,
      "grad_norm": 32.98625946044922,
      "learning_rate": 0.00017824,
      "loss": -117.2295,
      "step": 8160
    },
    {
      "epoch": 0.6536,
      "grad_norm": 56.24735641479492,
      "learning_rate": 0.00017821333333333333,
      "loss": -116.7363,
      "step": 8170
    },
    {
      "epoch": 0.6544,
      "grad_norm": 61.56997299194336,
      "learning_rate": 0.00017818666666666669,
      "loss": -117.373,
      "step": 8180
    },
    {
      "epoch": 0.6552,
      "grad_norm": 55.31660079956055,
      "learning_rate": 0.00017816000000000002,
      "loss": -116.2382,
      "step": 8190
    },
    {
      "epoch": 0.656,
      "grad_norm": 50.214683532714844,
      "learning_rate": 0.00017813333333333334,
      "loss": -116.5147,
      "step": 8200
    },
    {
      "epoch": 0.6568,
      "grad_norm": 27.964555740356445,
      "learning_rate": 0.00017810666666666667,
      "loss": -116.3624,
      "step": 8210
    },
    {
      "epoch": 0.6576,
      "grad_norm": 44.756526947021484,
      "learning_rate": 0.00017808,
      "loss": -116.5015,
      "step": 8220
    },
    {
      "epoch": 0.6584,
      "grad_norm": 44.36581802368164,
      "learning_rate": 0.00017805333333333333,
      "loss": -115.8949,
      "step": 8230
    },
    {
      "epoch": 0.6592,
      "grad_norm": 35.15604019165039,
      "learning_rate": 0.0001780266666666667,
      "loss": -116.1405,
      "step": 8240
    },
    {
      "epoch": 0.66,
      "grad_norm": 51.24054718017578,
      "learning_rate": 0.00017800000000000002,
      "loss": -116.5141,
      "step": 8250
    },
    {
      "epoch": 0.6608,
      "grad_norm": 34.18670654296875,
      "learning_rate": 0.00017797333333333335,
      "loss": -115.8745,
      "step": 8260
    },
    {
      "epoch": 0.6616,
      "grad_norm": 46.126312255859375,
      "learning_rate": 0.00017794666666666668,
      "loss": -115.6763,
      "step": 8270
    },
    {
      "epoch": 0.6624,
      "grad_norm": 34.94189453125,
      "learning_rate": 0.00017792,
      "loss": -118.0447,
      "step": 8280
    },
    {
      "epoch": 0.6632,
      "grad_norm": 31.322267532348633,
      "learning_rate": 0.00017789333333333333,
      "loss": -116.9949,
      "step": 8290
    },
    {
      "epoch": 0.664,
      "grad_norm": 35.45851135253906,
      "learning_rate": 0.00017786666666666666,
      "loss": -117.3494,
      "step": 8300
    },
    {
      "epoch": 0.6648,
      "grad_norm": 35.05217742919922,
      "learning_rate": 0.00017784000000000002,
      "loss": -117.5676,
      "step": 8310
    },
    {
      "epoch": 0.6656,
      "grad_norm": 48.82609558105469,
      "learning_rate": 0.00017781333333333335,
      "loss": -116.975,
      "step": 8320
    },
    {
      "epoch": 0.6664,
      "grad_norm": 40.97316360473633,
      "learning_rate": 0.00017778666666666668,
      "loss": -116.6808,
      "step": 8330
    },
    {
      "epoch": 0.6672,
      "grad_norm": 37.475433349609375,
      "learning_rate": 0.00017776,
      "loss": -116.7773,
      "step": 8340
    },
    {
      "epoch": 0.668,
      "grad_norm": 32.0035285949707,
      "learning_rate": 0.00017773333333333336,
      "loss": -116.5331,
      "step": 8350
    },
    {
      "epoch": 0.6688,
      "grad_norm": 45.81812286376953,
      "learning_rate": 0.00017770666666666666,
      "loss": -116.0437,
      "step": 8360
    },
    {
      "epoch": 0.6696,
      "grad_norm": 32.23917007446289,
      "learning_rate": 0.00017768,
      "loss": -118.425,
      "step": 8370
    },
    {
      "epoch": 0.6704,
      "grad_norm": 34.261878967285156,
      "learning_rate": 0.00017765333333333335,
      "loss": -116.5344,
      "step": 8380
    },
    {
      "epoch": 0.6712,
      "grad_norm": 49.778499603271484,
      "learning_rate": 0.00017762666666666668,
      "loss": -116.2209,
      "step": 8390
    },
    {
      "epoch": 0.672,
      "grad_norm": 44.060821533203125,
      "learning_rate": 0.0001776,
      "loss": -115.2077,
      "step": 8400
    },
    {
      "epoch": 0.6728,
      "grad_norm": 39.106258392333984,
      "learning_rate": 0.00017757333333333334,
      "loss": -117.6046,
      "step": 8410
    },
    {
      "epoch": 0.6736,
      "grad_norm": 35.700721740722656,
      "learning_rate": 0.0001775466666666667,
      "loss": -116.6779,
      "step": 8420
    },
    {
      "epoch": 0.6744,
      "grad_norm": 38.49739456176758,
      "learning_rate": 0.00017752,
      "loss": -116.8539,
      "step": 8430
    },
    {
      "epoch": 0.6752,
      "grad_norm": 36.63945388793945,
      "learning_rate": 0.00017749333333333332,
      "loss": -117.2094,
      "step": 8440
    },
    {
      "epoch": 0.676,
      "grad_norm": 179.92340087890625,
      "learning_rate": 0.00017746666666666668,
      "loss": -117.8736,
      "step": 8450
    },
    {
      "epoch": 0.6768,
      "grad_norm": 43.007362365722656,
      "learning_rate": 0.00017744,
      "loss": -116.5265,
      "step": 8460
    },
    {
      "epoch": 0.6776,
      "grad_norm": 37.76871109008789,
      "learning_rate": 0.00017741333333333334,
      "loss": -116.0239,
      "step": 8470
    },
    {
      "epoch": 0.6784,
      "grad_norm": 54.19415283203125,
      "learning_rate": 0.00017738666666666667,
      "loss": -117.2823,
      "step": 8480
    },
    {
      "epoch": 0.6792,
      "grad_norm": 45.628604888916016,
      "learning_rate": 0.00017736000000000002,
      "loss": -117.2208,
      "step": 8490
    },
    {
      "epoch": 0.68,
      "grad_norm": 37.583499908447266,
      "learning_rate": 0.00017733333333333335,
      "loss": -116.9004,
      "step": 8500
    },
    {
      "epoch": 0.6808,
      "grad_norm": 47.783626556396484,
      "learning_rate": 0.00017730666666666665,
      "loss": -117.8704,
      "step": 8510
    },
    {
      "epoch": 0.6816,
      "grad_norm": 37.981529235839844,
      "learning_rate": 0.00017728,
      "loss": -117.1079,
      "step": 8520
    },
    {
      "epoch": 0.6824,
      "grad_norm": 41.569759368896484,
      "learning_rate": 0.00017725333333333334,
      "loss": -117.124,
      "step": 8530
    },
    {
      "epoch": 0.6832,
      "grad_norm": 44.38874816894531,
      "learning_rate": 0.00017722666666666667,
      "loss": -116.388,
      "step": 8540
    },
    {
      "epoch": 0.684,
      "grad_norm": 30.202728271484375,
      "learning_rate": 0.0001772,
      "loss": -117.1989,
      "step": 8550
    },
    {
      "epoch": 0.6848,
      "grad_norm": 35.20930862426758,
      "learning_rate": 0.00017717333333333335,
      "loss": -115.8835,
      "step": 8560
    },
    {
      "epoch": 0.6856,
      "grad_norm": 36.52688217163086,
      "learning_rate": 0.00017714666666666668,
      "loss": -118.0904,
      "step": 8570
    },
    {
      "epoch": 0.6864,
      "grad_norm": 34.76478958129883,
      "learning_rate": 0.00017712,
      "loss": -117.7977,
      "step": 8580
    },
    {
      "epoch": 0.6872,
      "grad_norm": 40.544227600097656,
      "learning_rate": 0.00017709333333333334,
      "loss": -115.5086,
      "step": 8590
    },
    {
      "epoch": 0.688,
      "grad_norm": 38.520450592041016,
      "learning_rate": 0.00017706666666666667,
      "loss": -117.872,
      "step": 8600
    },
    {
      "epoch": 0.6888,
      "grad_norm": 130.09129333496094,
      "learning_rate": 0.00017704,
      "loss": -118.113,
      "step": 8610
    },
    {
      "epoch": 0.6896,
      "grad_norm": 43.78343200683594,
      "learning_rate": 0.00017701333333333336,
      "loss": -116.0582,
      "step": 8620
    },
    {
      "epoch": 0.6904,
      "grad_norm": 40.78398513793945,
      "learning_rate": 0.00017698666666666668,
      "loss": -116.8421,
      "step": 8630
    },
    {
      "epoch": 0.6912,
      "grad_norm": 33.13228225708008,
      "learning_rate": 0.00017696,
      "loss": -114.556,
      "step": 8640
    },
    {
      "epoch": 0.692,
      "grad_norm": 39.38179016113281,
      "learning_rate": 0.00017693333333333334,
      "loss": -116.4203,
      "step": 8650
    },
    {
      "epoch": 0.6928,
      "grad_norm": 54.1925163269043,
      "learning_rate": 0.00017690666666666667,
      "loss": -116.6408,
      "step": 8660
    },
    {
      "epoch": 0.6936,
      "grad_norm": 35.46259307861328,
      "learning_rate": 0.00017688,
      "loss": -117.6413,
      "step": 8670
    },
    {
      "epoch": 0.6944,
      "grad_norm": 40.612125396728516,
      "learning_rate": 0.00017685333333333333,
      "loss": -117.3251,
      "step": 8680
    },
    {
      "epoch": 0.6952,
      "grad_norm": 32.79493713378906,
      "learning_rate": 0.00017682666666666669,
      "loss": -116.3192,
      "step": 8690
    },
    {
      "epoch": 0.696,
      "grad_norm": 34.6302490234375,
      "learning_rate": 0.00017680000000000001,
      "loss": -117.6763,
      "step": 8700
    },
    {
      "epoch": 0.6968,
      "grad_norm": 37.50491714477539,
      "learning_rate": 0.00017677333333333334,
      "loss": -116.9201,
      "step": 8710
    },
    {
      "epoch": 0.6976,
      "grad_norm": 36.83292770385742,
      "learning_rate": 0.00017674666666666667,
      "loss": -117.813,
      "step": 8720
    },
    {
      "epoch": 0.6984,
      "grad_norm": 42.22932052612305,
      "learning_rate": 0.00017672000000000003,
      "loss": -116.1193,
      "step": 8730
    },
    {
      "epoch": 0.6992,
      "grad_norm": 36.96061325073242,
      "learning_rate": 0.00017669333333333333,
      "loss": -116.2352,
      "step": 8740
    },
    {
      "epoch": 0.7,
      "grad_norm": 36.22679901123047,
      "learning_rate": 0.00017666666666666666,
      "loss": -115.5243,
      "step": 8750
    },
    {
      "epoch": 0.7008,
      "grad_norm": 40.591957092285156,
      "learning_rate": 0.00017664000000000002,
      "loss": -117.5236,
      "step": 8760
    },
    {
      "epoch": 0.7016,
      "grad_norm": 51.056861877441406,
      "learning_rate": 0.00017661333333333335,
      "loss": -116.7633,
      "step": 8770
    },
    {
      "epoch": 0.7024,
      "grad_norm": 36.088687896728516,
      "learning_rate": 0.00017658666666666667,
      "loss": -116.8765,
      "step": 8780
    },
    {
      "epoch": 0.7032,
      "grad_norm": 49.614891052246094,
      "learning_rate": 0.00017656,
      "loss": -118.246,
      "step": 8790
    },
    {
      "epoch": 0.704,
      "grad_norm": 32.98260498046875,
      "learning_rate": 0.00017653333333333336,
      "loss": -116.8395,
      "step": 8800
    },
    {
      "epoch": 0.7048,
      "grad_norm": 37.58303451538086,
      "learning_rate": 0.00017650666666666666,
      "loss": -116.2576,
      "step": 8810
    },
    {
      "epoch": 0.7056,
      "grad_norm": 61.086238861083984,
      "learning_rate": 0.00017648,
      "loss": -116.5773,
      "step": 8820
    },
    {
      "epoch": 0.7064,
      "grad_norm": 31.73227310180664,
      "learning_rate": 0.00017645333333333335,
      "loss": -117.3061,
      "step": 8830
    },
    {
      "epoch": 0.7072,
      "grad_norm": 27.5671329498291,
      "learning_rate": 0.00017642666666666668,
      "loss": -117.2918,
      "step": 8840
    },
    {
      "epoch": 0.708,
      "grad_norm": 156.36019897460938,
      "learning_rate": 0.0001764,
      "loss": -117.2204,
      "step": 8850
    },
    {
      "epoch": 0.7088,
      "grad_norm": 48.73027801513672,
      "learning_rate": 0.00017637333333333333,
      "loss": -116.2298,
      "step": 8860
    },
    {
      "epoch": 0.7096,
      "grad_norm": 33.24005126953125,
      "learning_rate": 0.0001763466666666667,
      "loss": -117.226,
      "step": 8870
    },
    {
      "epoch": 0.7104,
      "grad_norm": 36.19174575805664,
      "learning_rate": 0.00017632000000000002,
      "loss": -115.1355,
      "step": 8880
    },
    {
      "epoch": 0.7112,
      "grad_norm": 32.27180862426758,
      "learning_rate": 0.00017629333333333332,
      "loss": -117.7675,
      "step": 8890
    },
    {
      "epoch": 0.712,
      "grad_norm": 42.77313995361328,
      "learning_rate": 0.00017626666666666668,
      "loss": -117.2102,
      "step": 8900
    },
    {
      "epoch": 0.7128,
      "grad_norm": 45.13800048828125,
      "learning_rate": 0.00017624,
      "loss": -116.2951,
      "step": 8910
    },
    {
      "epoch": 0.7136,
      "grad_norm": 62.660648345947266,
      "learning_rate": 0.00017621333333333334,
      "loss": -116.3383,
      "step": 8920
    },
    {
      "epoch": 0.7144,
      "grad_norm": 47.011653900146484,
      "learning_rate": 0.00017618666666666666,
      "loss": -117.1264,
      "step": 8930
    },
    {
      "epoch": 0.7152,
      "grad_norm": 33.322303771972656,
      "learning_rate": 0.00017616000000000002,
      "loss": -117.647,
      "step": 8940
    },
    {
      "epoch": 0.716,
      "grad_norm": 41.159523010253906,
      "learning_rate": 0.00017613333333333335,
      "loss": -117.6452,
      "step": 8950
    },
    {
      "epoch": 0.7168,
      "grad_norm": 41.147586822509766,
      "learning_rate": 0.00017610666666666665,
      "loss": -116.6981,
      "step": 8960
    },
    {
      "epoch": 0.7176,
      "grad_norm": 47.418766021728516,
      "learning_rate": 0.00017608,
      "loss": -116.6269,
      "step": 8970
    },
    {
      "epoch": 0.7184,
      "grad_norm": 41.500022888183594,
      "learning_rate": 0.00017605333333333334,
      "loss": -116.465,
      "step": 8980
    },
    {
      "epoch": 0.7192,
      "grad_norm": 28.2304744720459,
      "learning_rate": 0.00017602666666666667,
      "loss": -115.6281,
      "step": 8990
    },
    {
      "epoch": 0.72,
      "grad_norm": 37.49712371826172,
      "learning_rate": 0.00017600000000000002,
      "loss": -116.0471,
      "step": 9000
    },
    {
      "epoch": 0.7208,
      "grad_norm": 32.3600959777832,
      "learning_rate": 0.00017597333333333335,
      "loss": -117.6102,
      "step": 9010
    },
    {
      "epoch": 0.7216,
      "grad_norm": 32.07460403442383,
      "learning_rate": 0.00017594666666666668,
      "loss": -116.0979,
      "step": 9020
    },
    {
      "epoch": 0.7224,
      "grad_norm": 42.53641891479492,
      "learning_rate": 0.00017592,
      "loss": -116.4295,
      "step": 9030
    },
    {
      "epoch": 0.7232,
      "grad_norm": 43.53851318359375,
      "learning_rate": 0.00017589333333333334,
      "loss": -117.6577,
      "step": 9040
    },
    {
      "epoch": 0.724,
      "grad_norm": 45.58750915527344,
      "learning_rate": 0.00017586666666666667,
      "loss": -116.8985,
      "step": 9050
    },
    {
      "epoch": 0.7248,
      "grad_norm": 36.50088119506836,
      "learning_rate": 0.00017584,
      "loss": -117.1226,
      "step": 9060
    },
    {
      "epoch": 0.7256,
      "grad_norm": 35.25590896606445,
      "learning_rate": 0.00017581333333333335,
      "loss": -117.053,
      "step": 9070
    },
    {
      "epoch": 0.7264,
      "grad_norm": 46.611083984375,
      "learning_rate": 0.00017578666666666668,
      "loss": -117.6817,
      "step": 9080
    },
    {
      "epoch": 0.7272,
      "grad_norm": 37.277313232421875,
      "learning_rate": 0.00017576,
      "loss": -118.4638,
      "step": 9090
    },
    {
      "epoch": 0.728,
      "grad_norm": 38.4487419128418,
      "learning_rate": 0.00017573333333333334,
      "loss": -117.8905,
      "step": 9100
    },
    {
      "epoch": 0.7288,
      "grad_norm": 42.57809066772461,
      "learning_rate": 0.0001757066666666667,
      "loss": -117.9848,
      "step": 9110
    },
    {
      "epoch": 0.7296,
      "grad_norm": 38.56827163696289,
      "learning_rate": 0.00017568,
      "loss": -116.8407,
      "step": 9120
    },
    {
      "epoch": 0.7304,
      "grad_norm": 26.96900177001953,
      "learning_rate": 0.00017565333333333333,
      "loss": -117.8851,
      "step": 9130
    },
    {
      "epoch": 0.7312,
      "grad_norm": 36.88459396362305,
      "learning_rate": 0.00017562666666666668,
      "loss": -115.8911,
      "step": 9140
    },
    {
      "epoch": 0.732,
      "grad_norm": 34.27349090576172,
      "learning_rate": 0.0001756,
      "loss": -117.159,
      "step": 9150
    },
    {
      "epoch": 0.7328,
      "grad_norm": 34.07572937011719,
      "learning_rate": 0.00017557333333333334,
      "loss": -116.1964,
      "step": 9160
    },
    {
      "epoch": 0.7336,
      "grad_norm": 46.15312194824219,
      "learning_rate": 0.00017554666666666667,
      "loss": -117.7465,
      "step": 9170
    },
    {
      "epoch": 0.7344,
      "grad_norm": 36.199012756347656,
      "learning_rate": 0.00017552000000000003,
      "loss": -116.5112,
      "step": 9180
    },
    {
      "epoch": 0.7352,
      "grad_norm": 28.545446395874023,
      "learning_rate": 0.00017549333333333333,
      "loss": -118.1452,
      "step": 9190
    },
    {
      "epoch": 0.736,
      "grad_norm": 32.911617279052734,
      "learning_rate": 0.00017546666666666666,
      "loss": -116.1325,
      "step": 9200
    },
    {
      "epoch": 0.7368,
      "grad_norm": 37.589996337890625,
      "learning_rate": 0.00017544000000000001,
      "loss": -117.8555,
      "step": 9210
    },
    {
      "epoch": 0.7376,
      "grad_norm": 43.62628173828125,
      "learning_rate": 0.00017541333333333334,
      "loss": -116.2054,
      "step": 9220
    },
    {
      "epoch": 0.7384,
      "grad_norm": 40.8306999206543,
      "learning_rate": 0.00017538666666666667,
      "loss": -116.0067,
      "step": 9230
    },
    {
      "epoch": 0.7392,
      "grad_norm": 39.630210876464844,
      "learning_rate": 0.00017536,
      "loss": -116.7189,
      "step": 9240
    },
    {
      "epoch": 0.74,
      "grad_norm": 37.813682556152344,
      "learning_rate": 0.00017533333333333336,
      "loss": -118.126,
      "step": 9250
    },
    {
      "epoch": 0.7408,
      "grad_norm": 29.68982696533203,
      "learning_rate": 0.0001753066666666667,
      "loss": -117.0832,
      "step": 9260
    },
    {
      "epoch": 0.7416,
      "grad_norm": 34.11018753051758,
      "learning_rate": 0.00017528,
      "loss": -114.9562,
      "step": 9270
    },
    {
      "epoch": 0.7424,
      "grad_norm": 38.50886154174805,
      "learning_rate": 0.00017525333333333334,
      "loss": -115.7195,
      "step": 9280
    },
    {
      "epoch": 0.7432,
      "grad_norm": 43.83024215698242,
      "learning_rate": 0.00017522666666666667,
      "loss": -117.0039,
      "step": 9290
    },
    {
      "epoch": 0.744,
      "grad_norm": 44.01445007324219,
      "learning_rate": 0.0001752,
      "loss": -117.5827,
      "step": 9300
    },
    {
      "epoch": 0.7448,
      "grad_norm": 42.317962646484375,
      "learning_rate": 0.00017517333333333333,
      "loss": -117.1248,
      "step": 9310
    },
    {
      "epoch": 0.7456,
      "grad_norm": 41.37525939941406,
      "learning_rate": 0.0001751466666666667,
      "loss": -117.1991,
      "step": 9320
    },
    {
      "epoch": 0.7464,
      "grad_norm": 35.179283142089844,
      "learning_rate": 0.00017512000000000002,
      "loss": -116.7185,
      "step": 9330
    },
    {
      "epoch": 0.7472,
      "grad_norm": 29.872228622436523,
      "learning_rate": 0.00017509333333333332,
      "loss": -117.3814,
      "step": 9340
    },
    {
      "epoch": 0.748,
      "grad_norm": 36.73237609863281,
      "learning_rate": 0.00017506666666666668,
      "loss": -117.6131,
      "step": 9350
    },
    {
      "epoch": 0.7488,
      "grad_norm": 39.22553253173828,
      "learning_rate": 0.00017504,
      "loss": -116.827,
      "step": 9360
    },
    {
      "epoch": 0.7496,
      "grad_norm": 35.09416580200195,
      "learning_rate": 0.00017501333333333333,
      "loss": -116.2173,
      "step": 9370
    },
    {
      "epoch": 0.7504,
      "grad_norm": 39.46403121948242,
      "learning_rate": 0.0001749866666666667,
      "loss": -116.7392,
      "step": 9380
    },
    {
      "epoch": 0.7512,
      "grad_norm": 51.719024658203125,
      "learning_rate": 0.00017496000000000002,
      "loss": -117.0319,
      "step": 9390
    },
    {
      "epoch": 0.752,
      "grad_norm": 32.57200622558594,
      "learning_rate": 0.00017493333333333335,
      "loss": -115.1694,
      "step": 9400
    },
    {
      "epoch": 0.7528,
      "grad_norm": 45.162742614746094,
      "learning_rate": 0.00017490666666666668,
      "loss": -115.714,
      "step": 9410
    },
    {
      "epoch": 0.7536,
      "grad_norm": 37.24314880371094,
      "learning_rate": 0.00017488,
      "loss": -117.3681,
      "step": 9420
    },
    {
      "epoch": 0.7544,
      "grad_norm": 40.98931121826172,
      "learning_rate": 0.00017485333333333334,
      "loss": -117.7077,
      "step": 9430
    },
    {
      "epoch": 0.7552,
      "grad_norm": 37.85551834106445,
      "learning_rate": 0.00017482666666666666,
      "loss": -116.4321,
      "step": 9440
    },
    {
      "epoch": 0.756,
      "grad_norm": 56.13038635253906,
      "learning_rate": 0.00017480000000000002,
      "loss": -118.009,
      "step": 9450
    },
    {
      "epoch": 0.7568,
      "grad_norm": 40.8016357421875,
      "learning_rate": 0.00017477333333333335,
      "loss": -119.1305,
      "step": 9460
    },
    {
      "epoch": 0.7576,
      "grad_norm": 30.534414291381836,
      "learning_rate": 0.00017474666666666668,
      "loss": -117.2079,
      "step": 9470
    },
    {
      "epoch": 0.7584,
      "grad_norm": 37.898746490478516,
      "learning_rate": 0.00017472,
      "loss": -117.0391,
      "step": 9480
    },
    {
      "epoch": 0.7592,
      "grad_norm": 42.81519317626953,
      "learning_rate": 0.00017469333333333334,
      "loss": -116.5269,
      "step": 9490
    },
    {
      "epoch": 0.76,
      "grad_norm": 35.926368713378906,
      "learning_rate": 0.00017466666666666667,
      "loss": -116.5483,
      "step": 9500
    },
    {
      "epoch": 0.7608,
      "grad_norm": 35.566001892089844,
      "learning_rate": 0.00017464,
      "loss": -117.7541,
      "step": 9510
    },
    {
      "epoch": 0.7616,
      "grad_norm": 29.93995475769043,
      "learning_rate": 0.00017461333333333335,
      "loss": -117.3806,
      "step": 9520
    },
    {
      "epoch": 0.7624,
      "grad_norm": 28.539628982543945,
      "learning_rate": 0.00017458666666666668,
      "loss": -116.825,
      "step": 9530
    },
    {
      "epoch": 0.7632,
      "grad_norm": 50.70758056640625,
      "learning_rate": 0.00017456,
      "loss": -117.6176,
      "step": 9540
    },
    {
      "epoch": 0.764,
      "grad_norm": 36.3371696472168,
      "learning_rate": 0.00017453333333333334,
      "loss": -117.0015,
      "step": 9550
    },
    {
      "epoch": 0.7648,
      "grad_norm": 32.858863830566406,
      "learning_rate": 0.0001745066666666667,
      "loss": -117.6041,
      "step": 9560
    },
    {
      "epoch": 0.7656,
      "grad_norm": 42.86309051513672,
      "learning_rate": 0.00017448,
      "loss": -116.3444,
      "step": 9570
    },
    {
      "epoch": 0.7664,
      "grad_norm": 53.366912841796875,
      "learning_rate": 0.00017445333333333333,
      "loss": -116.296,
      "step": 9580
    },
    {
      "epoch": 0.7672,
      "grad_norm": 37.78923797607422,
      "learning_rate": 0.00017442666666666668,
      "loss": -117.2076,
      "step": 9590
    },
    {
      "epoch": 0.768,
      "grad_norm": 42.18913269042969,
      "learning_rate": 0.0001744,
      "loss": -117.1174,
      "step": 9600
    },
    {
      "epoch": 0.7688,
      "grad_norm": 42.92565155029297,
      "learning_rate": 0.00017437333333333334,
      "loss": -116.2135,
      "step": 9610
    },
    {
      "epoch": 0.7696,
      "grad_norm": 48.124366760253906,
      "learning_rate": 0.00017434666666666667,
      "loss": -115.8326,
      "step": 9620
    },
    {
      "epoch": 0.7704,
      "grad_norm": 36.04734420776367,
      "learning_rate": 0.00017432000000000003,
      "loss": -117.1097,
      "step": 9630
    },
    {
      "epoch": 0.7712,
      "grad_norm": 37.95924758911133,
      "learning_rate": 0.00017429333333333333,
      "loss": -117.987,
      "step": 9640
    },
    {
      "epoch": 0.772,
      "grad_norm": 37.06597137451172,
      "learning_rate": 0.00017426666666666666,
      "loss": -117.3225,
      "step": 9650
    },
    {
      "epoch": 0.7728,
      "grad_norm": 33.998260498046875,
      "learning_rate": 0.00017424,
      "loss": -117.0352,
      "step": 9660
    },
    {
      "epoch": 0.7736,
      "grad_norm": 32.9652099609375,
      "learning_rate": 0.00017421333333333334,
      "loss": -118.2414,
      "step": 9670
    },
    {
      "epoch": 0.7744,
      "grad_norm": 36.97661590576172,
      "learning_rate": 0.00017418666666666667,
      "loss": -115.7202,
      "step": 9680
    },
    {
      "epoch": 0.7752,
      "grad_norm": 39.901695251464844,
      "learning_rate": 0.00017416,
      "loss": -117.2575,
      "step": 9690
    },
    {
      "epoch": 0.776,
      "grad_norm": 57.448856353759766,
      "learning_rate": 0.00017413333333333336,
      "loss": -117.2167,
      "step": 9700
    },
    {
      "epoch": 0.7768,
      "grad_norm": 45.93378829956055,
      "learning_rate": 0.00017410666666666668,
      "loss": -118.1347,
      "step": 9710
    },
    {
      "epoch": 0.7776,
      "grad_norm": 51.501529693603516,
      "learning_rate": 0.00017408,
      "loss": -115.721,
      "step": 9720
    },
    {
      "epoch": 0.7784,
      "grad_norm": 36.28866958618164,
      "learning_rate": 0.00017405333333333334,
      "loss": -118.1411,
      "step": 9730
    },
    {
      "epoch": 0.7792,
      "grad_norm": 36.06340789794922,
      "learning_rate": 0.00017402666666666667,
      "loss": -118.0621,
      "step": 9740
    },
    {
      "epoch": 0.78,
      "grad_norm": 30.802003860473633,
      "learning_rate": 0.000174,
      "loss": -117.9844,
      "step": 9750
    },
    {
      "epoch": 0.7808,
      "grad_norm": 33.037010192871094,
      "learning_rate": 0.00017397333333333336,
      "loss": -117.3135,
      "step": 9760
    },
    {
      "epoch": 0.7816,
      "grad_norm": 50.03852844238281,
      "learning_rate": 0.00017394666666666669,
      "loss": -117.3165,
      "step": 9770
    },
    {
      "epoch": 0.7824,
      "grad_norm": 50.68558120727539,
      "learning_rate": 0.00017392000000000002,
      "loss": -117.2146,
      "step": 9780
    },
    {
      "epoch": 0.7832,
      "grad_norm": 38.66961669921875,
      "learning_rate": 0.00017389333333333334,
      "loss": -116.7565,
      "step": 9790
    },
    {
      "epoch": 0.784,
      "grad_norm": 37.93056869506836,
      "learning_rate": 0.00017386666666666667,
      "loss": -115.32,
      "step": 9800
    },
    {
      "epoch": 0.7848,
      "grad_norm": 31.89476776123047,
      "learning_rate": 0.00017384,
      "loss": -117.7201,
      "step": 9810
    },
    {
      "epoch": 0.7856,
      "grad_norm": 37.77830505371094,
      "learning_rate": 0.00017381333333333333,
      "loss": -116.4914,
      "step": 9820
    },
    {
      "epoch": 0.7864,
      "grad_norm": 33.47239685058594,
      "learning_rate": 0.0001737866666666667,
      "loss": -117.0523,
      "step": 9830
    },
    {
      "epoch": 0.7872,
      "grad_norm": 47.43251419067383,
      "learning_rate": 0.00017376000000000002,
      "loss": -117.3681,
      "step": 9840
    },
    {
      "epoch": 0.788,
      "grad_norm": 37.15602111816406,
      "learning_rate": 0.00017373333333333335,
      "loss": -117.6162,
      "step": 9850
    },
    {
      "epoch": 0.7888,
      "grad_norm": 31.92695426940918,
      "learning_rate": 0.00017370666666666668,
      "loss": -117.0347,
      "step": 9860
    },
    {
      "epoch": 0.7896,
      "grad_norm": 34.538578033447266,
      "learning_rate": 0.00017368,
      "loss": -116.8584,
      "step": 9870
    },
    {
      "epoch": 0.7904,
      "grad_norm": 44.177120208740234,
      "learning_rate": 0.00017365333333333333,
      "loss": -117.6602,
      "step": 9880
    },
    {
      "epoch": 0.7912,
      "grad_norm": 41.836727142333984,
      "learning_rate": 0.00017362666666666666,
      "loss": -117.2714,
      "step": 9890
    },
    {
      "epoch": 0.792,
      "grad_norm": 39.481483459472656,
      "learning_rate": 0.00017360000000000002,
      "loss": -118.0631,
      "step": 9900
    },
    {
      "epoch": 0.7928,
      "grad_norm": 39.051300048828125,
      "learning_rate": 0.00017357333333333335,
      "loss": -116.6002,
      "step": 9910
    },
    {
      "epoch": 0.7936,
      "grad_norm": 30.939611434936523,
      "learning_rate": 0.00017354666666666668,
      "loss": -116.9089,
      "step": 9920
    },
    {
      "epoch": 0.7944,
      "grad_norm": 41.90079116821289,
      "learning_rate": 0.00017352,
      "loss": -117.4229,
      "step": 9930
    },
    {
      "epoch": 0.7952,
      "grad_norm": 45.26663589477539,
      "learning_rate": 0.00017349333333333336,
      "loss": -117.4943,
      "step": 9940
    },
    {
      "epoch": 0.796,
      "grad_norm": 37.87739944458008,
      "learning_rate": 0.00017346666666666666,
      "loss": -117.2341,
      "step": 9950
    },
    {
      "epoch": 0.7968,
      "grad_norm": 33.9995002746582,
      "learning_rate": 0.00017344,
      "loss": -117.9946,
      "step": 9960
    },
    {
      "epoch": 0.7976,
      "grad_norm": 32.17803955078125,
      "learning_rate": 0.00017341333333333335,
      "loss": -118.1242,
      "step": 9970
    },
    {
      "epoch": 0.7984,
      "grad_norm": 33.92988967895508,
      "learning_rate": 0.00017338666666666668,
      "loss": -117.4049,
      "step": 9980
    },
    {
      "epoch": 0.7992,
      "grad_norm": 51.09221649169922,
      "learning_rate": 0.00017336,
      "loss": -116.9355,
      "step": 9990
    },
    {
      "epoch": 0.8,
      "grad_norm": 37.503170013427734,
      "learning_rate": 0.00017333333333333334,
      "loss": -117.2671,
      "step": 10000
    },
    {
      "epoch": 0.8008,
      "grad_norm": 37.772159576416016,
      "learning_rate": 0.0001733066666666667,
      "loss": -116.6958,
      "step": 10010
    },
    {
      "epoch": 0.8016,
      "grad_norm": 36.61260223388672,
      "learning_rate": 0.00017328,
      "loss": -116.3866,
      "step": 10020
    },
    {
      "epoch": 0.8024,
      "grad_norm": 39.40378189086914,
      "learning_rate": 0.00017325333333333332,
      "loss": -117.2326,
      "step": 10030
    },
    {
      "epoch": 0.8032,
      "grad_norm": 47.54624557495117,
      "learning_rate": 0.00017322666666666668,
      "loss": -117.5456,
      "step": 10040
    },
    {
      "epoch": 0.804,
      "grad_norm": 41.16878128051758,
      "learning_rate": 0.0001732,
      "loss": -117.0015,
      "step": 10050
    },
    {
      "epoch": 0.8048,
      "grad_norm": 39.90586471557617,
      "learning_rate": 0.00017317333333333334,
      "loss": -117.7439,
      "step": 10060
    },
    {
      "epoch": 0.8056,
      "grad_norm": 45.470333099365234,
      "learning_rate": 0.00017314666666666667,
      "loss": -117.4301,
      "step": 10070
    },
    {
      "epoch": 0.8064,
      "grad_norm": 43.886932373046875,
      "learning_rate": 0.00017312000000000002,
      "loss": -117.3564,
      "step": 10080
    },
    {
      "epoch": 0.8072,
      "grad_norm": 49.464515686035156,
      "learning_rate": 0.00017309333333333335,
      "loss": -117.2263,
      "step": 10090
    },
    {
      "epoch": 0.808,
      "grad_norm": 40.31156921386719,
      "learning_rate": 0.00017306666666666665,
      "loss": -117.438,
      "step": 10100
    },
    {
      "epoch": 0.8088,
      "grad_norm": 36.24325942993164,
      "learning_rate": 0.00017304,
      "loss": -116.7521,
      "step": 10110
    },
    {
      "epoch": 0.8096,
      "grad_norm": 45.666664123535156,
      "learning_rate": 0.00017301333333333334,
      "loss": -116.3852,
      "step": 10120
    },
    {
      "epoch": 0.8104,
      "grad_norm": 36.255680084228516,
      "learning_rate": 0.00017298666666666667,
      "loss": -117.3309,
      "step": 10130
    },
    {
      "epoch": 0.8112,
      "grad_norm": 42.89284133911133,
      "learning_rate": 0.00017296,
      "loss": -117.3657,
      "step": 10140
    },
    {
      "epoch": 0.812,
      "grad_norm": 40.64197540283203,
      "learning_rate": 0.00017293333333333335,
      "loss": -116.6611,
      "step": 10150
    },
    {
      "epoch": 0.8128,
      "grad_norm": 41.116615295410156,
      "learning_rate": 0.00017290666666666668,
      "loss": -116.1314,
      "step": 10160
    },
    {
      "epoch": 0.8136,
      "grad_norm": 43.88105392456055,
      "learning_rate": 0.00017287999999999998,
      "loss": -115.3332,
      "step": 10170
    },
    {
      "epoch": 0.8144,
      "grad_norm": 37.77107620239258,
      "learning_rate": 0.00017285333333333334,
      "loss": -117.3244,
      "step": 10180
    },
    {
      "epoch": 0.8152,
      "grad_norm": 42.688419342041016,
      "learning_rate": 0.00017282666666666667,
      "loss": -117.143,
      "step": 10190
    },
    {
      "epoch": 0.816,
      "grad_norm": 42.0047607421875,
      "learning_rate": 0.0001728,
      "loss": -116.3286,
      "step": 10200
    },
    {
      "epoch": 0.8168,
      "grad_norm": 35.11088180541992,
      "learning_rate": 0.00017277333333333336,
      "loss": -117.6834,
      "step": 10210
    },
    {
      "epoch": 0.8176,
      "grad_norm": 46.41966247558594,
      "learning_rate": 0.00017274666666666668,
      "loss": -117.7795,
      "step": 10220
    },
    {
      "epoch": 0.8184,
      "grad_norm": 39.221378326416016,
      "learning_rate": 0.00017272,
      "loss": -116.1563,
      "step": 10230
    },
    {
      "epoch": 0.8192,
      "grad_norm": 58.53653335571289,
      "learning_rate": 0.00017269333333333334,
      "loss": -116.3387,
      "step": 10240
    },
    {
      "epoch": 0.82,
      "grad_norm": 87.43763732910156,
      "learning_rate": 0.00017266666666666667,
      "loss": -117.8699,
      "step": 10250
    },
    {
      "epoch": 0.8208,
      "grad_norm": 38.228267669677734,
      "learning_rate": 0.00017264,
      "loss": -117.0538,
      "step": 10260
    },
    {
      "epoch": 0.8216,
      "grad_norm": 42.80567169189453,
      "learning_rate": 0.00017261333333333333,
      "loss": -117.4334,
      "step": 10270
    },
    {
      "epoch": 0.8224,
      "grad_norm": 39.02827072143555,
      "learning_rate": 0.00017258666666666669,
      "loss": -118.1673,
      "step": 10280
    },
    {
      "epoch": 0.8232,
      "grad_norm": 60.59226989746094,
      "learning_rate": 0.00017256000000000001,
      "loss": -116.5187,
      "step": 10290
    },
    {
      "epoch": 0.824,
      "grad_norm": 41.76185607910156,
      "learning_rate": 0.00017253333333333334,
      "loss": -116.8979,
      "step": 10300
    },
    {
      "epoch": 0.8248,
      "grad_norm": 37.19527816772461,
      "learning_rate": 0.00017250666666666667,
      "loss": -115.4639,
      "step": 10310
    },
    {
      "epoch": 0.8256,
      "grad_norm": 45.12327575683594,
      "learning_rate": 0.00017248000000000003,
      "loss": -116.9445,
      "step": 10320
    },
    {
      "epoch": 0.8264,
      "grad_norm": 52.68502426147461,
      "learning_rate": 0.00017245333333333333,
      "loss": -117.5002,
      "step": 10330
    },
    {
      "epoch": 0.8272,
      "grad_norm": 44.341888427734375,
      "learning_rate": 0.00017242666666666666,
      "loss": -116.784,
      "step": 10340
    },
    {
      "epoch": 0.828,
      "grad_norm": 38.46736145019531,
      "learning_rate": 0.00017240000000000002,
      "loss": -116.4557,
      "step": 10350
    },
    {
      "epoch": 0.8288,
      "grad_norm": 40.9367561340332,
      "learning_rate": 0.00017237333333333335,
      "loss": -116.2437,
      "step": 10360
    },
    {
      "epoch": 0.8296,
      "grad_norm": 37.091243743896484,
      "learning_rate": 0.00017234666666666667,
      "loss": -117.0054,
      "step": 10370
    },
    {
      "epoch": 0.8304,
      "grad_norm": 42.10951232910156,
      "learning_rate": 0.00017232,
      "loss": -117.3552,
      "step": 10380
    },
    {
      "epoch": 0.8312,
      "grad_norm": 44.67382049560547,
      "learning_rate": 0.00017229333333333336,
      "loss": -116.5488,
      "step": 10390
    },
    {
      "epoch": 0.832,
      "grad_norm": 46.90599060058594,
      "learning_rate": 0.00017226666666666666,
      "loss": -116.997,
      "step": 10400
    },
    {
      "epoch": 0.8328,
      "grad_norm": 43.2051887512207,
      "learning_rate": 0.00017224,
      "loss": -117.8617,
      "step": 10410
    },
    {
      "epoch": 0.8336,
      "grad_norm": 49.99418258666992,
      "learning_rate": 0.00017221333333333335,
      "loss": -117.4969,
      "step": 10420
    },
    {
      "epoch": 0.8344,
      "grad_norm": 47.406593322753906,
      "learning_rate": 0.00017218666666666668,
      "loss": -118.1681,
      "step": 10430
    },
    {
      "epoch": 0.8352,
      "grad_norm": 49.753013610839844,
      "learning_rate": 0.00017216,
      "loss": -116.1992,
      "step": 10440
    },
    {
      "epoch": 0.836,
      "grad_norm": 49.02772521972656,
      "learning_rate": 0.00017213333333333333,
      "loss": -116.1705,
      "step": 10450
    },
    {
      "epoch": 0.8368,
      "grad_norm": 43.43007278442383,
      "learning_rate": 0.0001721066666666667,
      "loss": -116.5815,
      "step": 10460
    },
    {
      "epoch": 0.8376,
      "grad_norm": 31.58871078491211,
      "learning_rate": 0.00017208000000000002,
      "loss": -117.5287,
      "step": 10470
    },
    {
      "epoch": 0.8384,
      "grad_norm": 49.23271942138672,
      "learning_rate": 0.00017205333333333332,
      "loss": -117.0649,
      "step": 10480
    },
    {
      "epoch": 0.8392,
      "grad_norm": 53.49134826660156,
      "learning_rate": 0.00017202666666666668,
      "loss": -116.4555,
      "step": 10490
    },
    {
      "epoch": 0.84,
      "grad_norm": 49.910499572753906,
      "learning_rate": 0.000172,
      "loss": -116.1321,
      "step": 10500
    },
    {
      "epoch": 0.8408,
      "grad_norm": 44.502891540527344,
      "learning_rate": 0.00017197333333333334,
      "loss": -117.3347,
      "step": 10510
    },
    {
      "epoch": 0.8416,
      "grad_norm": 34.5551872253418,
      "learning_rate": 0.00017194666666666666,
      "loss": -117.7789,
      "step": 10520
    },
    {
      "epoch": 0.8424,
      "grad_norm": 53.7381477355957,
      "learning_rate": 0.00017192000000000002,
      "loss": -116.8215,
      "step": 10530
    },
    {
      "epoch": 0.8432,
      "grad_norm": 41.34278106689453,
      "learning_rate": 0.00017189333333333335,
      "loss": -117.2384,
      "step": 10540
    },
    {
      "epoch": 0.844,
      "grad_norm": 43.545230865478516,
      "learning_rate": 0.00017186666666666665,
      "loss": -117.1495,
      "step": 10550
    },
    {
      "epoch": 0.8448,
      "grad_norm": 41.03671646118164,
      "learning_rate": 0.00017184,
      "loss": -117.6649,
      "step": 10560
    },
    {
      "epoch": 0.8456,
      "grad_norm": 49.4794807434082,
      "learning_rate": 0.00017181333333333334,
      "loss": -116.336,
      "step": 10570
    },
    {
      "epoch": 0.8464,
      "grad_norm": 50.667442321777344,
      "learning_rate": 0.00017178666666666667,
      "loss": -116.4479,
      "step": 10580
    },
    {
      "epoch": 0.8472,
      "grad_norm": 38.7114143371582,
      "learning_rate": 0.00017176000000000002,
      "loss": -116.2674,
      "step": 10590
    },
    {
      "epoch": 0.848,
      "grad_norm": 37.19160842895508,
      "learning_rate": 0.00017173333333333335,
      "loss": -116.617,
      "step": 10600
    },
    {
      "epoch": 0.8488,
      "grad_norm": 44.786376953125,
      "learning_rate": 0.00017170666666666668,
      "loss": -117.2163,
      "step": 10610
    },
    {
      "epoch": 0.8496,
      "grad_norm": 34.55177688598633,
      "learning_rate": 0.00017168,
      "loss": -118.1401,
      "step": 10620
    },
    {
      "epoch": 0.8504,
      "grad_norm": 40.394126892089844,
      "learning_rate": 0.00017165333333333334,
      "loss": -117.2233,
      "step": 10630
    },
    {
      "epoch": 0.8512,
      "grad_norm": 40.517032623291016,
      "learning_rate": 0.00017162666666666667,
      "loss": -117.2076,
      "step": 10640
    },
    {
      "epoch": 0.852,
      "grad_norm": 47.97421646118164,
      "learning_rate": 0.0001716,
      "loss": -116.997,
      "step": 10650
    },
    {
      "epoch": 0.8528,
      "grad_norm": 76.55226135253906,
      "learning_rate": 0.00017157333333333335,
      "loss": -115.8859,
      "step": 10660
    },
    {
      "epoch": 0.8536,
      "grad_norm": 43.8157844543457,
      "learning_rate": 0.00017154666666666668,
      "loss": -116.2482,
      "step": 10670
    },
    {
      "epoch": 0.8544,
      "grad_norm": 45.0828857421875,
      "learning_rate": 0.00017152,
      "loss": -117.9285,
      "step": 10680
    },
    {
      "epoch": 0.8552,
      "grad_norm": 55.63522720336914,
      "learning_rate": 0.00017149333333333334,
      "loss": -117.4379,
      "step": 10690
    },
    {
      "epoch": 0.856,
      "grad_norm": 39.49064254760742,
      "learning_rate": 0.00017146666666666667,
      "loss": -117.9574,
      "step": 10700
    },
    {
      "epoch": 0.8568,
      "grad_norm": 50.40845489501953,
      "learning_rate": 0.00017144,
      "loss": -117.0945,
      "step": 10710
    },
    {
      "epoch": 0.8576,
      "grad_norm": 40.5634765625,
      "learning_rate": 0.00017141333333333333,
      "loss": -116.5181,
      "step": 10720
    },
    {
      "epoch": 0.8584,
      "grad_norm": 39.477516174316406,
      "learning_rate": 0.00017138666666666668,
      "loss": -116.8594,
      "step": 10730
    },
    {
      "epoch": 0.8592,
      "grad_norm": 36.81827926635742,
      "learning_rate": 0.00017136,
      "loss": -117.475,
      "step": 10740
    },
    {
      "epoch": 0.86,
      "grad_norm": 44.508724212646484,
      "learning_rate": 0.00017133333333333334,
      "loss": -116.8335,
      "step": 10750
    },
    {
      "epoch": 0.8608,
      "grad_norm": 62.364986419677734,
      "learning_rate": 0.00017130666666666667,
      "loss": -117.4392,
      "step": 10760
    },
    {
      "epoch": 0.8616,
      "grad_norm": 43.73081588745117,
      "learning_rate": 0.00017128000000000003,
      "loss": -117.0599,
      "step": 10770
    },
    {
      "epoch": 0.8624,
      "grad_norm": 66.94947052001953,
      "learning_rate": 0.00017125333333333333,
      "loss": -116.784,
      "step": 10780
    },
    {
      "epoch": 0.8632,
      "grad_norm": 52.402645111083984,
      "learning_rate": 0.00017122666666666666,
      "loss": -117.4685,
      "step": 10790
    },
    {
      "epoch": 0.864,
      "grad_norm": 54.24318313598633,
      "learning_rate": 0.00017120000000000001,
      "loss": -116.8503,
      "step": 10800
    },
    {
      "epoch": 0.8648,
      "grad_norm": 32.41240310668945,
      "learning_rate": 0.00017117333333333334,
      "loss": -115.5198,
      "step": 10810
    },
    {
      "epoch": 0.8656,
      "grad_norm": 60.55221939086914,
      "learning_rate": 0.00017114666666666667,
      "loss": -117.6249,
      "step": 10820
    },
    {
      "epoch": 0.8664,
      "grad_norm": 45.93354415893555,
      "learning_rate": 0.00017112,
      "loss": -118.1,
      "step": 10830
    },
    {
      "epoch": 0.8672,
      "grad_norm": 32.65840148925781,
      "learning_rate": 0.00017109333333333336,
      "loss": -118.149,
      "step": 10840
    },
    {
      "epoch": 0.868,
      "grad_norm": 46.73919677734375,
      "learning_rate": 0.00017106666666666666,
      "loss": -117.4324,
      "step": 10850
    },
    {
      "epoch": 0.8688,
      "grad_norm": 68.0079116821289,
      "learning_rate": 0.00017104,
      "loss": -116.2782,
      "step": 10860
    },
    {
      "epoch": 0.8696,
      "grad_norm": 40.47300720214844,
      "learning_rate": 0.00017101333333333334,
      "loss": -116.9724,
      "step": 10870
    },
    {
      "epoch": 0.8704,
      "grad_norm": 45.03017044067383,
      "learning_rate": 0.00017098666666666667,
      "loss": -118.548,
      "step": 10880
    },
    {
      "epoch": 0.8712,
      "grad_norm": 54.10895919799805,
      "learning_rate": 0.00017096,
      "loss": -117.3672,
      "step": 10890
    },
    {
      "epoch": 0.872,
      "grad_norm": 52.7243537902832,
      "learning_rate": 0.00017093333333333333,
      "loss": -116.5961,
      "step": 10900
    },
    {
      "epoch": 0.8728,
      "grad_norm": 57.79999923706055,
      "learning_rate": 0.0001709066666666667,
      "loss": -116.2746,
      "step": 10910
    },
    {
      "epoch": 0.8736,
      "grad_norm": 40.860260009765625,
      "learning_rate": 0.00017088000000000002,
      "loss": -116.9371,
      "step": 10920
    },
    {
      "epoch": 0.8744,
      "grad_norm": 46.69642639160156,
      "learning_rate": 0.00017085333333333332,
      "loss": -116.1149,
      "step": 10930
    },
    {
      "epoch": 0.8752,
      "grad_norm": 53.12354278564453,
      "learning_rate": 0.00017082666666666668,
      "loss": -116.8472,
      "step": 10940
    },
    {
      "epoch": 0.876,
      "grad_norm": 47.51912307739258,
      "learning_rate": 0.0001708,
      "loss": -117.569,
      "step": 10950
    },
    {
      "epoch": 0.8768,
      "grad_norm": 42.66727066040039,
      "learning_rate": 0.00017077333333333333,
      "loss": -116.5379,
      "step": 10960
    },
    {
      "epoch": 0.8776,
      "grad_norm": 78.94515228271484,
      "learning_rate": 0.0001707466666666667,
      "loss": -115.5433,
      "step": 10970
    },
    {
      "epoch": 0.8784,
      "grad_norm": 79.12973022460938,
      "learning_rate": 0.00017072000000000002,
      "loss": -116.6542,
      "step": 10980
    },
    {
      "epoch": 0.8792,
      "grad_norm": 66.27777862548828,
      "learning_rate": 0.00017069333333333335,
      "loss": -116.4489,
      "step": 10990
    },
    {
      "epoch": 0.88,
      "grad_norm": 38.99708938598633,
      "learning_rate": 0.00017066666666666668,
      "loss": -117.8698,
      "step": 11000
    },
    {
      "epoch": 0.8808,
      "grad_norm": 58.28094482421875,
      "learning_rate": 0.00017064,
      "loss": -118.0992,
      "step": 11010
    },
    {
      "epoch": 0.8816,
      "grad_norm": 33.74074172973633,
      "learning_rate": 0.00017061333333333334,
      "loss": -118.4569,
      "step": 11020
    },
    {
      "epoch": 0.8824,
      "grad_norm": 56.2979621887207,
      "learning_rate": 0.00017058666666666666,
      "loss": -117.1643,
      "step": 11030
    },
    {
      "epoch": 0.8832,
      "grad_norm": 73.16983032226562,
      "learning_rate": 0.00017056000000000002,
      "loss": -117.9697,
      "step": 11040
    },
    {
      "epoch": 0.884,
      "grad_norm": 2345.68310546875,
      "learning_rate": 0.00017053333333333335,
      "loss": -117.2909,
      "step": 11050
    },
    {
      "epoch": 0.8848,
      "grad_norm": 51.043251037597656,
      "learning_rate": 0.00017050666666666668,
      "loss": -116.749,
      "step": 11060
    },
    {
      "epoch": 0.8856,
      "grad_norm": 49.97084426879883,
      "learning_rate": 0.00017048,
      "loss": -116.8081,
      "step": 11070
    },
    {
      "epoch": 0.8864,
      "grad_norm": 84.06050109863281,
      "learning_rate": 0.00017045333333333334,
      "loss": -115.6229,
      "step": 11080
    },
    {
      "epoch": 0.8872,
      "grad_norm": 64.27342987060547,
      "learning_rate": 0.00017042666666666667,
      "loss": -117.589,
      "step": 11090
    },
    {
      "epoch": 0.888,
      "grad_norm": 43.46214294433594,
      "learning_rate": 0.0001704,
      "loss": -116.3023,
      "step": 11100
    },
    {
      "epoch": 0.8888,
      "grad_norm": 49.09113693237305,
      "learning_rate": 0.00017037333333333335,
      "loss": -116.7576,
      "step": 11110
    },
    {
      "epoch": 0.8896,
      "grad_norm": 44.83989715576172,
      "learning_rate": 0.00017034666666666668,
      "loss": -117.6646,
      "step": 11120
    },
    {
      "epoch": 0.8904,
      "grad_norm": 36.398983001708984,
      "learning_rate": 0.00017032,
      "loss": -117.3703,
      "step": 11130
    },
    {
      "epoch": 0.8912,
      "grad_norm": 64.61691284179688,
      "learning_rate": 0.00017029333333333334,
      "loss": -116.1187,
      "step": 11140
    },
    {
      "epoch": 0.892,
      "grad_norm": 52.2093391418457,
      "learning_rate": 0.0001702666666666667,
      "loss": -116.9979,
      "step": 11150
    },
    {
      "epoch": 0.8928,
      "grad_norm": 95.13056182861328,
      "learning_rate": 0.00017024,
      "loss": -116.2487,
      "step": 11160
    },
    {
      "epoch": 0.8936,
      "grad_norm": 106.93550872802734,
      "learning_rate": 0.00017021333333333333,
      "loss": -117.1441,
      "step": 11170
    },
    {
      "epoch": 0.8944,
      "grad_norm": 53.875919342041016,
      "learning_rate": 0.00017018666666666668,
      "loss": -117.663,
      "step": 11180
    },
    {
      "epoch": 0.8952,
      "grad_norm": 59.16569900512695,
      "learning_rate": 0.00017016,
      "loss": -116.8862,
      "step": 11190
    },
    {
      "epoch": 0.896,
      "grad_norm": 35.7396125793457,
      "learning_rate": 0.00017013333333333334,
      "loss": -118.0181,
      "step": 11200
    },
    {
      "epoch": 0.8968,
      "grad_norm": 60.68769073486328,
      "learning_rate": 0.00017010666666666667,
      "loss": -117.0843,
      "step": 11210
    },
    {
      "epoch": 0.8976,
      "grad_norm": 69.48706817626953,
      "learning_rate": 0.00017008000000000002,
      "loss": -116.4234,
      "step": 11220
    },
    {
      "epoch": 0.8984,
      "grad_norm": 53.78166198730469,
      "learning_rate": 0.00017005333333333333,
      "loss": -116.5045,
      "step": 11230
    },
    {
      "epoch": 0.8992,
      "grad_norm": 75.88245391845703,
      "learning_rate": 0.00017002666666666666,
      "loss": -117.0709,
      "step": 11240
    },
    {
      "epoch": 0.9,
      "grad_norm": 53.00850296020508,
      "learning_rate": 0.00017,
      "loss": -117.6511,
      "step": 11250
    },
    {
      "epoch": 0.9008,
      "grad_norm": 129.80845642089844,
      "learning_rate": 0.00016997333333333334,
      "loss": -117.225,
      "step": 11260
    },
    {
      "epoch": 0.9016,
      "grad_norm": 66.79264068603516,
      "learning_rate": 0.00016994666666666667,
      "loss": -116.9115,
      "step": 11270
    },
    {
      "epoch": 0.9024,
      "grad_norm": 50.26638412475586,
      "learning_rate": 0.00016992,
      "loss": -115.9179,
      "step": 11280
    },
    {
      "epoch": 0.9032,
      "grad_norm": 44.79767608642578,
      "learning_rate": 0.00016989333333333336,
      "loss": -117.7906,
      "step": 11290
    },
    {
      "epoch": 0.904,
      "grad_norm": 98.13970947265625,
      "learning_rate": 0.00016986666666666668,
      "loss": -117.3611,
      "step": 11300
    },
    {
      "epoch": 0.9048,
      "grad_norm": 50.60435485839844,
      "learning_rate": 0.00016984,
      "loss": -117.1874,
      "step": 11310
    },
    {
      "epoch": 0.9056,
      "grad_norm": 44.26810836791992,
      "learning_rate": 0.00016981333333333334,
      "loss": -117.956,
      "step": 11320
    },
    {
      "epoch": 0.9064,
      "grad_norm": 78.46216583251953,
      "learning_rate": 0.00016978666666666667,
      "loss": -116.0973,
      "step": 11330
    },
    {
      "epoch": 0.9072,
      "grad_norm": 31.51936149597168,
      "learning_rate": 0.00016976,
      "loss": -117.7792,
      "step": 11340
    },
    {
      "epoch": 0.908,
      "grad_norm": 65.59077453613281,
      "learning_rate": 0.00016973333333333336,
      "loss": -117.0417,
      "step": 11350
    },
    {
      "epoch": 0.9088,
      "grad_norm": 92.92515563964844,
      "learning_rate": 0.00016970666666666669,
      "loss": -117.3844,
      "step": 11360
    },
    {
      "epoch": 0.9096,
      "grad_norm": 93.76172637939453,
      "learning_rate": 0.00016968000000000002,
      "loss": -116.4693,
      "step": 11370
    },
    {
      "epoch": 0.9104,
      "grad_norm": 57.56781005859375,
      "learning_rate": 0.00016965333333333332,
      "loss": -117.3151,
      "step": 11380
    },
    {
      "epoch": 0.9112,
      "grad_norm": 88.88276672363281,
      "learning_rate": 0.00016962666666666667,
      "loss": -117.8213,
      "step": 11390
    },
    {
      "epoch": 0.912,
      "grad_norm": 41.182071685791016,
      "learning_rate": 0.0001696,
      "loss": -117.2842,
      "step": 11400
    },
    {
      "epoch": 0.9128,
      "grad_norm": 89.75202941894531,
      "learning_rate": 0.00016957333333333333,
      "loss": -117.8226,
      "step": 11410
    },
    {
      "epoch": 0.9136,
      "grad_norm": 79.86038208007812,
      "learning_rate": 0.0001695466666666667,
      "loss": -116.5803,
      "step": 11420
    },
    {
      "epoch": 0.9144,
      "grad_norm": 38.13020706176758,
      "learning_rate": 0.00016952000000000002,
      "loss": -117.9943,
      "step": 11430
    },
    {
      "epoch": 0.9152,
      "grad_norm": 47.87346649169922,
      "learning_rate": 0.00016949333333333335,
      "loss": -117.9595,
      "step": 11440
    },
    {
      "epoch": 0.916,
      "grad_norm": 53.99599838256836,
      "learning_rate": 0.00016946666666666667,
      "loss": -117.7559,
      "step": 11450
    },
    {
      "epoch": 0.9168,
      "grad_norm": 58.622257232666016,
      "learning_rate": 0.00016944,
      "loss": -115.8508,
      "step": 11460
    },
    {
      "epoch": 0.9176,
      "grad_norm": 47.04182052612305,
      "learning_rate": 0.00016941333333333333,
      "loss": -115.8703,
      "step": 11470
    },
    {
      "epoch": 0.9184,
      "grad_norm": 39.412559509277344,
      "learning_rate": 0.00016938666666666666,
      "loss": -116.5477,
      "step": 11480
    },
    {
      "epoch": 0.9192,
      "grad_norm": 68.67630767822266,
      "learning_rate": 0.00016936000000000002,
      "loss": -116.9294,
      "step": 11490
    },
    {
      "epoch": 0.92,
      "grad_norm": 87.48612976074219,
      "learning_rate": 0.00016933333333333335,
      "loss": -117.8866,
      "step": 11500
    },
    {
      "epoch": 0.9208,
      "grad_norm": 65.42521667480469,
      "learning_rate": 0.00016930666666666668,
      "loss": -117.9344,
      "step": 11510
    },
    {
      "epoch": 0.9216,
      "grad_norm": 94.50769805908203,
      "learning_rate": 0.00016928,
      "loss": -115.4215,
      "step": 11520
    },
    {
      "epoch": 0.9224,
      "grad_norm": 57.769710540771484,
      "learning_rate": 0.00016925333333333333,
      "loss": -116.5389,
      "step": 11530
    },
    {
      "epoch": 0.9232,
      "grad_norm": 67.38834381103516,
      "learning_rate": 0.00016922666666666666,
      "loss": -115.7775,
      "step": 11540
    },
    {
      "epoch": 0.924,
      "grad_norm": 96.54400634765625,
      "learning_rate": 0.0001692,
      "loss": -116.6125,
      "step": 11550
    },
    {
      "epoch": 0.9248,
      "grad_norm": 55.41862106323242,
      "learning_rate": 0.00016917333333333335,
      "loss": -118.0456,
      "step": 11560
    },
    {
      "epoch": 0.9256,
      "grad_norm": 62.771636962890625,
      "learning_rate": 0.00016914666666666668,
      "loss": -117.1667,
      "step": 11570
    },
    {
      "epoch": 0.9264,
      "grad_norm": 50.08879852294922,
      "learning_rate": 0.00016912,
      "loss": -118.1164,
      "step": 11580
    },
    {
      "epoch": 0.9272,
      "grad_norm": 65.00941467285156,
      "learning_rate": 0.00016909333333333334,
      "loss": -118.5503,
      "step": 11590
    },
    {
      "epoch": 0.928,
      "grad_norm": 85.84394073486328,
      "learning_rate": 0.0001690666666666667,
      "loss": -117.5815,
      "step": 11600
    },
    {
      "epoch": 0.9288,
      "grad_norm": 53.24177551269531,
      "learning_rate": 0.00016904,
      "loss": -117.4457,
      "step": 11610
    },
    {
      "epoch": 0.9296,
      "grad_norm": 61.08743667602539,
      "learning_rate": 0.00016901333333333332,
      "loss": -117.5816,
      "step": 11620
    },
    {
      "epoch": 0.9304,
      "grad_norm": 81.92003631591797,
      "learning_rate": 0.00016898666666666668,
      "loss": -116.8873,
      "step": 11630
    },
    {
      "epoch": 0.9312,
      "grad_norm": 90.92742919921875,
      "learning_rate": 0.00016896,
      "loss": -116.7108,
      "step": 11640
    },
    {
      "epoch": 0.932,
      "grad_norm": 48.70656967163086,
      "learning_rate": 0.00016893333333333334,
      "loss": -117.1813,
      "step": 11650
    },
    {
      "epoch": 0.9328,
      "grad_norm": 42.55426025390625,
      "learning_rate": 0.00016890666666666667,
      "loss": -116.9715,
      "step": 11660
    },
    {
      "epoch": 0.9336,
      "grad_norm": 60.878662109375,
      "learning_rate": 0.00016888000000000002,
      "loss": -116.6686,
      "step": 11670
    },
    {
      "epoch": 0.9344,
      "grad_norm": 60.07355880737305,
      "learning_rate": 0.00016885333333333335,
      "loss": -117.0254,
      "step": 11680
    },
    {
      "epoch": 0.9352,
      "grad_norm": 89.84535217285156,
      "learning_rate": 0.00016882666666666665,
      "loss": -116.4059,
      "step": 11690
    },
    {
      "epoch": 0.936,
      "grad_norm": 62.63066482543945,
      "learning_rate": 0.0001688,
      "loss": -117.2381,
      "step": 11700
    },
    {
      "epoch": 0.9368,
      "grad_norm": 68.66329956054688,
      "learning_rate": 0.00016877333333333334,
      "loss": -117.5925,
      "step": 11710
    },
    {
      "epoch": 0.9376,
      "grad_norm": 100.86043548583984,
      "learning_rate": 0.00016874666666666667,
      "loss": -116.4201,
      "step": 11720
    },
    {
      "epoch": 0.9384,
      "grad_norm": 97.3386001586914,
      "learning_rate": 0.00016872000000000002,
      "loss": -116.3398,
      "step": 11730
    },
    {
      "epoch": 0.9392,
      "grad_norm": 91.05880737304688,
      "learning_rate": 0.00016869333333333335,
      "loss": -117.6229,
      "step": 11740
    },
    {
      "epoch": 0.94,
      "grad_norm": 103.01835632324219,
      "learning_rate": 0.00016866666666666668,
      "loss": -116.9775,
      "step": 11750
    },
    {
      "epoch": 0.9408,
      "grad_norm": 48.371612548828125,
      "learning_rate": 0.00016863999999999998,
      "loss": -116.3242,
      "step": 11760
    },
    {
      "epoch": 0.9416,
      "grad_norm": 126.0789794921875,
      "learning_rate": 0.00016861333333333334,
      "loss": -116.5924,
      "step": 11770
    },
    {
      "epoch": 0.9424,
      "grad_norm": 95.73176574707031,
      "learning_rate": 0.00016858666666666667,
      "loss": -116.8206,
      "step": 11780
    },
    {
      "epoch": 0.9432,
      "grad_norm": 92.86315155029297,
      "learning_rate": 0.00016856,
      "loss": -116.7017,
      "step": 11790
    },
    {
      "epoch": 0.944,
      "grad_norm": 150.93795776367188,
      "learning_rate": 0.00016853333333333336,
      "loss": -116.8603,
      "step": 11800
    },
    {
      "epoch": 0.9448,
      "grad_norm": 67.49483489990234,
      "learning_rate": 0.00016850666666666668,
      "loss": -117.1851,
      "step": 11810
    },
    {
      "epoch": 0.9456,
      "grad_norm": 59.44088363647461,
      "learning_rate": 0.00016848,
      "loss": -117.1036,
      "step": 11820
    },
    {
      "epoch": 0.9464,
      "grad_norm": 70.77653503417969,
      "learning_rate": 0.00016845333333333334,
      "loss": -116.9822,
      "step": 11830
    },
    {
      "epoch": 0.9472,
      "grad_norm": 78.12226867675781,
      "learning_rate": 0.00016842666666666667,
      "loss": -116.9894,
      "step": 11840
    },
    {
      "epoch": 0.948,
      "grad_norm": 58.28801727294922,
      "learning_rate": 0.0001684,
      "loss": -117.5537,
      "step": 11850
    },
    {
      "epoch": 0.9488,
      "grad_norm": 57.58671569824219,
      "learning_rate": 0.00016837333333333333,
      "loss": -116.5528,
      "step": 11860
    },
    {
      "epoch": 0.9496,
      "grad_norm": 59.427703857421875,
      "learning_rate": 0.00016834666666666669,
      "loss": -116.8795,
      "step": 11870
    },
    {
      "epoch": 0.9504,
      "grad_norm": 53.4565544128418,
      "learning_rate": 0.00016832000000000001,
      "loss": -117.6356,
      "step": 11880
    },
    {
      "epoch": 0.9512,
      "grad_norm": 60.467281341552734,
      "learning_rate": 0.00016829333333333334,
      "loss": -115.928,
      "step": 11890
    },
    {
      "epoch": 0.952,
      "grad_norm": 67.98981475830078,
      "learning_rate": 0.00016826666666666667,
      "loss": -115.659,
      "step": 11900
    },
    {
      "epoch": 0.9528,
      "grad_norm": 94.52641296386719,
      "learning_rate": 0.00016824,
      "loss": -116.5728,
      "step": 11910
    },
    {
      "epoch": 0.9536,
      "grad_norm": 98.77193450927734,
      "learning_rate": 0.00016821333333333333,
      "loss": -116.9253,
      "step": 11920
    },
    {
      "epoch": 0.9544,
      "grad_norm": 75.30245208740234,
      "learning_rate": 0.00016818666666666666,
      "loss": -115.8313,
      "step": 11930
    },
    {
      "epoch": 0.9552,
      "grad_norm": 102.65462493896484,
      "learning_rate": 0.00016816000000000002,
      "loss": -116.5553,
      "step": 11940
    },
    {
      "epoch": 0.956,
      "grad_norm": 56.96399688720703,
      "learning_rate": 0.00016813333333333335,
      "loss": -117.7242,
      "step": 11950
    },
    {
      "epoch": 0.9568,
      "grad_norm": 174.66162109375,
      "learning_rate": 0.00016810666666666667,
      "loss": -115.0269,
      "step": 11960
    },
    {
      "epoch": 0.9576,
      "grad_norm": 282.7590026855469,
      "learning_rate": 0.00016808,
      "loss": -116.7868,
      "step": 11970
    },
    {
      "epoch": 0.9584,
      "grad_norm": 109.38117218017578,
      "learning_rate": 0.00016805333333333336,
      "loss": -116.1076,
      "step": 11980
    },
    {
      "epoch": 0.9592,
      "grad_norm": 93.59733581542969,
      "learning_rate": 0.00016802666666666666,
      "loss": -117.6869,
      "step": 11990
    },
    {
      "epoch": 0.96,
      "grad_norm": 123.62594604492188,
      "learning_rate": 0.000168,
      "loss": -116.7614,
      "step": 12000
    },
    {
      "epoch": 0.9608,
      "grad_norm": 130.962158203125,
      "learning_rate": 0.00016797333333333335,
      "loss": -116.4338,
      "step": 12010
    },
    {
      "epoch": 0.9616,
      "grad_norm": 86.5330581665039,
      "learning_rate": 0.00016794666666666668,
      "loss": -115.407,
      "step": 12020
    },
    {
      "epoch": 0.9624,
      "grad_norm": 78.27218627929688,
      "learning_rate": 0.00016792,
      "loss": -118.2721,
      "step": 12030
    },
    {
      "epoch": 0.9632,
      "grad_norm": 55.548583984375,
      "learning_rate": 0.00016789333333333333,
      "loss": -116.7706,
      "step": 12040
    },
    {
      "epoch": 0.964,
      "grad_norm": 118.6967544555664,
      "learning_rate": 0.0001678666666666667,
      "loss": -115.772,
      "step": 12050
    },
    {
      "epoch": 0.9648,
      "grad_norm": 73.47734069824219,
      "learning_rate": 0.00016784,
      "loss": -116.682,
      "step": 12060
    },
    {
      "epoch": 0.9656,
      "grad_norm": 313.8199462890625,
      "learning_rate": 0.00016781333333333332,
      "loss": -116.7422,
      "step": 12070
    },
    {
      "epoch": 0.9664,
      "grad_norm": 73.9266357421875,
      "learning_rate": 0.00016778666666666668,
      "loss": -116.4822,
      "step": 12080
    },
    {
      "epoch": 0.9672,
      "grad_norm": 55.15097427368164,
      "learning_rate": 0.00016776,
      "loss": -116.2024,
      "step": 12090
    },
    {
      "epoch": 0.968,
      "grad_norm": 67.08422088623047,
      "learning_rate": 0.00016773333333333334,
      "loss": -116.0222,
      "step": 12100
    },
    {
      "epoch": 0.9688,
      "grad_norm": 55.75651168823242,
      "learning_rate": 0.0001677066666666667,
      "loss": -116.8702,
      "step": 12110
    },
    {
      "epoch": 0.9696,
      "grad_norm": 67.22200775146484,
      "learning_rate": 0.00016768000000000002,
      "loss": -117.4585,
      "step": 12120
    },
    {
      "epoch": 0.9704,
      "grad_norm": 54.228614807128906,
      "learning_rate": 0.00016765333333333335,
      "loss": -116.8407,
      "step": 12130
    },
    {
      "epoch": 0.9712,
      "grad_norm": 73.72709655761719,
      "learning_rate": 0.00016762666666666665,
      "loss": -117.1802,
      "step": 12140
    },
    {
      "epoch": 0.972,
      "grad_norm": 111.2944564819336,
      "learning_rate": 0.0001676,
      "loss": -117.1577,
      "step": 12150
    },
    {
      "epoch": 0.9728,
      "grad_norm": 102.09548950195312,
      "learning_rate": 0.00016757333333333334,
      "loss": -116.9526,
      "step": 12160
    },
    {
      "epoch": 0.9736,
      "grad_norm": 54.8115348815918,
      "learning_rate": 0.00016754666666666667,
      "loss": -117.2896,
      "step": 12170
    },
    {
      "epoch": 0.9744,
      "grad_norm": 63.34724807739258,
      "learning_rate": 0.00016752000000000002,
      "loss": -116.6205,
      "step": 12180
    },
    {
      "epoch": 0.9752,
      "grad_norm": 102.85255432128906,
      "learning_rate": 0.00016749333333333335,
      "loss": -115.8241,
      "step": 12190
    },
    {
      "epoch": 0.976,
      "grad_norm": 153.78504943847656,
      "learning_rate": 0.00016746666666666668,
      "loss": -116.9328,
      "step": 12200
    },
    {
      "epoch": 0.9768,
      "grad_norm": 96.98687744140625,
      "learning_rate": 0.00016744,
      "loss": -117.1988,
      "step": 12210
    },
    {
      "epoch": 0.9776,
      "grad_norm": 53.04060745239258,
      "learning_rate": 0.00016741333333333334,
      "loss": -117.3927,
      "step": 12220
    },
    {
      "epoch": 0.9784,
      "grad_norm": 120.947265625,
      "learning_rate": 0.00016738666666666667,
      "loss": -118.79,
      "step": 12230
    },
    {
      "epoch": 0.9792,
      "grad_norm": 63.88637924194336,
      "learning_rate": 0.00016736,
      "loss": -118.1137,
      "step": 12240
    },
    {
      "epoch": 0.98,
      "grad_norm": 79.90894317626953,
      "learning_rate": 0.00016733333333333335,
      "loss": -117.0547,
      "step": 12250
    },
    {
      "epoch": 0.9808,
      "grad_norm": 89.6541748046875,
      "learning_rate": 0.00016730666666666668,
      "loss": -117.8245,
      "step": 12260
    },
    {
      "epoch": 0.9816,
      "grad_norm": 90.37093353271484,
      "learning_rate": 0.00016728,
      "loss": -117.3316,
      "step": 12270
    },
    {
      "epoch": 0.9824,
      "grad_norm": 109.24824523925781,
      "learning_rate": 0.00016725333333333334,
      "loss": -116.7387,
      "step": 12280
    },
    {
      "epoch": 0.9832,
      "grad_norm": 109.61309814453125,
      "learning_rate": 0.00016722666666666667,
      "loss": -115.8086,
      "step": 12290
    },
    {
      "epoch": 0.984,
      "grad_norm": 94.31756591796875,
      "learning_rate": 0.0001672,
      "loss": -117.0687,
      "step": 12300
    },
    {
      "epoch": 0.9848,
      "grad_norm": 54.325374603271484,
      "learning_rate": 0.00016717333333333333,
      "loss": -117.6413,
      "step": 12310
    },
    {
      "epoch": 0.9856,
      "grad_norm": 61.49561309814453,
      "learning_rate": 0.00016714666666666668,
      "loss": -116.8855,
      "step": 12320
    },
    {
      "epoch": 0.9864,
      "grad_norm": 75.02899169921875,
      "learning_rate": 0.00016712,
      "loss": -116.7139,
      "step": 12330
    },
    {
      "epoch": 0.9872,
      "grad_norm": 77.19142150878906,
      "learning_rate": 0.00016709333333333334,
      "loss": -115.8201,
      "step": 12340
    },
    {
      "epoch": 0.988,
      "grad_norm": 69.97565460205078,
      "learning_rate": 0.00016706666666666667,
      "loss": -116.5596,
      "step": 12350
    },
    {
      "epoch": 0.9888,
      "grad_norm": 275.8172607421875,
      "learning_rate": 0.00016704000000000003,
      "loss": -111.323,
      "step": 12360
    },
    {
      "epoch": 0.9896,
      "grad_norm": 235.21502685546875,
      "learning_rate": 0.00016701333333333333,
      "loss": -114.5424,
      "step": 12370
    },
    {
      "epoch": 0.9904,
      "grad_norm": 87.40684509277344,
      "learning_rate": 0.00016698666666666666,
      "loss": -114.9749,
      "step": 12380
    },
    {
      "epoch": 0.9912,
      "grad_norm": 123.42211151123047,
      "learning_rate": 0.00016696000000000001,
      "loss": -115.4471,
      "step": 12390
    },
    {
      "epoch": 0.992,
      "grad_norm": 76.83316040039062,
      "learning_rate": 0.00016693333333333334,
      "loss": -115.2606,
      "step": 12400
    },
    {
      "epoch": 0.9928,
      "grad_norm": 63.09186553955078,
      "learning_rate": 0.00016690666666666667,
      "loss": -115.8853,
      "step": 12410
    },
    {
      "epoch": 0.9936,
      "grad_norm": 60.785831451416016,
      "learning_rate": 0.00016688,
      "loss": -115.6949,
      "step": 12420
    },
    {
      "epoch": 0.9944,
      "grad_norm": 97.13720703125,
      "learning_rate": 0.00016685333333333336,
      "loss": -115.1071,
      "step": 12430
    },
    {
      "epoch": 0.9952,
      "grad_norm": 67.03722381591797,
      "learning_rate": 0.00016682666666666666,
      "loss": -117.1334,
      "step": 12440
    },
    {
      "epoch": 0.996,
      "grad_norm": 54.17840576171875,
      "learning_rate": 0.0001668,
      "loss": -116.3746,
      "step": 12450
    },
    {
      "epoch": 0.9968,
      "grad_norm": 43.18458557128906,
      "learning_rate": 0.00016677333333333334,
      "loss": -116.0513,
      "step": 12460
    },
    {
      "epoch": 0.9976,
      "grad_norm": 49.142059326171875,
      "learning_rate": 0.00016674666666666667,
      "loss": -116.3116,
      "step": 12470
    },
    {
      "epoch": 0.9984,
      "grad_norm": 41.380279541015625,
      "learning_rate": 0.00016672,
      "loss": -114.7757,
      "step": 12480
    },
    {
      "epoch": 0.9992,
      "grad_norm": 61.18212127685547,
      "learning_rate": 0.00016669333333333336,
      "loss": -116.3841,
      "step": 12490
    },
    {
      "epoch": 1.0,
      "grad_norm": 69.9844970703125,
      "learning_rate": 0.0001666666666666667,
      "loss": -115.4973,
      "step": 12500
    },
    {
      "epoch": 1.0008,
      "grad_norm": 57.444488525390625,
      "learning_rate": 0.00016664000000000002,
      "loss": -116.3631,
      "step": 12510
    },
    {
      "epoch": 1.0016,
      "grad_norm": 70.29529571533203,
      "learning_rate": 0.00016661333333333332,
      "loss": -115.0418,
      "step": 12520
    },
    {
      "epoch": 1.0024,
      "grad_norm": 106.41748046875,
      "learning_rate": 0.00016658666666666668,
      "loss": -114.8772,
      "step": 12530
    },
    {
      "epoch": 1.0032,
      "grad_norm": 124.3656234741211,
      "learning_rate": 0.00016656,
      "loss": -114.6076,
      "step": 12540
    },
    {
      "epoch": 1.004,
      "grad_norm": 125.31057739257812,
      "learning_rate": 0.00016653333333333333,
      "loss": -114.167,
      "step": 12550
    },
    {
      "epoch": 1.0048,
      "grad_norm": 66.61286926269531,
      "learning_rate": 0.0001665066666666667,
      "loss": -115.1909,
      "step": 12560
    },
    {
      "epoch": 1.0056,
      "grad_norm": 79.3866958618164,
      "learning_rate": 0.00016648000000000002,
      "loss": -114.314,
      "step": 12570
    },
    {
      "epoch": 1.0064,
      "grad_norm": 59.62410354614258,
      "learning_rate": 0.00016645333333333335,
      "loss": -115.7286,
      "step": 12580
    },
    {
      "epoch": 1.0072,
      "grad_norm": 155.0753173828125,
      "learning_rate": 0.00016642666666666668,
      "loss": -115.4274,
      "step": 12590
    },
    {
      "epoch": 1.008,
      "grad_norm": 1971.216552734375,
      "learning_rate": 0.0001664,
      "loss": -115.3993,
      "step": 12600
    },
    {
      "epoch": 1.0088,
      "grad_norm": 177.792724609375,
      "learning_rate": 0.00016637333333333334,
      "loss": -115.0372,
      "step": 12610
    },
    {
      "epoch": 1.0096,
      "grad_norm": 74.5004653930664,
      "learning_rate": 0.00016634666666666666,
      "loss": -114.2532,
      "step": 12620
    },
    {
      "epoch": 1.0104,
      "grad_norm": 122.3790512084961,
      "learning_rate": 0.00016632000000000002,
      "loss": -114.0865,
      "step": 12630
    },
    {
      "epoch": 1.0112,
      "grad_norm": 65.56031036376953,
      "learning_rate": 0.00016629333333333335,
      "loss": -115.3709,
      "step": 12640
    },
    {
      "epoch": 1.012,
      "grad_norm": 68.56060791015625,
      "learning_rate": 0.00016626666666666668,
      "loss": -114.9609,
      "step": 12650
    },
    {
      "epoch": 1.0128,
      "grad_norm": 51.92560958862305,
      "learning_rate": 0.00016624,
      "loss": -114.6326,
      "step": 12660
    },
    {
      "epoch": 1.0136,
      "grad_norm": 47.39127731323242,
      "learning_rate": 0.00016621333333333334,
      "loss": -115.8666,
      "step": 12670
    },
    {
      "epoch": 1.0144,
      "grad_norm": 66.92921447753906,
      "learning_rate": 0.00016618666666666667,
      "loss": -114.5521,
      "step": 12680
    },
    {
      "epoch": 1.0152,
      "grad_norm": 62.572410583496094,
      "learning_rate": 0.00016616,
      "loss": -114.6716,
      "step": 12690
    },
    {
      "epoch": 1.016,
      "grad_norm": 69.18968200683594,
      "learning_rate": 0.00016613333333333335,
      "loss": -115.4283,
      "step": 12700
    },
    {
      "epoch": 1.0168,
      "grad_norm": 60.560245513916016,
      "learning_rate": 0.00016610666666666668,
      "loss": -115.2442,
      "step": 12710
    },
    {
      "epoch": 1.0176,
      "grad_norm": 63.34351348876953,
      "learning_rate": 0.00016608,
      "loss": -115.1461,
      "step": 12720
    },
    {
      "epoch": 1.0184,
      "grad_norm": 169.16542053222656,
      "learning_rate": 0.00016605333333333334,
      "loss": -114.9029,
      "step": 12730
    },
    {
      "epoch": 1.0192,
      "grad_norm": 93.17167663574219,
      "learning_rate": 0.00016602666666666667,
      "loss": -114.4474,
      "step": 12740
    },
    {
      "epoch": 1.02,
      "grad_norm": 66.56460571289062,
      "learning_rate": 0.000166,
      "loss": -116.1753,
      "step": 12750
    },
    {
      "epoch": 1.0208,
      "grad_norm": 130.25669860839844,
      "learning_rate": 0.00016597333333333333,
      "loss": -113.217,
      "step": 12760
    },
    {
      "epoch": 1.0216,
      "grad_norm": 104.13505554199219,
      "learning_rate": 0.00016594666666666668,
      "loss": -114.637,
      "step": 12770
    },
    {
      "epoch": 1.0224,
      "grad_norm": 124.0998306274414,
      "learning_rate": 0.00016592,
      "loss": -115.4509,
      "step": 12780
    },
    {
      "epoch": 1.0232,
      "grad_norm": 59.520263671875,
      "learning_rate": 0.00016589333333333334,
      "loss": -115.5241,
      "step": 12790
    },
    {
      "epoch": 1.024,
      "grad_norm": 57.271488189697266,
      "learning_rate": 0.00016586666666666667,
      "loss": -114.9571,
      "step": 12800
    },
    {
      "epoch": 1.0248,
      "grad_norm": 68.08418273925781,
      "learning_rate": 0.00016584000000000002,
      "loss": -115.9352,
      "step": 12810
    },
    {
      "epoch": 1.0256,
      "grad_norm": 69.32470703125,
      "learning_rate": 0.00016581333333333333,
      "loss": -116.2047,
      "step": 12820
    },
    {
      "epoch": 1.0264,
      "grad_norm": 55.850921630859375,
      "learning_rate": 0.00016578666666666666,
      "loss": -115.0182,
      "step": 12830
    },
    {
      "epoch": 1.0272,
      "grad_norm": 57.49272918701172,
      "learning_rate": 0.00016576,
      "loss": -115.9211,
      "step": 12840
    },
    {
      "epoch": 1.028,
      "grad_norm": 149.4462127685547,
      "learning_rate": 0.00016573333333333334,
      "loss": -114.6556,
      "step": 12850
    },
    {
      "epoch": 1.0288,
      "grad_norm": 101.10395050048828,
      "learning_rate": 0.00016570666666666667,
      "loss": -114.4656,
      "step": 12860
    },
    {
      "epoch": 1.0296,
      "grad_norm": 83.4496841430664,
      "learning_rate": 0.00016568000000000003,
      "loss": -115.4949,
      "step": 12870
    },
    {
      "epoch": 1.0304,
      "grad_norm": 79.38800048828125,
      "learning_rate": 0.00016565333333333336,
      "loss": -115.314,
      "step": 12880
    },
    {
      "epoch": 1.0312,
      "grad_norm": 47.0601692199707,
      "learning_rate": 0.00016562666666666668,
      "loss": -115.7994,
      "step": 12890
    },
    {
      "epoch": 1.032,
      "grad_norm": 78.96002197265625,
      "learning_rate": 0.0001656,
      "loss": -115.3958,
      "step": 12900
    },
    {
      "epoch": 1.0328,
      "grad_norm": 53.725425720214844,
      "learning_rate": 0.00016557333333333334,
      "loss": -115.6682,
      "step": 12910
    },
    {
      "epoch": 1.0336,
      "grad_norm": 105.03076171875,
      "learning_rate": 0.00016554666666666667,
      "loss": -114.5087,
      "step": 12920
    },
    {
      "epoch": 1.0344,
      "grad_norm": 69.72941589355469,
      "learning_rate": 0.00016552,
      "loss": -115.4678,
      "step": 12930
    },
    {
      "epoch": 1.0352,
      "grad_norm": 79.69670104980469,
      "learning_rate": 0.00016549333333333336,
      "loss": -116.223,
      "step": 12940
    },
    {
      "epoch": 1.036,
      "grad_norm": 51.0986442565918,
      "learning_rate": 0.00016546666666666669,
      "loss": -115.3467,
      "step": 12950
    },
    {
      "epoch": 1.0368,
      "grad_norm": 55.8572998046875,
      "learning_rate": 0.00016544000000000002,
      "loss": -115.5392,
      "step": 12960
    },
    {
      "epoch": 1.0376,
      "grad_norm": 45.91038513183594,
      "learning_rate": 0.00016541333333333334,
      "loss": -116.2,
      "step": 12970
    },
    {
      "epoch": 1.0384,
      "grad_norm": 68.55473327636719,
      "learning_rate": 0.00016538666666666667,
      "loss": -114.7337,
      "step": 12980
    },
    {
      "epoch": 1.0392,
      "grad_norm": 41.000667572021484,
      "learning_rate": 0.00016536,
      "loss": -115.0427,
      "step": 12990
    },
    {
      "epoch": 1.04,
      "grad_norm": 45.46289825439453,
      "learning_rate": 0.00016533333333333333,
      "loss": -115.4222,
      "step": 13000
    },
    {
      "epoch": 1.0408,
      "grad_norm": 48.935611724853516,
      "learning_rate": 0.0001653066666666667,
      "loss": -114.8559,
      "step": 13010
    },
    {
      "epoch": 1.0416,
      "grad_norm": 80.27906036376953,
      "learning_rate": 0.00016528000000000002,
      "loss": -114.1986,
      "step": 13020
    },
    {
      "epoch": 1.0424,
      "grad_norm": 80.3494644165039,
      "learning_rate": 0.00016525333333333335,
      "loss": -114.6985,
      "step": 13030
    },
    {
      "epoch": 1.0432,
      "grad_norm": 73.14787292480469,
      "learning_rate": 0.00016522666666666667,
      "loss": -114.595,
      "step": 13040
    },
    {
      "epoch": 1.044,
      "grad_norm": 52.88898468017578,
      "learning_rate": 0.0001652,
      "loss": -114.7232,
      "step": 13050
    },
    {
      "epoch": 1.0448,
      "grad_norm": 44.176761627197266,
      "learning_rate": 0.00016517333333333333,
      "loss": -113.4088,
      "step": 13060
    },
    {
      "epoch": 1.0456,
      "grad_norm": 54.76133728027344,
      "learning_rate": 0.00016514666666666666,
      "loss": -115.4364,
      "step": 13070
    },
    {
      "epoch": 1.0464,
      "grad_norm": 57.8114013671875,
      "learning_rate": 0.00016512000000000002,
      "loss": -116.1593,
      "step": 13080
    },
    {
      "epoch": 1.0472,
      "grad_norm": 85.52544403076172,
      "learning_rate": 0.00016509333333333335,
      "loss": -114.4524,
      "step": 13090
    },
    {
      "epoch": 1.048,
      "grad_norm": 107.3317642211914,
      "learning_rate": 0.00016506666666666668,
      "loss": -113.6253,
      "step": 13100
    },
    {
      "epoch": 1.0488,
      "grad_norm": 76.39970397949219,
      "learning_rate": 0.00016504,
      "loss": -116.4055,
      "step": 13110
    },
    {
      "epoch": 1.0496,
      "grad_norm": 43.01819610595703,
      "learning_rate": 0.00016501333333333333,
      "loss": -114.5133,
      "step": 13120
    },
    {
      "epoch": 1.0504,
      "grad_norm": 236.675048828125,
      "learning_rate": 0.00016498666666666666,
      "loss": -115.1052,
      "step": 13130
    },
    {
      "epoch": 1.0512,
      "grad_norm": 71.80680847167969,
      "learning_rate": 0.00016496,
      "loss": -113.8224,
      "step": 13140
    },
    {
      "epoch": 1.052,
      "grad_norm": 83.22933959960938,
      "learning_rate": 0.00016493333333333335,
      "loss": -114.9469,
      "step": 13150
    },
    {
      "epoch": 1.0528,
      "grad_norm": 83.58198547363281,
      "learning_rate": 0.00016490666666666668,
      "loss": -114.98,
      "step": 13160
    },
    {
      "epoch": 1.0536,
      "grad_norm": 66.27971649169922,
      "learning_rate": 0.00016488,
      "loss": -115.8482,
      "step": 13170
    },
    {
      "epoch": 1.0544,
      "grad_norm": 77.94251251220703,
      "learning_rate": 0.00016485333333333334,
      "loss": -114.6467,
      "step": 13180
    },
    {
      "epoch": 1.0552,
      "grad_norm": 121.12091827392578,
      "learning_rate": 0.0001648266666666667,
      "loss": -115.3359,
      "step": 13190
    },
    {
      "epoch": 1.056,
      "grad_norm": 112.35478973388672,
      "learning_rate": 0.0001648,
      "loss": -114.4378,
      "step": 13200
    },
    {
      "epoch": 1.0568,
      "grad_norm": 59.90471649169922,
      "learning_rate": 0.00016477333333333332,
      "loss": -115.5614,
      "step": 13210
    },
    {
      "epoch": 1.0576,
      "grad_norm": 63.89481735229492,
      "learning_rate": 0.00016474666666666668,
      "loss": -114.9456,
      "step": 13220
    },
    {
      "epoch": 1.0584,
      "grad_norm": 60.22328186035156,
      "learning_rate": 0.00016472,
      "loss": -115.2475,
      "step": 13230
    },
    {
      "epoch": 1.0592,
      "grad_norm": 59.807647705078125,
      "learning_rate": 0.00016469333333333334,
      "loss": -115.4971,
      "step": 13240
    },
    {
      "epoch": 1.06,
      "grad_norm": 44.87676239013672,
      "learning_rate": 0.00016466666666666667,
      "loss": -115.1662,
      "step": 13250
    },
    {
      "epoch": 1.0608,
      "grad_norm": 60.21931838989258,
      "learning_rate": 0.00016464000000000002,
      "loss": -115.484,
      "step": 13260
    },
    {
      "epoch": 1.0616,
      "grad_norm": 43.59270095825195,
      "learning_rate": 0.00016461333333333332,
      "loss": -115.4914,
      "step": 13270
    },
    {
      "epoch": 1.0624,
      "grad_norm": 57.739593505859375,
      "learning_rate": 0.00016458666666666665,
      "loss": -114.4952,
      "step": 13280
    },
    {
      "epoch": 1.0632,
      "grad_norm": 97.23734283447266,
      "learning_rate": 0.00016456,
      "loss": -114.477,
      "step": 13290
    },
    {
      "epoch": 1.064,
      "grad_norm": 78.90298461914062,
      "learning_rate": 0.00016453333333333334,
      "loss": -115.3649,
      "step": 13300
    },
    {
      "epoch": 1.0648,
      "grad_norm": 49.840721130371094,
      "learning_rate": 0.00016450666666666667,
      "loss": -114.1658,
      "step": 13310
    },
    {
      "epoch": 1.0656,
      "grad_norm": 52.0182991027832,
      "learning_rate": 0.00016448000000000002,
      "loss": -115.0638,
      "step": 13320
    },
    {
      "epoch": 1.0664,
      "grad_norm": 60.04191970825195,
      "learning_rate": 0.00016445333333333335,
      "loss": -114.7278,
      "step": 13330
    },
    {
      "epoch": 1.0672,
      "grad_norm": 75.60108947753906,
      "learning_rate": 0.00016442666666666668,
      "loss": -114.8332,
      "step": 13340
    },
    {
      "epoch": 1.068,
      "grad_norm": 43.453426361083984,
      "learning_rate": 0.0001644,
      "loss": -115.6211,
      "step": 13350
    },
    {
      "epoch": 1.0688,
      "grad_norm": 65.8123550415039,
      "learning_rate": 0.00016437333333333334,
      "loss": -114.4558,
      "step": 13360
    },
    {
      "epoch": 1.0695999999999999,
      "grad_norm": 92.98384094238281,
      "learning_rate": 0.00016434666666666667,
      "loss": -114.4373,
      "step": 13370
    },
    {
      "epoch": 1.0704,
      "grad_norm": 106.88571166992188,
      "learning_rate": 0.00016432,
      "loss": -114.1034,
      "step": 13380
    },
    {
      "epoch": 1.0712,
      "grad_norm": 122.26738739013672,
      "learning_rate": 0.00016429333333333336,
      "loss": -114.2246,
      "step": 13390
    },
    {
      "epoch": 1.072,
      "grad_norm": 300.93017578125,
      "learning_rate": 0.00016426666666666668,
      "loss": -112.2308,
      "step": 13400
    },
    {
      "epoch": 1.0728,
      "grad_norm": 84.00848388671875,
      "learning_rate": 0.00016424,
      "loss": -115.2729,
      "step": 13410
    },
    {
      "epoch": 1.0735999999999999,
      "grad_norm": 274.774169921875,
      "learning_rate": 0.00016421333333333334,
      "loss": -115.4674,
      "step": 13420
    },
    {
      "epoch": 1.0744,
      "grad_norm": 190.33518981933594,
      "learning_rate": 0.00016418666666666667,
      "loss": -114.7966,
      "step": 13430
    },
    {
      "epoch": 1.0752,
      "grad_norm": 57.54215621948242,
      "learning_rate": 0.00016416,
      "loss": -113.664,
      "step": 13440
    },
    {
      "epoch": 1.076,
      "grad_norm": 113.93291473388672,
      "learning_rate": 0.00016413333333333333,
      "loss": -113.4538,
      "step": 13450
    },
    {
      "epoch": 1.0768,
      "grad_norm": 47.90643310546875,
      "learning_rate": 0.00016410666666666669,
      "loss": -114.4839,
      "step": 13460
    },
    {
      "epoch": 1.0776,
      "grad_norm": 80.8706283569336,
      "learning_rate": 0.00016408000000000001,
      "loss": -114.9885,
      "step": 13470
    },
    {
      "epoch": 1.0784,
      "grad_norm": 214.82144165039062,
      "learning_rate": 0.00016405333333333334,
      "loss": -114.8993,
      "step": 13480
    },
    {
      "epoch": 1.0792,
      "grad_norm": 92.50384521484375,
      "learning_rate": 0.00016402666666666667,
      "loss": -114.3464,
      "step": 13490
    },
    {
      "epoch": 1.08,
      "grad_norm": 71.07521057128906,
      "learning_rate": 0.000164,
      "loss": -115.8328,
      "step": 13500
    },
    {
      "epoch": 1.0808,
      "grad_norm": 146.72665405273438,
      "learning_rate": 0.00016397333333333333,
      "loss": -115.2961,
      "step": 13510
    },
    {
      "epoch": 1.0816,
      "grad_norm": 108.21526336669922,
      "learning_rate": 0.00016394666666666666,
      "loss": -114.0761,
      "step": 13520
    },
    {
      "epoch": 1.0824,
      "grad_norm": 67.55166625976562,
      "learning_rate": 0.00016392000000000002,
      "loss": -112.8897,
      "step": 13530
    },
    {
      "epoch": 1.0832,
      "grad_norm": 228.00396728515625,
      "learning_rate": 0.00016389333333333335,
      "loss": -113.6284,
      "step": 13540
    },
    {
      "epoch": 1.084,
      "grad_norm": 64.09646606445312,
      "learning_rate": 0.00016386666666666667,
      "loss": -114.9048,
      "step": 13550
    },
    {
      "epoch": 1.0848,
      "grad_norm": 170.2908477783203,
      "learning_rate": 0.00016384,
      "loss": -114.2193,
      "step": 13560
    },
    {
      "epoch": 1.0856,
      "grad_norm": 61.023773193359375,
      "learning_rate": 0.00016381333333333336,
      "loss": -116.1081,
      "step": 13570
    },
    {
      "epoch": 1.0864,
      "grad_norm": 78.98086547851562,
      "learning_rate": 0.00016378666666666666,
      "loss": -115.2262,
      "step": 13580
    },
    {
      "epoch": 1.0872,
      "grad_norm": 108.76072692871094,
      "learning_rate": 0.00016376,
      "loss": -113.9149,
      "step": 13590
    },
    {
      "epoch": 1.088,
      "grad_norm": 87.88572692871094,
      "learning_rate": 0.00016373333333333335,
      "loss": -110.9109,
      "step": 13600
    },
    {
      "epoch": 1.0888,
      "grad_norm": 336.71142578125,
      "learning_rate": 0.00016370666666666668,
      "loss": -113.0002,
      "step": 13610
    },
    {
      "epoch": 1.0896,
      "grad_norm": 117.6514892578125,
      "learning_rate": 0.00016368,
      "loss": -113.9593,
      "step": 13620
    },
    {
      "epoch": 1.0904,
      "grad_norm": 143.6277618408203,
      "learning_rate": 0.00016365333333333333,
      "loss": -115.0592,
      "step": 13630
    },
    {
      "epoch": 1.0912,
      "grad_norm": 381.3492126464844,
      "learning_rate": 0.0001636266666666667,
      "loss": -113.1135,
      "step": 13640
    },
    {
      "epoch": 1.092,
      "grad_norm": 96.65679168701172,
      "learning_rate": 0.0001636,
      "loss": -114.185,
      "step": 13650
    },
    {
      "epoch": 1.0928,
      "grad_norm": 124.36814880371094,
      "learning_rate": 0.00016357333333333332,
      "loss": -112.9836,
      "step": 13660
    },
    {
      "epoch": 1.0936,
      "grad_norm": 127.62847137451172,
      "learning_rate": 0.00016354666666666668,
      "loss": -112.6027,
      "step": 13670
    },
    {
      "epoch": 1.0944,
      "grad_norm": 3490.801025390625,
      "learning_rate": 0.00016352,
      "loss": -110.4045,
      "step": 13680
    },
    {
      "epoch": 1.0952,
      "grad_norm": 87.25005340576172,
      "learning_rate": 0.00016349333333333334,
      "loss": -112.2021,
      "step": 13690
    },
    {
      "epoch": 1.096,
      "grad_norm": 54.09878158569336,
      "learning_rate": 0.0001634666666666667,
      "loss": -113.7252,
      "step": 13700
    },
    {
      "epoch": 1.0968,
      "grad_norm": 61.89280700683594,
      "learning_rate": 0.00016344000000000002,
      "loss": -115.0124,
      "step": 13710
    },
    {
      "epoch": 1.0976,
      "grad_norm": 89.01599884033203,
      "learning_rate": 0.00016341333333333335,
      "loss": -113.4949,
      "step": 13720
    },
    {
      "epoch": 1.0984,
      "grad_norm": 62.37044143676758,
      "learning_rate": 0.00016338666666666668,
      "loss": -112.539,
      "step": 13730
    },
    {
      "epoch": 1.0992,
      "grad_norm": 111.0685043334961,
      "learning_rate": 0.00016336,
      "loss": -114.5915,
      "step": 13740
    },
    {
      "epoch": 1.1,
      "grad_norm": 111.9040298461914,
      "learning_rate": 0.00016333333333333334,
      "loss": -114.2936,
      "step": 13750
    },
    {
      "epoch": 1.1008,
      "grad_norm": 66.1371841430664,
      "learning_rate": 0.00016330666666666667,
      "loss": -112.9309,
      "step": 13760
    },
    {
      "epoch": 1.1016,
      "grad_norm": 69.6031265258789,
      "learning_rate": 0.00016328000000000002,
      "loss": -115.0554,
      "step": 13770
    },
    {
      "epoch": 1.1024,
      "grad_norm": 87.27265167236328,
      "learning_rate": 0.00016325333333333335,
      "loss": -114.1071,
      "step": 13780
    },
    {
      "epoch": 1.1032,
      "grad_norm": 99.17670440673828,
      "learning_rate": 0.00016322666666666668,
      "loss": -113.8675,
      "step": 13790
    },
    {
      "epoch": 1.104,
      "grad_norm": 69.7165298461914,
      "learning_rate": 0.0001632,
      "loss": -114.4355,
      "step": 13800
    },
    {
      "epoch": 1.1048,
      "grad_norm": 3172.23681640625,
      "learning_rate": 0.00016317333333333334,
      "loss": -111.902,
      "step": 13810
    },
    {
      "epoch": 1.1056,
      "grad_norm": 97.14134979248047,
      "learning_rate": 0.00016314666666666667,
      "loss": -112.8422,
      "step": 13820
    },
    {
      "epoch": 1.1064,
      "grad_norm": 115.14803314208984,
      "learning_rate": 0.00016312,
      "loss": -114.1157,
      "step": 13830
    },
    {
      "epoch": 1.1072,
      "grad_norm": 62.27098846435547,
      "learning_rate": 0.00016309333333333335,
      "loss": -112.7835,
      "step": 13840
    },
    {
      "epoch": 1.108,
      "grad_norm": 63.48937225341797,
      "learning_rate": 0.00016306666666666668,
      "loss": -114.9854,
      "step": 13850
    },
    {
      "epoch": 1.1088,
      "grad_norm": 162.0688934326172,
      "learning_rate": 0.00016304,
      "loss": -112.9765,
      "step": 13860
    },
    {
      "epoch": 1.1096,
      "grad_norm": 75.43901824951172,
      "learning_rate": 0.00016301333333333334,
      "loss": -114.873,
      "step": 13870
    },
    {
      "epoch": 1.1104,
      "grad_norm": 66.77526092529297,
      "learning_rate": 0.00016298666666666667,
      "loss": -113.822,
      "step": 13880
    },
    {
      "epoch": 1.1112,
      "grad_norm": 55.36674880981445,
      "learning_rate": 0.00016296,
      "loss": -113.0135,
      "step": 13890
    },
    {
      "epoch": 1.112,
      "grad_norm": 80.53665161132812,
      "learning_rate": 0.00016293333333333333,
      "loss": -113.8379,
      "step": 13900
    },
    {
      "epoch": 1.1128,
      "grad_norm": 79.58307647705078,
      "learning_rate": 0.00016290666666666668,
      "loss": -114.7705,
      "step": 13910
    },
    {
      "epoch": 1.1136,
      "grad_norm": 61.555625915527344,
      "learning_rate": 0.00016288,
      "loss": -113.9358,
      "step": 13920
    },
    {
      "epoch": 1.1144,
      "grad_norm": 59.29352951049805,
      "learning_rate": 0.00016285333333333334,
      "loss": -113.5334,
      "step": 13930
    },
    {
      "epoch": 1.1152,
      "grad_norm": 229.0404510498047,
      "learning_rate": 0.00016282666666666667,
      "loss": -113.4232,
      "step": 13940
    },
    {
      "epoch": 1.116,
      "grad_norm": 52.39222717285156,
      "learning_rate": 0.0001628,
      "loss": -112.4976,
      "step": 13950
    },
    {
      "epoch": 1.1168,
      "grad_norm": 56.3404541015625,
      "learning_rate": 0.00016277333333333333,
      "loss": -114.6696,
      "step": 13960
    },
    {
      "epoch": 1.1176,
      "grad_norm": 73.5768814086914,
      "learning_rate": 0.00016274666666666666,
      "loss": -115.1199,
      "step": 13970
    },
    {
      "epoch": 1.1184,
      "grad_norm": 77.4969253540039,
      "learning_rate": 0.00016272000000000001,
      "loss": -113.4364,
      "step": 13980
    },
    {
      "epoch": 1.1192,
      "grad_norm": 62.998355865478516,
      "learning_rate": 0.00016269333333333334,
      "loss": -113.5368,
      "step": 13990
    },
    {
      "epoch": 1.12,
      "grad_norm": 99.32018280029297,
      "learning_rate": 0.00016266666666666667,
      "loss": -115.4776,
      "step": 14000
    },
    {
      "epoch": 1.1208,
      "grad_norm": 79.05542755126953,
      "learning_rate": 0.00016264,
      "loss": -113.9626,
      "step": 14010
    },
    {
      "epoch": 1.1216,
      "grad_norm": 51.01441192626953,
      "learning_rate": 0.00016261333333333336,
      "loss": -114.6037,
      "step": 14020
    },
    {
      "epoch": 1.1224,
      "grad_norm": 116.61627197265625,
      "learning_rate": 0.00016258666666666666,
      "loss": -114.8534,
      "step": 14030
    },
    {
      "epoch": 1.1232,
      "grad_norm": 76.09244537353516,
      "learning_rate": 0.00016256,
      "loss": -115.4368,
      "step": 14040
    },
    {
      "epoch": 1.124,
      "grad_norm": 43.99741744995117,
      "learning_rate": 0.00016253333333333334,
      "loss": -114.3363,
      "step": 14050
    },
    {
      "epoch": 1.1248,
      "grad_norm": 88.3769760131836,
      "learning_rate": 0.00016250666666666667,
      "loss": -113.0061,
      "step": 14060
    },
    {
      "epoch": 1.1256,
      "grad_norm": 75.30457305908203,
      "learning_rate": 0.00016248,
      "loss": -114.5534,
      "step": 14070
    },
    {
      "epoch": 1.1264,
      "grad_norm": 54.41944122314453,
      "learning_rate": 0.00016245333333333336,
      "loss": -114.0701,
      "step": 14080
    },
    {
      "epoch": 1.1272,
      "grad_norm": 65.72260284423828,
      "learning_rate": 0.0001624266666666667,
      "loss": -114.8251,
      "step": 14090
    },
    {
      "epoch": 1.1280000000000001,
      "grad_norm": 46.506141662597656,
      "learning_rate": 0.00016240000000000002,
      "loss": -114.5408,
      "step": 14100
    },
    {
      "epoch": 1.1288,
      "grad_norm": 53.8522834777832,
      "learning_rate": 0.00016237333333333335,
      "loss": -114.5605,
      "step": 14110
    },
    {
      "epoch": 1.1296,
      "grad_norm": 44.7677116394043,
      "learning_rate": 0.00016234666666666668,
      "loss": -114.5812,
      "step": 14120
    },
    {
      "epoch": 1.1304,
      "grad_norm": 71.27498626708984,
      "learning_rate": 0.00016232,
      "loss": -112.4784,
      "step": 14130
    },
    {
      "epoch": 1.1312,
      "grad_norm": 57.178611755371094,
      "learning_rate": 0.00016229333333333333,
      "loss": -112.5701,
      "step": 14140
    },
    {
      "epoch": 1.1320000000000001,
      "grad_norm": 78.73223114013672,
      "learning_rate": 0.0001622666666666667,
      "loss": -114.1829,
      "step": 14150
    },
    {
      "epoch": 1.1328,
      "grad_norm": 111.46888732910156,
      "learning_rate": 0.00016224000000000002,
      "loss": -114.5299,
      "step": 14160
    },
    {
      "epoch": 1.1336,
      "grad_norm": 36.51978302001953,
      "learning_rate": 0.00016221333333333335,
      "loss": -114.1932,
      "step": 14170
    },
    {
      "epoch": 1.1344,
      "grad_norm": 76.62730407714844,
      "learning_rate": 0.00016218666666666668,
      "loss": -113.4347,
      "step": 14180
    },
    {
      "epoch": 1.1352,
      "grad_norm": 51.94744873046875,
      "learning_rate": 0.00016216,
      "loss": -114.3651,
      "step": 14190
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 41.84835433959961,
      "learning_rate": 0.00016213333333333334,
      "loss": -113.3516,
      "step": 14200
    },
    {
      "epoch": 1.1368,
      "grad_norm": 82.71410369873047,
      "learning_rate": 0.00016210666666666666,
      "loss": -115.4211,
      "step": 14210
    },
    {
      "epoch": 1.1376,
      "grad_norm": 53.75401306152344,
      "learning_rate": 0.00016208000000000002,
      "loss": -114.5128,
      "step": 14220
    },
    {
      "epoch": 1.1384,
      "grad_norm": 39.13063049316406,
      "learning_rate": 0.00016205333333333335,
      "loss": -114.5284,
      "step": 14230
    },
    {
      "epoch": 1.1392,
      "grad_norm": 53.30813217163086,
      "learning_rate": 0.00016202666666666668,
      "loss": -114.7168,
      "step": 14240
    },
    {
      "epoch": 1.1400000000000001,
      "grad_norm": 48.50184631347656,
      "learning_rate": 0.000162,
      "loss": -115.8384,
      "step": 14250
    },
    {
      "epoch": 1.1408,
      "grad_norm": 47.92903137207031,
      "learning_rate": 0.00016197333333333334,
      "loss": -115.1587,
      "step": 14260
    },
    {
      "epoch": 1.1416,
      "grad_norm": 678.4876098632812,
      "learning_rate": 0.00016194666666666667,
      "loss": -114.4846,
      "step": 14270
    },
    {
      "epoch": 1.1424,
      "grad_norm": 67.0184555053711,
      "learning_rate": 0.00016192,
      "loss": -114.8807,
      "step": 14280
    },
    {
      "epoch": 1.1432,
      "grad_norm": 223.6226043701172,
      "learning_rate": 0.00016189333333333335,
      "loss": -113.6384,
      "step": 14290
    },
    {
      "epoch": 1.144,
      "grad_norm": 78.51970672607422,
      "learning_rate": 0.00016186666666666668,
      "loss": -114.5492,
      "step": 14300
    },
    {
      "epoch": 1.1448,
      "grad_norm": 65.01700592041016,
      "learning_rate": 0.00016184,
      "loss": -113.7324,
      "step": 14310
    },
    {
      "epoch": 1.1456,
      "grad_norm": 78.21405792236328,
      "learning_rate": 0.00016181333333333334,
      "loss": -113.4601,
      "step": 14320
    },
    {
      "epoch": 1.1464,
      "grad_norm": 49.31260299682617,
      "learning_rate": 0.00016178666666666667,
      "loss": -115.9972,
      "step": 14330
    },
    {
      "epoch": 1.1472,
      "grad_norm": 66.46142578125,
      "learning_rate": 0.00016176,
      "loss": -114.4609,
      "step": 14340
    },
    {
      "epoch": 1.148,
      "grad_norm": 39.69892501831055,
      "learning_rate": 0.00016173333333333333,
      "loss": -114.0408,
      "step": 14350
    },
    {
      "epoch": 1.1488,
      "grad_norm": 62.14849090576172,
      "learning_rate": 0.00016170666666666668,
      "loss": -114.0391,
      "step": 14360
    },
    {
      "epoch": 1.1496,
      "grad_norm": 71.3232421875,
      "learning_rate": 0.00016168,
      "loss": -114.4198,
      "step": 14370
    },
    {
      "epoch": 1.1504,
      "grad_norm": 64.00019836425781,
      "learning_rate": 0.00016165333333333334,
      "loss": -113.6421,
      "step": 14380
    },
    {
      "epoch": 1.1512,
      "grad_norm": 42.968788146972656,
      "learning_rate": 0.00016162666666666667,
      "loss": -115.1817,
      "step": 14390
    },
    {
      "epoch": 1.152,
      "grad_norm": 59.17898178100586,
      "learning_rate": 0.00016160000000000002,
      "loss": -114.5833,
      "step": 14400
    },
    {
      "epoch": 1.1528,
      "grad_norm": 63.233612060546875,
      "learning_rate": 0.00016157333333333333,
      "loss": -115.1844,
      "step": 14410
    },
    {
      "epoch": 1.1536,
      "grad_norm": 200.05108642578125,
      "learning_rate": 0.00016154666666666666,
      "loss": -114.2285,
      "step": 14420
    },
    {
      "epoch": 1.1544,
      "grad_norm": 80.76761627197266,
      "learning_rate": 0.00016152,
      "loss": -113.1399,
      "step": 14430
    },
    {
      "epoch": 1.1552,
      "grad_norm": 35.49745178222656,
      "learning_rate": 0.00016149333333333334,
      "loss": -115.0271,
      "step": 14440
    },
    {
      "epoch": 1.156,
      "grad_norm": 54.9648323059082,
      "learning_rate": 0.00016146666666666667,
      "loss": -114.4705,
      "step": 14450
    },
    {
      "epoch": 1.1568,
      "grad_norm": 65.74812316894531,
      "learning_rate": 0.00016144000000000003,
      "loss": -115.3363,
      "step": 14460
    },
    {
      "epoch": 1.1576,
      "grad_norm": 146.1957550048828,
      "learning_rate": 0.00016141333333333336,
      "loss": -114.1013,
      "step": 14470
    },
    {
      "epoch": 1.1584,
      "grad_norm": 59.42613983154297,
      "learning_rate": 0.00016138666666666666,
      "loss": -115.4057,
      "step": 14480
    },
    {
      "epoch": 1.1592,
      "grad_norm": 82.45623016357422,
      "learning_rate": 0.00016136,
      "loss": -114.184,
      "step": 14490
    },
    {
      "epoch": 1.16,
      "grad_norm": 182.42431640625,
      "learning_rate": 0.00016133333333333334,
      "loss": -113.7675,
      "step": 14500
    },
    {
      "epoch": 1.1608,
      "grad_norm": 49.045623779296875,
      "learning_rate": 0.00016130666666666667,
      "loss": -113.3182,
      "step": 14510
    },
    {
      "epoch": 1.1616,
      "grad_norm": 55.115596771240234,
      "learning_rate": 0.00016128,
      "loss": -112.1674,
      "step": 14520
    },
    {
      "epoch": 1.1623999999999999,
      "grad_norm": 73.9544448852539,
      "learning_rate": 0.00016125333333333336,
      "loss": -114.3374,
      "step": 14530
    },
    {
      "epoch": 1.1632,
      "grad_norm": 68.32061004638672,
      "learning_rate": 0.00016122666666666669,
      "loss": -112.855,
      "step": 14540
    },
    {
      "epoch": 1.164,
      "grad_norm": 125.96096801757812,
      "learning_rate": 0.00016120000000000002,
      "loss": -114.8678,
      "step": 14550
    },
    {
      "epoch": 1.1648,
      "grad_norm": 82.39248657226562,
      "learning_rate": 0.00016117333333333334,
      "loss": -111.9602,
      "step": 14560
    },
    {
      "epoch": 1.1656,
      "grad_norm": 91.87837982177734,
      "learning_rate": 0.00016114666666666667,
      "loss": -113.1756,
      "step": 14570
    },
    {
      "epoch": 1.1663999999999999,
      "grad_norm": 96.7498550415039,
      "learning_rate": 0.00016112,
      "loss": -112.8114,
      "step": 14580
    },
    {
      "epoch": 1.1672,
      "grad_norm": 78.33724212646484,
      "learning_rate": 0.00016109333333333333,
      "loss": -112.8242,
      "step": 14590
    },
    {
      "epoch": 1.168,
      "grad_norm": 54.88640594482422,
      "learning_rate": 0.0001610666666666667,
      "loss": -114.5931,
      "step": 14600
    },
    {
      "epoch": 1.1688,
      "grad_norm": 66.12509155273438,
      "learning_rate": 0.00016104000000000002,
      "loss": -112.4942,
      "step": 14610
    },
    {
      "epoch": 1.1696,
      "grad_norm": 54.728946685791016,
      "learning_rate": 0.00016101333333333335,
      "loss": -114.531,
      "step": 14620
    },
    {
      "epoch": 1.1703999999999999,
      "grad_norm": 920.3743896484375,
      "learning_rate": 0.00016098666666666667,
      "loss": -113.6337,
      "step": 14630
    },
    {
      "epoch": 1.1712,
      "grad_norm": 61.214996337890625,
      "learning_rate": 0.00016096,
      "loss": -112.6744,
      "step": 14640
    },
    {
      "epoch": 1.172,
      "grad_norm": 39.9565544128418,
      "learning_rate": 0.00016093333333333333,
      "loss": -114.5083,
      "step": 14650
    },
    {
      "epoch": 1.1728,
      "grad_norm": 86.42827606201172,
      "learning_rate": 0.00016090666666666666,
      "loss": -114.8449,
      "step": 14660
    },
    {
      "epoch": 1.1736,
      "grad_norm": 52.86133575439453,
      "learning_rate": 0.00016088000000000002,
      "loss": -113.7501,
      "step": 14670
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 117.95782470703125,
      "learning_rate": 0.00016085333333333335,
      "loss": -111.3892,
      "step": 14680
    },
    {
      "epoch": 1.1752,
      "grad_norm": 62.53562545776367,
      "learning_rate": 0.00016082666666666668,
      "loss": -112.4903,
      "step": 14690
    },
    {
      "epoch": 1.176,
      "grad_norm": 70.35801696777344,
      "learning_rate": 0.0001608,
      "loss": -112.9849,
      "step": 14700
    },
    {
      "epoch": 1.1768,
      "grad_norm": 128.37867736816406,
      "learning_rate": 0.00016077333333333333,
      "loss": -112.8498,
      "step": 14710
    },
    {
      "epoch": 1.1776,
      "grad_norm": 58.061744689941406,
      "learning_rate": 0.00016074666666666666,
      "loss": -114.2851,
      "step": 14720
    },
    {
      "epoch": 1.1784,
      "grad_norm": 60.228797912597656,
      "learning_rate": 0.00016072,
      "loss": -113.598,
      "step": 14730
    },
    {
      "epoch": 1.1792,
      "grad_norm": 70.79666900634766,
      "learning_rate": 0.00016069333333333335,
      "loss": -114.1338,
      "step": 14740
    },
    {
      "epoch": 1.18,
      "grad_norm": 84.44005584716797,
      "learning_rate": 0.00016066666666666668,
      "loss": -114.9061,
      "step": 14750
    },
    {
      "epoch": 1.1808,
      "grad_norm": 45.47882080078125,
      "learning_rate": 0.00016064,
      "loss": -114.2431,
      "step": 14760
    },
    {
      "epoch": 1.1816,
      "grad_norm": 44.39668655395508,
      "learning_rate": 0.00016061333333333334,
      "loss": -114.6838,
      "step": 14770
    },
    {
      "epoch": 1.1824,
      "grad_norm": 37.47141647338867,
      "learning_rate": 0.0001605866666666667,
      "loss": -114.734,
      "step": 14780
    },
    {
      "epoch": 1.1832,
      "grad_norm": 53.53913116455078,
      "learning_rate": 0.00016056,
      "loss": -114.8611,
      "step": 14790
    },
    {
      "epoch": 1.184,
      "grad_norm": 88.38733673095703,
      "learning_rate": 0.00016053333333333332,
      "loss": -114.0229,
      "step": 14800
    },
    {
      "epoch": 1.1848,
      "grad_norm": 47.562984466552734,
      "learning_rate": 0.00016050666666666668,
      "loss": -115.1125,
      "step": 14810
    },
    {
      "epoch": 1.1856,
      "grad_norm": 84.6954574584961,
      "learning_rate": 0.00016048,
      "loss": -112.843,
      "step": 14820
    },
    {
      "epoch": 1.1864,
      "grad_norm": 48.650264739990234,
      "learning_rate": 0.00016045333333333334,
      "loss": -115.6033,
      "step": 14830
    },
    {
      "epoch": 1.1872,
      "grad_norm": 51.40550231933594,
      "learning_rate": 0.0001604266666666667,
      "loss": -114.4726,
      "step": 14840
    },
    {
      "epoch": 1.188,
      "grad_norm": 138.5155487060547,
      "learning_rate": 0.00016040000000000002,
      "loss": -114.5705,
      "step": 14850
    },
    {
      "epoch": 1.1888,
      "grad_norm": 58.411346435546875,
      "learning_rate": 0.00016037333333333332,
      "loss": -114.5865,
      "step": 14860
    },
    {
      "epoch": 1.1896,
      "grad_norm": 127.46846008300781,
      "learning_rate": 0.00016034666666666665,
      "loss": -114.3462,
      "step": 14870
    },
    {
      "epoch": 1.1904,
      "grad_norm": 41.253753662109375,
      "learning_rate": 0.00016032,
      "loss": -112.7809,
      "step": 14880
    },
    {
      "epoch": 1.1912,
      "grad_norm": 65.10868835449219,
      "learning_rate": 0.00016029333333333334,
      "loss": -114.5055,
      "step": 14890
    },
    {
      "epoch": 1.192,
      "grad_norm": 111.02740478515625,
      "learning_rate": 0.00016026666666666667,
      "loss": -114.1921,
      "step": 14900
    },
    {
      "epoch": 1.1928,
      "grad_norm": 115.30587005615234,
      "learning_rate": 0.00016024000000000002,
      "loss": -114.1629,
      "step": 14910
    },
    {
      "epoch": 1.1936,
      "grad_norm": 305.91650390625,
      "learning_rate": 0.00016021333333333335,
      "loss": -113.5781,
      "step": 14920
    },
    {
      "epoch": 1.1944,
      "grad_norm": 57.49977493286133,
      "learning_rate": 0.00016018666666666668,
      "loss": -113.7346,
      "step": 14930
    },
    {
      "epoch": 1.1952,
      "grad_norm": 207.99302673339844,
      "learning_rate": 0.00016016,
      "loss": -115.2193,
      "step": 14940
    },
    {
      "epoch": 1.196,
      "grad_norm": 63.13298416137695,
      "learning_rate": 0.00016013333333333334,
      "loss": -114.3124,
      "step": 14950
    },
    {
      "epoch": 1.1968,
      "grad_norm": 48.55502700805664,
      "learning_rate": 0.00016010666666666667,
      "loss": -115.607,
      "step": 14960
    },
    {
      "epoch": 1.1976,
      "grad_norm": 73.01734161376953,
      "learning_rate": 0.00016008,
      "loss": -113.9201,
      "step": 14970
    },
    {
      "epoch": 1.1984,
      "grad_norm": 62.96584701538086,
      "learning_rate": 0.00016005333333333335,
      "loss": -115.7387,
      "step": 14980
    },
    {
      "epoch": 1.1992,
      "grad_norm": 54.21083068847656,
      "learning_rate": 0.00016002666666666668,
      "loss": -114.4531,
      "step": 14990
    },
    {
      "epoch": 1.2,
      "grad_norm": 49.96942138671875,
      "learning_rate": 0.00016,
      "loss": -115.4693,
      "step": 15000
    },
    {
      "epoch": 1.2008,
      "grad_norm": 51.35708236694336,
      "learning_rate": 0.00015997333333333334,
      "loss": -113.9798,
      "step": 15010
    },
    {
      "epoch": 1.2016,
      "grad_norm": 79.10597229003906,
      "learning_rate": 0.00015994666666666667,
      "loss": -113.8279,
      "step": 15020
    },
    {
      "epoch": 1.2024,
      "grad_norm": 66.75945281982422,
      "learning_rate": 0.00015992,
      "loss": -114.734,
      "step": 15030
    },
    {
      "epoch": 1.2032,
      "grad_norm": 43.77897644042969,
      "learning_rate": 0.00015989333333333333,
      "loss": -114.5037,
      "step": 15040
    },
    {
      "epoch": 1.204,
      "grad_norm": 43.64409637451172,
      "learning_rate": 0.00015986666666666669,
      "loss": -115.7891,
      "step": 15050
    },
    {
      "epoch": 1.2048,
      "grad_norm": 63.16431427001953,
      "learning_rate": 0.00015984000000000001,
      "loss": -113.8928,
      "step": 15060
    },
    {
      "epoch": 1.2056,
      "grad_norm": 75.33705139160156,
      "learning_rate": 0.00015981333333333334,
      "loss": -114.8421,
      "step": 15070
    },
    {
      "epoch": 1.2064,
      "grad_norm": 94.25210571289062,
      "learning_rate": 0.00015978666666666667,
      "loss": -114.8192,
      "step": 15080
    },
    {
      "epoch": 1.2072,
      "grad_norm": 55.10553741455078,
      "learning_rate": 0.00015976,
      "loss": -115.3056,
      "step": 15090
    },
    {
      "epoch": 1.208,
      "grad_norm": 42.45444869995117,
      "learning_rate": 0.00015973333333333333,
      "loss": -115.0364,
      "step": 15100
    },
    {
      "epoch": 1.2088,
      "grad_norm": 79.39749145507812,
      "learning_rate": 0.00015970666666666666,
      "loss": -114.3921,
      "step": 15110
    },
    {
      "epoch": 1.2096,
      "grad_norm": 55.45378875732422,
      "learning_rate": 0.00015968000000000002,
      "loss": -115.2707,
      "step": 15120
    },
    {
      "epoch": 1.2104,
      "grad_norm": 49.8899040222168,
      "learning_rate": 0.00015965333333333335,
      "loss": -115.0761,
      "step": 15130
    },
    {
      "epoch": 1.2112,
      "grad_norm": 57.1456413269043,
      "learning_rate": 0.00015962666666666667,
      "loss": -115.2233,
      "step": 15140
    },
    {
      "epoch": 1.212,
      "grad_norm": 52.02346420288086,
      "learning_rate": 0.0001596,
      "loss": -116.7128,
      "step": 15150
    },
    {
      "epoch": 1.2128,
      "grad_norm": 41.59880828857422,
      "learning_rate": 0.00015957333333333333,
      "loss": -113.9786,
      "step": 15160
    },
    {
      "epoch": 1.2136,
      "grad_norm": 52.671104431152344,
      "learning_rate": 0.00015954666666666666,
      "loss": -112.2436,
      "step": 15170
    },
    {
      "epoch": 1.2144,
      "grad_norm": 85.41568756103516,
      "learning_rate": 0.00015952,
      "loss": -114.5375,
      "step": 15180
    },
    {
      "epoch": 1.2152,
      "grad_norm": 55.96591567993164,
      "learning_rate": 0.00015949333333333335,
      "loss": -115.0811,
      "step": 15190
    },
    {
      "epoch": 1.216,
      "grad_norm": 48.3001594543457,
      "learning_rate": 0.00015946666666666668,
      "loss": -115.7755,
      "step": 15200
    },
    {
      "epoch": 1.2168,
      "grad_norm": 87.61382293701172,
      "learning_rate": 0.00015944,
      "loss": -114.9143,
      "step": 15210
    },
    {
      "epoch": 1.2176,
      "grad_norm": 70.22651672363281,
      "learning_rate": 0.00015941333333333336,
      "loss": -111.7849,
      "step": 15220
    },
    {
      "epoch": 1.2184,
      "grad_norm": 63.556907653808594,
      "learning_rate": 0.0001593866666666667,
      "loss": -114.6172,
      "step": 15230
    },
    {
      "epoch": 1.2192,
      "grad_norm": 57.69001007080078,
      "learning_rate": 0.00015936,
      "loss": -114.4575,
      "step": 15240
    },
    {
      "epoch": 1.22,
      "grad_norm": 60.874691009521484,
      "learning_rate": 0.00015933333333333332,
      "loss": -114.2617,
      "step": 15250
    },
    {
      "epoch": 1.2208,
      "grad_norm": 55.63650131225586,
      "learning_rate": 0.00015930666666666668,
      "loss": -115.4134,
      "step": 15260
    },
    {
      "epoch": 1.2216,
      "grad_norm": 57.570247650146484,
      "learning_rate": 0.00015928,
      "loss": -116.7804,
      "step": 15270
    },
    {
      "epoch": 1.2224,
      "grad_norm": 62.49252700805664,
      "learning_rate": 0.00015925333333333334,
      "loss": -115.6547,
      "step": 15280
    },
    {
      "epoch": 1.2232,
      "grad_norm": 60.40922164916992,
      "learning_rate": 0.0001592266666666667,
      "loss": -114.8436,
      "step": 15290
    },
    {
      "epoch": 1.224,
      "grad_norm": 58.417476654052734,
      "learning_rate": 0.00015920000000000002,
      "loss": -114.1861,
      "step": 15300
    },
    {
      "epoch": 1.2248,
      "grad_norm": 57.683162689208984,
      "learning_rate": 0.00015917333333333332,
      "loss": -114.7695,
      "step": 15310
    },
    {
      "epoch": 1.2256,
      "grad_norm": 83.02511596679688,
      "learning_rate": 0.00015914666666666668,
      "loss": -114.7505,
      "step": 15320
    },
    {
      "epoch": 1.2264,
      "grad_norm": 60.70705795288086,
      "learning_rate": 0.00015912,
      "loss": -115.1629,
      "step": 15330
    },
    {
      "epoch": 1.2272,
      "grad_norm": 43.713134765625,
      "learning_rate": 0.00015909333333333334,
      "loss": -114.1898,
      "step": 15340
    },
    {
      "epoch": 1.228,
      "grad_norm": 76.6175765991211,
      "learning_rate": 0.00015906666666666667,
      "loss": -116.3165,
      "step": 15350
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 87.33978271484375,
      "learning_rate": 0.00015904000000000002,
      "loss": -114.2578,
      "step": 15360
    },
    {
      "epoch": 1.2296,
      "grad_norm": 80.86941528320312,
      "learning_rate": 0.00015901333333333335,
      "loss": -114.4771,
      "step": 15370
    },
    {
      "epoch": 1.2304,
      "grad_norm": 55.61539077758789,
      "learning_rate": 0.00015898666666666668,
      "loss": -114.7596,
      "step": 15380
    },
    {
      "epoch": 1.2312,
      "grad_norm": 70.02622985839844,
      "learning_rate": 0.00015896,
      "loss": -114.8417,
      "step": 15390
    },
    {
      "epoch": 1.232,
      "grad_norm": 75.94625091552734,
      "learning_rate": 0.00015893333333333334,
      "loss": -114.9993,
      "step": 15400
    },
    {
      "epoch": 1.2328000000000001,
      "grad_norm": 76.9031982421875,
      "learning_rate": 0.00015890666666666667,
      "loss": -115.8023,
      "step": 15410
    },
    {
      "epoch": 1.2336,
      "grad_norm": 55.90003204345703,
      "learning_rate": 0.00015888,
      "loss": -114.6129,
      "step": 15420
    },
    {
      "epoch": 1.2344,
      "grad_norm": 48.79921340942383,
      "learning_rate": 0.00015885333333333335,
      "loss": -114.3365,
      "step": 15430
    },
    {
      "epoch": 1.2352,
      "grad_norm": 61.12717056274414,
      "learning_rate": 0.00015882666666666668,
      "loss": -114.0477,
      "step": 15440
    },
    {
      "epoch": 1.236,
      "grad_norm": 65.34709167480469,
      "learning_rate": 0.0001588,
      "loss": -114.1374,
      "step": 15450
    },
    {
      "epoch": 1.2368000000000001,
      "grad_norm": 75.36463165283203,
      "learning_rate": 0.00015877333333333334,
      "loss": -114.9193,
      "step": 15460
    },
    {
      "epoch": 1.2376,
      "grad_norm": 78.38164520263672,
      "learning_rate": 0.00015874666666666667,
      "loss": -113.4824,
      "step": 15470
    },
    {
      "epoch": 1.2384,
      "grad_norm": 84.18028259277344,
      "learning_rate": 0.00015872,
      "loss": -113.7796,
      "step": 15480
    },
    {
      "epoch": 1.2392,
      "grad_norm": 54.51068115234375,
      "learning_rate": 0.00015869333333333333,
      "loss": -114.6371,
      "step": 15490
    },
    {
      "epoch": 1.24,
      "grad_norm": 62.44926071166992,
      "learning_rate": 0.00015866666666666668,
      "loss": -113.5,
      "step": 15500
    },
    {
      "epoch": 1.2408,
      "grad_norm": 53.988346099853516,
      "learning_rate": 0.00015864,
      "loss": -114.3766,
      "step": 15510
    },
    {
      "epoch": 1.2416,
      "grad_norm": 68.44823455810547,
      "learning_rate": 0.00015861333333333334,
      "loss": -115.545,
      "step": 15520
    },
    {
      "epoch": 1.2424,
      "grad_norm": 52.19215774536133,
      "learning_rate": 0.00015858666666666667,
      "loss": -116.308,
      "step": 15530
    },
    {
      "epoch": 1.2432,
      "grad_norm": 67.90331268310547,
      "learning_rate": 0.00015856,
      "loss": -114.6244,
      "step": 15540
    },
    {
      "epoch": 1.244,
      "grad_norm": 69.59051513671875,
      "learning_rate": 0.00015853333333333333,
      "loss": -114.462,
      "step": 15550
    },
    {
      "epoch": 1.2448,
      "grad_norm": 114.30504608154297,
      "learning_rate": 0.00015850666666666666,
      "loss": -114.6299,
      "step": 15560
    },
    {
      "epoch": 1.2456,
      "grad_norm": 49.429080963134766,
      "learning_rate": 0.00015848000000000001,
      "loss": -114.2859,
      "step": 15570
    },
    {
      "epoch": 1.2464,
      "grad_norm": 66.91151428222656,
      "learning_rate": 0.00015845333333333334,
      "loss": -115.6816,
      "step": 15580
    },
    {
      "epoch": 1.2472,
      "grad_norm": 67.74938201904297,
      "learning_rate": 0.00015842666666666667,
      "loss": -115.43,
      "step": 15590
    },
    {
      "epoch": 1.248,
      "grad_norm": 63.28197479248047,
      "learning_rate": 0.00015840000000000003,
      "loss": -114.7126,
      "step": 15600
    },
    {
      "epoch": 1.2488,
      "grad_norm": 64.1523208618164,
      "learning_rate": 0.00015837333333333336,
      "loss": -115.2617,
      "step": 15610
    },
    {
      "epoch": 1.2496,
      "grad_norm": 61.21489715576172,
      "learning_rate": 0.00015834666666666666,
      "loss": -115.4385,
      "step": 15620
    },
    {
      "epoch": 1.2504,
      "grad_norm": 61.0362548828125,
      "learning_rate": 0.00015832,
      "loss": -113.9302,
      "step": 15630
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 52.52396011352539,
      "learning_rate": 0.00015829333333333334,
      "loss": -116.1879,
      "step": 15640
    },
    {
      "epoch": 1.252,
      "grad_norm": 71.1827163696289,
      "learning_rate": 0.00015826666666666667,
      "loss": -115.6405,
      "step": 15650
    },
    {
      "epoch": 1.2528000000000001,
      "grad_norm": 49.57060241699219,
      "learning_rate": 0.00015824,
      "loss": -114.704,
      "step": 15660
    },
    {
      "epoch": 1.2536,
      "grad_norm": 51.013240814208984,
      "learning_rate": 0.00015821333333333336,
      "loss": -114.7543,
      "step": 15670
    },
    {
      "epoch": 1.2544,
      "grad_norm": 78.68618774414062,
      "learning_rate": 0.0001581866666666667,
      "loss": -115.1937,
      "step": 15680
    },
    {
      "epoch": 1.2551999999999999,
      "grad_norm": 120.26351165771484,
      "learning_rate": 0.00015816,
      "loss": -114.2287,
      "step": 15690
    },
    {
      "epoch": 1.256,
      "grad_norm": 59.99007797241211,
      "learning_rate": 0.00015813333333333335,
      "loss": -115.7215,
      "step": 15700
    },
    {
      "epoch": 1.2568,
      "grad_norm": 61.28348922729492,
      "learning_rate": 0.00015810666666666668,
      "loss": -114.7988,
      "step": 15710
    },
    {
      "epoch": 1.2576,
      "grad_norm": 69.2947006225586,
      "learning_rate": 0.00015808,
      "loss": -114.9241,
      "step": 15720
    },
    {
      "epoch": 1.2584,
      "grad_norm": 64.64073944091797,
      "learning_rate": 0.00015805333333333333,
      "loss": -115.0072,
      "step": 15730
    },
    {
      "epoch": 1.2591999999999999,
      "grad_norm": 63.909542083740234,
      "learning_rate": 0.0001580266666666667,
      "loss": -114.6502,
      "step": 15740
    },
    {
      "epoch": 1.26,
      "grad_norm": 65.77407836914062,
      "learning_rate": 0.00015800000000000002,
      "loss": -115.1723,
      "step": 15750
    },
    {
      "epoch": 1.2608,
      "grad_norm": 58.566192626953125,
      "learning_rate": 0.00015797333333333335,
      "loss": -115.0826,
      "step": 15760
    },
    {
      "epoch": 1.2616,
      "grad_norm": 76.12036895751953,
      "learning_rate": 0.00015794666666666668,
      "loss": -113.6202,
      "step": 15770
    },
    {
      "epoch": 1.2624,
      "grad_norm": 71.55289459228516,
      "learning_rate": 0.00015792,
      "loss": -114.3429,
      "step": 15780
    },
    {
      "epoch": 1.2631999999999999,
      "grad_norm": 66.14604949951172,
      "learning_rate": 0.00015789333333333333,
      "loss": -115.4382,
      "step": 15790
    },
    {
      "epoch": 1.264,
      "grad_norm": 56.33871841430664,
      "learning_rate": 0.00015786666666666666,
      "loss": -114.8968,
      "step": 15800
    },
    {
      "epoch": 1.2648,
      "grad_norm": 54.69660186767578,
      "learning_rate": 0.00015784000000000002,
      "loss": -115.8587,
      "step": 15810
    },
    {
      "epoch": 1.2656,
      "grad_norm": 65.52230834960938,
      "learning_rate": 0.00015781333333333335,
      "loss": -115.7509,
      "step": 15820
    },
    {
      "epoch": 1.2664,
      "grad_norm": 79.84103393554688,
      "learning_rate": 0.00015778666666666668,
      "loss": -113.9721,
      "step": 15830
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 91.3614501953125,
      "learning_rate": 0.00015776,
      "loss": -113.1522,
      "step": 15840
    },
    {
      "epoch": 1.268,
      "grad_norm": 69.69999694824219,
      "learning_rate": 0.00015773333333333334,
      "loss": -114.5475,
      "step": 15850
    },
    {
      "epoch": 1.2688,
      "grad_norm": 70.65373992919922,
      "learning_rate": 0.00015770666666666667,
      "loss": -115.2246,
      "step": 15860
    },
    {
      "epoch": 1.2696,
      "grad_norm": 99.45852661132812,
      "learning_rate": 0.00015768,
      "loss": -114.7085,
      "step": 15870
    },
    {
      "epoch": 1.2704,
      "grad_norm": 77.83953094482422,
      "learning_rate": 0.00015765333333333335,
      "loss": -115.6257,
      "step": 15880
    },
    {
      "epoch": 1.2711999999999999,
      "grad_norm": 92.62419128417969,
      "learning_rate": 0.00015762666666666668,
      "loss": -114.9272,
      "step": 15890
    },
    {
      "epoch": 1.272,
      "grad_norm": 64.55335998535156,
      "learning_rate": 0.0001576,
      "loss": -113.7511,
      "step": 15900
    },
    {
      "epoch": 1.2728,
      "grad_norm": 81.64805603027344,
      "learning_rate": 0.00015757333333333334,
      "loss": -114.8882,
      "step": 15910
    },
    {
      "epoch": 1.2736,
      "grad_norm": 63.119537353515625,
      "learning_rate": 0.00015754666666666667,
      "loss": -114.7927,
      "step": 15920
    },
    {
      "epoch": 1.2744,
      "grad_norm": 65.25743103027344,
      "learning_rate": 0.00015752,
      "loss": -115.8652,
      "step": 15930
    },
    {
      "epoch": 1.2752,
      "grad_norm": 81.48893737792969,
      "learning_rate": 0.00015749333333333333,
      "loss": -115.8974,
      "step": 15940
    },
    {
      "epoch": 1.276,
      "grad_norm": 86.8906478881836,
      "learning_rate": 0.00015746666666666668,
      "loss": -115.0604,
      "step": 15950
    },
    {
      "epoch": 1.2768,
      "grad_norm": 62.310176849365234,
      "learning_rate": 0.00015744,
      "loss": -114.3477,
      "step": 15960
    },
    {
      "epoch": 1.2776,
      "grad_norm": 73.21205139160156,
      "learning_rate": 0.00015741333333333334,
      "loss": -114.7913,
      "step": 15970
    },
    {
      "epoch": 1.2784,
      "grad_norm": 71.38441467285156,
      "learning_rate": 0.0001573866666666667,
      "loss": -115.1027,
      "step": 15980
    },
    {
      "epoch": 1.2792,
      "grad_norm": 61.98588943481445,
      "learning_rate": 0.00015736000000000002,
      "loss": -114.764,
      "step": 15990
    },
    {
      "epoch": 1.28,
      "grad_norm": 78.85823059082031,
      "learning_rate": 0.00015733333333333333,
      "loss": -115.3353,
      "step": 16000
    },
    {
      "epoch": 1.2808,
      "grad_norm": 53.80997848510742,
      "learning_rate": 0.00015730666666666666,
      "loss": -116.2641,
      "step": 16010
    },
    {
      "epoch": 1.2816,
      "grad_norm": 65.71346282958984,
      "learning_rate": 0.00015728,
      "loss": -113.5889,
      "step": 16020
    },
    {
      "epoch": 1.2824,
      "grad_norm": 66.46076965332031,
      "learning_rate": 0.00015725333333333334,
      "loss": -115.1969,
      "step": 16030
    },
    {
      "epoch": 1.2832,
      "grad_norm": 70.2730941772461,
      "learning_rate": 0.00015722666666666667,
      "loss": -115.5045,
      "step": 16040
    },
    {
      "epoch": 1.284,
      "grad_norm": 74.98191833496094,
      "learning_rate": 0.00015720000000000003,
      "loss": -115.2227,
      "step": 16050
    },
    {
      "epoch": 1.2848,
      "grad_norm": 74.65802764892578,
      "learning_rate": 0.00015717333333333336,
      "loss": -115.2632,
      "step": 16060
    },
    {
      "epoch": 1.2856,
      "grad_norm": 51.1591911315918,
      "learning_rate": 0.00015714666666666666,
      "loss": -113.7338,
      "step": 16070
    },
    {
      "epoch": 1.2864,
      "grad_norm": 78.10047912597656,
      "learning_rate": 0.00015712000000000001,
      "loss": -115.5451,
      "step": 16080
    },
    {
      "epoch": 1.2872,
      "grad_norm": 82.2795639038086,
      "learning_rate": 0.00015709333333333334,
      "loss": -116.4968,
      "step": 16090
    },
    {
      "epoch": 1.288,
      "grad_norm": 83.01210021972656,
      "learning_rate": 0.00015706666666666667,
      "loss": -114.9422,
      "step": 16100
    },
    {
      "epoch": 1.2888,
      "grad_norm": 67.21568298339844,
      "learning_rate": 0.00015704,
      "loss": -115.3036,
      "step": 16110
    },
    {
      "epoch": 1.2896,
      "grad_norm": 63.897979736328125,
      "learning_rate": 0.00015701333333333336,
      "loss": -114.5853,
      "step": 16120
    },
    {
      "epoch": 1.2904,
      "grad_norm": 62.15467071533203,
      "learning_rate": 0.00015698666666666669,
      "loss": -115.3002,
      "step": 16130
    },
    {
      "epoch": 1.2912,
      "grad_norm": 115.12538146972656,
      "learning_rate": 0.00015696000000000002,
      "loss": -115.285,
      "step": 16140
    },
    {
      "epoch": 1.292,
      "grad_norm": 87.23673248291016,
      "learning_rate": 0.00015693333333333334,
      "loss": -116.0282,
      "step": 16150
    },
    {
      "epoch": 1.2928,
      "grad_norm": 67.70500946044922,
      "learning_rate": 0.00015690666666666667,
      "loss": -116.0644,
      "step": 16160
    },
    {
      "epoch": 1.2936,
      "grad_norm": 74.4842300415039,
      "learning_rate": 0.00015688,
      "loss": -114.3819,
      "step": 16170
    },
    {
      "epoch": 1.2944,
      "grad_norm": 122.40225219726562,
      "learning_rate": 0.00015685333333333333,
      "loss": -114.6418,
      "step": 16180
    },
    {
      "epoch": 1.2952,
      "grad_norm": 58.49176025390625,
      "learning_rate": 0.0001568266666666667,
      "loss": -115.1177,
      "step": 16190
    },
    {
      "epoch": 1.296,
      "grad_norm": 69.60902404785156,
      "learning_rate": 0.00015680000000000002,
      "loss": -114.4257,
      "step": 16200
    },
    {
      "epoch": 1.2968,
      "grad_norm": 772.8843994140625,
      "learning_rate": 0.00015677333333333335,
      "loss": -112.9261,
      "step": 16210
    },
    {
      "epoch": 1.2976,
      "grad_norm": 107.17015075683594,
      "learning_rate": 0.00015674666666666667,
      "loss": -111.7904,
      "step": 16220
    },
    {
      "epoch": 1.2984,
      "grad_norm": 78.95710754394531,
      "learning_rate": 0.00015672,
      "loss": -115.6779,
      "step": 16230
    },
    {
      "epoch": 1.2992,
      "grad_norm": 64.15848541259766,
      "learning_rate": 0.00015669333333333333,
      "loss": -113.9931,
      "step": 16240
    },
    {
      "epoch": 1.3,
      "grad_norm": 87.58484649658203,
      "learning_rate": 0.00015666666666666666,
      "loss": -114.8104,
      "step": 16250
    },
    {
      "epoch": 1.3008,
      "grad_norm": 58.95500564575195,
      "learning_rate": 0.00015664000000000002,
      "loss": -114.619,
      "step": 16260
    },
    {
      "epoch": 1.3016,
      "grad_norm": 84.90796661376953,
      "learning_rate": 0.00015661333333333335,
      "loss": -115.0985,
      "step": 16270
    },
    {
      "epoch": 1.3024,
      "grad_norm": 120.64408874511719,
      "learning_rate": 0.00015658666666666668,
      "loss": -115.5895,
      "step": 16280
    },
    {
      "epoch": 1.3032,
      "grad_norm": 93.01589965820312,
      "learning_rate": 0.00015656,
      "loss": -114.5221,
      "step": 16290
    },
    {
      "epoch": 1.304,
      "grad_norm": 67.82109069824219,
      "learning_rate": 0.00015653333333333333,
      "loss": -114.7282,
      "step": 16300
    },
    {
      "epoch": 1.3048,
      "grad_norm": 80.30514526367188,
      "learning_rate": 0.00015650666666666666,
      "loss": -114.4577,
      "step": 16310
    },
    {
      "epoch": 1.3056,
      "grad_norm": 60.241153717041016,
      "learning_rate": 0.00015648,
      "loss": -113.21,
      "step": 16320
    },
    {
      "epoch": 1.3064,
      "grad_norm": 75.52767181396484,
      "learning_rate": 0.00015645333333333335,
      "loss": -115.509,
      "step": 16330
    },
    {
      "epoch": 1.3072,
      "grad_norm": 106.75074005126953,
      "learning_rate": 0.00015642666666666668,
      "loss": -114.4855,
      "step": 16340
    },
    {
      "epoch": 1.308,
      "grad_norm": 81.57096862792969,
      "learning_rate": 0.0001564,
      "loss": -114.3223,
      "step": 16350
    },
    {
      "epoch": 1.3088,
      "grad_norm": 81.22389221191406,
      "learning_rate": 0.00015637333333333334,
      "loss": -114.4565,
      "step": 16360
    },
    {
      "epoch": 1.3096,
      "grad_norm": 163.5589599609375,
      "learning_rate": 0.00015634666666666667,
      "loss": -115.011,
      "step": 16370
    },
    {
      "epoch": 1.3104,
      "grad_norm": 58.090240478515625,
      "learning_rate": 0.00015632,
      "loss": -115.582,
      "step": 16380
    },
    {
      "epoch": 1.3112,
      "grad_norm": 98.97676849365234,
      "learning_rate": 0.00015629333333333332,
      "loss": -113.687,
      "step": 16390
    },
    {
      "epoch": 1.312,
      "grad_norm": 73.71510314941406,
      "learning_rate": 0.00015626666666666668,
      "loss": -114.7824,
      "step": 16400
    },
    {
      "epoch": 1.3128,
      "grad_norm": 90.36725616455078,
      "learning_rate": 0.00015624,
      "loss": -114.1302,
      "step": 16410
    },
    {
      "epoch": 1.3136,
      "grad_norm": 64.13126373291016,
      "learning_rate": 0.00015621333333333334,
      "loss": -114.1679,
      "step": 16420
    },
    {
      "epoch": 1.3144,
      "grad_norm": 82.82881164550781,
      "learning_rate": 0.0001561866666666667,
      "loss": -114.0715,
      "step": 16430
    },
    {
      "epoch": 1.3152,
      "grad_norm": 126.65470886230469,
      "learning_rate": 0.00015616000000000002,
      "loss": -113.7823,
      "step": 16440
    },
    {
      "epoch": 1.316,
      "grad_norm": 95.8511962890625,
      "learning_rate": 0.00015613333333333332,
      "loss": -115.2966,
      "step": 16450
    },
    {
      "epoch": 1.3168,
      "grad_norm": 62.27412414550781,
      "learning_rate": 0.00015610666666666668,
      "loss": -113.6475,
      "step": 16460
    },
    {
      "epoch": 1.3176,
      "grad_norm": 70.88752746582031,
      "learning_rate": 0.00015608,
      "loss": -114.2519,
      "step": 16470
    },
    {
      "epoch": 1.3184,
      "grad_norm": 71.811279296875,
      "learning_rate": 0.00015605333333333334,
      "loss": -113.5948,
      "step": 16480
    },
    {
      "epoch": 1.3192,
      "grad_norm": 75.82632446289062,
      "learning_rate": 0.00015602666666666667,
      "loss": -115.2355,
      "step": 16490
    },
    {
      "epoch": 1.32,
      "grad_norm": 57.94560241699219,
      "learning_rate": 0.00015600000000000002,
      "loss": -115.1534,
      "step": 16500
    },
    {
      "epoch": 1.3208,
      "grad_norm": 58.40652847290039,
      "learning_rate": 0.00015597333333333335,
      "loss": -113.7371,
      "step": 16510
    },
    {
      "epoch": 1.3216,
      "grad_norm": 64.99263000488281,
      "learning_rate": 0.00015594666666666666,
      "loss": -115.069,
      "step": 16520
    },
    {
      "epoch": 1.3224,
      "grad_norm": 163.9884490966797,
      "learning_rate": 0.00015592,
      "loss": -116.0522,
      "step": 16530
    },
    {
      "epoch": 1.3232,
      "grad_norm": 65.04570770263672,
      "learning_rate": 0.00015589333333333334,
      "loss": -114.1659,
      "step": 16540
    },
    {
      "epoch": 1.324,
      "grad_norm": 110.0140609741211,
      "learning_rate": 0.00015586666666666667,
      "loss": -116.5296,
      "step": 16550
    },
    {
      "epoch": 1.3248,
      "grad_norm": 115.91842651367188,
      "learning_rate": 0.00015584,
      "loss": -113.9111,
      "step": 16560
    },
    {
      "epoch": 1.3256000000000001,
      "grad_norm": 78.11107635498047,
      "learning_rate": 0.00015581333333333335,
      "loss": -114.7037,
      "step": 16570
    },
    {
      "epoch": 1.3264,
      "grad_norm": 59.704978942871094,
      "learning_rate": 0.00015578666666666668,
      "loss": -114.5512,
      "step": 16580
    },
    {
      "epoch": 1.3272,
      "grad_norm": 69.39225006103516,
      "learning_rate": 0.00015576,
      "loss": -114.8215,
      "step": 16590
    },
    {
      "epoch": 1.328,
      "grad_norm": 69.8138198852539,
      "learning_rate": 0.00015573333333333334,
      "loss": -115.8022,
      "step": 16600
    },
    {
      "epoch": 1.3288,
      "grad_norm": 44.7988166809082,
      "learning_rate": 0.00015570666666666667,
      "loss": -114.7751,
      "step": 16610
    },
    {
      "epoch": 1.3296000000000001,
      "grad_norm": 94.6048355102539,
      "learning_rate": 0.00015568,
      "loss": -114.4283,
      "step": 16620
    },
    {
      "epoch": 1.3304,
      "grad_norm": 64.53520965576172,
      "learning_rate": 0.00015565333333333333,
      "loss": -115.2046,
      "step": 16630
    },
    {
      "epoch": 1.3312,
      "grad_norm": 77.26148223876953,
      "learning_rate": 0.00015562666666666669,
      "loss": -114.7066,
      "step": 16640
    },
    {
      "epoch": 1.332,
      "grad_norm": 57.10806655883789,
      "learning_rate": 0.00015560000000000001,
      "loss": -114.4143,
      "step": 16650
    },
    {
      "epoch": 1.3328,
      "grad_norm": 126.77438354492188,
      "learning_rate": 0.00015557333333333334,
      "loss": -114.3425,
      "step": 16660
    },
    {
      "epoch": 1.3336000000000001,
      "grad_norm": 75.69691467285156,
      "learning_rate": 0.00015554666666666667,
      "loss": -113.7493,
      "step": 16670
    },
    {
      "epoch": 1.3344,
      "grad_norm": 86.56230163574219,
      "learning_rate": 0.00015552,
      "loss": -114.7643,
      "step": 16680
    },
    {
      "epoch": 1.3352,
      "grad_norm": 51.08270263671875,
      "learning_rate": 0.00015549333333333333,
      "loss": -113.9479,
      "step": 16690
    },
    {
      "epoch": 1.336,
      "grad_norm": 96.54212951660156,
      "learning_rate": 0.00015546666666666666,
      "loss": -115.8298,
      "step": 16700
    },
    {
      "epoch": 1.3368,
      "grad_norm": 97.56843566894531,
      "learning_rate": 0.00015544000000000002,
      "loss": -115.459,
      "step": 16710
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 68.85564422607422,
      "learning_rate": 0.00015541333333333335,
      "loss": -113.6072,
      "step": 16720
    },
    {
      "epoch": 1.3384,
      "grad_norm": 77.0892105102539,
      "learning_rate": 0.00015538666666666667,
      "loss": -114.7768,
      "step": 16730
    },
    {
      "epoch": 1.3392,
      "grad_norm": 97.71812438964844,
      "learning_rate": 0.00015536,
      "loss": -114.2472,
      "step": 16740
    },
    {
      "epoch": 1.34,
      "grad_norm": 85.23298645019531,
      "learning_rate": 0.00015533333333333333,
      "loss": -115.3711,
      "step": 16750
    },
    {
      "epoch": 1.3408,
      "grad_norm": 106.13204193115234,
      "learning_rate": 0.00015530666666666666,
      "loss": -115.0921,
      "step": 16760
    },
    {
      "epoch": 1.3416000000000001,
      "grad_norm": 78.591796875,
      "learning_rate": 0.00015528,
      "loss": -115.8826,
      "step": 16770
    },
    {
      "epoch": 1.3424,
      "grad_norm": 64.33605194091797,
      "learning_rate": 0.00015525333333333335,
      "loss": -114.0855,
      "step": 16780
    },
    {
      "epoch": 1.3432,
      "grad_norm": 101.95280456542969,
      "learning_rate": 0.00015522666666666668,
      "loss": -114.7928,
      "step": 16790
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 76.78368377685547,
      "learning_rate": 0.0001552,
      "loss": -115.7594,
      "step": 16800
    },
    {
      "epoch": 1.3448,
      "grad_norm": 88.67174530029297,
      "learning_rate": 0.00015517333333333336,
      "loss": -113.4053,
      "step": 16810
    },
    {
      "epoch": 1.3456000000000001,
      "grad_norm": 60.25768280029297,
      "learning_rate": 0.0001551466666666667,
      "loss": -114.7951,
      "step": 16820
    },
    {
      "epoch": 1.3464,
      "grad_norm": 48.45520782470703,
      "learning_rate": 0.00015512,
      "loss": -115.5058,
      "step": 16830
    },
    {
      "epoch": 1.3472,
      "grad_norm": 91.3466567993164,
      "learning_rate": 0.00015509333333333335,
      "loss": -114.3789,
      "step": 16840
    },
    {
      "epoch": 1.3479999999999999,
      "grad_norm": 97.52607727050781,
      "learning_rate": 0.00015506666666666668,
      "loss": -114.9315,
      "step": 16850
    },
    {
      "epoch": 1.3488,
      "grad_norm": 78.79407501220703,
      "learning_rate": 0.00015504,
      "loss": -116.1952,
      "step": 16860
    },
    {
      "epoch": 1.3496000000000001,
      "grad_norm": 105.06755065917969,
      "learning_rate": 0.00015501333333333334,
      "loss": -115.9658,
      "step": 16870
    },
    {
      "epoch": 1.3504,
      "grad_norm": 101.53050231933594,
      "learning_rate": 0.0001549866666666667,
      "loss": -115.4141,
      "step": 16880
    },
    {
      "epoch": 1.3512,
      "grad_norm": 102.50237274169922,
      "learning_rate": 0.00015496000000000002,
      "loss": -114.3934,
      "step": 16890
    },
    {
      "epoch": 1.3519999999999999,
      "grad_norm": 80.12030792236328,
      "learning_rate": 0.00015493333333333332,
      "loss": -116.6766,
      "step": 16900
    },
    {
      "epoch": 1.3528,
      "grad_norm": 69.00224304199219,
      "learning_rate": 0.00015490666666666668,
      "loss": -115.0785,
      "step": 16910
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 111.02576446533203,
      "learning_rate": 0.00015488,
      "loss": -114.8326,
      "step": 16920
    },
    {
      "epoch": 1.3544,
      "grad_norm": 62.76764678955078,
      "learning_rate": 0.00015485333333333334,
      "loss": -115.6438,
      "step": 16930
    },
    {
      "epoch": 1.3552,
      "grad_norm": 171.39315795898438,
      "learning_rate": 0.00015482666666666667,
      "loss": -115.6589,
      "step": 16940
    },
    {
      "epoch": 1.3559999999999999,
      "grad_norm": 79.42778015136719,
      "learning_rate": 0.00015480000000000002,
      "loss": -114.4614,
      "step": 16950
    },
    {
      "epoch": 1.3568,
      "grad_norm": 80.10526275634766,
      "learning_rate": 0.00015477333333333335,
      "loss": -115.2829,
      "step": 16960
    },
    {
      "epoch": 1.3576,
      "grad_norm": 107.54898834228516,
      "learning_rate": 0.00015474666666666668,
      "loss": -114.9441,
      "step": 16970
    },
    {
      "epoch": 1.3584,
      "grad_norm": 90.0042724609375,
      "learning_rate": 0.00015472,
      "loss": -114.2897,
      "step": 16980
    },
    {
      "epoch": 1.3592,
      "grad_norm": 95.03338623046875,
      "learning_rate": 0.00015469333333333334,
      "loss": -115.3061,
      "step": 16990
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 103.2951431274414,
      "learning_rate": 0.00015466666666666667,
      "loss": -116.4417,
      "step": 17000
    },
    {
      "epoch": 1.3608,
      "grad_norm": 128.1478729248047,
      "learning_rate": 0.00015464,
      "loss": -114.8642,
      "step": 17010
    },
    {
      "epoch": 1.3616,
      "grad_norm": 83.5162582397461,
      "learning_rate": 0.00015461333333333335,
      "loss": -115.6854,
      "step": 17020
    },
    {
      "epoch": 1.3624,
      "grad_norm": 96.93244171142578,
      "learning_rate": 0.00015458666666666668,
      "loss": -114.787,
      "step": 17030
    },
    {
      "epoch": 1.3632,
      "grad_norm": 107.56535339355469,
      "learning_rate": 0.00015456,
      "loss": -114.3117,
      "step": 17040
    },
    {
      "epoch": 1.3639999999999999,
      "grad_norm": 126.6244888305664,
      "learning_rate": 0.00015453333333333334,
      "loss": -113.9875,
      "step": 17050
    },
    {
      "epoch": 1.3648,
      "grad_norm": 91.03846740722656,
      "learning_rate": 0.00015450666666666667,
      "loss": -114.5103,
      "step": 17060
    },
    {
      "epoch": 1.3656,
      "grad_norm": 62.16288757324219,
      "learning_rate": 0.00015448,
      "loss": -114.7584,
      "step": 17070
    },
    {
      "epoch": 1.3664,
      "grad_norm": 212.02532958984375,
      "learning_rate": 0.00015445333333333333,
      "loss": -114.1839,
      "step": 17080
    },
    {
      "epoch": 1.3672,
      "grad_norm": 151.69691467285156,
      "learning_rate": 0.00015442666666666668,
      "loss": -113.764,
      "step": 17090
    },
    {
      "epoch": 1.3679999999999999,
      "grad_norm": 99.244873046875,
      "learning_rate": 0.0001544,
      "loss": -113.894,
      "step": 17100
    },
    {
      "epoch": 1.3688,
      "grad_norm": 146.77548217773438,
      "learning_rate": 0.00015437333333333334,
      "loss": -113.6039,
      "step": 17110
    },
    {
      "epoch": 1.3696,
      "grad_norm": 89.54518127441406,
      "learning_rate": 0.00015434666666666667,
      "loss": -114.9148,
      "step": 17120
    },
    {
      "epoch": 1.3704,
      "grad_norm": 57.71653366088867,
      "learning_rate": 0.00015432,
      "loss": -116.0776,
      "step": 17130
    },
    {
      "epoch": 1.3712,
      "grad_norm": 87.0379867553711,
      "learning_rate": 0.00015429333333333333,
      "loss": -115.7216,
      "step": 17140
    },
    {
      "epoch": 1.3719999999999999,
      "grad_norm": 84.24197387695312,
      "learning_rate": 0.00015426666666666666,
      "loss": -114.6993,
      "step": 17150
    },
    {
      "epoch": 1.3728,
      "grad_norm": 123.99501037597656,
      "learning_rate": 0.00015424000000000001,
      "loss": -113.2966,
      "step": 17160
    },
    {
      "epoch": 1.3736,
      "grad_norm": 105.42259216308594,
      "learning_rate": 0.00015421333333333334,
      "loss": -116.18,
      "step": 17170
    },
    {
      "epoch": 1.3744,
      "grad_norm": 78.63032531738281,
      "learning_rate": 0.00015418666666666667,
      "loss": -114.9193,
      "step": 17180
    },
    {
      "epoch": 1.3752,
      "grad_norm": 138.77560424804688,
      "learning_rate": 0.00015416000000000003,
      "loss": -115.0201,
      "step": 17190
    },
    {
      "epoch": 1.376,
      "grad_norm": 98.47191619873047,
      "learning_rate": 0.00015413333333333336,
      "loss": -115.1145,
      "step": 17200
    },
    {
      "epoch": 1.3768,
      "grad_norm": 118.61788940429688,
      "learning_rate": 0.00015410666666666666,
      "loss": -114.1625,
      "step": 17210
    },
    {
      "epoch": 1.3776,
      "grad_norm": 129.3617706298828,
      "learning_rate": 0.00015408000000000002,
      "loss": -114.9613,
      "step": 17220
    },
    {
      "epoch": 1.3784,
      "grad_norm": 72.84434509277344,
      "learning_rate": 0.00015405333333333334,
      "loss": -115.0775,
      "step": 17230
    },
    {
      "epoch": 1.3792,
      "grad_norm": 107.966064453125,
      "learning_rate": 0.00015402666666666667,
      "loss": -115.7508,
      "step": 17240
    },
    {
      "epoch": 1.38,
      "grad_norm": 101.05348205566406,
      "learning_rate": 0.000154,
      "loss": -115.761,
      "step": 17250
    },
    {
      "epoch": 1.3808,
      "grad_norm": 114.09282684326172,
      "learning_rate": 0.00015397333333333336,
      "loss": -115.2056,
      "step": 17260
    },
    {
      "epoch": 1.3816,
      "grad_norm": 102.27743530273438,
      "learning_rate": 0.0001539466666666667,
      "loss": -115.9483,
      "step": 17270
    },
    {
      "epoch": 1.3824,
      "grad_norm": 176.86581420898438,
      "learning_rate": 0.00015392,
      "loss": -116.0894,
      "step": 17280
    },
    {
      "epoch": 1.3832,
      "grad_norm": 113.4091796875,
      "learning_rate": 0.00015389333333333335,
      "loss": -113.1431,
      "step": 17290
    },
    {
      "epoch": 1.384,
      "grad_norm": 132.01315307617188,
      "learning_rate": 0.00015386666666666668,
      "loss": -114.5842,
      "step": 17300
    },
    {
      "epoch": 1.3848,
      "grad_norm": 176.1749725341797,
      "learning_rate": 0.00015384,
      "loss": -115.4279,
      "step": 17310
    },
    {
      "epoch": 1.3856,
      "grad_norm": 86.22509765625,
      "learning_rate": 0.00015381333333333333,
      "loss": -115.0501,
      "step": 17320
    },
    {
      "epoch": 1.3864,
      "grad_norm": 97.61686706542969,
      "learning_rate": 0.0001537866666666667,
      "loss": -114.7695,
      "step": 17330
    },
    {
      "epoch": 1.3872,
      "grad_norm": 111.5750503540039,
      "learning_rate": 0.00015376000000000002,
      "loss": -114.9692,
      "step": 17340
    },
    {
      "epoch": 1.388,
      "grad_norm": 87.89727020263672,
      "learning_rate": 0.00015373333333333335,
      "loss": -116.2915,
      "step": 17350
    },
    {
      "epoch": 1.3888,
      "grad_norm": 73.56796264648438,
      "learning_rate": 0.00015370666666666668,
      "loss": -115.4429,
      "step": 17360
    },
    {
      "epoch": 1.3896,
      "grad_norm": 87.98904418945312,
      "learning_rate": 0.00015368,
      "loss": -114.2506,
      "step": 17370
    },
    {
      "epoch": 1.3904,
      "grad_norm": 131.310791015625,
      "learning_rate": 0.00015365333333333333,
      "loss": -114.5834,
      "step": 17380
    },
    {
      "epoch": 1.3912,
      "grad_norm": 104.18254852294922,
      "learning_rate": 0.00015362666666666666,
      "loss": -115.1767,
      "step": 17390
    },
    {
      "epoch": 1.392,
      "grad_norm": 61.713905334472656,
      "learning_rate": 0.00015360000000000002,
      "loss": -115.6226,
      "step": 17400
    },
    {
      "epoch": 1.3928,
      "grad_norm": 117.40975952148438,
      "learning_rate": 0.00015357333333333335,
      "loss": -115.1487,
      "step": 17410
    },
    {
      "epoch": 1.3936,
      "grad_norm": 75.73595428466797,
      "learning_rate": 0.00015354666666666668,
      "loss": -113.9623,
      "step": 17420
    },
    {
      "epoch": 1.3944,
      "grad_norm": 122.25657653808594,
      "learning_rate": 0.00015352,
      "loss": -115.3132,
      "step": 17430
    },
    {
      "epoch": 1.3952,
      "grad_norm": 101.19903564453125,
      "learning_rate": 0.00015349333333333334,
      "loss": -116.3418,
      "step": 17440
    },
    {
      "epoch": 1.396,
      "grad_norm": 96.31082916259766,
      "learning_rate": 0.00015346666666666667,
      "loss": -115.7541,
      "step": 17450
    },
    {
      "epoch": 1.3968,
      "grad_norm": 94.24244689941406,
      "learning_rate": 0.00015344,
      "loss": -113.7735,
      "step": 17460
    },
    {
      "epoch": 1.3976,
      "grad_norm": 75.62796020507812,
      "learning_rate": 0.00015341333333333335,
      "loss": -115.0794,
      "step": 17470
    },
    {
      "epoch": 1.3984,
      "grad_norm": 161.42845153808594,
      "learning_rate": 0.00015338666666666668,
      "loss": -115.2572,
      "step": 17480
    },
    {
      "epoch": 1.3992,
      "grad_norm": 94.28527069091797,
      "learning_rate": 0.00015336,
      "loss": -116.0811,
      "step": 17490
    },
    {
      "epoch": 1.4,
      "grad_norm": 73.40411376953125,
      "learning_rate": 0.00015333333333333334,
      "loss": -116.3788,
      "step": 17500
    },
    {
      "epoch": 1.4008,
      "grad_norm": 106.11282348632812,
      "learning_rate": 0.00015330666666666667,
      "loss": -116.1084,
      "step": 17510
    },
    {
      "epoch": 1.4016,
      "grad_norm": 189.94876098632812,
      "learning_rate": 0.00015328,
      "loss": -114.7419,
      "step": 17520
    },
    {
      "epoch": 1.4024,
      "grad_norm": 149.2256622314453,
      "learning_rate": 0.00015325333333333333,
      "loss": -115.6957,
      "step": 17530
    },
    {
      "epoch": 1.4032,
      "grad_norm": 66.15582275390625,
      "learning_rate": 0.00015322666666666668,
      "loss": -114.199,
      "step": 17540
    },
    {
      "epoch": 1.404,
      "grad_norm": 49.109466552734375,
      "learning_rate": 0.0001532,
      "loss": -115.4166,
      "step": 17550
    },
    {
      "epoch": 1.4048,
      "grad_norm": 93.0813980102539,
      "learning_rate": 0.00015317333333333334,
      "loss": -115.1899,
      "step": 17560
    },
    {
      "epoch": 1.4056,
      "grad_norm": 91.90589141845703,
      "learning_rate": 0.0001531466666666667,
      "loss": -115.6583,
      "step": 17570
    },
    {
      "epoch": 1.4064,
      "grad_norm": 147.1775665283203,
      "learning_rate": 0.00015312,
      "loss": -116.6126,
      "step": 17580
    },
    {
      "epoch": 1.4072,
      "grad_norm": 87.8643798828125,
      "learning_rate": 0.00015309333333333333,
      "loss": -115.8774,
      "step": 17590
    },
    {
      "epoch": 1.408,
      "grad_norm": 142.28460693359375,
      "learning_rate": 0.00015306666666666666,
      "loss": -114.9736,
      "step": 17600
    },
    {
      "epoch": 1.4088,
      "grad_norm": 131.9186553955078,
      "learning_rate": 0.00015304,
      "loss": -116.0493,
      "step": 17610
    },
    {
      "epoch": 1.4096,
      "grad_norm": 140.47332763671875,
      "learning_rate": 0.00015301333333333334,
      "loss": -114.7704,
      "step": 17620
    },
    {
      "epoch": 1.4104,
      "grad_norm": 69.4457015991211,
      "learning_rate": 0.00015298666666666667,
      "loss": -115.6843,
      "step": 17630
    },
    {
      "epoch": 1.4112,
      "grad_norm": 165.82275390625,
      "learning_rate": 0.00015296000000000003,
      "loss": -115.0891,
      "step": 17640
    },
    {
      "epoch": 1.412,
      "grad_norm": 97.14949035644531,
      "learning_rate": 0.00015293333333333336,
      "loss": -116.2839,
      "step": 17650
    },
    {
      "epoch": 1.4128,
      "grad_norm": 63.30683135986328,
      "learning_rate": 0.00015290666666666666,
      "loss": -115.1248,
      "step": 17660
    },
    {
      "epoch": 1.4136,
      "grad_norm": 92.84410095214844,
      "learning_rate": 0.00015288,
      "loss": -116.0479,
      "step": 17670
    },
    {
      "epoch": 1.4144,
      "grad_norm": 137.94024658203125,
      "learning_rate": 0.00015285333333333334,
      "loss": -114.7177,
      "step": 17680
    },
    {
      "epoch": 1.4152,
      "grad_norm": 98.75066375732422,
      "learning_rate": 0.00015282666666666667,
      "loss": -114.3561,
      "step": 17690
    },
    {
      "epoch": 1.416,
      "grad_norm": 88.15868377685547,
      "learning_rate": 0.0001528,
      "loss": -116.4391,
      "step": 17700
    },
    {
      "epoch": 1.4168,
      "grad_norm": 89.73106384277344,
      "learning_rate": 0.00015277333333333336,
      "loss": -115.8323,
      "step": 17710
    },
    {
      "epoch": 1.4176,
      "grad_norm": 110.55674743652344,
      "learning_rate": 0.00015274666666666669,
      "loss": -116.3034,
      "step": 17720
    },
    {
      "epoch": 1.4184,
      "grad_norm": 112.49617004394531,
      "learning_rate": 0.00015272,
      "loss": -116.0716,
      "step": 17730
    },
    {
      "epoch": 1.4192,
      "grad_norm": 161.62319946289062,
      "learning_rate": 0.00015269333333333334,
      "loss": -115.2642,
      "step": 17740
    },
    {
      "epoch": 1.42,
      "grad_norm": 76.63613891601562,
      "learning_rate": 0.00015266666666666667,
      "loss": -116.1433,
      "step": 17750
    },
    {
      "epoch": 1.4208,
      "grad_norm": 121.79415130615234,
      "learning_rate": 0.00015264,
      "loss": -114.882,
      "step": 17760
    },
    {
      "epoch": 1.4216,
      "grad_norm": 104.71410369873047,
      "learning_rate": 0.00015261333333333333,
      "loss": -115.8487,
      "step": 17770
    },
    {
      "epoch": 1.4224,
      "grad_norm": 93.88151550292969,
      "learning_rate": 0.0001525866666666667,
      "loss": -113.944,
      "step": 17780
    },
    {
      "epoch": 1.4232,
      "grad_norm": 109.43047332763672,
      "learning_rate": 0.00015256000000000002,
      "loss": -114.9016,
      "step": 17790
    },
    {
      "epoch": 1.424,
      "grad_norm": 187.8695831298828,
      "learning_rate": 0.00015253333333333335,
      "loss": -115.9902,
      "step": 17800
    },
    {
      "epoch": 1.4248,
      "grad_norm": 66.53206634521484,
      "learning_rate": 0.00015250666666666667,
      "loss": -115.319,
      "step": 17810
    },
    {
      "epoch": 1.4256,
      "grad_norm": 71.1916732788086,
      "learning_rate": 0.00015248,
      "loss": -116.2813,
      "step": 17820
    },
    {
      "epoch": 1.4264000000000001,
      "grad_norm": 92.41678619384766,
      "learning_rate": 0.00015245333333333333,
      "loss": -115.0892,
      "step": 17830
    },
    {
      "epoch": 1.4272,
      "grad_norm": 72.15184783935547,
      "learning_rate": 0.00015242666666666666,
      "loss": -115.6256,
      "step": 17840
    },
    {
      "epoch": 1.428,
      "grad_norm": 114.06668090820312,
      "learning_rate": 0.00015240000000000002,
      "loss": -115.6842,
      "step": 17850
    },
    {
      "epoch": 1.4288,
      "grad_norm": 250.27066040039062,
      "learning_rate": 0.00015237333333333335,
      "loss": -115.0798,
      "step": 17860
    },
    {
      "epoch": 1.4296,
      "grad_norm": 141.8844757080078,
      "learning_rate": 0.00015234666666666668,
      "loss": -114.4561,
      "step": 17870
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 79.8505630493164,
      "learning_rate": 0.00015232,
      "loss": -117.3087,
      "step": 17880
    },
    {
      "epoch": 1.4312,
      "grad_norm": 154.81089782714844,
      "learning_rate": 0.00015229333333333333,
      "loss": -115.0128,
      "step": 17890
    },
    {
      "epoch": 1.432,
      "grad_norm": 113.76787567138672,
      "learning_rate": 0.00015226666666666666,
      "loss": -115.2107,
      "step": 17900
    },
    {
      "epoch": 1.4328,
      "grad_norm": 99.85737609863281,
      "learning_rate": 0.00015224,
      "loss": -116.0187,
      "step": 17910
    },
    {
      "epoch": 1.4336,
      "grad_norm": 171.55677795410156,
      "learning_rate": 0.00015221333333333335,
      "loss": -114.2756,
      "step": 17920
    },
    {
      "epoch": 1.4344000000000001,
      "grad_norm": 163.2982940673828,
      "learning_rate": 0.00015218666666666668,
      "loss": -115.774,
      "step": 17930
    },
    {
      "epoch": 1.4352,
      "grad_norm": 149.0499725341797,
      "learning_rate": 0.00015216,
      "loss": -113.5721,
      "step": 17940
    },
    {
      "epoch": 1.436,
      "grad_norm": 146.47386169433594,
      "learning_rate": 0.00015213333333333336,
      "loss": -115.4023,
      "step": 17950
    },
    {
      "epoch": 1.4368,
      "grad_norm": 195.08944702148438,
      "learning_rate": 0.00015210666666666666,
      "loss": -115.3525,
      "step": 17960
    },
    {
      "epoch": 1.4376,
      "grad_norm": 140.802001953125,
      "learning_rate": 0.00015208,
      "loss": -115.2593,
      "step": 17970
    },
    {
      "epoch": 1.4384000000000001,
      "grad_norm": 107.49595642089844,
      "learning_rate": 0.00015205333333333332,
      "loss": -114.9901,
      "step": 17980
    },
    {
      "epoch": 1.4392,
      "grad_norm": 80.19282531738281,
      "learning_rate": 0.00015202666666666668,
      "loss": -116.5206,
      "step": 17990
    },
    {
      "epoch": 1.44,
      "grad_norm": 184.98959350585938,
      "learning_rate": 0.000152,
      "loss": -114.5135,
      "step": 18000
    },
    {
      "epoch": 1.4408,
      "grad_norm": 142.6279296875,
      "learning_rate": 0.00015197333333333334,
      "loss": -114.8644,
      "step": 18010
    },
    {
      "epoch": 1.4416,
      "grad_norm": 232.3841094970703,
      "learning_rate": 0.0001519466666666667,
      "loss": -115.6808,
      "step": 18020
    },
    {
      "epoch": 1.4424000000000001,
      "grad_norm": 168.32005310058594,
      "learning_rate": 0.00015192000000000002,
      "loss": -114.5913,
      "step": 18030
    },
    {
      "epoch": 1.4432,
      "grad_norm": 128.15982055664062,
      "learning_rate": 0.00015189333333333332,
      "loss": -115.5277,
      "step": 18040
    },
    {
      "epoch": 1.444,
      "grad_norm": 152.1522674560547,
      "learning_rate": 0.00015186666666666668,
      "loss": -114.645,
      "step": 18050
    },
    {
      "epoch": 1.4447999999999999,
      "grad_norm": 225.24452209472656,
      "learning_rate": 0.00015184,
      "loss": -114.9344,
      "step": 18060
    },
    {
      "epoch": 1.4456,
      "grad_norm": 227.25100708007812,
      "learning_rate": 0.00015181333333333334,
      "loss": -114.4414,
      "step": 18070
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 149.58572387695312,
      "learning_rate": 0.00015178666666666667,
      "loss": -115.7105,
      "step": 18080
    },
    {
      "epoch": 1.4472,
      "grad_norm": 97.9288101196289,
      "learning_rate": 0.00015176000000000002,
      "loss": -116.3823,
      "step": 18090
    },
    {
      "epoch": 1.448,
      "grad_norm": 142.1650390625,
      "learning_rate": 0.00015173333333333335,
      "loss": -116.2564,
      "step": 18100
    },
    {
      "epoch": 1.4487999999999999,
      "grad_norm": 96.28116607666016,
      "learning_rate": 0.00015170666666666666,
      "loss": -115.9265,
      "step": 18110
    },
    {
      "epoch": 1.4496,
      "grad_norm": 106.31333923339844,
      "learning_rate": 0.00015168,
      "loss": -116.2889,
      "step": 18120
    },
    {
      "epoch": 1.4504000000000001,
      "grad_norm": 129.47879028320312,
      "learning_rate": 0.00015165333333333334,
      "loss": -115.4424,
      "step": 18130
    },
    {
      "epoch": 1.4512,
      "grad_norm": 90.49705505371094,
      "learning_rate": 0.00015162666666666667,
      "loss": -114.9517,
      "step": 18140
    },
    {
      "epoch": 1.452,
      "grad_norm": 138.20303344726562,
      "learning_rate": 0.0001516,
      "loss": -115.0532,
      "step": 18150
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 191.59596252441406,
      "learning_rate": 0.00015157333333333335,
      "loss": -115.6228,
      "step": 18160
    },
    {
      "epoch": 1.4536,
      "grad_norm": 124.96900939941406,
      "learning_rate": 0.00015154666666666668,
      "loss": -114.1716,
      "step": 18170
    },
    {
      "epoch": 1.4544000000000001,
      "grad_norm": 169.7991180419922,
      "learning_rate": 0.00015152,
      "loss": -116.0929,
      "step": 18180
    },
    {
      "epoch": 1.4552,
      "grad_norm": 184.28074645996094,
      "learning_rate": 0.00015149333333333334,
      "loss": -115.3819,
      "step": 18190
    },
    {
      "epoch": 1.456,
      "grad_norm": 105.40851593017578,
      "learning_rate": 0.00015146666666666667,
      "loss": -115.881,
      "step": 18200
    },
    {
      "epoch": 1.4567999999999999,
      "grad_norm": 118.74126434326172,
      "learning_rate": 0.00015144,
      "loss": -115.3833,
      "step": 18210
    },
    {
      "epoch": 1.4576,
      "grad_norm": 110.56692504882812,
      "learning_rate": 0.00015141333333333333,
      "loss": -115.6113,
      "step": 18220
    },
    {
      "epoch": 1.4584,
      "grad_norm": 105.51863098144531,
      "learning_rate": 0.00015138666666666669,
      "loss": -114.6124,
      "step": 18230
    },
    {
      "epoch": 1.4592,
      "grad_norm": 76.89215087890625,
      "learning_rate": 0.00015136000000000001,
      "loss": -116.3684,
      "step": 18240
    },
    {
      "epoch": 1.46,
      "grad_norm": 97.41831970214844,
      "learning_rate": 0.00015133333333333334,
      "loss": -114.6146,
      "step": 18250
    },
    {
      "epoch": 1.4607999999999999,
      "grad_norm": 172.13441467285156,
      "learning_rate": 0.00015130666666666667,
      "loss": -115.2355,
      "step": 18260
    },
    {
      "epoch": 1.4616,
      "grad_norm": 111.99754333496094,
      "learning_rate": 0.00015128,
      "loss": -115.4821,
      "step": 18270
    },
    {
      "epoch": 1.4624,
      "grad_norm": 131.30337524414062,
      "learning_rate": 0.00015125333333333333,
      "loss": -115.6157,
      "step": 18280
    },
    {
      "epoch": 1.4632,
      "grad_norm": 125.83778381347656,
      "learning_rate": 0.00015122666666666666,
      "loss": -115.9215,
      "step": 18290
    },
    {
      "epoch": 1.464,
      "grad_norm": 106.89964294433594,
      "learning_rate": 0.00015120000000000002,
      "loss": -116.9854,
      "step": 18300
    },
    {
      "epoch": 1.4647999999999999,
      "grad_norm": 133.2691650390625,
      "learning_rate": 0.00015117333333333335,
      "loss": -115.6006,
      "step": 18310
    },
    {
      "epoch": 1.4656,
      "grad_norm": 86.31963348388672,
      "learning_rate": 0.00015114666666666667,
      "loss": -116.2542,
      "step": 18320
    },
    {
      "epoch": 1.4664,
      "grad_norm": 159.8810272216797,
      "learning_rate": 0.00015112000000000003,
      "loss": -115.1782,
      "step": 18330
    },
    {
      "epoch": 1.4672,
      "grad_norm": 205.4381103515625,
      "learning_rate": 0.00015109333333333333,
      "loss": -114.9158,
      "step": 18340
    },
    {
      "epoch": 1.468,
      "grad_norm": 185.31114196777344,
      "learning_rate": 0.00015106666666666666,
      "loss": -114.8556,
      "step": 18350
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 178.12501525878906,
      "learning_rate": 0.00015104,
      "loss": -116.0519,
      "step": 18360
    },
    {
      "epoch": 1.4696,
      "grad_norm": 185.0387725830078,
      "learning_rate": 0.00015101333333333335,
      "loss": -115.2125,
      "step": 18370
    },
    {
      "epoch": 1.4704,
      "grad_norm": 104.61895751953125,
      "learning_rate": 0.00015098666666666668,
      "loss": -115.3439,
      "step": 18380
    },
    {
      "epoch": 1.4712,
      "grad_norm": 124.14167022705078,
      "learning_rate": 0.00015096,
      "loss": -115.6739,
      "step": 18390
    },
    {
      "epoch": 1.472,
      "grad_norm": 87.70626831054688,
      "learning_rate": 0.00015093333333333336,
      "loss": -115.7477,
      "step": 18400
    },
    {
      "epoch": 1.4727999999999999,
      "grad_norm": 110.68903350830078,
      "learning_rate": 0.00015090666666666666,
      "loss": -114.5295,
      "step": 18410
    },
    {
      "epoch": 1.4736,
      "grad_norm": 120.46629333496094,
      "learning_rate": 0.00015088,
      "loss": -114.7688,
      "step": 18420
    },
    {
      "epoch": 1.4744,
      "grad_norm": 156.96359252929688,
      "learning_rate": 0.00015085333333333335,
      "loss": -115.0222,
      "step": 18430
    },
    {
      "epoch": 1.4752,
      "grad_norm": 166.93624877929688,
      "learning_rate": 0.00015082666666666668,
      "loss": -116.2763,
      "step": 18440
    },
    {
      "epoch": 1.476,
      "grad_norm": 140.16505432128906,
      "learning_rate": 0.0001508,
      "loss": -115.983,
      "step": 18450
    },
    {
      "epoch": 1.4768,
      "grad_norm": 112.8608169555664,
      "learning_rate": 0.00015077333333333334,
      "loss": -114.7219,
      "step": 18460
    },
    {
      "epoch": 1.4776,
      "grad_norm": 222.93637084960938,
      "learning_rate": 0.0001507466666666667,
      "loss": -115.226,
      "step": 18470
    },
    {
      "epoch": 1.4784,
      "grad_norm": 173.019287109375,
      "learning_rate": 0.00015072000000000002,
      "loss": -115.8579,
      "step": 18480
    },
    {
      "epoch": 1.4792,
      "grad_norm": 126.49242401123047,
      "learning_rate": 0.00015069333333333332,
      "loss": -115.9774,
      "step": 18490
    },
    {
      "epoch": 1.48,
      "grad_norm": 199.53759765625,
      "learning_rate": 0.00015066666666666668,
      "loss": -115.6807,
      "step": 18500
    },
    {
      "epoch": 1.4808,
      "grad_norm": 95.5223388671875,
      "learning_rate": 0.00015064,
      "loss": -115.8208,
      "step": 18510
    },
    {
      "epoch": 1.4816,
      "grad_norm": 167.42669677734375,
      "learning_rate": 0.00015061333333333334,
      "loss": -116.7128,
      "step": 18520
    },
    {
      "epoch": 1.4824,
      "grad_norm": 117.57063293457031,
      "learning_rate": 0.00015058666666666667,
      "loss": -115.8243,
      "step": 18530
    },
    {
      "epoch": 1.4832,
      "grad_norm": 155.7910919189453,
      "learning_rate": 0.00015056000000000002,
      "loss": -115.8633,
      "step": 18540
    },
    {
      "epoch": 1.484,
      "grad_norm": 131.78286743164062,
      "learning_rate": 0.00015053333333333335,
      "loss": -116.1946,
      "step": 18550
    },
    {
      "epoch": 1.4848,
      "grad_norm": 113.91564178466797,
      "learning_rate": 0.00015050666666666668,
      "loss": -115.6914,
      "step": 18560
    },
    {
      "epoch": 1.4856,
      "grad_norm": 130.55397033691406,
      "learning_rate": 0.00015048,
      "loss": -115.9993,
      "step": 18570
    },
    {
      "epoch": 1.4864,
      "grad_norm": 113.47891235351562,
      "learning_rate": 0.00015045333333333334,
      "loss": -115.1526,
      "step": 18580
    },
    {
      "epoch": 1.4872,
      "grad_norm": 108.70222473144531,
      "learning_rate": 0.00015042666666666667,
      "loss": -115.6815,
      "step": 18590
    },
    {
      "epoch": 1.488,
      "grad_norm": 88.6073989868164,
      "learning_rate": 0.0001504,
      "loss": -116.8552,
      "step": 18600
    },
    {
      "epoch": 1.4888,
      "grad_norm": 125.49088287353516,
      "learning_rate": 0.00015037333333333335,
      "loss": -117.0074,
      "step": 18610
    },
    {
      "epoch": 1.4896,
      "grad_norm": 104.73514556884766,
      "learning_rate": 0.00015034666666666668,
      "loss": -116.7605,
      "step": 18620
    },
    {
      "epoch": 1.4904,
      "grad_norm": 154.51980590820312,
      "learning_rate": 0.00015032,
      "loss": -116.7597,
      "step": 18630
    },
    {
      "epoch": 1.4912,
      "grad_norm": 127.53409576416016,
      "learning_rate": 0.00015029333333333334,
      "loss": -115.6439,
      "step": 18640
    },
    {
      "epoch": 1.492,
      "grad_norm": 151.90438842773438,
      "learning_rate": 0.00015026666666666667,
      "loss": -116.9286,
      "step": 18650
    },
    {
      "epoch": 1.4928,
      "grad_norm": 104.15673828125,
      "learning_rate": 0.00015024,
      "loss": -115.9152,
      "step": 18660
    },
    {
      "epoch": 1.4936,
      "grad_norm": 141.10568237304688,
      "learning_rate": 0.00015021333333333333,
      "loss": -114.8121,
      "step": 18670
    },
    {
      "epoch": 1.4944,
      "grad_norm": 136.17318725585938,
      "learning_rate": 0.00015018666666666668,
      "loss": -116.292,
      "step": 18680
    },
    {
      "epoch": 1.4952,
      "grad_norm": 209.73873901367188,
      "learning_rate": 0.00015016,
      "loss": -114.8259,
      "step": 18690
    },
    {
      "epoch": 1.496,
      "grad_norm": 200.49478149414062,
      "learning_rate": 0.00015013333333333334,
      "loss": -114.8994,
      "step": 18700
    },
    {
      "epoch": 1.4968,
      "grad_norm": 140.9697265625,
      "learning_rate": 0.0001501066666666667,
      "loss": -115.1965,
      "step": 18710
    },
    {
      "epoch": 1.4976,
      "grad_norm": 151.14404296875,
      "learning_rate": 0.00015008,
      "loss": -116.9369,
      "step": 18720
    },
    {
      "epoch": 1.4984,
      "grad_norm": 123.34342193603516,
      "learning_rate": 0.00015005333333333333,
      "loss": -115.7607,
      "step": 18730
    },
    {
      "epoch": 1.4992,
      "grad_norm": 94.84044647216797,
      "learning_rate": 0.00015002666666666666,
      "loss": -116.9432,
      "step": 18740
    },
    {
      "epoch": 1.5,
      "grad_norm": 100.33491516113281,
      "learning_rate": 0.00015000000000000001,
      "loss": -116.7705,
      "step": 18750
    },
    {
      "epoch": 1.5008,
      "grad_norm": 127.5469970703125,
      "learning_rate": 0.00014997333333333334,
      "loss": -116.3576,
      "step": 18760
    },
    {
      "epoch": 1.5016,
      "grad_norm": 144.01626586914062,
      "learning_rate": 0.00014994666666666667,
      "loss": -116.1518,
      "step": 18770
    },
    {
      "epoch": 1.5024,
      "grad_norm": 81.61959075927734,
      "learning_rate": 0.00014992000000000003,
      "loss": -115.5265,
      "step": 18780
    },
    {
      "epoch": 1.5032,
      "grad_norm": 98.15473175048828,
      "learning_rate": 0.00014989333333333333,
      "loss": -116.0967,
      "step": 18790
    },
    {
      "epoch": 1.504,
      "grad_norm": 106.14464569091797,
      "learning_rate": 0.00014986666666666666,
      "loss": -115.5247,
      "step": 18800
    },
    {
      "epoch": 1.5048,
      "grad_norm": 145.24951171875,
      "learning_rate": 0.00014984000000000002,
      "loss": -117.3233,
      "step": 18810
    },
    {
      "epoch": 1.5056,
      "grad_norm": 183.17974853515625,
      "learning_rate": 0.00014981333333333334,
      "loss": -115.0235,
      "step": 18820
    },
    {
      "epoch": 1.5064,
      "grad_norm": 90.98548889160156,
      "learning_rate": 0.00014978666666666667,
      "loss": -116.6618,
      "step": 18830
    },
    {
      "epoch": 1.5072,
      "grad_norm": 122.4155044555664,
      "learning_rate": 0.00014976,
      "loss": -115.9697,
      "step": 18840
    },
    {
      "epoch": 1.508,
      "grad_norm": 113.578125,
      "learning_rate": 0.00014973333333333336,
      "loss": -115.859,
      "step": 18850
    },
    {
      "epoch": 1.5088,
      "grad_norm": 91.15010833740234,
      "learning_rate": 0.0001497066666666667,
      "loss": -116.6,
      "step": 18860
    },
    {
      "epoch": 1.5096,
      "grad_norm": 142.23367309570312,
      "learning_rate": 0.00014968,
      "loss": -115.8733,
      "step": 18870
    },
    {
      "epoch": 1.5104,
      "grad_norm": 133.78623962402344,
      "learning_rate": 0.00014965333333333335,
      "loss": -115.9124,
      "step": 18880
    },
    {
      "epoch": 1.5112,
      "grad_norm": 91.92915344238281,
      "learning_rate": 0.00014962666666666668,
      "loss": -115.0981,
      "step": 18890
    },
    {
      "epoch": 1.512,
      "grad_norm": 134.7578582763672,
      "learning_rate": 0.0001496,
      "loss": -116.3391,
      "step": 18900
    },
    {
      "epoch": 1.5128,
      "grad_norm": 83.88250732421875,
      "learning_rate": 0.00014957333333333333,
      "loss": -116.9993,
      "step": 18910
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 254.49420166015625,
      "learning_rate": 0.0001495466666666667,
      "loss": -117.2141,
      "step": 18920
    },
    {
      "epoch": 1.5144,
      "grad_norm": 102.33089447021484,
      "learning_rate": 0.00014952000000000002,
      "loss": -115.7563,
      "step": 18930
    },
    {
      "epoch": 1.5152,
      "grad_norm": 129.2823944091797,
      "learning_rate": 0.00014949333333333332,
      "loss": -115.1999,
      "step": 18940
    },
    {
      "epoch": 1.516,
      "grad_norm": 128.7246856689453,
      "learning_rate": 0.00014946666666666668,
      "loss": -116.217,
      "step": 18950
    },
    {
      "epoch": 1.5168,
      "grad_norm": 149.83409118652344,
      "learning_rate": 0.00014944,
      "loss": -116.8016,
      "step": 18960
    },
    {
      "epoch": 1.5175999999999998,
      "grad_norm": 136.04066467285156,
      "learning_rate": 0.00014941333333333333,
      "loss": -115.747,
      "step": 18970
    },
    {
      "epoch": 1.5184,
      "grad_norm": 94.2752914428711,
      "learning_rate": 0.00014938666666666666,
      "loss": -117.4533,
      "step": 18980
    },
    {
      "epoch": 1.5192,
      "grad_norm": 102.44189453125,
      "learning_rate": 0.00014936000000000002,
      "loss": -116.3212,
      "step": 18990
    },
    {
      "epoch": 1.52,
      "grad_norm": 74.66812133789062,
      "learning_rate": 0.00014933333333333335,
      "loss": -116.59,
      "step": 19000
    },
    {
      "epoch": 1.5208,
      "grad_norm": 120.82369232177734,
      "learning_rate": 0.00014930666666666668,
      "loss": -116.2878,
      "step": 19010
    },
    {
      "epoch": 1.5215999999999998,
      "grad_norm": 71.06863403320312,
      "learning_rate": 0.00014928,
      "loss": -115.4726,
      "step": 19020
    },
    {
      "epoch": 1.5224,
      "grad_norm": 128.14987182617188,
      "learning_rate": 0.00014925333333333334,
      "loss": -117.4018,
      "step": 19030
    },
    {
      "epoch": 1.5232,
      "grad_norm": 144.6573486328125,
      "learning_rate": 0.00014922666666666667,
      "loss": -116.1538,
      "step": 19040
    },
    {
      "epoch": 1.524,
      "grad_norm": 138.1166534423828,
      "learning_rate": 0.0001492,
      "loss": -116.5576,
      "step": 19050
    },
    {
      "epoch": 1.5248,
      "grad_norm": 124.34595489501953,
      "learning_rate": 0.00014917333333333335,
      "loss": -116.4695,
      "step": 19060
    },
    {
      "epoch": 1.5255999999999998,
      "grad_norm": 111.53895568847656,
      "learning_rate": 0.00014914666666666668,
      "loss": -116.6372,
      "step": 19070
    },
    {
      "epoch": 1.5264,
      "grad_norm": 116.14805603027344,
      "learning_rate": 0.00014912,
      "loss": -114.6015,
      "step": 19080
    },
    {
      "epoch": 1.5272000000000001,
      "grad_norm": 114.05790710449219,
      "learning_rate": 0.00014909333333333337,
      "loss": -115.3978,
      "step": 19090
    },
    {
      "epoch": 1.528,
      "grad_norm": 126.66028594970703,
      "learning_rate": 0.00014906666666666667,
      "loss": -116.1419,
      "step": 19100
    },
    {
      "epoch": 1.5288,
      "grad_norm": 103.25312805175781,
      "learning_rate": 0.00014904,
      "loss": -115.9657,
      "step": 19110
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 114.5788803100586,
      "learning_rate": 0.00014901333333333333,
      "loss": -115.0715,
      "step": 19120
    },
    {
      "epoch": 1.5304,
      "grad_norm": 104.76988220214844,
      "learning_rate": 0.00014898666666666668,
      "loss": -116.6733,
      "step": 19130
    },
    {
      "epoch": 1.5312000000000001,
      "grad_norm": 73.1800308227539,
      "learning_rate": 0.00014896,
      "loss": -115.5177,
      "step": 19140
    },
    {
      "epoch": 1.532,
      "grad_norm": 120.61203002929688,
      "learning_rate": 0.00014893333333333334,
      "loss": -116.8138,
      "step": 19150
    },
    {
      "epoch": 1.5328,
      "grad_norm": 121.82381439208984,
      "learning_rate": 0.0001489066666666667,
      "loss": -116.9085,
      "step": 19160
    },
    {
      "epoch": 1.5335999999999999,
      "grad_norm": 123.35436248779297,
      "learning_rate": 0.00014888,
      "loss": -115.9249,
      "step": 19170
    },
    {
      "epoch": 1.5344,
      "grad_norm": 138.04934692382812,
      "learning_rate": 0.00014885333333333333,
      "loss": -115.2447,
      "step": 19180
    },
    {
      "epoch": 1.5352000000000001,
      "grad_norm": 135.4754638671875,
      "learning_rate": 0.00014882666666666668,
      "loss": -115.6018,
      "step": 19190
    },
    {
      "epoch": 1.536,
      "grad_norm": 100.87679290771484,
      "learning_rate": 0.0001488,
      "loss": -115.5707,
      "step": 19200
    },
    {
      "epoch": 1.5368,
      "grad_norm": 172.39833068847656,
      "learning_rate": 0.00014877333333333334,
      "loss": -117.1926,
      "step": 19210
    },
    {
      "epoch": 1.5375999999999999,
      "grad_norm": 177.44622802734375,
      "learning_rate": 0.00014874666666666667,
      "loss": -115.1973,
      "step": 19220
    },
    {
      "epoch": 1.5384,
      "grad_norm": 127.31195831298828,
      "learning_rate": 0.00014872000000000003,
      "loss": -115.3427,
      "step": 19230
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 132.6955108642578,
      "learning_rate": 0.00014869333333333336,
      "loss": -116.5287,
      "step": 19240
    },
    {
      "epoch": 1.54,
      "grad_norm": 80.42927551269531,
      "learning_rate": 0.00014866666666666666,
      "loss": -116.4717,
      "step": 19250
    },
    {
      "epoch": 1.5408,
      "grad_norm": 89.02778625488281,
      "learning_rate": 0.00014864,
      "loss": -116.6381,
      "step": 19260
    },
    {
      "epoch": 1.5415999999999999,
      "grad_norm": 90.66706848144531,
      "learning_rate": 0.00014861333333333334,
      "loss": -115.588,
      "step": 19270
    },
    {
      "epoch": 1.5424,
      "grad_norm": 109.04704284667969,
      "learning_rate": 0.00014858666666666667,
      "loss": -116.842,
      "step": 19280
    },
    {
      "epoch": 1.5432000000000001,
      "grad_norm": 110.11137390136719,
      "learning_rate": 0.00014856,
      "loss": -116.7763,
      "step": 19290
    },
    {
      "epoch": 1.544,
      "grad_norm": 104.01205444335938,
      "learning_rate": 0.00014853333333333336,
      "loss": -116.7504,
      "step": 19300
    },
    {
      "epoch": 1.5448,
      "grad_norm": 97.30353546142578,
      "learning_rate": 0.00014850666666666669,
      "loss": -117.0224,
      "step": 19310
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 116.41276550292969,
      "learning_rate": 0.00014848,
      "loss": -116.2044,
      "step": 19320
    },
    {
      "epoch": 1.5464,
      "grad_norm": 133.76397705078125,
      "learning_rate": 0.00014845333333333334,
      "loss": -116.2004,
      "step": 19330
    },
    {
      "epoch": 1.5472000000000001,
      "grad_norm": 249.94387817382812,
      "learning_rate": 0.00014842666666666667,
      "loss": -116.1458,
      "step": 19340
    },
    {
      "epoch": 1.548,
      "grad_norm": 125.9734878540039,
      "learning_rate": 0.0001484,
      "loss": -114.2951,
      "step": 19350
    },
    {
      "epoch": 1.5488,
      "grad_norm": 200.18612670898438,
      "learning_rate": 0.00014837333333333333,
      "loss": -114.7211,
      "step": 19360
    },
    {
      "epoch": 1.5495999999999999,
      "grad_norm": 160.85667419433594,
      "learning_rate": 0.0001483466666666667,
      "loss": -116.9289,
      "step": 19370
    },
    {
      "epoch": 1.5504,
      "grad_norm": 111.0391845703125,
      "learning_rate": 0.00014832000000000002,
      "loss": -117.3408,
      "step": 19380
    },
    {
      "epoch": 1.5512000000000001,
      "grad_norm": 134.70606994628906,
      "learning_rate": 0.00014829333333333335,
      "loss": -117.3403,
      "step": 19390
    },
    {
      "epoch": 1.552,
      "grad_norm": 107.76385498046875,
      "learning_rate": 0.00014826666666666667,
      "loss": -117.3867,
      "step": 19400
    },
    {
      "epoch": 1.5528,
      "grad_norm": 76.1854476928711,
      "learning_rate": 0.00014824,
      "loss": -116.4979,
      "step": 19410
    },
    {
      "epoch": 1.5535999999999999,
      "grad_norm": 119.35245513916016,
      "learning_rate": 0.00014821333333333333,
      "loss": -116.3877,
      "step": 19420
    },
    {
      "epoch": 1.5544,
      "grad_norm": 79.49254608154297,
      "learning_rate": 0.00014818666666666666,
      "loss": -116.1273,
      "step": 19430
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 161.76995849609375,
      "learning_rate": 0.00014816000000000002,
      "loss": -115.7292,
      "step": 19440
    },
    {
      "epoch": 1.556,
      "grad_norm": 122.37934875488281,
      "learning_rate": 0.00014813333333333335,
      "loss": -116.0222,
      "step": 19450
    },
    {
      "epoch": 1.5568,
      "grad_norm": 87.25267028808594,
      "learning_rate": 0.00014810666666666668,
      "loss": -116.8125,
      "step": 19460
    },
    {
      "epoch": 1.5575999999999999,
      "grad_norm": 189.1361083984375,
      "learning_rate": 0.00014808,
      "loss": -116.9809,
      "step": 19470
    },
    {
      "epoch": 1.5584,
      "grad_norm": 88.21812438964844,
      "learning_rate": 0.00014805333333333333,
      "loss": -116.1219,
      "step": 19480
    },
    {
      "epoch": 1.5592000000000001,
      "grad_norm": 83.27996826171875,
      "learning_rate": 0.00014802666666666666,
      "loss": -115.8315,
      "step": 19490
    },
    {
      "epoch": 1.56,
      "grad_norm": 159.9205780029297,
      "learning_rate": 0.000148,
      "loss": -116.6735,
      "step": 19500
    },
    {
      "epoch": 1.5608,
      "grad_norm": 95.56216430664062,
      "learning_rate": 0.00014797333333333335,
      "loss": -116.7024,
      "step": 19510
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 143.1485595703125,
      "learning_rate": 0.00014794666666666668,
      "loss": -116.4601,
      "step": 19520
    },
    {
      "epoch": 1.5624,
      "grad_norm": 88.66869354248047,
      "learning_rate": 0.00014792,
      "loss": -115.7877,
      "step": 19530
    },
    {
      "epoch": 1.5632000000000001,
      "grad_norm": 164.39608764648438,
      "learning_rate": 0.00014789333333333336,
      "loss": -117.3889,
      "step": 19540
    },
    {
      "epoch": 1.564,
      "grad_norm": 193.72377014160156,
      "learning_rate": 0.00014786666666666666,
      "loss": -117.4002,
      "step": 19550
    },
    {
      "epoch": 1.5648,
      "grad_norm": 129.1273651123047,
      "learning_rate": 0.00014784,
      "loss": -115.2125,
      "step": 19560
    },
    {
      "epoch": 1.5655999999999999,
      "grad_norm": 134.1826629638672,
      "learning_rate": 0.00014781333333333335,
      "loss": -116.4155,
      "step": 19570
    },
    {
      "epoch": 1.5664,
      "grad_norm": 87.6408462524414,
      "learning_rate": 0.00014778666666666668,
      "loss": -116.5539,
      "step": 19580
    },
    {
      "epoch": 1.5672000000000001,
      "grad_norm": 163.57167053222656,
      "learning_rate": 0.00014776,
      "loss": -115.9918,
      "step": 19590
    },
    {
      "epoch": 1.568,
      "grad_norm": 135.43563842773438,
      "learning_rate": 0.00014773333333333334,
      "loss": -117.3572,
      "step": 19600
    },
    {
      "epoch": 1.5688,
      "grad_norm": 109.62943267822266,
      "learning_rate": 0.0001477066666666667,
      "loss": -117.4992,
      "step": 19610
    },
    {
      "epoch": 1.5695999999999999,
      "grad_norm": 106.80635070800781,
      "learning_rate": 0.00014768,
      "loss": -116.3359,
      "step": 19620
    },
    {
      "epoch": 1.5704,
      "grad_norm": 95.21373748779297,
      "learning_rate": 0.00014765333333333332,
      "loss": -117.0256,
      "step": 19630
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 82.431884765625,
      "learning_rate": 0.00014762666666666668,
      "loss": -116.0853,
      "step": 19640
    },
    {
      "epoch": 1.572,
      "grad_norm": 129.8419952392578,
      "learning_rate": 0.0001476,
      "loss": -117.0482,
      "step": 19650
    },
    {
      "epoch": 1.5728,
      "grad_norm": 103.40672302246094,
      "learning_rate": 0.00014757333333333334,
      "loss": -116.2959,
      "step": 19660
    },
    {
      "epoch": 1.5735999999999999,
      "grad_norm": 112.77284240722656,
      "learning_rate": 0.00014754666666666667,
      "loss": -116.4051,
      "step": 19670
    },
    {
      "epoch": 1.5744,
      "grad_norm": 236.16851806640625,
      "learning_rate": 0.00014752000000000002,
      "loss": -116.8925,
      "step": 19680
    },
    {
      "epoch": 1.5752000000000002,
      "grad_norm": 132.54627990722656,
      "learning_rate": 0.00014749333333333335,
      "loss": -115.48,
      "step": 19690
    },
    {
      "epoch": 1.576,
      "grad_norm": 127.16676330566406,
      "learning_rate": 0.00014746666666666666,
      "loss": -116.5439,
      "step": 19700
    },
    {
      "epoch": 1.5768,
      "grad_norm": 113.45777130126953,
      "learning_rate": 0.00014744,
      "loss": -116.8272,
      "step": 19710
    },
    {
      "epoch": 1.5776,
      "grad_norm": 86.49960327148438,
      "learning_rate": 0.00014741333333333334,
      "loss": -116.8372,
      "step": 19720
    },
    {
      "epoch": 1.5784,
      "grad_norm": 149.4482421875,
      "learning_rate": 0.00014738666666666667,
      "loss": -115.9916,
      "step": 19730
    },
    {
      "epoch": 1.5792000000000002,
      "grad_norm": 103.72601318359375,
      "learning_rate": 0.00014736,
      "loss": -116.3009,
      "step": 19740
    },
    {
      "epoch": 1.58,
      "grad_norm": 164.57545471191406,
      "learning_rate": 0.00014733333333333335,
      "loss": -118.3756,
      "step": 19750
    },
    {
      "epoch": 1.5808,
      "grad_norm": 133.71591186523438,
      "learning_rate": 0.00014730666666666668,
      "loss": -116.5399,
      "step": 19760
    },
    {
      "epoch": 1.5816,
      "grad_norm": 132.42906188964844,
      "learning_rate": 0.00014728,
      "loss": -116.4902,
      "step": 19770
    },
    {
      "epoch": 1.5824,
      "grad_norm": 123.28034210205078,
      "learning_rate": 0.00014725333333333334,
      "loss": -117.2663,
      "step": 19780
    },
    {
      "epoch": 1.5832000000000002,
      "grad_norm": 243.39816284179688,
      "learning_rate": 0.00014722666666666667,
      "loss": -116.1664,
      "step": 19790
    },
    {
      "epoch": 1.584,
      "grad_norm": 107.01947784423828,
      "learning_rate": 0.0001472,
      "loss": -116.4382,
      "step": 19800
    },
    {
      "epoch": 1.5848,
      "grad_norm": 102.12876892089844,
      "learning_rate": 0.00014717333333333333,
      "loss": -116.0692,
      "step": 19810
    },
    {
      "epoch": 1.5856,
      "grad_norm": 85.3353500366211,
      "learning_rate": 0.00014714666666666669,
      "loss": -116.8512,
      "step": 19820
    },
    {
      "epoch": 1.5864,
      "grad_norm": 97.76821899414062,
      "learning_rate": 0.00014712000000000001,
      "loss": -117.2457,
      "step": 19830
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 147.8069610595703,
      "learning_rate": 0.00014709333333333334,
      "loss": -116.9917,
      "step": 19840
    },
    {
      "epoch": 1.588,
      "grad_norm": 82.14156341552734,
      "learning_rate": 0.00014706666666666667,
      "loss": -116.8784,
      "step": 19850
    },
    {
      "epoch": 1.5888,
      "grad_norm": 92.46713256835938,
      "learning_rate": 0.00014704,
      "loss": -116.2898,
      "step": 19860
    },
    {
      "epoch": 1.5896,
      "grad_norm": 123.14788055419922,
      "learning_rate": 0.00014701333333333333,
      "loss": -117.365,
      "step": 19870
    },
    {
      "epoch": 1.5904,
      "grad_norm": 129.58099365234375,
      "learning_rate": 0.00014698666666666666,
      "loss": -117.1368,
      "step": 19880
    },
    {
      "epoch": 1.5912,
      "grad_norm": 119.22148895263672,
      "learning_rate": 0.00014696000000000002,
      "loss": -116.9669,
      "step": 19890
    },
    {
      "epoch": 1.592,
      "grad_norm": 117.46598815917969,
      "learning_rate": 0.00014693333333333335,
      "loss": -116.991,
      "step": 19900
    },
    {
      "epoch": 1.5928,
      "grad_norm": 108.69014739990234,
      "learning_rate": 0.00014690666666666667,
      "loss": -115.573,
      "step": 19910
    },
    {
      "epoch": 1.5936,
      "grad_norm": 523.8753662109375,
      "learning_rate": 0.00014688000000000003,
      "loss": -117.5982,
      "step": 19920
    },
    {
      "epoch": 1.5944,
      "grad_norm": 77.24771118164062,
      "learning_rate": 0.00014685333333333333,
      "loss": -116.814,
      "step": 19930
    },
    {
      "epoch": 1.5952,
      "grad_norm": 132.02964782714844,
      "learning_rate": 0.00014682666666666666,
      "loss": -116.5155,
      "step": 19940
    },
    {
      "epoch": 1.596,
      "grad_norm": 114.06372833251953,
      "learning_rate": 0.00014680000000000002,
      "loss": -116.8122,
      "step": 19950
    },
    {
      "epoch": 1.5968,
      "grad_norm": 129.67605590820312,
      "learning_rate": 0.00014677333333333335,
      "loss": -114.9593,
      "step": 19960
    },
    {
      "epoch": 1.5976,
      "grad_norm": 142.0677490234375,
      "learning_rate": 0.00014674666666666668,
      "loss": -117.7615,
      "step": 19970
    },
    {
      "epoch": 1.5984,
      "grad_norm": 137.57656860351562,
      "learning_rate": 0.00014672,
      "loss": -116.8631,
      "step": 19980
    },
    {
      "epoch": 1.5992,
      "grad_norm": 81.97193145751953,
      "learning_rate": 0.00014669333333333336,
      "loss": -118.1873,
      "step": 19990
    },
    {
      "epoch": 1.6,
      "grad_norm": 82.2662124633789,
      "learning_rate": 0.00014666666666666666,
      "loss": -116.6136,
      "step": 20000
    },
    {
      "epoch": 1.6008,
      "grad_norm": 100.04751586914062,
      "learning_rate": 0.00014664,
      "loss": -117.1446,
      "step": 20010
    },
    {
      "epoch": 1.6016,
      "grad_norm": 109.58934020996094,
      "learning_rate": 0.00014661333333333335,
      "loss": -117.1743,
      "step": 20020
    },
    {
      "epoch": 1.6024,
      "grad_norm": 139.27162170410156,
      "learning_rate": 0.00014658666666666668,
      "loss": -116.5539,
      "step": 20030
    },
    {
      "epoch": 1.6032,
      "grad_norm": 122.33140563964844,
      "learning_rate": 0.00014656,
      "loss": -116.8536,
      "step": 20040
    },
    {
      "epoch": 1.604,
      "grad_norm": 108.24520111083984,
      "learning_rate": 0.00014653333333333334,
      "loss": -117.2151,
      "step": 20050
    },
    {
      "epoch": 1.6048,
      "grad_norm": 104.11248016357422,
      "learning_rate": 0.0001465066666666667,
      "loss": -115.5828,
      "step": 20060
    },
    {
      "epoch": 1.6056,
      "grad_norm": 147.59666442871094,
      "learning_rate": 0.00014648000000000002,
      "loss": -116.3601,
      "step": 20070
    },
    {
      "epoch": 1.6064,
      "grad_norm": 129.79592895507812,
      "learning_rate": 0.00014645333333333332,
      "loss": -116.5945,
      "step": 20080
    },
    {
      "epoch": 1.6072,
      "grad_norm": 169.98928833007812,
      "learning_rate": 0.00014642666666666668,
      "loss": -116.8972,
      "step": 20090
    },
    {
      "epoch": 1.608,
      "grad_norm": 107.95149230957031,
      "learning_rate": 0.0001464,
      "loss": -116.0396,
      "step": 20100
    },
    {
      "epoch": 1.6088,
      "grad_norm": 76.93455505371094,
      "learning_rate": 0.00014637333333333334,
      "loss": -117.445,
      "step": 20110
    },
    {
      "epoch": 1.6096,
      "grad_norm": 139.754150390625,
      "learning_rate": 0.00014634666666666667,
      "loss": -115.5976,
      "step": 20120
    },
    {
      "epoch": 1.6104,
      "grad_norm": 94.96983337402344,
      "learning_rate": 0.00014632000000000002,
      "loss": -117.3311,
      "step": 20130
    },
    {
      "epoch": 1.6112,
      "grad_norm": 102.95079040527344,
      "learning_rate": 0.00014629333333333335,
      "loss": -117.4734,
      "step": 20140
    },
    {
      "epoch": 1.612,
      "grad_norm": 95.1776123046875,
      "learning_rate": 0.00014626666666666665,
      "loss": -116.553,
      "step": 20150
    },
    {
      "epoch": 1.6128,
      "grad_norm": 140.70599365234375,
      "learning_rate": 0.00014624,
      "loss": -115.4532,
      "step": 20160
    },
    {
      "epoch": 1.6136,
      "grad_norm": 108.72329711914062,
      "learning_rate": 0.00014621333333333334,
      "loss": -117.431,
      "step": 20170
    },
    {
      "epoch": 1.6143999999999998,
      "grad_norm": 101.01323699951172,
      "learning_rate": 0.00014618666666666667,
      "loss": -116.5575,
      "step": 20180
    },
    {
      "epoch": 1.6152,
      "grad_norm": 225.7964324951172,
      "learning_rate": 0.00014616,
      "loss": -116.5914,
      "step": 20190
    },
    {
      "epoch": 1.616,
      "grad_norm": 146.5361328125,
      "learning_rate": 0.00014613333333333335,
      "loss": -117.8876,
      "step": 20200
    },
    {
      "epoch": 1.6168,
      "grad_norm": 169.22344970703125,
      "learning_rate": 0.00014610666666666668,
      "loss": -117.2978,
      "step": 20210
    },
    {
      "epoch": 1.6176,
      "grad_norm": 76.30571746826172,
      "learning_rate": 0.00014608,
      "loss": -116.4054,
      "step": 20220
    },
    {
      "epoch": 1.6183999999999998,
      "grad_norm": 141.74363708496094,
      "learning_rate": 0.00014605333333333334,
      "loss": -116.4151,
      "step": 20230
    },
    {
      "epoch": 1.6192,
      "grad_norm": 127.22698974609375,
      "learning_rate": 0.00014602666666666667,
      "loss": -117.0326,
      "step": 20240
    },
    {
      "epoch": 1.62,
      "grad_norm": 204.96083068847656,
      "learning_rate": 0.000146,
      "loss": -117.3873,
      "step": 20250
    },
    {
      "epoch": 1.6208,
      "grad_norm": 239.80471801757812,
      "learning_rate": 0.00014597333333333333,
      "loss": -116.4289,
      "step": 20260
    },
    {
      "epoch": 1.6216,
      "grad_norm": 130.60830688476562,
      "learning_rate": 0.00014594666666666668,
      "loss": -116.7635,
      "step": 20270
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 100.23526000976562,
      "learning_rate": 0.00014592,
      "loss": -117.3628,
      "step": 20280
    },
    {
      "epoch": 1.6232,
      "grad_norm": 115.65084838867188,
      "learning_rate": 0.00014589333333333334,
      "loss": -117.4152,
      "step": 20290
    },
    {
      "epoch": 1.624,
      "grad_norm": 104.68840026855469,
      "learning_rate": 0.00014586666666666667,
      "loss": -117.1534,
      "step": 20300
    },
    {
      "epoch": 1.6248,
      "grad_norm": 120.30870056152344,
      "learning_rate": 0.00014584,
      "loss": -117.5583,
      "step": 20310
    },
    {
      "epoch": 1.6256,
      "grad_norm": 97.23719787597656,
      "learning_rate": 0.00014581333333333333,
      "loss": -117.402,
      "step": 20320
    },
    {
      "epoch": 1.6263999999999998,
      "grad_norm": 94.8646011352539,
      "learning_rate": 0.00014578666666666668,
      "loss": -116.5829,
      "step": 20330
    },
    {
      "epoch": 1.6272,
      "grad_norm": 85.47941589355469,
      "learning_rate": 0.00014576000000000001,
      "loss": -117.6972,
      "step": 20340
    },
    {
      "epoch": 1.6280000000000001,
      "grad_norm": 145.8193817138672,
      "learning_rate": 0.00014573333333333334,
      "loss": -116.5192,
      "step": 20350
    },
    {
      "epoch": 1.6288,
      "grad_norm": 150.4110565185547,
      "learning_rate": 0.00014570666666666667,
      "loss": -116.8682,
      "step": 20360
    },
    {
      "epoch": 1.6296,
      "grad_norm": 144.62277221679688,
      "learning_rate": 0.00014568000000000003,
      "loss": -116.71,
      "step": 20370
    },
    {
      "epoch": 1.6303999999999998,
      "grad_norm": 78.92864990234375,
      "learning_rate": 0.00014565333333333333,
      "loss": -118.0216,
      "step": 20380
    },
    {
      "epoch": 1.6312,
      "grad_norm": 135.98143005371094,
      "learning_rate": 0.00014562666666666666,
      "loss": -117.3766,
      "step": 20390
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 138.99961853027344,
      "learning_rate": 0.00014560000000000002,
      "loss": -117.7505,
      "step": 20400
    },
    {
      "epoch": 1.6328,
      "grad_norm": 128.9060516357422,
      "learning_rate": 0.00014557333333333334,
      "loss": -117.8081,
      "step": 20410
    },
    {
      "epoch": 1.6336,
      "grad_norm": 222.5364227294922,
      "learning_rate": 0.00014554666666666667,
      "loss": -117.3155,
      "step": 20420
    },
    {
      "epoch": 1.6343999999999999,
      "grad_norm": 87.83069610595703,
      "learning_rate": 0.00014552,
      "loss": -117.3535,
      "step": 20430
    },
    {
      "epoch": 1.6352,
      "grad_norm": 158.40176391601562,
      "learning_rate": 0.00014549333333333336,
      "loss": -117.0712,
      "step": 20440
    },
    {
      "epoch": 1.6360000000000001,
      "grad_norm": 137.69898986816406,
      "learning_rate": 0.0001454666666666667,
      "loss": -118.1904,
      "step": 20450
    },
    {
      "epoch": 1.6368,
      "grad_norm": 77.1274642944336,
      "learning_rate": 0.00014544,
      "loss": -117.6644,
      "step": 20460
    },
    {
      "epoch": 1.6376,
      "grad_norm": 158.26608276367188,
      "learning_rate": 0.00014541333333333335,
      "loss": -115.9137,
      "step": 20470
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 64.8165054321289,
      "learning_rate": 0.00014538666666666668,
      "loss": -116.8513,
      "step": 20480
    },
    {
      "epoch": 1.6392,
      "grad_norm": 60.57124328613281,
      "learning_rate": 0.00014536,
      "loss": -117.6331,
      "step": 20490
    },
    {
      "epoch": 1.6400000000000001,
      "grad_norm": 99.06470489501953,
      "learning_rate": 0.00014533333333333333,
      "loss": -116.5075,
      "step": 20500
    },
    {
      "epoch": 1.6408,
      "grad_norm": 118.67098236083984,
      "learning_rate": 0.0001453066666666667,
      "loss": -117.0322,
      "step": 20510
    },
    {
      "epoch": 1.6416,
      "grad_norm": 110.51368713378906,
      "learning_rate": 0.00014528000000000002,
      "loss": -116.71,
      "step": 20520
    },
    {
      "epoch": 1.6423999999999999,
      "grad_norm": 171.12509155273438,
      "learning_rate": 0.00014525333333333332,
      "loss": -117.6782,
      "step": 20530
    },
    {
      "epoch": 1.6432,
      "grad_norm": 114.42481231689453,
      "learning_rate": 0.00014522666666666668,
      "loss": -116.9454,
      "step": 20540
    },
    {
      "epoch": 1.6440000000000001,
      "grad_norm": 212.899169921875,
      "learning_rate": 0.0001452,
      "loss": -117.2481,
      "step": 20550
    },
    {
      "epoch": 1.6448,
      "grad_norm": 86.80517578125,
      "learning_rate": 0.00014517333333333333,
      "loss": -117.0163,
      "step": 20560
    },
    {
      "epoch": 1.6456,
      "grad_norm": 99.5357894897461,
      "learning_rate": 0.00014514666666666666,
      "loss": -116.7017,
      "step": 20570
    },
    {
      "epoch": 1.6463999999999999,
      "grad_norm": 126.91100311279297,
      "learning_rate": 0.00014512000000000002,
      "loss": -117.2318,
      "step": 20580
    },
    {
      "epoch": 1.6472,
      "grad_norm": 96.62116241455078,
      "learning_rate": 0.00014509333333333335,
      "loss": -116.1376,
      "step": 20590
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 100.1251449584961,
      "learning_rate": 0.00014506666666666668,
      "loss": -116.8892,
      "step": 20600
    },
    {
      "epoch": 1.6488,
      "grad_norm": 120.7854995727539,
      "learning_rate": 0.00014504,
      "loss": -116.8177,
      "step": 20610
    },
    {
      "epoch": 1.6496,
      "grad_norm": 132.33042907714844,
      "learning_rate": 0.00014501333333333334,
      "loss": -117.7601,
      "step": 20620
    },
    {
      "epoch": 1.6503999999999999,
      "grad_norm": 137.3877410888672,
      "learning_rate": 0.00014498666666666667,
      "loss": -116.7357,
      "step": 20630
    },
    {
      "epoch": 1.6512,
      "grad_norm": 68.55204010009766,
      "learning_rate": 0.00014496,
      "loss": -116.7355,
      "step": 20640
    },
    {
      "epoch": 1.6520000000000001,
      "grad_norm": 143.84986877441406,
      "learning_rate": 0.00014493333333333335,
      "loss": -118.0148,
      "step": 20650
    },
    {
      "epoch": 1.6528,
      "grad_norm": 93.58677673339844,
      "learning_rate": 0.00014490666666666668,
      "loss": -117.4891,
      "step": 20660
    },
    {
      "epoch": 1.6536,
      "grad_norm": 78.91841125488281,
      "learning_rate": 0.00014488,
      "loss": -117.8729,
      "step": 20670
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 101.32467651367188,
      "learning_rate": 0.00014485333333333334,
      "loss": -117.1577,
      "step": 20680
    },
    {
      "epoch": 1.6552,
      "grad_norm": 159.2547149658203,
      "learning_rate": 0.00014482666666666667,
      "loss": -117.6146,
      "step": 20690
    },
    {
      "epoch": 1.6560000000000001,
      "grad_norm": 104.71642303466797,
      "learning_rate": 0.0001448,
      "loss": -116.7355,
      "step": 20700
    },
    {
      "epoch": 1.6568,
      "grad_norm": 140.05426025390625,
      "learning_rate": 0.00014477333333333333,
      "loss": -116.9588,
      "step": 20710
    },
    {
      "epoch": 1.6576,
      "grad_norm": 116.21290588378906,
      "learning_rate": 0.00014474666666666668,
      "loss": -118.1058,
      "step": 20720
    },
    {
      "epoch": 1.6583999999999999,
      "grad_norm": 151.5200958251953,
      "learning_rate": 0.00014472,
      "loss": -116.9791,
      "step": 20730
    },
    {
      "epoch": 1.6592,
      "grad_norm": 110.93804931640625,
      "learning_rate": 0.00014469333333333334,
      "loss": -117.5216,
      "step": 20740
    },
    {
      "epoch": 1.6600000000000001,
      "grad_norm": 115.55313873291016,
      "learning_rate": 0.0001446666666666667,
      "loss": -116.07,
      "step": 20750
    },
    {
      "epoch": 1.6608,
      "grad_norm": 136.283935546875,
      "learning_rate": 0.00014464,
      "loss": -117.8956,
      "step": 20760
    },
    {
      "epoch": 1.6616,
      "grad_norm": 95.75353240966797,
      "learning_rate": 0.00014461333333333333,
      "loss": -117.4582,
      "step": 20770
    },
    {
      "epoch": 1.6623999999999999,
      "grad_norm": 143.55455017089844,
      "learning_rate": 0.00014458666666666668,
      "loss": -117.4676,
      "step": 20780
    },
    {
      "epoch": 1.6632,
      "grad_norm": 168.59898376464844,
      "learning_rate": 0.00014456,
      "loss": -117.7208,
      "step": 20790
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 64.42981719970703,
      "learning_rate": 0.00014453333333333334,
      "loss": -117.0733,
      "step": 20800
    },
    {
      "epoch": 1.6648,
      "grad_norm": 133.5110626220703,
      "learning_rate": 0.00014450666666666667,
      "loss": -118.055,
      "step": 20810
    },
    {
      "epoch": 1.6656,
      "grad_norm": 194.1951141357422,
      "learning_rate": 0.00014448000000000003,
      "loss": -116.9653,
      "step": 20820
    },
    {
      "epoch": 1.6663999999999999,
      "grad_norm": 119.00901794433594,
      "learning_rate": 0.00014445333333333333,
      "loss": -118.025,
      "step": 20830
    },
    {
      "epoch": 1.6672,
      "grad_norm": 116.98336029052734,
      "learning_rate": 0.00014442666666666666,
      "loss": -118.5379,
      "step": 20840
    },
    {
      "epoch": 1.6680000000000001,
      "grad_norm": 70.08898162841797,
      "learning_rate": 0.0001444,
      "loss": -117.0747,
      "step": 20850
    },
    {
      "epoch": 1.6688,
      "grad_norm": 108.08590698242188,
      "learning_rate": 0.00014437333333333334,
      "loss": -116.1376,
      "step": 20860
    },
    {
      "epoch": 1.6696,
      "grad_norm": 135.81239318847656,
      "learning_rate": 0.00014434666666666667,
      "loss": -118.8292,
      "step": 20870
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 163.4408416748047,
      "learning_rate": 0.00014432,
      "loss": -117.0197,
      "step": 20880
    },
    {
      "epoch": 1.6712,
      "grad_norm": 103.78763580322266,
      "learning_rate": 0.00014429333333333336,
      "loss": -117.334,
      "step": 20890
    },
    {
      "epoch": 1.6720000000000002,
      "grad_norm": 96.69371795654297,
      "learning_rate": 0.00014426666666666669,
      "loss": -117.8336,
      "step": 20900
    },
    {
      "epoch": 1.6728,
      "grad_norm": 141.59535217285156,
      "learning_rate": 0.00014424,
      "loss": -116.6907,
      "step": 20910
    },
    {
      "epoch": 1.6736,
      "grad_norm": 178.39256286621094,
      "learning_rate": 0.00014421333333333334,
      "loss": -116.3869,
      "step": 20920
    },
    {
      "epoch": 1.6743999999999999,
      "grad_norm": 102.0096206665039,
      "learning_rate": 0.00014418666666666667,
      "loss": -116.5785,
      "step": 20930
    },
    {
      "epoch": 1.6752,
      "grad_norm": 78.376220703125,
      "learning_rate": 0.00014416,
      "loss": -117.874,
      "step": 20940
    },
    {
      "epoch": 1.6760000000000002,
      "grad_norm": 117.75306701660156,
      "learning_rate": 0.00014413333333333333,
      "loss": -117.1584,
      "step": 20950
    },
    {
      "epoch": 1.6768,
      "grad_norm": 57.8436279296875,
      "learning_rate": 0.0001441066666666667,
      "loss": -116.6185,
      "step": 20960
    },
    {
      "epoch": 1.6776,
      "grad_norm": 68.39032745361328,
      "learning_rate": 0.00014408000000000002,
      "loss": -115.9156,
      "step": 20970
    },
    {
      "epoch": 1.6784,
      "grad_norm": 105.41043090820312,
      "learning_rate": 0.00014405333333333335,
      "loss": -117.9359,
      "step": 20980
    },
    {
      "epoch": 1.6792,
      "grad_norm": 149.011474609375,
      "learning_rate": 0.00014402666666666667,
      "loss": -117.9975,
      "step": 20990
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 82.45010375976562,
      "learning_rate": 0.000144,
      "loss": -116.5887,
      "step": 21000
    },
    {
      "epoch": 1.6808,
      "grad_norm": 128.9319610595703,
      "learning_rate": 0.00014397333333333333,
      "loss": -116.6453,
      "step": 21010
    },
    {
      "epoch": 1.6816,
      "grad_norm": 145.59107971191406,
      "learning_rate": 0.00014394666666666666,
      "loss": -117.5801,
      "step": 21020
    },
    {
      "epoch": 1.6824,
      "grad_norm": 96.08916473388672,
      "learning_rate": 0.00014392000000000002,
      "loss": -117.4557,
      "step": 21030
    },
    {
      "epoch": 1.6832,
      "grad_norm": 75.65036010742188,
      "learning_rate": 0.00014389333333333335,
      "loss": -116.7863,
      "step": 21040
    },
    {
      "epoch": 1.6840000000000002,
      "grad_norm": 94.98621368408203,
      "learning_rate": 0.00014386666666666668,
      "loss": -116.407,
      "step": 21050
    },
    {
      "epoch": 1.6848,
      "grad_norm": 186.9213104248047,
      "learning_rate": 0.00014384,
      "loss": -117.6309,
      "step": 21060
    },
    {
      "epoch": 1.6856,
      "grad_norm": 67.68608093261719,
      "learning_rate": 0.00014381333333333333,
      "loss": -117.3867,
      "step": 21070
    },
    {
      "epoch": 1.6864,
      "grad_norm": 135.38690185546875,
      "learning_rate": 0.00014378666666666666,
      "loss": -117.7927,
      "step": 21080
    },
    {
      "epoch": 1.6872,
      "grad_norm": 82.42550659179688,
      "learning_rate": 0.00014376,
      "loss": -117.0962,
      "step": 21090
    },
    {
      "epoch": 1.688,
      "grad_norm": 182.0130157470703,
      "learning_rate": 0.00014373333333333335,
      "loss": -116.7124,
      "step": 21100
    },
    {
      "epoch": 1.6888,
      "grad_norm": 79.94859313964844,
      "learning_rate": 0.00014370666666666668,
      "loss": -116.9261,
      "step": 21110
    },
    {
      "epoch": 1.6896,
      "grad_norm": 117.9031982421875,
      "learning_rate": 0.00014368,
      "loss": -117.3008,
      "step": 21120
    },
    {
      "epoch": 1.6904,
      "grad_norm": 154.15667724609375,
      "learning_rate": 0.00014365333333333336,
      "loss": -117.1978,
      "step": 21130
    },
    {
      "epoch": 1.6912,
      "grad_norm": 139.2431640625,
      "learning_rate": 0.00014362666666666666,
      "loss": -117.1602,
      "step": 21140
    },
    {
      "epoch": 1.692,
      "grad_norm": 176.96499633789062,
      "learning_rate": 0.0001436,
      "loss": -118.2246,
      "step": 21150
    },
    {
      "epoch": 1.6928,
      "grad_norm": 156.42396545410156,
      "learning_rate": 0.00014357333333333335,
      "loss": -117.3423,
      "step": 21160
    },
    {
      "epoch": 1.6936,
      "grad_norm": 76.20317840576172,
      "learning_rate": 0.00014354666666666668,
      "loss": -116.7079,
      "step": 21170
    },
    {
      "epoch": 1.6944,
      "grad_norm": 78.37528991699219,
      "learning_rate": 0.00014352,
      "loss": -117.6718,
      "step": 21180
    },
    {
      "epoch": 1.6952,
      "grad_norm": 112.67237854003906,
      "learning_rate": 0.00014349333333333334,
      "loss": -116.7933,
      "step": 21190
    },
    {
      "epoch": 1.696,
      "grad_norm": 133.49400329589844,
      "learning_rate": 0.0001434666666666667,
      "loss": -118.2381,
      "step": 21200
    },
    {
      "epoch": 1.6968,
      "grad_norm": 71.3167953491211,
      "learning_rate": 0.00014344,
      "loss": -117.8704,
      "step": 21210
    },
    {
      "epoch": 1.6976,
      "grad_norm": 169.15859985351562,
      "learning_rate": 0.00014341333333333332,
      "loss": -115.2648,
      "step": 21220
    },
    {
      "epoch": 1.6984,
      "grad_norm": 110.77522277832031,
      "learning_rate": 0.00014338666666666668,
      "loss": -116.8731,
      "step": 21230
    },
    {
      "epoch": 1.6992,
      "grad_norm": 105.10784912109375,
      "learning_rate": 0.00014336,
      "loss": -118.0252,
      "step": 21240
    },
    {
      "epoch": 1.7,
      "grad_norm": 86.47347259521484,
      "learning_rate": 0.00014333333333333334,
      "loss": -117.5296,
      "step": 21250
    },
    {
      "epoch": 1.7008,
      "grad_norm": 119.92674255371094,
      "learning_rate": 0.00014330666666666667,
      "loss": -118.9337,
      "step": 21260
    },
    {
      "epoch": 1.7016,
      "grad_norm": 163.46817016601562,
      "learning_rate": 0.00014328000000000002,
      "loss": -117.781,
      "step": 21270
    },
    {
      "epoch": 1.7024,
      "grad_norm": 86.09630584716797,
      "learning_rate": 0.00014325333333333335,
      "loss": -117.9716,
      "step": 21280
    },
    {
      "epoch": 1.7032,
      "grad_norm": 128.40658569335938,
      "learning_rate": 0.00014322666666666666,
      "loss": -118.187,
      "step": 21290
    },
    {
      "epoch": 1.704,
      "grad_norm": 172.5924072265625,
      "learning_rate": 0.0001432,
      "loss": -117.594,
      "step": 21300
    },
    {
      "epoch": 1.7048,
      "grad_norm": 266.437255859375,
      "learning_rate": 0.00014317333333333334,
      "loss": -117.7533,
      "step": 21310
    },
    {
      "epoch": 1.7056,
      "grad_norm": 99.87174987792969,
      "learning_rate": 0.00014314666666666667,
      "loss": -116.9479,
      "step": 21320
    },
    {
      "epoch": 1.7064,
      "grad_norm": 91.60237884521484,
      "learning_rate": 0.00014312,
      "loss": -117.6554,
      "step": 21330
    },
    {
      "epoch": 1.7072,
      "grad_norm": 110.40779113769531,
      "learning_rate": 0.00014309333333333335,
      "loss": -118.479,
      "step": 21340
    },
    {
      "epoch": 1.708,
      "grad_norm": 107.05506896972656,
      "learning_rate": 0.00014306666666666668,
      "loss": -117.7435,
      "step": 21350
    },
    {
      "epoch": 1.7088,
      "grad_norm": 128.846923828125,
      "learning_rate": 0.00014303999999999999,
      "loss": -117.2186,
      "step": 21360
    },
    {
      "epoch": 1.7096,
      "grad_norm": 113.69147491455078,
      "learning_rate": 0.00014301333333333334,
      "loss": -116.916,
      "step": 21370
    },
    {
      "epoch": 1.7104,
      "grad_norm": 109.09063720703125,
      "learning_rate": 0.00014298666666666667,
      "loss": -117.5523,
      "step": 21380
    },
    {
      "epoch": 1.7112,
      "grad_norm": 98.37505340576172,
      "learning_rate": 0.00014296,
      "loss": -117.7286,
      "step": 21390
    },
    {
      "epoch": 1.712,
      "grad_norm": 108.70370483398438,
      "learning_rate": 0.00014293333333333333,
      "loss": -117.9707,
      "step": 21400
    },
    {
      "epoch": 1.7128,
      "grad_norm": 199.9634246826172,
      "learning_rate": 0.00014290666666666669,
      "loss": -118.9822,
      "step": 21410
    },
    {
      "epoch": 1.7136,
      "grad_norm": 90.33207702636719,
      "learning_rate": 0.00014288000000000001,
      "loss": -116.9101,
      "step": 21420
    },
    {
      "epoch": 1.7144,
      "grad_norm": 146.22491455078125,
      "learning_rate": 0.00014285333333333334,
      "loss": -117.9447,
      "step": 21430
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 126.9264144897461,
      "learning_rate": 0.00014282666666666667,
      "loss": -117.9642,
      "step": 21440
    },
    {
      "epoch": 1.716,
      "grad_norm": 205.84548950195312,
      "learning_rate": 0.0001428,
      "loss": -117.9556,
      "step": 21450
    },
    {
      "epoch": 1.7168,
      "grad_norm": 111.46984100341797,
      "learning_rate": 0.00014277333333333333,
      "loss": -116.7583,
      "step": 21460
    },
    {
      "epoch": 1.7176,
      "grad_norm": 66.47722625732422,
      "learning_rate": 0.00014274666666666666,
      "loss": -116.9939,
      "step": 21470
    },
    {
      "epoch": 1.7184,
      "grad_norm": 109.26993560791016,
      "learning_rate": 0.00014272000000000002,
      "loss": -117.4525,
      "step": 21480
    },
    {
      "epoch": 1.7191999999999998,
      "grad_norm": 162.09786987304688,
      "learning_rate": 0.00014269333333333334,
      "loss": -116.3334,
      "step": 21490
    },
    {
      "epoch": 1.72,
      "grad_norm": 134.69924926757812,
      "learning_rate": 0.00014266666666666667,
      "loss": -117.3991,
      "step": 21500
    },
    {
      "epoch": 1.7208,
      "grad_norm": 134.82286071777344,
      "learning_rate": 0.00014264,
      "loss": -116.4077,
      "step": 21510
    },
    {
      "epoch": 1.7216,
      "grad_norm": 110.79271697998047,
      "learning_rate": 0.00014261333333333333,
      "loss": -117.2146,
      "step": 21520
    },
    {
      "epoch": 1.7224,
      "grad_norm": 119.01075744628906,
      "learning_rate": 0.00014258666666666666,
      "loss": -117.1385,
      "step": 21530
    },
    {
      "epoch": 1.7231999999999998,
      "grad_norm": 94.17277526855469,
      "learning_rate": 0.00014256000000000002,
      "loss": -117.7593,
      "step": 21540
    },
    {
      "epoch": 1.724,
      "grad_norm": 139.79212951660156,
      "learning_rate": 0.00014253333333333335,
      "loss": -118.3575,
      "step": 21550
    },
    {
      "epoch": 1.7248,
      "grad_norm": 125.60498046875,
      "learning_rate": 0.00014250666666666668,
      "loss": -117.3433,
      "step": 21560
    },
    {
      "epoch": 1.7256,
      "grad_norm": 174.0134735107422,
      "learning_rate": 0.00014248,
      "loss": -118.7572,
      "step": 21570
    },
    {
      "epoch": 1.7264,
      "grad_norm": 118.44242095947266,
      "learning_rate": 0.00014245333333333336,
      "loss": -118.3135,
      "step": 21580
    },
    {
      "epoch": 1.7271999999999998,
      "grad_norm": 94.2713851928711,
      "learning_rate": 0.00014242666666666666,
      "loss": -117.2534,
      "step": 21590
    },
    {
      "epoch": 1.728,
      "grad_norm": 77.73735046386719,
      "learning_rate": 0.0001424,
      "loss": -118.4665,
      "step": 21600
    },
    {
      "epoch": 1.7288000000000001,
      "grad_norm": 133.96902465820312,
      "learning_rate": 0.00014237333333333335,
      "loss": -117.2149,
      "step": 21610
    },
    {
      "epoch": 1.7296,
      "grad_norm": 153.99501037597656,
      "learning_rate": 0.00014234666666666668,
      "loss": -116.6213,
      "step": 21620
    },
    {
      "epoch": 1.7304,
      "grad_norm": 95.37873840332031,
      "learning_rate": 0.00014232,
      "loss": -116.6331,
      "step": 21630
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 95.88835144042969,
      "learning_rate": 0.00014229333333333334,
      "loss": -115.4532,
      "step": 21640
    },
    {
      "epoch": 1.732,
      "grad_norm": 73.80635070800781,
      "learning_rate": 0.0001422666666666667,
      "loss": -117.5006,
      "step": 21650
    },
    {
      "epoch": 1.7328000000000001,
      "grad_norm": 126.00849151611328,
      "learning_rate": 0.00014224000000000002,
      "loss": -116.5698,
      "step": 21660
    },
    {
      "epoch": 1.7336,
      "grad_norm": 111.24874114990234,
      "learning_rate": 0.00014221333333333332,
      "loss": -118.5657,
      "step": 21670
    },
    {
      "epoch": 1.7344,
      "grad_norm": 136.89901733398438,
      "learning_rate": 0.00014218666666666668,
      "loss": -117.4946,
      "step": 21680
    },
    {
      "epoch": 1.7351999999999999,
      "grad_norm": 109.22914123535156,
      "learning_rate": 0.00014216,
      "loss": -118.8437,
      "step": 21690
    },
    {
      "epoch": 1.736,
      "grad_norm": 90.77896118164062,
      "learning_rate": 0.00014213333333333334,
      "loss": -117.291,
      "step": 21700
    },
    {
      "epoch": 1.7368000000000001,
      "grad_norm": 144.29148864746094,
      "learning_rate": 0.00014210666666666667,
      "loss": -117.8109,
      "step": 21710
    },
    {
      "epoch": 1.7376,
      "grad_norm": 110.87276458740234,
      "learning_rate": 0.00014208000000000002,
      "loss": -116.5786,
      "step": 21720
    },
    {
      "epoch": 1.7384,
      "grad_norm": 156.88389587402344,
      "learning_rate": 0.00014205333333333335,
      "loss": -118.3864,
      "step": 21730
    },
    {
      "epoch": 1.7391999999999999,
      "grad_norm": 179.0191192626953,
      "learning_rate": 0.00014202666666666665,
      "loss": -118.4033,
      "step": 21740
    },
    {
      "epoch": 1.74,
      "grad_norm": 107.75253295898438,
      "learning_rate": 0.000142,
      "loss": -117.6997,
      "step": 21750
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 84.03477478027344,
      "learning_rate": 0.00014197333333333334,
      "loss": -117.9153,
      "step": 21760
    },
    {
      "epoch": 1.7416,
      "grad_norm": 105.59805297851562,
      "learning_rate": 0.00014194666666666667,
      "loss": -118.4249,
      "step": 21770
    },
    {
      "epoch": 1.7424,
      "grad_norm": 87.26025390625,
      "learning_rate": 0.00014192,
      "loss": -116.3435,
      "step": 21780
    },
    {
      "epoch": 1.7431999999999999,
      "grad_norm": 93.32868194580078,
      "learning_rate": 0.00014189333333333335,
      "loss": -118.5289,
      "step": 21790
    },
    {
      "epoch": 1.744,
      "grad_norm": 93.11421966552734,
      "learning_rate": 0.00014186666666666668,
      "loss": -117.1647,
      "step": 21800
    },
    {
      "epoch": 1.7448000000000001,
      "grad_norm": 120.74813079833984,
      "learning_rate": 0.00014184,
      "loss": -117.4331,
      "step": 21810
    },
    {
      "epoch": 1.7456,
      "grad_norm": 105.77117156982422,
      "learning_rate": 0.00014181333333333334,
      "loss": -117.1856,
      "step": 21820
    },
    {
      "epoch": 1.7464,
      "grad_norm": 101.01219940185547,
      "learning_rate": 0.00014178666666666667,
      "loss": -117.5192,
      "step": 21830
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 111.84796142578125,
      "learning_rate": 0.00014176,
      "loss": -117.976,
      "step": 21840
    },
    {
      "epoch": 1.748,
      "grad_norm": 121.39177703857422,
      "learning_rate": 0.00014173333333333333,
      "loss": -116.5378,
      "step": 21850
    },
    {
      "epoch": 1.7488000000000001,
      "grad_norm": 117.37361907958984,
      "learning_rate": 0.00014170666666666668,
      "loss": -117.6641,
      "step": 21860
    },
    {
      "epoch": 1.7496,
      "grad_norm": 90.0982666015625,
      "learning_rate": 0.00014168,
      "loss": -117.0952,
      "step": 21870
    },
    {
      "epoch": 1.7504,
      "grad_norm": 102.59356689453125,
      "learning_rate": 0.00014165333333333334,
      "loss": -117.4425,
      "step": 21880
    },
    {
      "epoch": 1.7511999999999999,
      "grad_norm": 96.99845123291016,
      "learning_rate": 0.00014162666666666667,
      "loss": -118.8403,
      "step": 21890
    },
    {
      "epoch": 1.752,
      "grad_norm": 111.16230010986328,
      "learning_rate": 0.0001416,
      "loss": -117.5652,
      "step": 21900
    },
    {
      "epoch": 1.7528000000000001,
      "grad_norm": 92.16417694091797,
      "learning_rate": 0.00014157333333333333,
      "loss": -118.3428,
      "step": 21910
    },
    {
      "epoch": 1.7536,
      "grad_norm": 277.38934326171875,
      "learning_rate": 0.00014154666666666668,
      "loss": -117.7863,
      "step": 21920
    },
    {
      "epoch": 1.7544,
      "grad_norm": 157.0863494873047,
      "learning_rate": 0.00014152000000000001,
      "loss": -117.6325,
      "step": 21930
    },
    {
      "epoch": 1.7551999999999999,
      "grad_norm": 113.96224975585938,
      "learning_rate": 0.00014149333333333334,
      "loss": -117.8901,
      "step": 21940
    },
    {
      "epoch": 1.756,
      "grad_norm": 173.2769317626953,
      "learning_rate": 0.00014146666666666667,
      "loss": -117.1311,
      "step": 21950
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 110.23790740966797,
      "learning_rate": 0.00014144000000000003,
      "loss": -116.9325,
      "step": 21960
    },
    {
      "epoch": 1.7576,
      "grad_norm": 223.2334747314453,
      "learning_rate": 0.00014141333333333333,
      "loss": -117.4446,
      "step": 21970
    },
    {
      "epoch": 1.7584,
      "grad_norm": 136.60142517089844,
      "learning_rate": 0.00014138666666666666,
      "loss": -117.7462,
      "step": 21980
    },
    {
      "epoch": 1.7591999999999999,
      "grad_norm": 92.75164031982422,
      "learning_rate": 0.00014136000000000002,
      "loss": -117.2288,
      "step": 21990
    },
    {
      "epoch": 1.76,
      "grad_norm": 78.33927154541016,
      "learning_rate": 0.00014133333333333334,
      "loss": -117.285,
      "step": 22000
    },
    {
      "epoch": 1.7608000000000001,
      "grad_norm": 87.61392974853516,
      "learning_rate": 0.00014130666666666667,
      "loss": -118.1772,
      "step": 22010
    },
    {
      "epoch": 1.7616,
      "grad_norm": 458.6822509765625,
      "learning_rate": 0.00014128,
      "loss": -119.1212,
      "step": 22020
    },
    {
      "epoch": 1.7624,
      "grad_norm": 102.13831329345703,
      "learning_rate": 0.00014125333333333336,
      "loss": -118.7052,
      "step": 22030
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 108.65095520019531,
      "learning_rate": 0.00014122666666666666,
      "loss": -118.0583,
      "step": 22040
    },
    {
      "epoch": 1.764,
      "grad_norm": 193.93666076660156,
      "learning_rate": 0.0001412,
      "loss": -117.7729,
      "step": 22050
    },
    {
      "epoch": 1.7648000000000001,
      "grad_norm": 210.4442138671875,
      "learning_rate": 0.00014117333333333335,
      "loss": -118.4098,
      "step": 22060
    },
    {
      "epoch": 1.7656,
      "grad_norm": 76.15414428710938,
      "learning_rate": 0.00014114666666666668,
      "loss": -118.6252,
      "step": 22070
    },
    {
      "epoch": 1.7664,
      "grad_norm": 102.72509765625,
      "learning_rate": 0.00014112,
      "loss": -116.9027,
      "step": 22080
    },
    {
      "epoch": 1.7671999999999999,
      "grad_norm": 95.5727310180664,
      "learning_rate": 0.00014109333333333333,
      "loss": -119.3713,
      "step": 22090
    },
    {
      "epoch": 1.768,
      "grad_norm": 128.1208038330078,
      "learning_rate": 0.0001410666666666667,
      "loss": -117.4732,
      "step": 22100
    },
    {
      "epoch": 1.7688000000000001,
      "grad_norm": 139.46409606933594,
      "learning_rate": 0.00014104000000000002,
      "loss": -118.3504,
      "step": 22110
    },
    {
      "epoch": 1.7696,
      "grad_norm": 91.26404571533203,
      "learning_rate": 0.00014101333333333332,
      "loss": -117.5862,
      "step": 22120
    },
    {
      "epoch": 1.7704,
      "grad_norm": 117.76810455322266,
      "learning_rate": 0.00014098666666666668,
      "loss": -117.5891,
      "step": 22130
    },
    {
      "epoch": 1.7711999999999999,
      "grad_norm": 134.4468994140625,
      "learning_rate": 0.00014096,
      "loss": -117.7722,
      "step": 22140
    },
    {
      "epoch": 1.772,
      "grad_norm": 112.76036071777344,
      "learning_rate": 0.00014093333333333333,
      "loss": -117.5274,
      "step": 22150
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 142.8599090576172,
      "learning_rate": 0.00014090666666666666,
      "loss": -116.9533,
      "step": 22160
    },
    {
      "epoch": 1.7736,
      "grad_norm": 78.87012481689453,
      "learning_rate": 0.00014088000000000002,
      "loss": -115.7129,
      "step": 22170
    },
    {
      "epoch": 1.7744,
      "grad_norm": 97.26058959960938,
      "learning_rate": 0.00014085333333333335,
      "loss": -118.4466,
      "step": 22180
    },
    {
      "epoch": 1.7752,
      "grad_norm": 131.3668975830078,
      "learning_rate": 0.00014082666666666668,
      "loss": -117.809,
      "step": 22190
    },
    {
      "epoch": 1.776,
      "grad_norm": 234.4195098876953,
      "learning_rate": 0.0001408,
      "loss": -117.9194,
      "step": 22200
    },
    {
      "epoch": 1.7768000000000002,
      "grad_norm": 104.40890502929688,
      "learning_rate": 0.00014077333333333334,
      "loss": -115.8264,
      "step": 22210
    },
    {
      "epoch": 1.7776,
      "grad_norm": 129.59832763671875,
      "learning_rate": 0.00014074666666666667,
      "loss": -116.2948,
      "step": 22220
    },
    {
      "epoch": 1.7784,
      "grad_norm": 87.51205444335938,
      "learning_rate": 0.00014072,
      "loss": -117.5055,
      "step": 22230
    },
    {
      "epoch": 1.7792,
      "grad_norm": 79.8331069946289,
      "learning_rate": 0.00014069333333333335,
      "loss": -117.8691,
      "step": 22240
    },
    {
      "epoch": 1.78,
      "grad_norm": 100.36366271972656,
      "learning_rate": 0.00014066666666666668,
      "loss": -116.6076,
      "step": 22250
    },
    {
      "epoch": 1.7808000000000002,
      "grad_norm": 123.96247863769531,
      "learning_rate": 0.00014064,
      "loss": -117.7918,
      "step": 22260
    },
    {
      "epoch": 1.7816,
      "grad_norm": 78.99567413330078,
      "learning_rate": 0.00014061333333333334,
      "loss": -117.5938,
      "step": 22270
    },
    {
      "epoch": 1.7824,
      "grad_norm": 115.14379119873047,
      "learning_rate": 0.00014058666666666667,
      "loss": -117.4703,
      "step": 22280
    },
    {
      "epoch": 1.7832,
      "grad_norm": 86.95055389404297,
      "learning_rate": 0.00014056,
      "loss": -117.0588,
      "step": 22290
    },
    {
      "epoch": 1.784,
      "grad_norm": 109.979736328125,
      "learning_rate": 0.00014053333333333335,
      "loss": -117.5578,
      "step": 22300
    },
    {
      "epoch": 1.7848000000000002,
      "grad_norm": 113.82459259033203,
      "learning_rate": 0.00014050666666666668,
      "loss": -118.4687,
      "step": 22310
    },
    {
      "epoch": 1.7856,
      "grad_norm": 151.75267028808594,
      "learning_rate": 0.00014048,
      "loss": -117.1651,
      "step": 22320
    },
    {
      "epoch": 1.7864,
      "grad_norm": 118.48372650146484,
      "learning_rate": 0.00014045333333333334,
      "loss": -116.8661,
      "step": 22330
    },
    {
      "epoch": 1.7872,
      "grad_norm": 84.06302642822266,
      "learning_rate": 0.0001404266666666667,
      "loss": -119.1597,
      "step": 22340
    },
    {
      "epoch": 1.788,
      "grad_norm": 85.18440246582031,
      "learning_rate": 0.0001404,
      "loss": -117.1403,
      "step": 22350
    },
    {
      "epoch": 1.7888,
      "grad_norm": 133.21377563476562,
      "learning_rate": 0.00014037333333333333,
      "loss": -118.651,
      "step": 22360
    },
    {
      "epoch": 1.7896,
      "grad_norm": 110.9341049194336,
      "learning_rate": 0.00014034666666666668,
      "loss": -118.0374,
      "step": 22370
    },
    {
      "epoch": 1.7904,
      "grad_norm": 129.9039764404297,
      "learning_rate": 0.00014032,
      "loss": -117.0326,
      "step": 22380
    },
    {
      "epoch": 1.7912,
      "grad_norm": 134.5853729248047,
      "learning_rate": 0.00014029333333333334,
      "loss": -117.1393,
      "step": 22390
    },
    {
      "epoch": 1.792,
      "grad_norm": 116.19127655029297,
      "learning_rate": 0.00014026666666666667,
      "loss": -116.5736,
      "step": 22400
    },
    {
      "epoch": 1.7928,
      "grad_norm": 135.37208557128906,
      "learning_rate": 0.00014024000000000003,
      "loss": -118.2213,
      "step": 22410
    },
    {
      "epoch": 1.7936,
      "grad_norm": 115.35769653320312,
      "learning_rate": 0.00014021333333333333,
      "loss": -117.4611,
      "step": 22420
    },
    {
      "epoch": 1.7944,
      "grad_norm": 217.50794982910156,
      "learning_rate": 0.00014018666666666666,
      "loss": -117.3239,
      "step": 22430
    },
    {
      "epoch": 1.7952,
      "grad_norm": 118.56290435791016,
      "learning_rate": 0.00014016,
      "loss": -116.2234,
      "step": 22440
    },
    {
      "epoch": 1.796,
      "grad_norm": 172.16384887695312,
      "learning_rate": 0.00014013333333333334,
      "loss": -118.1972,
      "step": 22450
    },
    {
      "epoch": 1.7968,
      "grad_norm": 89.06912994384766,
      "learning_rate": 0.00014010666666666667,
      "loss": -118.0478,
      "step": 22460
    },
    {
      "epoch": 1.7976,
      "grad_norm": 87.63711547851562,
      "learning_rate": 0.00014008,
      "loss": -118.3162,
      "step": 22470
    },
    {
      "epoch": 1.7984,
      "grad_norm": 107.68477630615234,
      "learning_rate": 0.00014005333333333336,
      "loss": -118.0231,
      "step": 22480
    },
    {
      "epoch": 1.7992,
      "grad_norm": 135.2403106689453,
      "learning_rate": 0.00014002666666666669,
      "loss": -116.6444,
      "step": 22490
    },
    {
      "epoch": 1.8,
      "grad_norm": 143.03514099121094,
      "learning_rate": 0.00014,
      "loss": -117.3556,
      "step": 22500
    },
    {
      "epoch": 1.8008,
      "grad_norm": 146.66973876953125,
      "learning_rate": 0.00013997333333333334,
      "loss": -118.7819,
      "step": 22510
    },
    {
      "epoch": 1.8016,
      "grad_norm": 79.34587097167969,
      "learning_rate": 0.00013994666666666667,
      "loss": -115.4024,
      "step": 22520
    },
    {
      "epoch": 1.8024,
      "grad_norm": 188.5207061767578,
      "learning_rate": 0.00013992,
      "loss": -118.152,
      "step": 22530
    },
    {
      "epoch": 1.8032,
      "grad_norm": 127.65412902832031,
      "learning_rate": 0.00013989333333333333,
      "loss": -117.881,
      "step": 22540
    },
    {
      "epoch": 1.804,
      "grad_norm": 75.25199890136719,
      "learning_rate": 0.0001398666666666667,
      "loss": -117.4406,
      "step": 22550
    },
    {
      "epoch": 1.8048,
      "grad_norm": 103.89360046386719,
      "learning_rate": 0.00013984000000000002,
      "loss": -117.2472,
      "step": 22560
    },
    {
      "epoch": 1.8056,
      "grad_norm": 91.27590942382812,
      "learning_rate": 0.00013981333333333332,
      "loss": -117.4985,
      "step": 22570
    },
    {
      "epoch": 1.8064,
      "grad_norm": 113.98329162597656,
      "learning_rate": 0.00013978666666666667,
      "loss": -118.378,
      "step": 22580
    },
    {
      "epoch": 1.8072,
      "grad_norm": 144.74371337890625,
      "learning_rate": 0.00013976,
      "loss": -117.1222,
      "step": 22590
    },
    {
      "epoch": 1.808,
      "grad_norm": 100.48687744140625,
      "learning_rate": 0.00013973333333333333,
      "loss": -118.3463,
      "step": 22600
    },
    {
      "epoch": 1.8088,
      "grad_norm": 137.31581115722656,
      "learning_rate": 0.00013970666666666666,
      "loss": -118.6469,
      "step": 22610
    },
    {
      "epoch": 1.8096,
      "grad_norm": 84.97254180908203,
      "learning_rate": 0.00013968000000000002,
      "loss": -117.2972,
      "step": 22620
    },
    {
      "epoch": 1.8104,
      "grad_norm": 193.55813598632812,
      "learning_rate": 0.00013965333333333335,
      "loss": -118.6941,
      "step": 22630
    },
    {
      "epoch": 1.8112,
      "grad_norm": 133.1509246826172,
      "learning_rate": 0.00013962666666666668,
      "loss": -117.3693,
      "step": 22640
    },
    {
      "epoch": 1.812,
      "grad_norm": 100.29277801513672,
      "learning_rate": 0.0001396,
      "loss": -116.9814,
      "step": 22650
    },
    {
      "epoch": 1.8128,
      "grad_norm": 168.05023193359375,
      "learning_rate": 0.00013957333333333333,
      "loss": -118.1187,
      "step": 22660
    },
    {
      "epoch": 1.8136,
      "grad_norm": 72.56122589111328,
      "learning_rate": 0.00013954666666666666,
      "loss": -117.4942,
      "step": 22670
    },
    {
      "epoch": 1.8144,
      "grad_norm": 79.99974060058594,
      "learning_rate": 0.00013952000000000002,
      "loss": -118.7306,
      "step": 22680
    },
    {
      "epoch": 1.8152,
      "grad_norm": 124.31189727783203,
      "learning_rate": 0.00013949333333333335,
      "loss": -117.3871,
      "step": 22690
    },
    {
      "epoch": 1.8159999999999998,
      "grad_norm": 70.82140350341797,
      "learning_rate": 0.00013946666666666668,
      "loss": -117.656,
      "step": 22700
    },
    {
      "epoch": 1.8168,
      "grad_norm": 159.6553192138672,
      "learning_rate": 0.00013944,
      "loss": -117.694,
      "step": 22710
    },
    {
      "epoch": 1.8176,
      "grad_norm": 109.97676086425781,
      "learning_rate": 0.00013941333333333334,
      "loss": -118.4757,
      "step": 22720
    },
    {
      "epoch": 1.8184,
      "grad_norm": 106.34336853027344,
      "learning_rate": 0.00013938666666666666,
      "loss": -117.4182,
      "step": 22730
    },
    {
      "epoch": 1.8192,
      "grad_norm": 144.16552734375,
      "learning_rate": 0.00013936,
      "loss": -117.0958,
      "step": 22740
    },
    {
      "epoch": 1.8199999999999998,
      "grad_norm": 92.06669616699219,
      "learning_rate": 0.00013933333333333335,
      "loss": -117.2893,
      "step": 22750
    },
    {
      "epoch": 1.8208,
      "grad_norm": 75.39954376220703,
      "learning_rate": 0.00013930666666666668,
      "loss": -117.2444,
      "step": 22760
    },
    {
      "epoch": 1.8216,
      "grad_norm": 107.7993392944336,
      "learning_rate": 0.00013928,
      "loss": -119.3336,
      "step": 22770
    },
    {
      "epoch": 1.8224,
      "grad_norm": 88.05425262451172,
      "learning_rate": 0.00013925333333333334,
      "loss": -117.3305,
      "step": 22780
    },
    {
      "epoch": 1.8232,
      "grad_norm": 113.47895050048828,
      "learning_rate": 0.0001392266666666667,
      "loss": -116.7672,
      "step": 22790
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 107.03244018554688,
      "learning_rate": 0.0001392,
      "loss": -117.5031,
      "step": 22800
    },
    {
      "epoch": 1.8248,
      "grad_norm": 159.11866760253906,
      "learning_rate": 0.00013917333333333332,
      "loss": -116.3019,
      "step": 22810
    },
    {
      "epoch": 1.8256000000000001,
      "grad_norm": 126.9562759399414,
      "learning_rate": 0.00013914666666666668,
      "loss": -118.9834,
      "step": 22820
    },
    {
      "epoch": 1.8264,
      "grad_norm": 153.5963134765625,
      "learning_rate": 0.00013912,
      "loss": -117.5839,
      "step": 22830
    },
    {
      "epoch": 1.8272,
      "grad_norm": 107.4268798828125,
      "learning_rate": 0.00013909333333333334,
      "loss": -117.1437,
      "step": 22840
    },
    {
      "epoch": 1.8279999999999998,
      "grad_norm": 87.6954574584961,
      "learning_rate": 0.00013906666666666667,
      "loss": -116.9901,
      "step": 22850
    },
    {
      "epoch": 1.8288,
      "grad_norm": 108.67058563232422,
      "learning_rate": 0.00013904000000000002,
      "loss": -117.2976,
      "step": 22860
    },
    {
      "epoch": 1.8296000000000001,
      "grad_norm": 107.28866577148438,
      "learning_rate": 0.00013901333333333335,
      "loss": -117.4166,
      "step": 22870
    },
    {
      "epoch": 1.8304,
      "grad_norm": 87.71604919433594,
      "learning_rate": 0.00013898666666666666,
      "loss": -117.9535,
      "step": 22880
    },
    {
      "epoch": 1.8312,
      "grad_norm": 117.08271789550781,
      "learning_rate": 0.00013896,
      "loss": -117.6839,
      "step": 22890
    },
    {
      "epoch": 1.8319999999999999,
      "grad_norm": 114.99169158935547,
      "learning_rate": 0.00013893333333333334,
      "loss": -118.1948,
      "step": 22900
    },
    {
      "epoch": 1.8328,
      "grad_norm": 79.8559799194336,
      "learning_rate": 0.00013890666666666667,
      "loss": -116.8486,
      "step": 22910
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 80.66548919677734,
      "learning_rate": 0.00013888,
      "loss": -117.7783,
      "step": 22920
    },
    {
      "epoch": 1.8344,
      "grad_norm": 96.12264251708984,
      "learning_rate": 0.00013885333333333335,
      "loss": -116.5443,
      "step": 22930
    },
    {
      "epoch": 1.8352,
      "grad_norm": 112.78910064697266,
      "learning_rate": 0.00013882666666666668,
      "loss": -118.2843,
      "step": 22940
    },
    {
      "epoch": 1.8359999999999999,
      "grad_norm": 82.52011108398438,
      "learning_rate": 0.00013879999999999999,
      "loss": -117.6007,
      "step": 22950
    },
    {
      "epoch": 1.8368,
      "grad_norm": 79.79143524169922,
      "learning_rate": 0.00013877333333333334,
      "loss": -117.7729,
      "step": 22960
    },
    {
      "epoch": 1.8376000000000001,
      "grad_norm": 144.0502471923828,
      "learning_rate": 0.00013874666666666667,
      "loss": -118.2586,
      "step": 22970
    },
    {
      "epoch": 1.8384,
      "grad_norm": 51.19733428955078,
      "learning_rate": 0.00013872,
      "loss": -118.6525,
      "step": 22980
    },
    {
      "epoch": 1.8392,
      "grad_norm": 141.2048797607422,
      "learning_rate": 0.00013869333333333333,
      "loss": -118.0206,
      "step": 22990
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 98.62580108642578,
      "learning_rate": 0.00013866666666666669,
      "loss": -117.1415,
      "step": 23000
    },
    {
      "epoch": 1.8408,
      "grad_norm": 110.08068084716797,
      "learning_rate": 0.00013864000000000001,
      "loss": -118.4721,
      "step": 23010
    },
    {
      "epoch": 1.8416000000000001,
      "grad_norm": 104.27410888671875,
      "learning_rate": 0.00013861333333333334,
      "loss": -116.9286,
      "step": 23020
    },
    {
      "epoch": 1.8424,
      "grad_norm": 109.96876525878906,
      "learning_rate": 0.00013858666666666667,
      "loss": -117.9631,
      "step": 23030
    },
    {
      "epoch": 1.8432,
      "grad_norm": 114.36573028564453,
      "learning_rate": 0.00013856,
      "loss": -118.1153,
      "step": 23040
    },
    {
      "epoch": 1.8439999999999999,
      "grad_norm": 91.20442962646484,
      "learning_rate": 0.00013853333333333333,
      "loss": -117.1393,
      "step": 23050
    },
    {
      "epoch": 1.8448,
      "grad_norm": 83.44336700439453,
      "learning_rate": 0.0001385066666666667,
      "loss": -117.6536,
      "step": 23060
    },
    {
      "epoch": 1.8456000000000001,
      "grad_norm": 78.16769409179688,
      "learning_rate": 0.00013848000000000002,
      "loss": -117.3986,
      "step": 23070
    },
    {
      "epoch": 1.8464,
      "grad_norm": 114.66144561767578,
      "learning_rate": 0.00013845333333333334,
      "loss": -117.8666,
      "step": 23080
    },
    {
      "epoch": 1.8472,
      "grad_norm": 180.78048706054688,
      "learning_rate": 0.00013842666666666667,
      "loss": -117.8107,
      "step": 23090
    },
    {
      "epoch": 1.8479999999999999,
      "grad_norm": 320.7082824707031,
      "learning_rate": 0.0001384,
      "loss": -117.7013,
      "step": 23100
    },
    {
      "epoch": 1.8488,
      "grad_norm": 133.9211883544922,
      "learning_rate": 0.00013837333333333333,
      "loss": -117.1267,
      "step": 23110
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 105.09502410888672,
      "learning_rate": 0.00013834666666666666,
      "loss": -118.0779,
      "step": 23120
    },
    {
      "epoch": 1.8504,
      "grad_norm": 125.79664611816406,
      "learning_rate": 0.00013832000000000002,
      "loss": -118.0682,
      "step": 23130
    },
    {
      "epoch": 1.8512,
      "grad_norm": 165.64022827148438,
      "learning_rate": 0.00013829333333333335,
      "loss": -117.1855,
      "step": 23140
    },
    {
      "epoch": 1.8519999999999999,
      "grad_norm": 106.2031478881836,
      "learning_rate": 0.00013826666666666668,
      "loss": -118.3087,
      "step": 23150
    },
    {
      "epoch": 1.8528,
      "grad_norm": 103.11967468261719,
      "learning_rate": 0.00013824,
      "loss": -118.376,
      "step": 23160
    },
    {
      "epoch": 1.8536000000000001,
      "grad_norm": 124.67056274414062,
      "learning_rate": 0.00013821333333333336,
      "loss": -116.7671,
      "step": 23170
    },
    {
      "epoch": 1.8544,
      "grad_norm": 150.06442260742188,
      "learning_rate": 0.00013818666666666666,
      "loss": -117.1746,
      "step": 23180
    },
    {
      "epoch": 1.8552,
      "grad_norm": 77.26154327392578,
      "learning_rate": 0.00013816,
      "loss": -117.8168,
      "step": 23190
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 85.85511016845703,
      "learning_rate": 0.00013813333333333335,
      "loss": -117.4258,
      "step": 23200
    },
    {
      "epoch": 1.8568,
      "grad_norm": 98.0694580078125,
      "learning_rate": 0.00013810666666666668,
      "loss": -117.4552,
      "step": 23210
    },
    {
      "epoch": 1.8576000000000001,
      "grad_norm": 90.81693267822266,
      "learning_rate": 0.00013808,
      "loss": -117.7991,
      "step": 23220
    },
    {
      "epoch": 1.8584,
      "grad_norm": 77.4237060546875,
      "learning_rate": 0.00013805333333333334,
      "loss": -116.8156,
      "step": 23230
    },
    {
      "epoch": 1.8592,
      "grad_norm": 133.4761505126953,
      "learning_rate": 0.0001380266666666667,
      "loss": -117.7763,
      "step": 23240
    },
    {
      "epoch": 1.8599999999999999,
      "grad_norm": 131.1409454345703,
      "learning_rate": 0.000138,
      "loss": -117.721,
      "step": 23250
    },
    {
      "epoch": 1.8608,
      "grad_norm": 91.5448226928711,
      "learning_rate": 0.00013797333333333332,
      "loss": -118.1073,
      "step": 23260
    },
    {
      "epoch": 1.8616000000000001,
      "grad_norm": 79.88502502441406,
      "learning_rate": 0.00013794666666666668,
      "loss": -117.4775,
      "step": 23270
    },
    {
      "epoch": 1.8624,
      "grad_norm": 109.98638153076172,
      "learning_rate": 0.00013792,
      "loss": -117.0792,
      "step": 23280
    },
    {
      "epoch": 1.8632,
      "grad_norm": 112.39797973632812,
      "learning_rate": 0.00013789333333333334,
      "loss": -118.0066,
      "step": 23290
    },
    {
      "epoch": 1.8639999999999999,
      "grad_norm": 97.51570892333984,
      "learning_rate": 0.00013786666666666667,
      "loss": -118.3233,
      "step": 23300
    },
    {
      "epoch": 1.8648,
      "grad_norm": 112.46782684326172,
      "learning_rate": 0.00013784000000000002,
      "loss": -117.3813,
      "step": 23310
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 117.44682312011719,
      "learning_rate": 0.00013781333333333335,
      "loss": -118.439,
      "step": 23320
    },
    {
      "epoch": 1.8664,
      "grad_norm": 117.50173950195312,
      "learning_rate": 0.00013778666666666665,
      "loss": -117.3164,
      "step": 23330
    },
    {
      "epoch": 1.8672,
      "grad_norm": 134.14718627929688,
      "learning_rate": 0.00013776,
      "loss": -117.1567,
      "step": 23340
    },
    {
      "epoch": 1.8679999999999999,
      "grad_norm": 213.8859100341797,
      "learning_rate": 0.00013773333333333334,
      "loss": -118.3767,
      "step": 23350
    },
    {
      "epoch": 1.8688,
      "grad_norm": 189.86279296875,
      "learning_rate": 0.00013770666666666667,
      "loss": -117.7865,
      "step": 23360
    },
    {
      "epoch": 1.8696000000000002,
      "grad_norm": 65.90254974365234,
      "learning_rate": 0.00013768,
      "loss": -119.1926,
      "step": 23370
    },
    {
      "epoch": 1.8704,
      "grad_norm": 89.93797302246094,
      "learning_rate": 0.00013765333333333335,
      "loss": -117.3112,
      "step": 23380
    },
    {
      "epoch": 1.8712,
      "grad_norm": 72.58149719238281,
      "learning_rate": 0.00013762666666666668,
      "loss": -116.9562,
      "step": 23390
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 97.27368927001953,
      "learning_rate": 0.00013759999999999998,
      "loss": -118.9137,
      "step": 23400
    },
    {
      "epoch": 1.8728,
      "grad_norm": 114.3816909790039,
      "learning_rate": 0.00013757333333333334,
      "loss": -118.225,
      "step": 23410
    },
    {
      "epoch": 1.8736000000000002,
      "grad_norm": 128.3567657470703,
      "learning_rate": 0.00013754666666666667,
      "loss": -117.6671,
      "step": 23420
    },
    {
      "epoch": 1.8744,
      "grad_norm": 109.31155395507812,
      "learning_rate": 0.00013752,
      "loss": -118.3229,
      "step": 23430
    },
    {
      "epoch": 1.8752,
      "grad_norm": 128.0842742919922,
      "learning_rate": 0.00013749333333333335,
      "loss": -119.0015,
      "step": 23440
    },
    {
      "epoch": 1.876,
      "grad_norm": 133.3376007080078,
      "learning_rate": 0.00013746666666666668,
      "loss": -117.8206,
      "step": 23450
    },
    {
      "epoch": 1.8768,
      "grad_norm": 125.82426452636719,
      "learning_rate": 0.00013744,
      "loss": -115.8089,
      "step": 23460
    },
    {
      "epoch": 1.8776000000000002,
      "grad_norm": 141.4514923095703,
      "learning_rate": 0.00013741333333333334,
      "loss": -118.0845,
      "step": 23470
    },
    {
      "epoch": 1.8784,
      "grad_norm": 267.8012390136719,
      "learning_rate": 0.00013738666666666667,
      "loss": -117.377,
      "step": 23480
    },
    {
      "epoch": 1.8792,
      "grad_norm": 108.42788696289062,
      "learning_rate": 0.00013736,
      "loss": -118.7408,
      "step": 23490
    },
    {
      "epoch": 1.88,
      "grad_norm": 86.27704620361328,
      "learning_rate": 0.00013733333333333333,
      "loss": -117.4664,
      "step": 23500
    },
    {
      "epoch": 1.8808,
      "grad_norm": 99.85841369628906,
      "learning_rate": 0.00013730666666666668,
      "loss": -118.7181,
      "step": 23510
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 106.37813568115234,
      "learning_rate": 0.00013728000000000001,
      "loss": -117.4686,
      "step": 23520
    },
    {
      "epoch": 1.8824,
      "grad_norm": 93.6478042602539,
      "learning_rate": 0.00013725333333333334,
      "loss": -118.5074,
      "step": 23530
    },
    {
      "epoch": 1.8832,
      "grad_norm": 76.59298706054688,
      "learning_rate": 0.00013722666666666667,
      "loss": -117.7194,
      "step": 23540
    },
    {
      "epoch": 1.884,
      "grad_norm": 96.98573303222656,
      "learning_rate": 0.00013720000000000003,
      "loss": -118.0254,
      "step": 23550
    },
    {
      "epoch": 1.8848,
      "grad_norm": 192.13571166992188,
      "learning_rate": 0.00013717333333333333,
      "loss": -118.2709,
      "step": 23560
    },
    {
      "epoch": 1.8856000000000002,
      "grad_norm": 95.98944854736328,
      "learning_rate": 0.00013714666666666666,
      "loss": -117.4658,
      "step": 23570
    },
    {
      "epoch": 1.8864,
      "grad_norm": 117.63314056396484,
      "learning_rate": 0.00013712000000000002,
      "loss": -117.4223,
      "step": 23580
    },
    {
      "epoch": 1.8872,
      "grad_norm": 101.22889709472656,
      "learning_rate": 0.00013709333333333334,
      "loss": -117.0374,
      "step": 23590
    },
    {
      "epoch": 1.888,
      "grad_norm": 65.39649200439453,
      "learning_rate": 0.00013706666666666667,
      "loss": -118.4383,
      "step": 23600
    },
    {
      "epoch": 1.8888,
      "grad_norm": 100.10811614990234,
      "learning_rate": 0.00013704,
      "loss": -118.7022,
      "step": 23610
    },
    {
      "epoch": 1.8896,
      "grad_norm": 75.73208618164062,
      "learning_rate": 0.00013701333333333336,
      "loss": -116.4791,
      "step": 23620
    },
    {
      "epoch": 1.8904,
      "grad_norm": 91.99449920654297,
      "learning_rate": 0.00013698666666666666,
      "loss": -119.213,
      "step": 23630
    },
    {
      "epoch": 1.8912,
      "grad_norm": 123.58291625976562,
      "learning_rate": 0.00013696,
      "loss": -118.8085,
      "step": 23640
    },
    {
      "epoch": 1.892,
      "grad_norm": 88.39807891845703,
      "learning_rate": 0.00013693333333333335,
      "loss": -118.1115,
      "step": 23650
    },
    {
      "epoch": 1.8928,
      "grad_norm": 121.17621612548828,
      "learning_rate": 0.00013690666666666667,
      "loss": -118.0665,
      "step": 23660
    },
    {
      "epoch": 1.8936,
      "grad_norm": 93.3237533569336,
      "learning_rate": 0.00013688,
      "loss": -117.8672,
      "step": 23670
    },
    {
      "epoch": 1.8944,
      "grad_norm": 127.5502700805664,
      "learning_rate": 0.00013685333333333333,
      "loss": -117.0767,
      "step": 23680
    },
    {
      "epoch": 1.8952,
      "grad_norm": 100.6087646484375,
      "learning_rate": 0.0001368266666666667,
      "loss": -116.2013,
      "step": 23690
    },
    {
      "epoch": 1.896,
      "grad_norm": 114.98143005371094,
      "learning_rate": 0.00013680000000000002,
      "loss": -117.7055,
      "step": 23700
    },
    {
      "epoch": 1.8968,
      "grad_norm": 118.45972442626953,
      "learning_rate": 0.00013677333333333332,
      "loss": -118.1573,
      "step": 23710
    },
    {
      "epoch": 1.8976,
      "grad_norm": 115.96829223632812,
      "learning_rate": 0.00013674666666666668,
      "loss": -118.6273,
      "step": 23720
    },
    {
      "epoch": 1.8984,
      "grad_norm": 120.97747802734375,
      "learning_rate": 0.00013672,
      "loss": -117.0847,
      "step": 23730
    },
    {
      "epoch": 1.8992,
      "grad_norm": 113.00474548339844,
      "learning_rate": 0.00013669333333333333,
      "loss": -118.8606,
      "step": 23740
    },
    {
      "epoch": 1.9,
      "grad_norm": 100.61886596679688,
      "learning_rate": 0.00013666666666666666,
      "loss": -117.463,
      "step": 23750
    },
    {
      "epoch": 1.9008,
      "grad_norm": 99.33324432373047,
      "learning_rate": 0.00013664000000000002,
      "loss": -117.5678,
      "step": 23760
    },
    {
      "epoch": 1.9016,
      "grad_norm": 103.69805145263672,
      "learning_rate": 0.00013661333333333335,
      "loss": -117.8744,
      "step": 23770
    },
    {
      "epoch": 1.9024,
      "grad_norm": 74.7794418334961,
      "learning_rate": 0.00013658666666666665,
      "loss": -117.4783,
      "step": 23780
    },
    {
      "epoch": 1.9032,
      "grad_norm": 128.47938537597656,
      "learning_rate": 0.00013656,
      "loss": -117.8629,
      "step": 23790
    },
    {
      "epoch": 1.904,
      "grad_norm": 163.08016967773438,
      "learning_rate": 0.00013653333333333334,
      "loss": -118.8132,
      "step": 23800
    },
    {
      "epoch": 1.9048,
      "grad_norm": 140.59481811523438,
      "learning_rate": 0.00013650666666666667,
      "loss": -117.5949,
      "step": 23810
    },
    {
      "epoch": 1.9056,
      "grad_norm": 61.903663635253906,
      "learning_rate": 0.00013648,
      "loss": -117.5439,
      "step": 23820
    },
    {
      "epoch": 1.9064,
      "grad_norm": 180.4200439453125,
      "learning_rate": 0.00013645333333333335,
      "loss": -118.0506,
      "step": 23830
    },
    {
      "epoch": 1.9072,
      "grad_norm": 129.4546661376953,
      "learning_rate": 0.00013642666666666668,
      "loss": -118.4588,
      "step": 23840
    },
    {
      "epoch": 1.908,
      "grad_norm": 104.81307983398438,
      "learning_rate": 0.0001364,
      "loss": -116.7434,
      "step": 23850
    },
    {
      "epoch": 1.9088,
      "grad_norm": 130.80426025390625,
      "learning_rate": 0.00013637333333333334,
      "loss": -116.7176,
      "step": 23860
    },
    {
      "epoch": 1.9096,
      "grad_norm": 67.09895324707031,
      "learning_rate": 0.00013634666666666667,
      "loss": -119.2459,
      "step": 23870
    },
    {
      "epoch": 1.9104,
      "grad_norm": 102.35134887695312,
      "learning_rate": 0.00013632,
      "loss": -118.2811,
      "step": 23880
    },
    {
      "epoch": 1.9112,
      "grad_norm": 138.4624786376953,
      "learning_rate": 0.00013629333333333335,
      "loss": -117.49,
      "step": 23890
    },
    {
      "epoch": 1.912,
      "grad_norm": 103.8604736328125,
      "learning_rate": 0.00013626666666666668,
      "loss": -118.7029,
      "step": 23900
    },
    {
      "epoch": 1.9127999999999998,
      "grad_norm": 90.23529052734375,
      "learning_rate": 0.00013624,
      "loss": -118.8328,
      "step": 23910
    },
    {
      "epoch": 1.9136,
      "grad_norm": 95.983154296875,
      "learning_rate": 0.00013621333333333334,
      "loss": -116.5431,
      "step": 23920
    },
    {
      "epoch": 1.9144,
      "grad_norm": 155.3740234375,
      "learning_rate": 0.00013618666666666667,
      "loss": -118.8075,
      "step": 23930
    },
    {
      "epoch": 1.9152,
      "grad_norm": 76.27371978759766,
      "learning_rate": 0.00013616,
      "loss": -116.7283,
      "step": 23940
    },
    {
      "epoch": 1.916,
      "grad_norm": 113.76548767089844,
      "learning_rate": 0.00013613333333333333,
      "loss": -118.5096,
      "step": 23950
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 132.27841186523438,
      "learning_rate": 0.00013610666666666668,
      "loss": -118.0387,
      "step": 23960
    },
    {
      "epoch": 1.9176,
      "grad_norm": 80.89615631103516,
      "learning_rate": 0.00013608,
      "loss": -118.1347,
      "step": 23970
    },
    {
      "epoch": 1.9184,
      "grad_norm": 408.4048767089844,
      "learning_rate": 0.00013605333333333334,
      "loss": -117.8865,
      "step": 23980
    },
    {
      "epoch": 1.9192,
      "grad_norm": 104.59228515625,
      "learning_rate": 0.00013602666666666667,
      "loss": -116.4928,
      "step": 23990
    },
    {
      "epoch": 1.92,
      "grad_norm": 111.0843734741211,
      "learning_rate": 0.00013600000000000003,
      "loss": -117.5598,
      "step": 24000
    },
    {
      "epoch": 1.9207999999999998,
      "grad_norm": 157.7613983154297,
      "learning_rate": 0.00013597333333333333,
      "loss": -117.3271,
      "step": 24010
    },
    {
      "epoch": 1.9216,
      "grad_norm": 76.37005615234375,
      "learning_rate": 0.00013594666666666666,
      "loss": -117.7984,
      "step": 24020
    },
    {
      "epoch": 1.9224,
      "grad_norm": 107.8878173828125,
      "learning_rate": 0.00013592,
      "loss": -117.7911,
      "step": 24030
    },
    {
      "epoch": 1.9232,
      "grad_norm": 66.83638000488281,
      "learning_rate": 0.00013589333333333334,
      "loss": -117.9266,
      "step": 24040
    },
    {
      "epoch": 1.924,
      "grad_norm": 108.36610412597656,
      "learning_rate": 0.00013586666666666667,
      "loss": -118.748,
      "step": 24050
    },
    {
      "epoch": 1.9247999999999998,
      "grad_norm": 131.11819458007812,
      "learning_rate": 0.00013584,
      "loss": -117.1028,
      "step": 24060
    },
    {
      "epoch": 1.9256,
      "grad_norm": 105.25041198730469,
      "learning_rate": 0.00013581333333333336,
      "loss": -118.1991,
      "step": 24070
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 164.13253784179688,
      "learning_rate": 0.00013578666666666669,
      "loss": -117.3188,
      "step": 24080
    },
    {
      "epoch": 1.9272,
      "grad_norm": 121.98918914794922,
      "learning_rate": 0.00013576,
      "loss": -118.8083,
      "step": 24090
    },
    {
      "epoch": 1.928,
      "grad_norm": 105.91295623779297,
      "learning_rate": 0.00013573333333333334,
      "loss": -117.5133,
      "step": 24100
    },
    {
      "epoch": 1.9287999999999998,
      "grad_norm": 72.77693176269531,
      "learning_rate": 0.00013570666666666667,
      "loss": -118.4627,
      "step": 24110
    },
    {
      "epoch": 1.9296,
      "grad_norm": 131.10519409179688,
      "learning_rate": 0.00013568,
      "loss": -117.9389,
      "step": 24120
    },
    {
      "epoch": 1.9304000000000001,
      "grad_norm": 129.57591247558594,
      "learning_rate": 0.00013565333333333333,
      "loss": -118.0801,
      "step": 24130
    },
    {
      "epoch": 1.9312,
      "grad_norm": 111.85015106201172,
      "learning_rate": 0.0001356266666666667,
      "loss": -117.8569,
      "step": 24140
    },
    {
      "epoch": 1.932,
      "grad_norm": 77.79075622558594,
      "learning_rate": 0.00013560000000000002,
      "loss": -118.4647,
      "step": 24150
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 86.20305633544922,
      "learning_rate": 0.00013557333333333332,
      "loss": -118.4778,
      "step": 24160
    },
    {
      "epoch": 1.9336,
      "grad_norm": 99.17242431640625,
      "learning_rate": 0.00013554666666666667,
      "loss": -118.5223,
      "step": 24170
    },
    {
      "epoch": 1.9344000000000001,
      "grad_norm": 113.65077209472656,
      "learning_rate": 0.00013552,
      "loss": -119.5429,
      "step": 24180
    },
    {
      "epoch": 1.9352,
      "grad_norm": 134.9378662109375,
      "learning_rate": 0.00013549333333333333,
      "loss": -118.7124,
      "step": 24190
    },
    {
      "epoch": 1.936,
      "grad_norm": 126.98643493652344,
      "learning_rate": 0.00013546666666666666,
      "loss": -118.566,
      "step": 24200
    },
    {
      "epoch": 1.9367999999999999,
      "grad_norm": 81.68743896484375,
      "learning_rate": 0.00013544000000000002,
      "loss": -118.6293,
      "step": 24210
    },
    {
      "epoch": 1.9376,
      "grad_norm": 109.44458770751953,
      "learning_rate": 0.00013541333333333335,
      "loss": -117.6482,
      "step": 24220
    },
    {
      "epoch": 1.9384000000000001,
      "grad_norm": 151.802490234375,
      "learning_rate": 0.00013538666666666668,
      "loss": -117.5933,
      "step": 24230
    },
    {
      "epoch": 1.9392,
      "grad_norm": 149.4102325439453,
      "learning_rate": 0.00013536,
      "loss": -117.9963,
      "step": 24240
    },
    {
      "epoch": 1.94,
      "grad_norm": 79.47612762451172,
      "learning_rate": 0.00013533333333333333,
      "loss": -117.5358,
      "step": 24250
    },
    {
      "epoch": 1.9407999999999999,
      "grad_norm": 148.71603393554688,
      "learning_rate": 0.00013530666666666666,
      "loss": -118.984,
      "step": 24260
    },
    {
      "epoch": 1.9416,
      "grad_norm": 88.01901245117188,
      "learning_rate": 0.00013528000000000002,
      "loss": -117.1142,
      "step": 24270
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 116.84193420410156,
      "learning_rate": 0.00013525333333333335,
      "loss": -117.6218,
      "step": 24280
    },
    {
      "epoch": 1.9432,
      "grad_norm": 103.29841613769531,
      "learning_rate": 0.00013522666666666668,
      "loss": -118.2767,
      "step": 24290
    },
    {
      "epoch": 1.944,
      "grad_norm": 104.01677703857422,
      "learning_rate": 0.0001352,
      "loss": -118.2921,
      "step": 24300
    },
    {
      "epoch": 1.9447999999999999,
      "grad_norm": 99.08296966552734,
      "learning_rate": 0.00013517333333333334,
      "loss": -118.2624,
      "step": 24310
    },
    {
      "epoch": 1.9456,
      "grad_norm": 90.3076171875,
      "learning_rate": 0.00013514666666666666,
      "loss": -118.9037,
      "step": 24320
    },
    {
      "epoch": 1.9464000000000001,
      "grad_norm": 263.1220397949219,
      "learning_rate": 0.00013512,
      "loss": -119.0696,
      "step": 24330
    },
    {
      "epoch": 1.9472,
      "grad_norm": 165.18597412109375,
      "learning_rate": 0.00013509333333333335,
      "loss": -118.0635,
      "step": 24340
    },
    {
      "epoch": 1.948,
      "grad_norm": 119.09326171875,
      "learning_rate": 0.00013506666666666668,
      "loss": -116.7523,
      "step": 24350
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 109.76736450195312,
      "learning_rate": 0.00013504,
      "loss": -118.2885,
      "step": 24360
    },
    {
      "epoch": 1.9496,
      "grad_norm": 89.56295776367188,
      "learning_rate": 0.00013501333333333334,
      "loss": -118.783,
      "step": 24370
    },
    {
      "epoch": 1.9504000000000001,
      "grad_norm": 157.54937744140625,
      "learning_rate": 0.0001349866666666667,
      "loss": -116.928,
      "step": 24380
    },
    {
      "epoch": 1.9512,
      "grad_norm": 104.68958282470703,
      "learning_rate": 0.00013496,
      "loss": -118.3018,
      "step": 24390
    },
    {
      "epoch": 1.952,
      "grad_norm": 127.69114685058594,
      "learning_rate": 0.00013493333333333332,
      "loss": -118.3642,
      "step": 24400
    },
    {
      "epoch": 1.9527999999999999,
      "grad_norm": 92.7698974609375,
      "learning_rate": 0.00013490666666666668,
      "loss": -117.657,
      "step": 24410
    },
    {
      "epoch": 1.9536,
      "grad_norm": 419.3840637207031,
      "learning_rate": 0.00013488,
      "loss": -118.3699,
      "step": 24420
    },
    {
      "epoch": 1.9544000000000001,
      "grad_norm": 122.53406524658203,
      "learning_rate": 0.00013485333333333334,
      "loss": -117.4873,
      "step": 24430
    },
    {
      "epoch": 1.9552,
      "grad_norm": 96.57937622070312,
      "learning_rate": 0.00013482666666666667,
      "loss": -118.5533,
      "step": 24440
    },
    {
      "epoch": 1.956,
      "grad_norm": 94.94673919677734,
      "learning_rate": 0.00013480000000000002,
      "loss": -117.7188,
      "step": 24450
    },
    {
      "epoch": 1.9567999999999999,
      "grad_norm": 104.45388793945312,
      "learning_rate": 0.00013477333333333333,
      "loss": -118.5538,
      "step": 24460
    },
    {
      "epoch": 1.9576,
      "grad_norm": 69.71794128417969,
      "learning_rate": 0.00013474666666666666,
      "loss": -117.905,
      "step": 24470
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 116.8606185913086,
      "learning_rate": 0.00013472,
      "loss": -117.1936,
      "step": 24480
    },
    {
      "epoch": 1.9592,
      "grad_norm": 157.67576599121094,
      "learning_rate": 0.00013469333333333334,
      "loss": -118.555,
      "step": 24490
    },
    {
      "epoch": 1.96,
      "grad_norm": 168.65237426757812,
      "learning_rate": 0.00013466666666666667,
      "loss": -117.6457,
      "step": 24500
    },
    {
      "epoch": 1.9607999999999999,
      "grad_norm": 139.12538146972656,
      "learning_rate": 0.00013464,
      "loss": -118.4148,
      "step": 24510
    },
    {
      "epoch": 1.9616,
      "grad_norm": 90.2447280883789,
      "learning_rate": 0.00013461333333333335,
      "loss": -118.2219,
      "step": 24520
    },
    {
      "epoch": 1.9624000000000001,
      "grad_norm": 105.69734191894531,
      "learning_rate": 0.00013458666666666668,
      "loss": -117.7237,
      "step": 24530
    },
    {
      "epoch": 1.9632,
      "grad_norm": 103.47518920898438,
      "learning_rate": 0.00013455999999999999,
      "loss": -118.1298,
      "step": 24540
    },
    {
      "epoch": 1.964,
      "grad_norm": 135.30751037597656,
      "learning_rate": 0.00013453333333333334,
      "loss": -117.7829,
      "step": 24550
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 110.1793212890625,
      "learning_rate": 0.00013450666666666667,
      "loss": -116.941,
      "step": 24560
    },
    {
      "epoch": 1.9656,
      "grad_norm": 109.47084045410156,
      "learning_rate": 0.00013448,
      "loss": -119.422,
      "step": 24570
    },
    {
      "epoch": 1.9664000000000001,
      "grad_norm": 109.48389434814453,
      "learning_rate": 0.00013445333333333333,
      "loss": -118.3525,
      "step": 24580
    },
    {
      "epoch": 1.9672,
      "grad_norm": 73.60432434082031,
      "learning_rate": 0.00013442666666666669,
      "loss": -118.7449,
      "step": 24590
    },
    {
      "epoch": 1.968,
      "grad_norm": 144.72451782226562,
      "learning_rate": 0.00013440000000000001,
      "loss": -118.4516,
      "step": 24600
    },
    {
      "epoch": 1.9687999999999999,
      "grad_norm": 356.8720703125,
      "learning_rate": 0.00013437333333333332,
      "loss": -118.1206,
      "step": 24610
    },
    {
      "epoch": 1.9696,
      "grad_norm": 126.33008575439453,
      "learning_rate": 0.00013434666666666667,
      "loss": -119.2894,
      "step": 24620
    },
    {
      "epoch": 1.9704000000000002,
      "grad_norm": 68.49043273925781,
      "learning_rate": 0.00013432,
      "loss": -118.0494,
      "step": 24630
    },
    {
      "epoch": 1.9712,
      "grad_norm": 97.12480163574219,
      "learning_rate": 0.00013429333333333333,
      "loss": -118.0186,
      "step": 24640
    },
    {
      "epoch": 1.972,
      "grad_norm": 66.72996520996094,
      "learning_rate": 0.0001342666666666667,
      "loss": -118.2175,
      "step": 24650
    },
    {
      "epoch": 1.9727999999999999,
      "grad_norm": 70.84854125976562,
      "learning_rate": 0.00013424000000000002,
      "loss": -118.4499,
      "step": 24660
    },
    {
      "epoch": 1.9736,
      "grad_norm": 209.5277099609375,
      "learning_rate": 0.00013421333333333334,
      "loss": -117.6302,
      "step": 24670
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 146.6021270751953,
      "learning_rate": 0.00013418666666666667,
      "loss": -117.8227,
      "step": 24680
    },
    {
      "epoch": 1.9752,
      "grad_norm": 90.37133026123047,
      "learning_rate": 0.00013416,
      "loss": -118.8544,
      "step": 24690
    },
    {
      "epoch": 1.976,
      "grad_norm": 84.8250961303711,
      "learning_rate": 0.00013413333333333333,
      "loss": -119.4925,
      "step": 24700
    },
    {
      "epoch": 1.9768,
      "grad_norm": 114.18083190917969,
      "learning_rate": 0.00013410666666666666,
      "loss": -117.4722,
      "step": 24710
    },
    {
      "epoch": 1.9776,
      "grad_norm": 152.93983459472656,
      "learning_rate": 0.00013408000000000002,
      "loss": -117.9559,
      "step": 24720
    },
    {
      "epoch": 1.9784000000000002,
      "grad_norm": 125.05746459960938,
      "learning_rate": 0.00013405333333333335,
      "loss": -118.3213,
      "step": 24730
    },
    {
      "epoch": 1.9792,
      "grad_norm": 128.99169921875,
      "learning_rate": 0.00013402666666666668,
      "loss": -116.9065,
      "step": 24740
    },
    {
      "epoch": 1.98,
      "grad_norm": 81.2417221069336,
      "learning_rate": 0.000134,
      "loss": -118.6249,
      "step": 24750
    },
    {
      "epoch": 1.9808,
      "grad_norm": 126.28367614746094,
      "learning_rate": 0.00013397333333333336,
      "loss": -117.5727,
      "step": 24760
    },
    {
      "epoch": 1.9816,
      "grad_norm": 78.53572845458984,
      "learning_rate": 0.00013394666666666666,
      "loss": -117.0971,
      "step": 24770
    },
    {
      "epoch": 1.9824000000000002,
      "grad_norm": 72.16437530517578,
      "learning_rate": 0.00013392,
      "loss": -117.7882,
      "step": 24780
    },
    {
      "epoch": 1.9832,
      "grad_norm": 107.86722564697266,
      "learning_rate": 0.00013389333333333335,
      "loss": -117.6921,
      "step": 24790
    },
    {
      "epoch": 1.984,
      "grad_norm": 80.85317993164062,
      "learning_rate": 0.00013386666666666668,
      "loss": -118.4502,
      "step": 24800
    },
    {
      "epoch": 1.9848,
      "grad_norm": 96.128173828125,
      "learning_rate": 0.00013384,
      "loss": -117.393,
      "step": 24810
    },
    {
      "epoch": 1.9856,
      "grad_norm": 96.45445251464844,
      "learning_rate": 0.00013381333333333334,
      "loss": -117.7791,
      "step": 24820
    },
    {
      "epoch": 1.9864000000000002,
      "grad_norm": 118.28086853027344,
      "learning_rate": 0.0001337866666666667,
      "loss": -116.9038,
      "step": 24830
    },
    {
      "epoch": 1.9872,
      "grad_norm": 172.73236083984375,
      "learning_rate": 0.00013376,
      "loss": -118.1125,
      "step": 24840
    },
    {
      "epoch": 1.988,
      "grad_norm": 73.68335723876953,
      "learning_rate": 0.00013373333333333332,
      "loss": -118.6879,
      "step": 24850
    },
    {
      "epoch": 1.9888,
      "grad_norm": 103.2510757446289,
      "learning_rate": 0.00013370666666666668,
      "loss": -117.5662,
      "step": 24860
    },
    {
      "epoch": 1.9896,
      "grad_norm": 112.78011322021484,
      "learning_rate": 0.00013368,
      "loss": -117.1932,
      "step": 24870
    },
    {
      "epoch": 1.9904,
      "grad_norm": 98.56373596191406,
      "learning_rate": 0.00013365333333333334,
      "loss": -118.8768,
      "step": 24880
    },
    {
      "epoch": 1.9912,
      "grad_norm": 74.93357849121094,
      "learning_rate": 0.00013362666666666667,
      "loss": -117.8014,
      "step": 24890
    },
    {
      "epoch": 1.992,
      "grad_norm": 106.95183563232422,
      "learning_rate": 0.00013360000000000002,
      "loss": -117.2533,
      "step": 24900
    },
    {
      "epoch": 1.9928,
      "grad_norm": 77.56739044189453,
      "learning_rate": 0.00013357333333333335,
      "loss": -117.7859,
      "step": 24910
    },
    {
      "epoch": 1.9936,
      "grad_norm": 98.52327728271484,
      "learning_rate": 0.00013354666666666665,
      "loss": -118.7727,
      "step": 24920
    },
    {
      "epoch": 1.9944,
      "grad_norm": 116.89501953125,
      "learning_rate": 0.00013352,
      "loss": -117.4083,
      "step": 24930
    },
    {
      "epoch": 1.9952,
      "grad_norm": 96.05475616455078,
      "learning_rate": 0.00013349333333333334,
      "loss": -118.034,
      "step": 24940
    },
    {
      "epoch": 1.996,
      "grad_norm": 871.739013671875,
      "learning_rate": 0.00013346666666666667,
      "loss": -118.1128,
      "step": 24950
    },
    {
      "epoch": 1.9968,
      "grad_norm": 79.98875427246094,
      "learning_rate": 0.00013344,
      "loss": -118.152,
      "step": 24960
    },
    {
      "epoch": 1.9976,
      "grad_norm": 216.5790252685547,
      "learning_rate": 0.00013341333333333335,
      "loss": -117.9948,
      "step": 24970
    },
    {
      "epoch": 1.9984,
      "grad_norm": 99.08905029296875,
      "learning_rate": 0.00013338666666666668,
      "loss": -119.1888,
      "step": 24980
    },
    {
      "epoch": 1.9992,
      "grad_norm": 122.162841796875,
      "learning_rate": 0.00013335999999999998,
      "loss": -116.9862,
      "step": 24990
    },
    {
      "epoch": 2.0,
      "grad_norm": 117.1363525390625,
      "learning_rate": 0.00013333333333333334,
      "loss": -118.5684,
      "step": 25000
    },
    {
      "epoch": 2.0008,
      "grad_norm": 97.83677673339844,
      "learning_rate": 0.00013330666666666667,
      "loss": -117.0358,
      "step": 25010
    },
    {
      "epoch": 2.0016,
      "grad_norm": 84.36266326904297,
      "learning_rate": 0.00013328,
      "loss": -118.6682,
      "step": 25020
    },
    {
      "epoch": 2.0024,
      "grad_norm": 79.53657531738281,
      "learning_rate": 0.00013325333333333335,
      "loss": -118.1448,
      "step": 25030
    },
    {
      "epoch": 2.0032,
      "grad_norm": 87.82369995117188,
      "learning_rate": 0.00013322666666666668,
      "loss": -118.6657,
      "step": 25040
    },
    {
      "epoch": 2.004,
      "grad_norm": 133.62661743164062,
      "learning_rate": 0.0001332,
      "loss": -118.6533,
      "step": 25050
    },
    {
      "epoch": 2.0048,
      "grad_norm": 81.1432876586914,
      "learning_rate": 0.00013317333333333334,
      "loss": -117.1854,
      "step": 25060
    },
    {
      "epoch": 2.0056,
      "grad_norm": 121.58953857421875,
      "learning_rate": 0.00013314666666666667,
      "loss": -116.4384,
      "step": 25070
    },
    {
      "epoch": 2.0064,
      "grad_norm": 116.19116973876953,
      "learning_rate": 0.00013312,
      "loss": -118.3727,
      "step": 25080
    },
    {
      "epoch": 2.0072,
      "grad_norm": 102.92301940917969,
      "learning_rate": 0.00013309333333333333,
      "loss": -117.421,
      "step": 25090
    },
    {
      "epoch": 2.008,
      "grad_norm": 72.52200317382812,
      "learning_rate": 0.00013306666666666668,
      "loss": -117.9706,
      "step": 25100
    },
    {
      "epoch": 2.0088,
      "grad_norm": 145.8455810546875,
      "learning_rate": 0.00013304000000000001,
      "loss": -118.3093,
      "step": 25110
    },
    {
      "epoch": 2.0096,
      "grad_norm": 153.55677795410156,
      "learning_rate": 0.00013301333333333334,
      "loss": -118.5119,
      "step": 25120
    },
    {
      "epoch": 2.0104,
      "grad_norm": 63.402061462402344,
      "learning_rate": 0.00013298666666666667,
      "loss": -118.13,
      "step": 25130
    },
    {
      "epoch": 2.0112,
      "grad_norm": 157.3351287841797,
      "learning_rate": 0.00013296,
      "loss": -118.1514,
      "step": 25140
    },
    {
      "epoch": 2.012,
      "grad_norm": 178.16294860839844,
      "learning_rate": 0.00013293333333333333,
      "loss": -116.9481,
      "step": 25150
    },
    {
      "epoch": 2.0128,
      "grad_norm": 80.44873809814453,
      "learning_rate": 0.00013290666666666666,
      "loss": -117.2261,
      "step": 25160
    },
    {
      "epoch": 2.0136,
      "grad_norm": 82.6534652709961,
      "learning_rate": 0.00013288000000000002,
      "loss": -117.8943,
      "step": 25170
    },
    {
      "epoch": 2.0144,
      "grad_norm": 88.98912811279297,
      "learning_rate": 0.00013285333333333334,
      "loss": -118.6266,
      "step": 25180
    },
    {
      "epoch": 2.0152,
      "grad_norm": 102.89404296875,
      "learning_rate": 0.00013282666666666667,
      "loss": -118.3426,
      "step": 25190
    },
    {
      "epoch": 2.016,
      "grad_norm": 75.63817596435547,
      "learning_rate": 0.0001328,
      "loss": -118.8634,
      "step": 25200
    },
    {
      "epoch": 2.0168,
      "grad_norm": 83.49317169189453,
      "learning_rate": 0.00013277333333333336,
      "loss": -118.1951,
      "step": 25210
    },
    {
      "epoch": 2.0176,
      "grad_norm": 91.74312591552734,
      "learning_rate": 0.00013274666666666666,
      "loss": -119.0816,
      "step": 25220
    },
    {
      "epoch": 2.0184,
      "grad_norm": 73.32015991210938,
      "learning_rate": 0.00013272,
      "loss": -117.2105,
      "step": 25230
    },
    {
      "epoch": 2.0192,
      "grad_norm": 100.44511413574219,
      "learning_rate": 0.00013269333333333335,
      "loss": -117.9927,
      "step": 25240
    },
    {
      "epoch": 2.02,
      "grad_norm": 125.21249389648438,
      "learning_rate": 0.00013266666666666667,
      "loss": -118.3752,
      "step": 25250
    },
    {
      "epoch": 2.0208,
      "grad_norm": 179.35403442382812,
      "learning_rate": 0.00013264,
      "loss": -117.9808,
      "step": 25260
    },
    {
      "epoch": 2.0216,
      "grad_norm": 62.9988899230957,
      "learning_rate": 0.00013261333333333333,
      "loss": -117.9701,
      "step": 25270
    },
    {
      "epoch": 2.0224,
      "grad_norm": 132.6657257080078,
      "learning_rate": 0.0001325866666666667,
      "loss": -117.4665,
      "step": 25280
    },
    {
      "epoch": 2.0232,
      "grad_norm": 91.76557922363281,
      "learning_rate": 0.00013256,
      "loss": -118.519,
      "step": 25290
    },
    {
      "epoch": 2.024,
      "grad_norm": 191.60678100585938,
      "learning_rate": 0.00013253333333333332,
      "loss": -118.7059,
      "step": 25300
    },
    {
      "epoch": 2.0248,
      "grad_norm": 69.58525848388672,
      "learning_rate": 0.00013250666666666668,
      "loss": -117.1647,
      "step": 25310
    },
    {
      "epoch": 2.0256,
      "grad_norm": 108.18190002441406,
      "learning_rate": 0.00013248,
      "loss": -119.1223,
      "step": 25320
    },
    {
      "epoch": 2.0264,
      "grad_norm": 85.51094818115234,
      "learning_rate": 0.00013245333333333333,
      "loss": -117.9973,
      "step": 25330
    },
    {
      "epoch": 2.0272,
      "grad_norm": 80.50064849853516,
      "learning_rate": 0.00013242666666666666,
      "loss": -117.7552,
      "step": 25340
    },
    {
      "epoch": 2.028,
      "grad_norm": 129.47915649414062,
      "learning_rate": 0.00013240000000000002,
      "loss": -118.7706,
      "step": 25350
    },
    {
      "epoch": 2.0288,
      "grad_norm": 80.6418685913086,
      "learning_rate": 0.00013237333333333335,
      "loss": -118.4927,
      "step": 25360
    },
    {
      "epoch": 2.0296,
      "grad_norm": 77.45824432373047,
      "learning_rate": 0.00013234666666666665,
      "loss": -117.3209,
      "step": 25370
    },
    {
      "epoch": 2.0304,
      "grad_norm": 89.03446197509766,
      "learning_rate": 0.00013232,
      "loss": -118.5528,
      "step": 25380
    },
    {
      "epoch": 2.0312,
      "grad_norm": 90.9383773803711,
      "learning_rate": 0.00013229333333333334,
      "loss": -117.7907,
      "step": 25390
    },
    {
      "epoch": 2.032,
      "grad_norm": 193.67250061035156,
      "learning_rate": 0.00013226666666666667,
      "loss": -118.0737,
      "step": 25400
    },
    {
      "epoch": 2.0328,
      "grad_norm": 111.52002716064453,
      "learning_rate": 0.00013224000000000002,
      "loss": -117.8963,
      "step": 25410
    },
    {
      "epoch": 2.0336,
      "grad_norm": 162.04513549804688,
      "learning_rate": 0.00013221333333333335,
      "loss": -117.3724,
      "step": 25420
    },
    {
      "epoch": 2.0344,
      "grad_norm": 96.48155212402344,
      "learning_rate": 0.00013218666666666668,
      "loss": -118.0035,
      "step": 25430
    },
    {
      "epoch": 2.0352,
      "grad_norm": 80.5115966796875,
      "learning_rate": 0.00013216,
      "loss": -117.2102,
      "step": 25440
    },
    {
      "epoch": 2.036,
      "grad_norm": 111.00471496582031,
      "learning_rate": 0.00013213333333333334,
      "loss": -118.4986,
      "step": 25450
    },
    {
      "epoch": 2.0368,
      "grad_norm": 85.6977767944336,
      "learning_rate": 0.00013210666666666667,
      "loss": -117.7026,
      "step": 25460
    },
    {
      "epoch": 2.0376,
      "grad_norm": 97.83641815185547,
      "learning_rate": 0.00013208,
      "loss": -119.0693,
      "step": 25470
    },
    {
      "epoch": 2.0384,
      "grad_norm": 88.49517822265625,
      "learning_rate": 0.00013205333333333335,
      "loss": -117.7853,
      "step": 25480
    },
    {
      "epoch": 2.0392,
      "grad_norm": 109.04502868652344,
      "learning_rate": 0.00013202666666666668,
      "loss": -116.8819,
      "step": 25490
    },
    {
      "epoch": 2.04,
      "grad_norm": 126.1618881225586,
      "learning_rate": 0.000132,
      "loss": -118.1577,
      "step": 25500
    },
    {
      "epoch": 2.0408,
      "grad_norm": 103.02240753173828,
      "learning_rate": 0.00013197333333333334,
      "loss": -118.6303,
      "step": 25510
    },
    {
      "epoch": 2.0416,
      "grad_norm": 90.65360260009766,
      "learning_rate": 0.00013194666666666667,
      "loss": -118.2623,
      "step": 25520
    },
    {
      "epoch": 2.0424,
      "grad_norm": 80.20585632324219,
      "learning_rate": 0.00013192,
      "loss": -119.1427,
      "step": 25530
    },
    {
      "epoch": 2.0432,
      "grad_norm": 86.02635192871094,
      "learning_rate": 0.00013189333333333333,
      "loss": -118.9682,
      "step": 25540
    },
    {
      "epoch": 2.044,
      "grad_norm": 98.82022094726562,
      "learning_rate": 0.00013186666666666668,
      "loss": -118.5518,
      "step": 25550
    },
    {
      "epoch": 2.0448,
      "grad_norm": 71.26118469238281,
      "learning_rate": 0.00013184,
      "loss": -118.9057,
      "step": 25560
    },
    {
      "epoch": 2.0456,
      "grad_norm": 98.45511627197266,
      "learning_rate": 0.00013181333333333334,
      "loss": -118.3624,
      "step": 25570
    },
    {
      "epoch": 2.0464,
      "grad_norm": 89.33442687988281,
      "learning_rate": 0.00013178666666666667,
      "loss": -118.1743,
      "step": 25580
    },
    {
      "epoch": 2.0472,
      "grad_norm": 154.38018798828125,
      "learning_rate": 0.00013176000000000003,
      "loss": -118.4881,
      "step": 25590
    },
    {
      "epoch": 2.048,
      "grad_norm": 126.71818542480469,
      "learning_rate": 0.00013173333333333333,
      "loss": -118.3784,
      "step": 25600
    },
    {
      "epoch": 2.0488,
      "grad_norm": 84.11554718017578,
      "learning_rate": 0.00013170666666666666,
      "loss": -118.7983,
      "step": 25610
    },
    {
      "epoch": 2.0496,
      "grad_norm": 104.70954132080078,
      "learning_rate": 0.00013168,
      "loss": -117.7599,
      "step": 25620
    },
    {
      "epoch": 2.0504,
      "grad_norm": 78.85484313964844,
      "learning_rate": 0.00013165333333333334,
      "loss": -118.0395,
      "step": 25630
    },
    {
      "epoch": 2.0512,
      "grad_norm": 110.08421325683594,
      "learning_rate": 0.00013162666666666667,
      "loss": -117.4969,
      "step": 25640
    },
    {
      "epoch": 2.052,
      "grad_norm": 104.02359008789062,
      "learning_rate": 0.0001316,
      "loss": -116.8563,
      "step": 25650
    },
    {
      "epoch": 2.0528,
      "grad_norm": 81.87976837158203,
      "learning_rate": 0.00013157333333333336,
      "loss": -116.811,
      "step": 25660
    },
    {
      "epoch": 2.0536,
      "grad_norm": 87.02912902832031,
      "learning_rate": 0.00013154666666666666,
      "loss": -117.8291,
      "step": 25670
    },
    {
      "epoch": 2.0544,
      "grad_norm": 89.28426361083984,
      "learning_rate": 0.00013152,
      "loss": -118.1189,
      "step": 25680
    },
    {
      "epoch": 2.0552,
      "grad_norm": 111.49933624267578,
      "learning_rate": 0.00013149333333333334,
      "loss": -118.5028,
      "step": 25690
    },
    {
      "epoch": 2.056,
      "grad_norm": 157.59254455566406,
      "learning_rate": 0.00013146666666666667,
      "loss": -117.399,
      "step": 25700
    },
    {
      "epoch": 2.0568,
      "grad_norm": 104.64362335205078,
      "learning_rate": 0.00013144,
      "loss": -118.3644,
      "step": 25710
    },
    {
      "epoch": 2.0576,
      "grad_norm": 97.99585723876953,
      "learning_rate": 0.00013141333333333333,
      "loss": -118.6894,
      "step": 25720
    },
    {
      "epoch": 2.0584,
      "grad_norm": 111.12142181396484,
      "learning_rate": 0.0001313866666666667,
      "loss": -117.1679,
      "step": 25730
    },
    {
      "epoch": 2.0592,
      "grad_norm": 110.21776580810547,
      "learning_rate": 0.00013136000000000002,
      "loss": -118.3379,
      "step": 25740
    },
    {
      "epoch": 2.06,
      "grad_norm": 152.13214111328125,
      "learning_rate": 0.00013133333333333332,
      "loss": -118.4079,
      "step": 25750
    },
    {
      "epoch": 2.0608,
      "grad_norm": 75.17443084716797,
      "learning_rate": 0.00013130666666666667,
      "loss": -117.132,
      "step": 25760
    },
    {
      "epoch": 2.0616,
      "grad_norm": 118.60912322998047,
      "learning_rate": 0.00013128,
      "loss": -117.4415,
      "step": 25770
    },
    {
      "epoch": 2.0624,
      "grad_norm": 147.95779418945312,
      "learning_rate": 0.00013125333333333333,
      "loss": -117.7594,
      "step": 25780
    },
    {
      "epoch": 2.0632,
      "grad_norm": 109.17765808105469,
      "learning_rate": 0.0001312266666666667,
      "loss": -118.261,
      "step": 25790
    },
    {
      "epoch": 2.064,
      "grad_norm": 91.77960968017578,
      "learning_rate": 0.00013120000000000002,
      "loss": -118.896,
      "step": 25800
    },
    {
      "epoch": 2.0648,
      "grad_norm": 95.2138900756836,
      "learning_rate": 0.00013117333333333335,
      "loss": -118.6571,
      "step": 25810
    },
    {
      "epoch": 2.0656,
      "grad_norm": 101.65275573730469,
      "learning_rate": 0.00013114666666666665,
      "loss": -117.9903,
      "step": 25820
    },
    {
      "epoch": 2.0664,
      "grad_norm": 101.45059967041016,
      "learning_rate": 0.00013112,
      "loss": -119.3016,
      "step": 25830
    },
    {
      "epoch": 2.0672,
      "grad_norm": 105.60050964355469,
      "learning_rate": 0.00013109333333333333,
      "loss": -118.2993,
      "step": 25840
    },
    {
      "epoch": 2.068,
      "grad_norm": 73.92279815673828,
      "learning_rate": 0.00013106666666666666,
      "loss": -118.6689,
      "step": 25850
    },
    {
      "epoch": 2.0688,
      "grad_norm": 84.00021362304688,
      "learning_rate": 0.00013104000000000002,
      "loss": -117.7416,
      "step": 25860
    },
    {
      "epoch": 2.0696,
      "grad_norm": 136.4718017578125,
      "learning_rate": 0.00013101333333333335,
      "loss": -117.408,
      "step": 25870
    },
    {
      "epoch": 2.0704,
      "grad_norm": 114.89116668701172,
      "learning_rate": 0.00013098666666666668,
      "loss": -118.998,
      "step": 25880
    },
    {
      "epoch": 2.0712,
      "grad_norm": 97.79564666748047,
      "learning_rate": 0.00013096,
      "loss": -117.6436,
      "step": 25890
    },
    {
      "epoch": 2.072,
      "grad_norm": 73.88325500488281,
      "learning_rate": 0.00013093333333333334,
      "loss": -118.0967,
      "step": 25900
    },
    {
      "epoch": 2.0728,
      "grad_norm": 110.24718475341797,
      "learning_rate": 0.00013090666666666666,
      "loss": -118.4229,
      "step": 25910
    },
    {
      "epoch": 2.0736,
      "grad_norm": 125.54470825195312,
      "learning_rate": 0.00013088,
      "loss": -118.2481,
      "step": 25920
    },
    {
      "epoch": 2.0744,
      "grad_norm": 121.35933685302734,
      "learning_rate": 0.00013085333333333335,
      "loss": -116.6969,
      "step": 25930
    },
    {
      "epoch": 2.0752,
      "grad_norm": 86.427734375,
      "learning_rate": 0.00013082666666666668,
      "loss": -117.5206,
      "step": 25940
    },
    {
      "epoch": 2.076,
      "grad_norm": 94.08523559570312,
      "learning_rate": 0.0001308,
      "loss": -118.1893,
      "step": 25950
    },
    {
      "epoch": 2.0768,
      "grad_norm": 109.92499542236328,
      "learning_rate": 0.00013077333333333334,
      "loss": -119.1763,
      "step": 25960
    },
    {
      "epoch": 2.0776,
      "grad_norm": 85.98056030273438,
      "learning_rate": 0.0001307466666666667,
      "loss": -118.3711,
      "step": 25970
    },
    {
      "epoch": 2.0784,
      "grad_norm": 115.681396484375,
      "learning_rate": 0.00013072,
      "loss": -118.5854,
      "step": 25980
    },
    {
      "epoch": 2.0792,
      "grad_norm": 112.4853286743164,
      "learning_rate": 0.00013069333333333332,
      "loss": -118.7889,
      "step": 25990
    },
    {
      "epoch": 2.08,
      "grad_norm": 150.94639587402344,
      "learning_rate": 0.00013066666666666668,
      "loss": -118.7595,
      "step": 26000
    },
    {
      "epoch": 2.0808,
      "grad_norm": 80.53926086425781,
      "learning_rate": 0.00013064,
      "loss": -117.8821,
      "step": 26010
    },
    {
      "epoch": 2.0816,
      "grad_norm": 94.52615356445312,
      "learning_rate": 0.00013061333333333334,
      "loss": -119.3755,
      "step": 26020
    },
    {
      "epoch": 2.0824,
      "grad_norm": 99.41621398925781,
      "learning_rate": 0.00013058666666666667,
      "loss": -119.6025,
      "step": 26030
    },
    {
      "epoch": 2.0832,
      "grad_norm": 79.8027114868164,
      "learning_rate": 0.00013056000000000002,
      "loss": -117.5405,
      "step": 26040
    },
    {
      "epoch": 2.084,
      "grad_norm": 67.0384292602539,
      "learning_rate": 0.00013053333333333333,
      "loss": -118.9812,
      "step": 26050
    },
    {
      "epoch": 2.0848,
      "grad_norm": 67.45057678222656,
      "learning_rate": 0.00013050666666666665,
      "loss": -118.8474,
      "step": 26060
    },
    {
      "epoch": 2.0856,
      "grad_norm": 135.0259246826172,
      "learning_rate": 0.00013048,
      "loss": -117.8316,
      "step": 26070
    },
    {
      "epoch": 2.0864,
      "grad_norm": 232.57933044433594,
      "learning_rate": 0.00013045333333333334,
      "loss": -117.2419,
      "step": 26080
    },
    {
      "epoch": 2.0872,
      "grad_norm": 140.4812469482422,
      "learning_rate": 0.00013042666666666667,
      "loss": -117.4375,
      "step": 26090
    },
    {
      "epoch": 2.088,
      "grad_norm": 100.03366088867188,
      "learning_rate": 0.0001304,
      "loss": -118.2335,
      "step": 26100
    },
    {
      "epoch": 2.0888,
      "grad_norm": 92.86851501464844,
      "learning_rate": 0.00013037333333333335,
      "loss": -118.036,
      "step": 26110
    },
    {
      "epoch": 2.0896,
      "grad_norm": 138.35870361328125,
      "learning_rate": 0.00013034666666666668,
      "loss": -119.125,
      "step": 26120
    },
    {
      "epoch": 2.0904,
      "grad_norm": 76.50119018554688,
      "learning_rate": 0.00013031999999999999,
      "loss": -118.3918,
      "step": 26130
    },
    {
      "epoch": 2.0912,
      "grad_norm": 130.5116729736328,
      "learning_rate": 0.00013029333333333334,
      "loss": -118.2844,
      "step": 26140
    },
    {
      "epoch": 2.092,
      "grad_norm": 156.28402709960938,
      "learning_rate": 0.00013026666666666667,
      "loss": -119.2964,
      "step": 26150
    },
    {
      "epoch": 2.0928,
      "grad_norm": 91.47146606445312,
      "learning_rate": 0.00013024,
      "loss": -118.0208,
      "step": 26160
    },
    {
      "epoch": 2.0936,
      "grad_norm": 116.72403717041016,
      "learning_rate": 0.00013021333333333336,
      "loss": -118.1193,
      "step": 26170
    },
    {
      "epoch": 2.0944,
      "grad_norm": 79.2093505859375,
      "learning_rate": 0.00013018666666666669,
      "loss": -117.8771,
      "step": 26180
    },
    {
      "epoch": 2.0952,
      "grad_norm": 86.20939636230469,
      "learning_rate": 0.00013016000000000001,
      "loss": -118.5832,
      "step": 26190
    },
    {
      "epoch": 2.096,
      "grad_norm": 119.96263885498047,
      "learning_rate": 0.00013013333333333332,
      "loss": -118.4264,
      "step": 26200
    },
    {
      "epoch": 2.0968,
      "grad_norm": 92.57007598876953,
      "learning_rate": 0.00013010666666666667,
      "loss": -119.1479,
      "step": 26210
    },
    {
      "epoch": 2.0976,
      "grad_norm": 82.46781158447266,
      "learning_rate": 0.00013008,
      "loss": -117.6626,
      "step": 26220
    },
    {
      "epoch": 2.0984,
      "grad_norm": 87.95130920410156,
      "learning_rate": 0.00013005333333333333,
      "loss": -116.5171,
      "step": 26230
    },
    {
      "epoch": 2.0992,
      "grad_norm": 111.88626098632812,
      "learning_rate": 0.0001300266666666667,
      "loss": -116.5663,
      "step": 26240
    },
    {
      "epoch": 2.1,
      "grad_norm": 88.56040954589844,
      "learning_rate": 0.00013000000000000002,
      "loss": -118.8763,
      "step": 26250
    },
    {
      "epoch": 2.1008,
      "grad_norm": 91.25239562988281,
      "learning_rate": 0.00012997333333333334,
      "loss": -117.1379,
      "step": 26260
    },
    {
      "epoch": 2.1016,
      "grad_norm": 102.56920623779297,
      "learning_rate": 0.00012994666666666667,
      "loss": -117.8775,
      "step": 26270
    },
    {
      "epoch": 2.1024,
      "grad_norm": 146.46543884277344,
      "learning_rate": 0.00012992,
      "loss": -117.9937,
      "step": 26280
    },
    {
      "epoch": 2.1032,
      "grad_norm": 85.40632629394531,
      "learning_rate": 0.00012989333333333333,
      "loss": -116.6232,
      "step": 26290
    },
    {
      "epoch": 2.104,
      "grad_norm": 88.84393310546875,
      "learning_rate": 0.00012986666666666666,
      "loss": -117.0274,
      "step": 26300
    },
    {
      "epoch": 2.1048,
      "grad_norm": 123.13351440429688,
      "learning_rate": 0.00012984000000000002,
      "loss": -118.0949,
      "step": 26310
    },
    {
      "epoch": 2.1056,
      "grad_norm": 144.52809143066406,
      "learning_rate": 0.00012981333333333335,
      "loss": -117.3592,
      "step": 26320
    },
    {
      "epoch": 2.1064,
      "grad_norm": 108.20899200439453,
      "learning_rate": 0.00012978666666666668,
      "loss": -118.3619,
      "step": 26330
    },
    {
      "epoch": 2.1072,
      "grad_norm": 98.3409423828125,
      "learning_rate": 0.00012976,
      "loss": -118.7091,
      "step": 26340
    },
    {
      "epoch": 2.108,
      "grad_norm": 107.22232055664062,
      "learning_rate": 0.00012973333333333333,
      "loss": -118.164,
      "step": 26350
    },
    {
      "epoch": 2.1088,
      "grad_norm": 172.4918212890625,
      "learning_rate": 0.00012970666666666666,
      "loss": -117.5995,
      "step": 26360
    },
    {
      "epoch": 2.1096,
      "grad_norm": 113.41094207763672,
      "learning_rate": 0.00012968,
      "loss": -117.1279,
      "step": 26370
    },
    {
      "epoch": 2.1104,
      "grad_norm": 88.7760009765625,
      "learning_rate": 0.00012965333333333335,
      "loss": -118.99,
      "step": 26380
    },
    {
      "epoch": 2.1112,
      "grad_norm": 78.33216857910156,
      "learning_rate": 0.00012962666666666668,
      "loss": -117.4704,
      "step": 26390
    },
    {
      "epoch": 2.112,
      "grad_norm": 89.60122680664062,
      "learning_rate": 0.0001296,
      "loss": -116.6325,
      "step": 26400
    },
    {
      "epoch": 2.1128,
      "grad_norm": 103.58299255371094,
      "learning_rate": 0.00012957333333333334,
      "loss": -117.7065,
      "step": 26410
    },
    {
      "epoch": 2.1136,
      "grad_norm": 90.43571472167969,
      "learning_rate": 0.0001295466666666667,
      "loss": -118.5254,
      "step": 26420
    },
    {
      "epoch": 2.1144,
      "grad_norm": 90.80906677246094,
      "learning_rate": 0.00012952,
      "loss": -117.3166,
      "step": 26430
    },
    {
      "epoch": 2.1152,
      "grad_norm": 120.15596771240234,
      "learning_rate": 0.00012949333333333332,
      "loss": -117.3532,
      "step": 26440
    },
    {
      "epoch": 2.116,
      "grad_norm": 92.24126434326172,
      "learning_rate": 0.00012946666666666668,
      "loss": -116.4546,
      "step": 26450
    },
    {
      "epoch": 2.1168,
      "grad_norm": 143.80856323242188,
      "learning_rate": 0.00012944,
      "loss": -117.9539,
      "step": 26460
    },
    {
      "epoch": 2.1176,
      "grad_norm": 58.62761688232422,
      "learning_rate": 0.00012941333333333334,
      "loss": -118.8149,
      "step": 26470
    },
    {
      "epoch": 2.1184,
      "grad_norm": 71.49378204345703,
      "learning_rate": 0.00012938666666666667,
      "loss": -117.8961,
      "step": 26480
    },
    {
      "epoch": 2.1192,
      "grad_norm": 97.4747543334961,
      "learning_rate": 0.00012936000000000002,
      "loss": -117.9588,
      "step": 26490
    },
    {
      "epoch": 2.12,
      "grad_norm": 76.8814926147461,
      "learning_rate": 0.00012933333333333332,
      "loss": -118.3243,
      "step": 26500
    },
    {
      "epoch": 2.1208,
      "grad_norm": 72.40987396240234,
      "learning_rate": 0.00012930666666666665,
      "loss": -118.7147,
      "step": 26510
    },
    {
      "epoch": 2.1216,
      "grad_norm": 57.89259719848633,
      "learning_rate": 0.00012928,
      "loss": -118.6822,
      "step": 26520
    },
    {
      "epoch": 2.1224,
      "grad_norm": 83.44439697265625,
      "learning_rate": 0.00012925333333333334,
      "loss": -119.64,
      "step": 26530
    },
    {
      "epoch": 2.1232,
      "grad_norm": 94.72044372558594,
      "learning_rate": 0.00012922666666666667,
      "loss": -117.3234,
      "step": 26540
    },
    {
      "epoch": 2.124,
      "grad_norm": 70.18206024169922,
      "learning_rate": 0.00012920000000000002,
      "loss": -117.9441,
      "step": 26550
    },
    {
      "epoch": 2.1248,
      "grad_norm": 63.28776168823242,
      "learning_rate": 0.00012917333333333335,
      "loss": -117.9697,
      "step": 26560
    },
    {
      "epoch": 2.1256,
      "grad_norm": 96.38327026367188,
      "learning_rate": 0.00012914666666666668,
      "loss": -118.0149,
      "step": 26570
    },
    {
      "epoch": 2.1264,
      "grad_norm": 132.59014892578125,
      "learning_rate": 0.00012911999999999998,
      "loss": -118.4909,
      "step": 26580
    },
    {
      "epoch": 2.1272,
      "grad_norm": 89.65678405761719,
      "learning_rate": 0.00012909333333333334,
      "loss": -117.8257,
      "step": 26590
    },
    {
      "epoch": 2.128,
      "grad_norm": 132.99237060546875,
      "learning_rate": 0.00012906666666666667,
      "loss": -118.313,
      "step": 26600
    },
    {
      "epoch": 2.1288,
      "grad_norm": 137.54837036132812,
      "learning_rate": 0.00012904,
      "loss": -118.1878,
      "step": 26610
    },
    {
      "epoch": 2.1296,
      "grad_norm": 110.88528442382812,
      "learning_rate": 0.00012901333333333335,
      "loss": -119.0012,
      "step": 26620
    },
    {
      "epoch": 2.1304,
      "grad_norm": 106.97388458251953,
      "learning_rate": 0.00012898666666666668,
      "loss": -117.2581,
      "step": 26630
    },
    {
      "epoch": 2.1312,
      "grad_norm": 110.13580322265625,
      "learning_rate": 0.00012896,
      "loss": -117.6044,
      "step": 26640
    },
    {
      "epoch": 2.132,
      "grad_norm": 95.52217864990234,
      "learning_rate": 0.00012893333333333334,
      "loss": -117.8008,
      "step": 26650
    },
    {
      "epoch": 2.1328,
      "grad_norm": 75.56371307373047,
      "learning_rate": 0.00012890666666666667,
      "loss": -118.021,
      "step": 26660
    },
    {
      "epoch": 2.1336,
      "grad_norm": 84.69121551513672,
      "learning_rate": 0.00012888,
      "loss": -118.8341,
      "step": 26670
    },
    {
      "epoch": 2.1344,
      "grad_norm": 84.52616882324219,
      "learning_rate": 0.00012885333333333333,
      "loss": -118.0189,
      "step": 26680
    },
    {
      "epoch": 2.1352,
      "grad_norm": 82.76292419433594,
      "learning_rate": 0.00012882666666666668,
      "loss": -117.9426,
      "step": 26690
    },
    {
      "epoch": 2.136,
      "grad_norm": 133.35252380371094,
      "learning_rate": 0.00012880000000000001,
      "loss": -118.4913,
      "step": 26700
    },
    {
      "epoch": 2.1368,
      "grad_norm": 126.89437103271484,
      "learning_rate": 0.00012877333333333334,
      "loss": -119.2918,
      "step": 26710
    },
    {
      "epoch": 2.1376,
      "grad_norm": 118.28239440917969,
      "learning_rate": 0.00012874666666666667,
      "loss": -117.3672,
      "step": 26720
    },
    {
      "epoch": 2.1384,
      "grad_norm": 102.77091217041016,
      "learning_rate": 0.00012872,
      "loss": -116.6299,
      "step": 26730
    },
    {
      "epoch": 2.1391999999999998,
      "grad_norm": 114.02002716064453,
      "learning_rate": 0.00012869333333333333,
      "loss": -119.102,
      "step": 26740
    },
    {
      "epoch": 2.14,
      "grad_norm": 121.64019775390625,
      "learning_rate": 0.00012866666666666666,
      "loss": -117.6503,
      "step": 26750
    },
    {
      "epoch": 2.1408,
      "grad_norm": 186.46343994140625,
      "learning_rate": 0.00012864000000000002,
      "loss": -117.0409,
      "step": 26760
    },
    {
      "epoch": 2.1416,
      "grad_norm": 106.81282043457031,
      "learning_rate": 0.00012861333333333334,
      "loss": -118.0916,
      "step": 26770
    },
    {
      "epoch": 2.1424,
      "grad_norm": 109.8625717163086,
      "learning_rate": 0.00012858666666666667,
      "loss": -118.4666,
      "step": 26780
    },
    {
      "epoch": 2.1432,
      "grad_norm": 138.6976318359375,
      "learning_rate": 0.00012856,
      "loss": -118.0842,
      "step": 26790
    },
    {
      "epoch": 2.144,
      "grad_norm": 46.66010284423828,
      "learning_rate": 0.00012853333333333336,
      "loss": -117.9192,
      "step": 26800
    },
    {
      "epoch": 2.1448,
      "grad_norm": 97.00675964355469,
      "learning_rate": 0.00012850666666666666,
      "loss": -118.8635,
      "step": 26810
    },
    {
      "epoch": 2.1456,
      "grad_norm": 88.3427963256836,
      "learning_rate": 0.00012848,
      "loss": -117.8763,
      "step": 26820
    },
    {
      "epoch": 2.1464,
      "grad_norm": 127.02154541015625,
      "learning_rate": 0.00012845333333333335,
      "loss": -118.7408,
      "step": 26830
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 111.5925064086914,
      "learning_rate": 0.00012842666666666667,
      "loss": -118.5248,
      "step": 26840
    },
    {
      "epoch": 2.148,
      "grad_norm": 98.0197982788086,
      "learning_rate": 0.0001284,
      "loss": -118.591,
      "step": 26850
    },
    {
      "epoch": 2.1488,
      "grad_norm": 98.9951171875,
      "learning_rate": 0.00012837333333333333,
      "loss": -117.3177,
      "step": 26860
    },
    {
      "epoch": 2.1496,
      "grad_norm": 159.8082275390625,
      "learning_rate": 0.0001283466666666667,
      "loss": -118.7311,
      "step": 26870
    },
    {
      "epoch": 2.1504,
      "grad_norm": 137.7856903076172,
      "learning_rate": 0.00012832,
      "loss": -118.5856,
      "step": 26880
    },
    {
      "epoch": 2.1512000000000002,
      "grad_norm": 72.72872161865234,
      "learning_rate": 0.00012829333333333332,
      "loss": -117.1301,
      "step": 26890
    },
    {
      "epoch": 2.152,
      "grad_norm": 114.1430435180664,
      "learning_rate": 0.00012826666666666668,
      "loss": -117.8932,
      "step": 26900
    },
    {
      "epoch": 2.1528,
      "grad_norm": 112.56275939941406,
      "learning_rate": 0.00012824,
      "loss": -118.5469,
      "step": 26910
    },
    {
      "epoch": 2.1536,
      "grad_norm": 161.0902099609375,
      "learning_rate": 0.00012821333333333333,
      "loss": -118.7502,
      "step": 26920
    },
    {
      "epoch": 2.1544,
      "grad_norm": 123.68367767333984,
      "learning_rate": 0.00012818666666666666,
      "loss": -118.109,
      "step": 26930
    },
    {
      "epoch": 2.1552,
      "grad_norm": 93.48045349121094,
      "learning_rate": 0.00012816000000000002,
      "loss": -118.58,
      "step": 26940
    },
    {
      "epoch": 2.156,
      "grad_norm": 97.80717468261719,
      "learning_rate": 0.00012813333333333335,
      "loss": -118.6895,
      "step": 26950
    },
    {
      "epoch": 2.1568,
      "grad_norm": 112.56474304199219,
      "learning_rate": 0.00012810666666666665,
      "loss": -117.9003,
      "step": 26960
    },
    {
      "epoch": 2.1576,
      "grad_norm": 68.23170471191406,
      "learning_rate": 0.00012808,
      "loss": -117.2408,
      "step": 26970
    },
    {
      "epoch": 2.1584,
      "grad_norm": 83.85253143310547,
      "learning_rate": 0.00012805333333333334,
      "loss": -118.6895,
      "step": 26980
    },
    {
      "epoch": 2.1592000000000002,
      "grad_norm": 136.57830810546875,
      "learning_rate": 0.00012802666666666667,
      "loss": -117.7417,
      "step": 26990
    },
    {
      "epoch": 2.16,
      "grad_norm": 84.722412109375,
      "learning_rate": 0.00012800000000000002,
      "loss": -118.4595,
      "step": 27000
    },
    {
      "epoch": 2.1608,
      "grad_norm": 67.97477722167969,
      "learning_rate": 0.00012797333333333335,
      "loss": -118.9624,
      "step": 27010
    },
    {
      "epoch": 2.1616,
      "grad_norm": 73.56653594970703,
      "learning_rate": 0.00012794666666666668,
      "loss": -118.1646,
      "step": 27020
    },
    {
      "epoch": 2.1624,
      "grad_norm": 72.23886108398438,
      "learning_rate": 0.00012792,
      "loss": -118.5871,
      "step": 27030
    },
    {
      "epoch": 2.1632,
      "grad_norm": 111.1138687133789,
      "learning_rate": 0.00012789333333333334,
      "loss": -118.8765,
      "step": 27040
    },
    {
      "epoch": 2.164,
      "grad_norm": 93.9005126953125,
      "learning_rate": 0.00012786666666666667,
      "loss": -120.0345,
      "step": 27050
    },
    {
      "epoch": 2.1648,
      "grad_norm": 98.82412719726562,
      "learning_rate": 0.00012784,
      "loss": -118.9138,
      "step": 27060
    },
    {
      "epoch": 2.1656,
      "grad_norm": 107.85150909423828,
      "learning_rate": 0.00012781333333333335,
      "loss": -117.7048,
      "step": 27070
    },
    {
      "epoch": 2.1664,
      "grad_norm": 142.10682678222656,
      "learning_rate": 0.00012778666666666668,
      "loss": -118.1214,
      "step": 27080
    },
    {
      "epoch": 2.1672,
      "grad_norm": 90.44872283935547,
      "learning_rate": 0.00012776,
      "loss": -118.1935,
      "step": 27090
    },
    {
      "epoch": 2.168,
      "grad_norm": 64.9609375,
      "learning_rate": 0.00012773333333333334,
      "loss": -117.1683,
      "step": 27100
    },
    {
      "epoch": 2.1688,
      "grad_norm": 90.18330383300781,
      "learning_rate": 0.00012770666666666667,
      "loss": -118.281,
      "step": 27110
    },
    {
      "epoch": 2.1696,
      "grad_norm": 92.91368865966797,
      "learning_rate": 0.00012768,
      "loss": -117.7666,
      "step": 27120
    },
    {
      "epoch": 2.1704,
      "grad_norm": 117.52896118164062,
      "learning_rate": 0.00012765333333333333,
      "loss": -117.6842,
      "step": 27130
    },
    {
      "epoch": 2.1712,
      "grad_norm": 75.81399536132812,
      "learning_rate": 0.00012762666666666668,
      "loss": -117.718,
      "step": 27140
    },
    {
      "epoch": 2.172,
      "grad_norm": 64.78254699707031,
      "learning_rate": 0.0001276,
      "loss": -117.4621,
      "step": 27150
    },
    {
      "epoch": 2.1728,
      "grad_norm": 98.10847473144531,
      "learning_rate": 0.00012757333333333334,
      "loss": -118.8183,
      "step": 27160
    },
    {
      "epoch": 2.1736,
      "grad_norm": 284.9759826660156,
      "learning_rate": 0.00012754666666666667,
      "loss": -117.8782,
      "step": 27170
    },
    {
      "epoch": 2.1744,
      "grad_norm": 70.83503723144531,
      "learning_rate": 0.00012752,
      "loss": -117.9965,
      "step": 27180
    },
    {
      "epoch": 2.1752,
      "grad_norm": 166.08453369140625,
      "learning_rate": 0.00012749333333333333,
      "loss": -117.0964,
      "step": 27190
    },
    {
      "epoch": 2.176,
      "grad_norm": 118.49510192871094,
      "learning_rate": 0.00012746666666666666,
      "loss": -118.02,
      "step": 27200
    },
    {
      "epoch": 2.1768,
      "grad_norm": 88.40169525146484,
      "learning_rate": 0.00012744,
      "loss": -119.6474,
      "step": 27210
    },
    {
      "epoch": 2.1776,
      "grad_norm": 77.13116455078125,
      "learning_rate": 0.00012741333333333334,
      "loss": -118.1287,
      "step": 27220
    },
    {
      "epoch": 2.1784,
      "grad_norm": 232.6377410888672,
      "learning_rate": 0.00012738666666666667,
      "loss": -118.3815,
      "step": 27230
    },
    {
      "epoch": 2.1792,
      "grad_norm": 123.02808380126953,
      "learning_rate": 0.00012736,
      "loss": -118.0006,
      "step": 27240
    },
    {
      "epoch": 2.18,
      "grad_norm": 100.00346374511719,
      "learning_rate": 0.00012733333333333336,
      "loss": -118.509,
      "step": 27250
    },
    {
      "epoch": 2.1808,
      "grad_norm": 72.31608581542969,
      "learning_rate": 0.00012730666666666666,
      "loss": -118.8469,
      "step": 27260
    },
    {
      "epoch": 2.1816,
      "grad_norm": 88.08342742919922,
      "learning_rate": 0.00012728,
      "loss": -118.2752,
      "step": 27270
    },
    {
      "epoch": 2.1824,
      "grad_norm": 98.15245056152344,
      "learning_rate": 0.00012725333333333334,
      "loss": -116.0563,
      "step": 27280
    },
    {
      "epoch": 2.1832,
      "grad_norm": 156.24420166015625,
      "learning_rate": 0.00012722666666666667,
      "loss": -118.7895,
      "step": 27290
    },
    {
      "epoch": 2.184,
      "grad_norm": 60.747337341308594,
      "learning_rate": 0.0001272,
      "loss": -115.6335,
      "step": 27300
    },
    {
      "epoch": 2.1848,
      "grad_norm": 126.6981201171875,
      "learning_rate": 0.00012717333333333333,
      "loss": -119.4623,
      "step": 27310
    },
    {
      "epoch": 2.1856,
      "grad_norm": 180.5953826904297,
      "learning_rate": 0.0001271466666666667,
      "loss": -119.7983,
      "step": 27320
    },
    {
      "epoch": 2.1864,
      "grad_norm": 103.37518310546875,
      "learning_rate": 0.00012712000000000002,
      "loss": -117.5579,
      "step": 27330
    },
    {
      "epoch": 2.1872,
      "grad_norm": 115.28675079345703,
      "learning_rate": 0.00012709333333333332,
      "loss": -116.004,
      "step": 27340
    },
    {
      "epoch": 2.188,
      "grad_norm": 97.55614471435547,
      "learning_rate": 0.00012706666666666667,
      "loss": -118.0803,
      "step": 27350
    },
    {
      "epoch": 2.1888,
      "grad_norm": 72.06694030761719,
      "learning_rate": 0.00012704,
      "loss": -118.7239,
      "step": 27360
    },
    {
      "epoch": 2.1896,
      "grad_norm": 113.61669921875,
      "learning_rate": 0.00012701333333333333,
      "loss": -118.1586,
      "step": 27370
    },
    {
      "epoch": 2.1904,
      "grad_norm": 80.09246826171875,
      "learning_rate": 0.0001269866666666667,
      "loss": -118.6452,
      "step": 27380
    },
    {
      "epoch": 2.1912,
      "grad_norm": 66.50984191894531,
      "learning_rate": 0.00012696000000000002,
      "loss": -117.985,
      "step": 27390
    },
    {
      "epoch": 2.192,
      "grad_norm": 108.96724700927734,
      "learning_rate": 0.00012693333333333335,
      "loss": -118.1215,
      "step": 27400
    },
    {
      "epoch": 2.1928,
      "grad_norm": 151.30308532714844,
      "learning_rate": 0.00012690666666666668,
      "loss": -117.6702,
      "step": 27410
    },
    {
      "epoch": 2.1936,
      "grad_norm": 101.81623077392578,
      "learning_rate": 0.00012688,
      "loss": -116.9521,
      "step": 27420
    },
    {
      "epoch": 2.1944,
      "grad_norm": 89.71809387207031,
      "learning_rate": 0.00012685333333333333,
      "loss": -119.4268,
      "step": 27430
    },
    {
      "epoch": 2.1952,
      "grad_norm": 130.8458709716797,
      "learning_rate": 0.00012682666666666666,
      "loss": -117.3331,
      "step": 27440
    },
    {
      "epoch": 2.196,
      "grad_norm": 103.59590148925781,
      "learning_rate": 0.00012680000000000002,
      "loss": -118.4799,
      "step": 27450
    },
    {
      "epoch": 2.1968,
      "grad_norm": 112.43901062011719,
      "learning_rate": 0.00012677333333333335,
      "loss": -119.0074,
      "step": 27460
    },
    {
      "epoch": 2.1976,
      "grad_norm": 73.25749206542969,
      "learning_rate": 0.00012674666666666668,
      "loss": -118.8397,
      "step": 27470
    },
    {
      "epoch": 2.1984,
      "grad_norm": 76.94786834716797,
      "learning_rate": 0.00012672,
      "loss": -118.9557,
      "step": 27480
    },
    {
      "epoch": 2.1992,
      "grad_norm": 95.3871841430664,
      "learning_rate": 0.00012669333333333334,
      "loss": -119.8808,
      "step": 27490
    },
    {
      "epoch": 2.2,
      "grad_norm": 75.16353607177734,
      "learning_rate": 0.00012666666666666666,
      "loss": -118.2655,
      "step": 27500
    },
    {
      "epoch": 2.2008,
      "grad_norm": 98.65262603759766,
      "learning_rate": 0.00012664,
      "loss": -118.6242,
      "step": 27510
    },
    {
      "epoch": 2.2016,
      "grad_norm": 113.51773834228516,
      "learning_rate": 0.00012661333333333335,
      "loss": -118.5072,
      "step": 27520
    },
    {
      "epoch": 2.2024,
      "grad_norm": 83.80751037597656,
      "learning_rate": 0.00012658666666666668,
      "loss": -117.6176,
      "step": 27530
    },
    {
      "epoch": 2.2032,
      "grad_norm": 75.38849639892578,
      "learning_rate": 0.00012656,
      "loss": -118.5223,
      "step": 27540
    },
    {
      "epoch": 2.204,
      "grad_norm": 89.51189422607422,
      "learning_rate": 0.00012653333333333334,
      "loss": -117.6139,
      "step": 27550
    },
    {
      "epoch": 2.2048,
      "grad_norm": 83.04005432128906,
      "learning_rate": 0.00012650666666666667,
      "loss": -118.8159,
      "step": 27560
    },
    {
      "epoch": 2.2056,
      "grad_norm": 87.96343231201172,
      "learning_rate": 0.00012648,
      "loss": -118.2901,
      "step": 27570
    },
    {
      "epoch": 2.2064,
      "grad_norm": 109.05625915527344,
      "learning_rate": 0.00012645333333333332,
      "loss": -118.6462,
      "step": 27580
    },
    {
      "epoch": 2.2072,
      "grad_norm": 75.85649108886719,
      "learning_rate": 0.00012642666666666668,
      "loss": -118.1679,
      "step": 27590
    },
    {
      "epoch": 2.208,
      "grad_norm": 122.38715362548828,
      "learning_rate": 0.0001264,
      "loss": -118.2074,
      "step": 27600
    },
    {
      "epoch": 2.2088,
      "grad_norm": 72.36248016357422,
      "learning_rate": 0.00012637333333333334,
      "loss": -118.6285,
      "step": 27610
    },
    {
      "epoch": 2.2096,
      "grad_norm": 104.14201354980469,
      "learning_rate": 0.00012634666666666667,
      "loss": -117.6952,
      "step": 27620
    },
    {
      "epoch": 2.2104,
      "grad_norm": 135.6602020263672,
      "learning_rate": 0.00012632000000000002,
      "loss": -117.3827,
      "step": 27630
    },
    {
      "epoch": 2.2112,
      "grad_norm": 64.22212219238281,
      "learning_rate": 0.00012629333333333333,
      "loss": -118.5022,
      "step": 27640
    },
    {
      "epoch": 2.212,
      "grad_norm": 57.78990936279297,
      "learning_rate": 0.00012626666666666665,
      "loss": -118.633,
      "step": 27650
    },
    {
      "epoch": 2.2128,
      "grad_norm": 139.4123992919922,
      "learning_rate": 0.00012624,
      "loss": -117.2316,
      "step": 27660
    },
    {
      "epoch": 2.2136,
      "grad_norm": 193.4602508544922,
      "learning_rate": 0.00012621333333333334,
      "loss": -118.2387,
      "step": 27670
    },
    {
      "epoch": 2.2144,
      "grad_norm": 196.3419952392578,
      "learning_rate": 0.00012618666666666667,
      "loss": -117.1971,
      "step": 27680
    },
    {
      "epoch": 2.2152,
      "grad_norm": 178.67303466796875,
      "learning_rate": 0.00012616,
      "loss": -117.7899,
      "step": 27690
    },
    {
      "epoch": 2.216,
      "grad_norm": 193.13572692871094,
      "learning_rate": 0.00012613333333333335,
      "loss": -117.4336,
      "step": 27700
    },
    {
      "epoch": 2.2168,
      "grad_norm": 70.00818634033203,
      "learning_rate": 0.00012610666666666666,
      "loss": -117.5211,
      "step": 27710
    },
    {
      "epoch": 2.2176,
      "grad_norm": 118.14261627197266,
      "learning_rate": 0.00012607999999999999,
      "loss": -118.1329,
      "step": 27720
    },
    {
      "epoch": 2.2184,
      "grad_norm": 51.441097259521484,
      "learning_rate": 0.00012605333333333334,
      "loss": -118.4621,
      "step": 27730
    },
    {
      "epoch": 2.2192,
      "grad_norm": 143.0526123046875,
      "learning_rate": 0.00012602666666666667,
      "loss": -118.7621,
      "step": 27740
    },
    {
      "epoch": 2.22,
      "grad_norm": 96.9173583984375,
      "learning_rate": 0.000126,
      "loss": -118.4668,
      "step": 27750
    },
    {
      "epoch": 2.2208,
      "grad_norm": 122.8900146484375,
      "learning_rate": 0.00012597333333333336,
      "loss": -118.6822,
      "step": 27760
    },
    {
      "epoch": 2.2216,
      "grad_norm": 150.66261291503906,
      "learning_rate": 0.00012594666666666669,
      "loss": -117.2389,
      "step": 27770
    },
    {
      "epoch": 2.2224,
      "grad_norm": 104.60474395751953,
      "learning_rate": 0.00012592000000000001,
      "loss": -118.1344,
      "step": 27780
    },
    {
      "epoch": 2.2232,
      "grad_norm": 123.82437133789062,
      "learning_rate": 0.00012589333333333334,
      "loss": -118.8418,
      "step": 27790
    },
    {
      "epoch": 2.224,
      "grad_norm": 108.76968383789062,
      "learning_rate": 0.00012586666666666667,
      "loss": -118.4191,
      "step": 27800
    },
    {
      "epoch": 2.2248,
      "grad_norm": 102.55032348632812,
      "learning_rate": 0.00012584,
      "loss": -117.9249,
      "step": 27810
    },
    {
      "epoch": 2.2256,
      "grad_norm": 88.40697479248047,
      "learning_rate": 0.00012581333333333333,
      "loss": -117.1489,
      "step": 27820
    },
    {
      "epoch": 2.2264,
      "grad_norm": 117.14844512939453,
      "learning_rate": 0.0001257866666666667,
      "loss": -118.2848,
      "step": 27830
    },
    {
      "epoch": 2.2272,
      "grad_norm": 245.0868377685547,
      "learning_rate": 0.00012576000000000002,
      "loss": -117.9073,
      "step": 27840
    },
    {
      "epoch": 2.228,
      "grad_norm": 92.0250015258789,
      "learning_rate": 0.00012573333333333334,
      "loss": -118.3969,
      "step": 27850
    },
    {
      "epoch": 2.2288,
      "grad_norm": 188.07302856445312,
      "learning_rate": 0.00012570666666666667,
      "loss": -117.2294,
      "step": 27860
    },
    {
      "epoch": 2.2296,
      "grad_norm": 168.21597290039062,
      "learning_rate": 0.00012568,
      "loss": -118.3065,
      "step": 27870
    },
    {
      "epoch": 2.2304,
      "grad_norm": 132.85939025878906,
      "learning_rate": 0.00012565333333333333,
      "loss": -117.9112,
      "step": 27880
    },
    {
      "epoch": 2.2312,
      "grad_norm": 86.40807342529297,
      "learning_rate": 0.00012562666666666666,
      "loss": -118.3596,
      "step": 27890
    },
    {
      "epoch": 2.232,
      "grad_norm": 109.10916900634766,
      "learning_rate": 0.00012560000000000002,
      "loss": -117.9338,
      "step": 27900
    },
    {
      "epoch": 2.2328,
      "grad_norm": 122.32319641113281,
      "learning_rate": 0.00012557333333333335,
      "loss": -117.6374,
      "step": 27910
    },
    {
      "epoch": 2.2336,
      "grad_norm": 101.38660430908203,
      "learning_rate": 0.00012554666666666668,
      "loss": -118.3867,
      "step": 27920
    },
    {
      "epoch": 2.2344,
      "grad_norm": 64.14376068115234,
      "learning_rate": 0.00012552,
      "loss": -117.0469,
      "step": 27930
    },
    {
      "epoch": 2.2352,
      "grad_norm": 111.2819595336914,
      "learning_rate": 0.00012549333333333333,
      "loss": -119.1014,
      "step": 27940
    },
    {
      "epoch": 2.2359999999999998,
      "grad_norm": 113.84172821044922,
      "learning_rate": 0.00012546666666666666,
      "loss": -117.6301,
      "step": 27950
    },
    {
      "epoch": 2.2368,
      "grad_norm": 123.7387466430664,
      "learning_rate": 0.00012544,
      "loss": -118.0485,
      "step": 27960
    },
    {
      "epoch": 2.2376,
      "grad_norm": 94.37022399902344,
      "learning_rate": 0.00012541333333333335,
      "loss": -116.9393,
      "step": 27970
    },
    {
      "epoch": 2.2384,
      "grad_norm": 91.97169494628906,
      "learning_rate": 0.00012538666666666668,
      "loss": -117.5266,
      "step": 27980
    },
    {
      "epoch": 2.2392,
      "grad_norm": 102.50453186035156,
      "learning_rate": 0.00012536,
      "loss": -117.5538,
      "step": 27990
    },
    {
      "epoch": 2.24,
      "grad_norm": 125.87114715576172,
      "learning_rate": 0.00012533333333333334,
      "loss": -117.0412,
      "step": 28000
    },
    {
      "epoch": 2.2408,
      "grad_norm": 97.15777587890625,
      "learning_rate": 0.0001253066666666667,
      "loss": -118.6963,
      "step": 28010
    },
    {
      "epoch": 2.2416,
      "grad_norm": 249.19308471679688,
      "learning_rate": 0.00012528,
      "loss": -118.917,
      "step": 28020
    },
    {
      "epoch": 2.2424,
      "grad_norm": 123.10993957519531,
      "learning_rate": 0.00012525333333333332,
      "loss": -119.0116,
      "step": 28030
    },
    {
      "epoch": 2.2432,
      "grad_norm": 121.2576675415039,
      "learning_rate": 0.00012522666666666668,
      "loss": -118.8918,
      "step": 28040
    },
    {
      "epoch": 2.2439999999999998,
      "grad_norm": 101.05651092529297,
      "learning_rate": 0.0001252,
      "loss": -118.2097,
      "step": 28050
    },
    {
      "epoch": 2.2448,
      "grad_norm": 138.33087158203125,
      "learning_rate": 0.00012517333333333334,
      "loss": -117.0734,
      "step": 28060
    },
    {
      "epoch": 2.2456,
      "grad_norm": 203.805908203125,
      "learning_rate": 0.00012514666666666667,
      "loss": -118.1682,
      "step": 28070
    },
    {
      "epoch": 2.2464,
      "grad_norm": 96.3591079711914,
      "learning_rate": 0.00012512000000000002,
      "loss": -117.5757,
      "step": 28080
    },
    {
      "epoch": 2.2472,
      "grad_norm": 91.38610076904297,
      "learning_rate": 0.00012509333333333332,
      "loss": -118.8111,
      "step": 28090
    },
    {
      "epoch": 2.248,
      "grad_norm": 115.91426086425781,
      "learning_rate": 0.00012506666666666665,
      "loss": -118.3279,
      "step": 28100
    },
    {
      "epoch": 2.2488,
      "grad_norm": 86.40065002441406,
      "learning_rate": 0.00012504,
      "loss": -117.9356,
      "step": 28110
    },
    {
      "epoch": 2.2496,
      "grad_norm": 101.10360717773438,
      "learning_rate": 0.00012501333333333334,
      "loss": -119.0057,
      "step": 28120
    },
    {
      "epoch": 2.2504,
      "grad_norm": 89.5708999633789,
      "learning_rate": 0.00012498666666666667,
      "loss": -117.1519,
      "step": 28130
    },
    {
      "epoch": 2.2512,
      "grad_norm": 161.1297607421875,
      "learning_rate": 0.00012496000000000002,
      "loss": -118.0822,
      "step": 28140
    },
    {
      "epoch": 2.252,
      "grad_norm": 98.46571350097656,
      "learning_rate": 0.00012493333333333335,
      "loss": -118.3655,
      "step": 28150
    },
    {
      "epoch": 2.2528,
      "grad_norm": 119.22724151611328,
      "learning_rate": 0.00012490666666666668,
      "loss": -118.3379,
      "step": 28160
    },
    {
      "epoch": 2.2536,
      "grad_norm": 95.37788391113281,
      "learning_rate": 0.00012488,
      "loss": -117.6499,
      "step": 28170
    },
    {
      "epoch": 2.2544,
      "grad_norm": 94.33460998535156,
      "learning_rate": 0.00012485333333333334,
      "loss": -119.1272,
      "step": 28180
    },
    {
      "epoch": 2.2552,
      "grad_norm": 49.42594909667969,
      "learning_rate": 0.00012482666666666667,
      "loss": -119.0399,
      "step": 28190
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 97.7533187866211,
      "learning_rate": 0.0001248,
      "loss": -118.0097,
      "step": 28200
    },
    {
      "epoch": 2.2568,
      "grad_norm": 104.7216567993164,
      "learning_rate": 0.00012477333333333335,
      "loss": -118.3655,
      "step": 28210
    },
    {
      "epoch": 2.2576,
      "grad_norm": 156.78530883789062,
      "learning_rate": 0.00012474666666666668,
      "loss": -118.0823,
      "step": 28220
    },
    {
      "epoch": 2.2584,
      "grad_norm": 74.60966491699219,
      "learning_rate": 0.00012472,
      "loss": -119.3251,
      "step": 28230
    },
    {
      "epoch": 2.2592,
      "grad_norm": 98.10820770263672,
      "learning_rate": 0.00012469333333333334,
      "loss": -118.5003,
      "step": 28240
    },
    {
      "epoch": 2.26,
      "grad_norm": 106.53456115722656,
      "learning_rate": 0.00012466666666666667,
      "loss": -118.0718,
      "step": 28250
    },
    {
      "epoch": 2.2608,
      "grad_norm": 98.84359741210938,
      "learning_rate": 0.00012464,
      "loss": -117.6835,
      "step": 28260
    },
    {
      "epoch": 2.2616,
      "grad_norm": 134.6191864013672,
      "learning_rate": 0.00012461333333333333,
      "loss": -118.7637,
      "step": 28270
    },
    {
      "epoch": 2.2624,
      "grad_norm": 113.1009521484375,
      "learning_rate": 0.00012458666666666668,
      "loss": -118.2158,
      "step": 28280
    },
    {
      "epoch": 2.2632,
      "grad_norm": 95.90684509277344,
      "learning_rate": 0.00012456000000000001,
      "loss": -118.3733,
      "step": 28290
    },
    {
      "epoch": 2.2640000000000002,
      "grad_norm": 45.04378128051758,
      "learning_rate": 0.00012453333333333334,
      "loss": -117.8629,
      "step": 28300
    },
    {
      "epoch": 2.2648,
      "grad_norm": 121.4488296508789,
      "learning_rate": 0.00012450666666666667,
      "loss": -117.687,
      "step": 28310
    },
    {
      "epoch": 2.2656,
      "grad_norm": 82.19888305664062,
      "learning_rate": 0.00012448,
      "loss": -118.1057,
      "step": 28320
    },
    {
      "epoch": 2.2664,
      "grad_norm": 70.42913818359375,
      "learning_rate": 0.00012445333333333333,
      "loss": -118.7487,
      "step": 28330
    },
    {
      "epoch": 2.2672,
      "grad_norm": 135.13641357421875,
      "learning_rate": 0.00012442666666666666,
      "loss": -117.8059,
      "step": 28340
    },
    {
      "epoch": 2.268,
      "grad_norm": 131.74436950683594,
      "learning_rate": 0.00012440000000000002,
      "loss": -118.8052,
      "step": 28350
    },
    {
      "epoch": 2.2688,
      "grad_norm": 75.61400604248047,
      "learning_rate": 0.00012437333333333334,
      "loss": -117.4487,
      "step": 28360
    },
    {
      "epoch": 2.2696,
      "grad_norm": 111.23505401611328,
      "learning_rate": 0.00012434666666666667,
      "loss": -118.3405,
      "step": 28370
    },
    {
      "epoch": 2.2704,
      "grad_norm": 162.801513671875,
      "learning_rate": 0.00012432,
      "loss": -118.7928,
      "step": 28380
    },
    {
      "epoch": 2.2712,
      "grad_norm": 71.60926055908203,
      "learning_rate": 0.00012429333333333333,
      "loss": -118.2838,
      "step": 28390
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 135.8428955078125,
      "learning_rate": 0.00012426666666666666,
      "loss": -118.7288,
      "step": 28400
    },
    {
      "epoch": 2.2728,
      "grad_norm": 139.0768280029297,
      "learning_rate": 0.00012424,
      "loss": -118.0524,
      "step": 28410
    },
    {
      "epoch": 2.2736,
      "grad_norm": 132.85092163085938,
      "learning_rate": 0.00012421333333333335,
      "loss": -118.9152,
      "step": 28420
    },
    {
      "epoch": 2.2744,
      "grad_norm": 137.57696533203125,
      "learning_rate": 0.00012418666666666667,
      "loss": -118.6458,
      "step": 28430
    },
    {
      "epoch": 2.2752,
      "grad_norm": 82.74612426757812,
      "learning_rate": 0.00012416,
      "loss": -118.1665,
      "step": 28440
    },
    {
      "epoch": 2.276,
      "grad_norm": 110.7293930053711,
      "learning_rate": 0.00012413333333333333,
      "loss": -117.9898,
      "step": 28450
    },
    {
      "epoch": 2.2768,
      "grad_norm": 102.34049224853516,
      "learning_rate": 0.0001241066666666667,
      "loss": -117.3144,
      "step": 28460
    },
    {
      "epoch": 2.2776,
      "grad_norm": 93.84770202636719,
      "learning_rate": 0.00012408,
      "loss": -118.222,
      "step": 28470
    },
    {
      "epoch": 2.2784,
      "grad_norm": 100.06925201416016,
      "learning_rate": 0.00012405333333333332,
      "loss": -118.3675,
      "step": 28480
    },
    {
      "epoch": 2.2792,
      "grad_norm": 93.33308410644531,
      "learning_rate": 0.00012402666666666668,
      "loss": -119.3294,
      "step": 28490
    },
    {
      "epoch": 2.2800000000000002,
      "grad_norm": 79.5688705444336,
      "learning_rate": 0.000124,
      "loss": -117.9853,
      "step": 28500
    },
    {
      "epoch": 2.2808,
      "grad_norm": 56.87533950805664,
      "learning_rate": 0.00012397333333333333,
      "loss": -118.7449,
      "step": 28510
    },
    {
      "epoch": 2.2816,
      "grad_norm": 81.2832260131836,
      "learning_rate": 0.0001239466666666667,
      "loss": -119.0965,
      "step": 28520
    },
    {
      "epoch": 2.2824,
      "grad_norm": 84.30413055419922,
      "learning_rate": 0.00012392000000000002,
      "loss": -118.1398,
      "step": 28530
    },
    {
      "epoch": 2.2832,
      "grad_norm": 187.80560302734375,
      "learning_rate": 0.00012389333333333335,
      "loss": -117.4802,
      "step": 28540
    },
    {
      "epoch": 2.284,
      "grad_norm": 184.5175018310547,
      "learning_rate": 0.00012386666666666665,
      "loss": -118.4922,
      "step": 28550
    },
    {
      "epoch": 2.2848,
      "grad_norm": 143.43540954589844,
      "learning_rate": 0.00012384,
      "loss": -118.3213,
      "step": 28560
    },
    {
      "epoch": 2.2856,
      "grad_norm": 136.7332000732422,
      "learning_rate": 0.00012381333333333334,
      "loss": -118.7155,
      "step": 28570
    },
    {
      "epoch": 2.2864,
      "grad_norm": 113.89579772949219,
      "learning_rate": 0.00012378666666666667,
      "loss": -117.8931,
      "step": 28580
    },
    {
      "epoch": 2.2872,
      "grad_norm": 97.16645812988281,
      "learning_rate": 0.00012376000000000002,
      "loss": -119.349,
      "step": 28590
    },
    {
      "epoch": 2.288,
      "grad_norm": 77.1113052368164,
      "learning_rate": 0.00012373333333333335,
      "loss": -117.3267,
      "step": 28600
    },
    {
      "epoch": 2.2888,
      "grad_norm": 139.75796508789062,
      "learning_rate": 0.00012370666666666668,
      "loss": -118.6066,
      "step": 28610
    },
    {
      "epoch": 2.2896,
      "grad_norm": 98.16402435302734,
      "learning_rate": 0.00012368,
      "loss": -118.1031,
      "step": 28620
    },
    {
      "epoch": 2.2904,
      "grad_norm": 126.9868392944336,
      "learning_rate": 0.00012365333333333334,
      "loss": -117.7236,
      "step": 28630
    },
    {
      "epoch": 2.2912,
      "grad_norm": 85.16913604736328,
      "learning_rate": 0.00012362666666666667,
      "loss": -118.0558,
      "step": 28640
    },
    {
      "epoch": 2.292,
      "grad_norm": 214.47998046875,
      "learning_rate": 0.0001236,
      "loss": -120.0671,
      "step": 28650
    },
    {
      "epoch": 2.2928,
      "grad_norm": 88.06082916259766,
      "learning_rate": 0.00012357333333333335,
      "loss": -118.8492,
      "step": 28660
    },
    {
      "epoch": 2.2936,
      "grad_norm": 99.38301086425781,
      "learning_rate": 0.00012354666666666668,
      "loss": -117.5013,
      "step": 28670
    },
    {
      "epoch": 2.2944,
      "grad_norm": 104.52302551269531,
      "learning_rate": 0.00012352,
      "loss": -118.3794,
      "step": 28680
    },
    {
      "epoch": 2.2952,
      "grad_norm": 108.01099395751953,
      "learning_rate": 0.00012349333333333334,
      "loss": -119.1078,
      "step": 28690
    },
    {
      "epoch": 2.296,
      "grad_norm": 54.56790542602539,
      "learning_rate": 0.00012346666666666667,
      "loss": -118.3787,
      "step": 28700
    },
    {
      "epoch": 2.2968,
      "grad_norm": 87.24987030029297,
      "learning_rate": 0.00012344,
      "loss": -117.5683,
      "step": 28710
    },
    {
      "epoch": 2.2976,
      "grad_norm": 103.85446166992188,
      "learning_rate": 0.00012341333333333333,
      "loss": -117.5327,
      "step": 28720
    },
    {
      "epoch": 2.2984,
      "grad_norm": 121.16923522949219,
      "learning_rate": 0.00012338666666666668,
      "loss": -118.9661,
      "step": 28730
    },
    {
      "epoch": 2.2992,
      "grad_norm": 82.23512268066406,
      "learning_rate": 0.00012336,
      "loss": -119.0144,
      "step": 28740
    },
    {
      "epoch": 2.3,
      "grad_norm": 58.8128776550293,
      "learning_rate": 0.00012333333333333334,
      "loss": -118.2481,
      "step": 28750
    },
    {
      "epoch": 2.3008,
      "grad_norm": 134.67294311523438,
      "learning_rate": 0.00012330666666666667,
      "loss": -117.548,
      "step": 28760
    },
    {
      "epoch": 2.3016,
      "grad_norm": 138.21449279785156,
      "learning_rate": 0.00012328,
      "loss": -117.2899,
      "step": 28770
    },
    {
      "epoch": 2.3024,
      "grad_norm": 105.35401916503906,
      "learning_rate": 0.00012325333333333333,
      "loss": -117.9261,
      "step": 28780
    },
    {
      "epoch": 2.3032,
      "grad_norm": 147.78428649902344,
      "learning_rate": 0.00012322666666666666,
      "loss": -118.8154,
      "step": 28790
    },
    {
      "epoch": 2.304,
      "grad_norm": 117.5743637084961,
      "learning_rate": 0.0001232,
      "loss": -117.4193,
      "step": 28800
    },
    {
      "epoch": 2.3048,
      "grad_norm": 92.34632110595703,
      "learning_rate": 0.00012317333333333334,
      "loss": -116.9266,
      "step": 28810
    },
    {
      "epoch": 2.3056,
      "grad_norm": 95.85973358154297,
      "learning_rate": 0.00012314666666666667,
      "loss": -118.1495,
      "step": 28820
    },
    {
      "epoch": 2.3064,
      "grad_norm": 85.46466064453125,
      "learning_rate": 0.00012312,
      "loss": -118.5359,
      "step": 28830
    },
    {
      "epoch": 2.3072,
      "grad_norm": 97.27700805664062,
      "learning_rate": 0.00012309333333333336,
      "loss": -118.4415,
      "step": 28840
    },
    {
      "epoch": 2.308,
      "grad_norm": 69.9666519165039,
      "learning_rate": 0.00012306666666666666,
      "loss": -117.6663,
      "step": 28850
    },
    {
      "epoch": 2.3088,
      "grad_norm": 98.46455383300781,
      "learning_rate": 0.00012304,
      "loss": -118.2497,
      "step": 28860
    },
    {
      "epoch": 2.3096,
      "grad_norm": 138.84669494628906,
      "learning_rate": 0.00012301333333333334,
      "loss": -117.1375,
      "step": 28870
    },
    {
      "epoch": 2.3104,
      "grad_norm": 98.15851593017578,
      "learning_rate": 0.00012298666666666667,
      "loss": -118.9274,
      "step": 28880
    },
    {
      "epoch": 2.3112,
      "grad_norm": 87.07219696044922,
      "learning_rate": 0.00012296,
      "loss": -118.3834,
      "step": 28890
    },
    {
      "epoch": 2.312,
      "grad_norm": 71.0914306640625,
      "learning_rate": 0.00012293333333333336,
      "loss": -118.7111,
      "step": 28900
    },
    {
      "epoch": 2.3128,
      "grad_norm": 87.30158233642578,
      "learning_rate": 0.0001229066666666667,
      "loss": -117.8244,
      "step": 28910
    },
    {
      "epoch": 2.3136,
      "grad_norm": 94.48722076416016,
      "learning_rate": 0.00012288,
      "loss": -118.7237,
      "step": 28920
    },
    {
      "epoch": 2.3144,
      "grad_norm": 103.70032501220703,
      "learning_rate": 0.00012285333333333332,
      "loss": -119.0793,
      "step": 28930
    },
    {
      "epoch": 2.3152,
      "grad_norm": 56.823646545410156,
      "learning_rate": 0.00012282666666666667,
      "loss": -119.1238,
      "step": 28940
    },
    {
      "epoch": 2.316,
      "grad_norm": 177.25927734375,
      "learning_rate": 0.0001228,
      "loss": -118.4627,
      "step": 28950
    },
    {
      "epoch": 2.3168,
      "grad_norm": 75.54878997802734,
      "learning_rate": 0.00012277333333333333,
      "loss": -118.4422,
      "step": 28960
    },
    {
      "epoch": 2.3176,
      "grad_norm": 158.55172729492188,
      "learning_rate": 0.0001227466666666667,
      "loss": -118.4446,
      "step": 28970
    },
    {
      "epoch": 2.3184,
      "grad_norm": 91.213623046875,
      "learning_rate": 0.00012272000000000002,
      "loss": -118.8234,
      "step": 28980
    },
    {
      "epoch": 2.3192,
      "grad_norm": 111.80045318603516,
      "learning_rate": 0.00012269333333333335,
      "loss": -118.577,
      "step": 28990
    },
    {
      "epoch": 2.32,
      "grad_norm": 124.56631469726562,
      "learning_rate": 0.00012266666666666668,
      "loss": -119.199,
      "step": 29000
    },
    {
      "epoch": 2.3208,
      "grad_norm": 225.5561065673828,
      "learning_rate": 0.00012264,
      "loss": -117.8731,
      "step": 29010
    },
    {
      "epoch": 2.3216,
      "grad_norm": 125.60942840576172,
      "learning_rate": 0.00012261333333333333,
      "loss": -117.7432,
      "step": 29020
    },
    {
      "epoch": 2.3224,
      "grad_norm": 74.57764434814453,
      "learning_rate": 0.00012258666666666666,
      "loss": -116.9192,
      "step": 29030
    },
    {
      "epoch": 2.3232,
      "grad_norm": 96.42951965332031,
      "learning_rate": 0.00012256000000000002,
      "loss": -117.5985,
      "step": 29040
    },
    {
      "epoch": 2.324,
      "grad_norm": 129.2724609375,
      "learning_rate": 0.00012253333333333335,
      "loss": -118.4073,
      "step": 29050
    },
    {
      "epoch": 2.3247999999999998,
      "grad_norm": 117.62068939208984,
      "learning_rate": 0.00012250666666666668,
      "loss": -118.6163,
      "step": 29060
    },
    {
      "epoch": 2.3256,
      "grad_norm": 124.48380279541016,
      "learning_rate": 0.00012248,
      "loss": -119.3176,
      "step": 29070
    },
    {
      "epoch": 2.3264,
      "grad_norm": 65.1235580444336,
      "learning_rate": 0.00012245333333333334,
      "loss": -118.3636,
      "step": 29080
    },
    {
      "epoch": 2.3272,
      "grad_norm": 138.10107421875,
      "learning_rate": 0.00012242666666666666,
      "loss": -118.9612,
      "step": 29090
    },
    {
      "epoch": 2.328,
      "grad_norm": 160.6376190185547,
      "learning_rate": 0.0001224,
      "loss": -117.7401,
      "step": 29100
    },
    {
      "epoch": 2.3288,
      "grad_norm": 88.57634735107422,
      "learning_rate": 0.00012237333333333335,
      "loss": -118.8327,
      "step": 29110
    },
    {
      "epoch": 2.3296,
      "grad_norm": 193.10690307617188,
      "learning_rate": 0.00012234666666666668,
      "loss": -116.6517,
      "step": 29120
    },
    {
      "epoch": 2.3304,
      "grad_norm": 57.69572448730469,
      "learning_rate": 0.00012232,
      "loss": -117.8143,
      "step": 29130
    },
    {
      "epoch": 2.3312,
      "grad_norm": 110.85166931152344,
      "learning_rate": 0.00012229333333333334,
      "loss": -118.8767,
      "step": 29140
    },
    {
      "epoch": 2.332,
      "grad_norm": 100.90094757080078,
      "learning_rate": 0.00012226666666666667,
      "loss": -118.169,
      "step": 29150
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 77.62089538574219,
      "learning_rate": 0.00012224,
      "loss": -118.3485,
      "step": 29160
    },
    {
      "epoch": 2.3336,
      "grad_norm": 131.40635681152344,
      "learning_rate": 0.00012221333333333332,
      "loss": -117.9942,
      "step": 29170
    },
    {
      "epoch": 2.3344,
      "grad_norm": 112.75199890136719,
      "learning_rate": 0.00012218666666666668,
      "loss": -118.8397,
      "step": 29180
    },
    {
      "epoch": 2.3352,
      "grad_norm": 138.86509704589844,
      "learning_rate": 0.00012216,
      "loss": -118.2872,
      "step": 29190
    },
    {
      "epoch": 2.336,
      "grad_norm": 93.57382202148438,
      "learning_rate": 0.00012213333333333334,
      "loss": -117.8899,
      "step": 29200
    },
    {
      "epoch": 2.3368,
      "grad_norm": 108.86569213867188,
      "learning_rate": 0.00012210666666666667,
      "loss": -119.7015,
      "step": 29210
    },
    {
      "epoch": 2.3376,
      "grad_norm": 83.06369018554688,
      "learning_rate": 0.00012208000000000002,
      "loss": -117.5824,
      "step": 29220
    },
    {
      "epoch": 2.3384,
      "grad_norm": 96.11064910888672,
      "learning_rate": 0.00012205333333333333,
      "loss": -118.0127,
      "step": 29230
    },
    {
      "epoch": 2.3392,
      "grad_norm": 64.7352294921875,
      "learning_rate": 0.00012202666666666667,
      "loss": -117.677,
      "step": 29240
    },
    {
      "epoch": 2.34,
      "grad_norm": 98.14253997802734,
      "learning_rate": 0.000122,
      "loss": -118.13,
      "step": 29250
    },
    {
      "epoch": 2.3407999999999998,
      "grad_norm": 51.83148193359375,
      "learning_rate": 0.00012197333333333334,
      "loss": -118.8965,
      "step": 29260
    },
    {
      "epoch": 2.3416,
      "grad_norm": 118.19429779052734,
      "learning_rate": 0.00012194666666666667,
      "loss": -118.3047,
      "step": 29270
    },
    {
      "epoch": 2.3424,
      "grad_norm": 99.42742919921875,
      "learning_rate": 0.00012192000000000001,
      "loss": -118.4301,
      "step": 29280
    },
    {
      "epoch": 2.3432,
      "grad_norm": 82.7088851928711,
      "learning_rate": 0.00012189333333333335,
      "loss": -119.1015,
      "step": 29290
    },
    {
      "epoch": 2.344,
      "grad_norm": 114.5810317993164,
      "learning_rate": 0.00012186666666666666,
      "loss": -118.6757,
      "step": 29300
    },
    {
      "epoch": 2.3448,
      "grad_norm": 119.8671875,
      "learning_rate": 0.00012184,
      "loss": -119.1674,
      "step": 29310
    },
    {
      "epoch": 2.3456,
      "grad_norm": 54.459659576416016,
      "learning_rate": 0.00012181333333333334,
      "loss": -117.7062,
      "step": 29320
    },
    {
      "epoch": 2.3464,
      "grad_norm": 60.92290115356445,
      "learning_rate": 0.00012178666666666667,
      "loss": -118.1398,
      "step": 29330
    },
    {
      "epoch": 2.3472,
      "grad_norm": 198.33090209960938,
      "learning_rate": 0.00012176000000000001,
      "loss": -118.1665,
      "step": 29340
    },
    {
      "epoch": 2.348,
      "grad_norm": 167.24452209472656,
      "learning_rate": 0.00012173333333333334,
      "loss": -116.4368,
      "step": 29350
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 78.54459381103516,
      "learning_rate": 0.00012170666666666668,
      "loss": -119.168,
      "step": 29360
    },
    {
      "epoch": 2.3496,
      "grad_norm": 133.3719482421875,
      "learning_rate": 0.00012168000000000001,
      "loss": -119.4028,
      "step": 29370
    },
    {
      "epoch": 2.3504,
      "grad_norm": 107.62860107421875,
      "learning_rate": 0.00012165333333333333,
      "loss": -116.5458,
      "step": 29380
    },
    {
      "epoch": 2.3512,
      "grad_norm": 79.73686981201172,
      "learning_rate": 0.00012162666666666667,
      "loss": -117.6507,
      "step": 29390
    },
    {
      "epoch": 2.352,
      "grad_norm": 132.12362670898438,
      "learning_rate": 0.0001216,
      "loss": -117.7616,
      "step": 29400
    },
    {
      "epoch": 2.3528000000000002,
      "grad_norm": 83.00360870361328,
      "learning_rate": 0.00012157333333333334,
      "loss": -117.016,
      "step": 29410
    },
    {
      "epoch": 2.3536,
      "grad_norm": 110.02069091796875,
      "learning_rate": 0.00012154666666666667,
      "loss": -118.5165,
      "step": 29420
    },
    {
      "epoch": 2.3544,
      "grad_norm": 78.72071075439453,
      "learning_rate": 0.00012152000000000002,
      "loss": -117.7004,
      "step": 29430
    },
    {
      "epoch": 2.3552,
      "grad_norm": 102.7467269897461,
      "learning_rate": 0.00012149333333333334,
      "loss": -117.6937,
      "step": 29440
    },
    {
      "epoch": 2.356,
      "grad_norm": 119.35926818847656,
      "learning_rate": 0.00012146666666666666,
      "loss": -119.1216,
      "step": 29450
    },
    {
      "epoch": 2.3568,
      "grad_norm": 73.12837982177734,
      "learning_rate": 0.00012144,
      "loss": -118.96,
      "step": 29460
    },
    {
      "epoch": 2.3576,
      "grad_norm": 97.97498321533203,
      "learning_rate": 0.00012141333333333333,
      "loss": -119.0309,
      "step": 29470
    },
    {
      "epoch": 2.3584,
      "grad_norm": 62.90920639038086,
      "learning_rate": 0.00012138666666666667,
      "loss": -117.3868,
      "step": 29480
    },
    {
      "epoch": 2.3592,
      "grad_norm": 65.27051544189453,
      "learning_rate": 0.00012136,
      "loss": -118.0583,
      "step": 29490
    },
    {
      "epoch": 2.36,
      "grad_norm": 87.70083618164062,
      "learning_rate": 0.00012133333333333335,
      "loss": -117.5454,
      "step": 29500
    },
    {
      "epoch": 2.3608000000000002,
      "grad_norm": 79.73161315917969,
      "learning_rate": 0.00012130666666666668,
      "loss": -118.0355,
      "step": 29510
    },
    {
      "epoch": 2.3616,
      "grad_norm": 73.19963073730469,
      "learning_rate": 0.00012128000000000002,
      "loss": -118.8961,
      "step": 29520
    },
    {
      "epoch": 2.3624,
      "grad_norm": 100.53579711914062,
      "learning_rate": 0.00012125333333333333,
      "loss": -119.3669,
      "step": 29530
    },
    {
      "epoch": 2.3632,
      "grad_norm": 98.09383392333984,
      "learning_rate": 0.00012122666666666666,
      "loss": -117.4284,
      "step": 29540
    },
    {
      "epoch": 2.364,
      "grad_norm": 127.91370391845703,
      "learning_rate": 0.0001212,
      "loss": -118.3328,
      "step": 29550
    },
    {
      "epoch": 2.3648,
      "grad_norm": 108.99182891845703,
      "learning_rate": 0.00012117333333333333,
      "loss": -117.0685,
      "step": 29560
    },
    {
      "epoch": 2.3656,
      "grad_norm": 103.48531341552734,
      "learning_rate": 0.00012114666666666668,
      "loss": -118.7836,
      "step": 29570
    },
    {
      "epoch": 2.3664,
      "grad_norm": 99.2496337890625,
      "learning_rate": 0.00012112,
      "loss": -118.426,
      "step": 29580
    },
    {
      "epoch": 2.3672,
      "grad_norm": 77.86892700195312,
      "learning_rate": 0.00012109333333333335,
      "loss": -119.212,
      "step": 29590
    },
    {
      "epoch": 2.368,
      "grad_norm": 79.10995483398438,
      "learning_rate": 0.00012106666666666666,
      "loss": -119.133,
      "step": 29600
    },
    {
      "epoch": 2.3688000000000002,
      "grad_norm": 68.99404907226562,
      "learning_rate": 0.00012103999999999999,
      "loss": -117.728,
      "step": 29610
    },
    {
      "epoch": 2.3696,
      "grad_norm": 91.2498550415039,
      "learning_rate": 0.00012101333333333334,
      "loss": -117.8335,
      "step": 29620
    },
    {
      "epoch": 2.3704,
      "grad_norm": 130.71133422851562,
      "learning_rate": 0.00012098666666666666,
      "loss": -117.6707,
      "step": 29630
    },
    {
      "epoch": 2.3712,
      "grad_norm": 90.02490997314453,
      "learning_rate": 0.00012096000000000001,
      "loss": -118.5843,
      "step": 29640
    },
    {
      "epoch": 2.372,
      "grad_norm": 81.36107635498047,
      "learning_rate": 0.00012093333333333334,
      "loss": -118.0249,
      "step": 29650
    },
    {
      "epoch": 2.3728,
      "grad_norm": 76.97008514404297,
      "learning_rate": 0.00012090666666666668,
      "loss": -118.3325,
      "step": 29660
    },
    {
      "epoch": 2.3736,
      "grad_norm": 65.1598892211914,
      "learning_rate": 0.00012088000000000002,
      "loss": -117.9903,
      "step": 29670
    },
    {
      "epoch": 2.3744,
      "grad_norm": 98.62158203125,
      "learning_rate": 0.00012085333333333332,
      "loss": -117.6328,
      "step": 29680
    },
    {
      "epoch": 2.3752,
      "grad_norm": 80.82176208496094,
      "learning_rate": 0.00012082666666666667,
      "loss": -117.7209,
      "step": 29690
    },
    {
      "epoch": 2.376,
      "grad_norm": 75.19068908691406,
      "learning_rate": 0.0001208,
      "loss": -117.8807,
      "step": 29700
    },
    {
      "epoch": 2.3768000000000002,
      "grad_norm": 122.45323181152344,
      "learning_rate": 0.00012077333333333334,
      "loss": -117.8844,
      "step": 29710
    },
    {
      "epoch": 2.3776,
      "grad_norm": 73.76390838623047,
      "learning_rate": 0.00012074666666666668,
      "loss": -118.6238,
      "step": 29720
    },
    {
      "epoch": 2.3784,
      "grad_norm": 71.34960174560547,
      "learning_rate": 0.00012072000000000001,
      "loss": -118.1717,
      "step": 29730
    },
    {
      "epoch": 2.3792,
      "grad_norm": 77.76664733886719,
      "learning_rate": 0.00012069333333333335,
      "loss": -119.1876,
      "step": 29740
    },
    {
      "epoch": 2.38,
      "grad_norm": 189.57374572753906,
      "learning_rate": 0.00012066666666666668,
      "loss": -118.8518,
      "step": 29750
    },
    {
      "epoch": 2.3808,
      "grad_norm": 116.78498840332031,
      "learning_rate": 0.00012064,
      "loss": -118.4297,
      "step": 29760
    },
    {
      "epoch": 2.3816,
      "grad_norm": 158.04995727539062,
      "learning_rate": 0.00012061333333333334,
      "loss": -118.6949,
      "step": 29770
    },
    {
      "epoch": 2.3824,
      "grad_norm": 81.75322723388672,
      "learning_rate": 0.00012058666666666667,
      "loss": -118.4742,
      "step": 29780
    },
    {
      "epoch": 2.3832,
      "grad_norm": 103.02178192138672,
      "learning_rate": 0.00012056000000000001,
      "loss": -118.3348,
      "step": 29790
    },
    {
      "epoch": 2.384,
      "grad_norm": 60.745784759521484,
      "learning_rate": 0.00012053333333333334,
      "loss": -118.2273,
      "step": 29800
    },
    {
      "epoch": 2.3848,
      "grad_norm": 110.7608413696289,
      "learning_rate": 0.00012050666666666668,
      "loss": -117.4044,
      "step": 29810
    },
    {
      "epoch": 2.3856,
      "grad_norm": 70.88404846191406,
      "learning_rate": 0.00012048000000000001,
      "loss": -118.1271,
      "step": 29820
    },
    {
      "epoch": 2.3864,
      "grad_norm": 87.5908432006836,
      "learning_rate": 0.00012045333333333333,
      "loss": -118.1045,
      "step": 29830
    },
    {
      "epoch": 2.3872,
      "grad_norm": 112.39907836914062,
      "learning_rate": 0.00012042666666666667,
      "loss": -116.4197,
      "step": 29840
    },
    {
      "epoch": 2.388,
      "grad_norm": 156.94786071777344,
      "learning_rate": 0.0001204,
      "loss": -118.612,
      "step": 29850
    },
    {
      "epoch": 2.3888,
      "grad_norm": 109.05876159667969,
      "learning_rate": 0.00012037333333333334,
      "loss": -118.5822,
      "step": 29860
    },
    {
      "epoch": 2.3896,
      "grad_norm": 91.53813934326172,
      "learning_rate": 0.00012034666666666667,
      "loss": -117.4422,
      "step": 29870
    },
    {
      "epoch": 2.3904,
      "grad_norm": 70.4241714477539,
      "learning_rate": 0.00012032000000000001,
      "loss": -118.0779,
      "step": 29880
    },
    {
      "epoch": 2.3912,
      "grad_norm": 72.45689392089844,
      "learning_rate": 0.00012029333333333334,
      "loss": -118.0043,
      "step": 29890
    },
    {
      "epoch": 2.392,
      "grad_norm": 73.55793762207031,
      "learning_rate": 0.00012026666666666669,
      "loss": -118.018,
      "step": 29900
    },
    {
      "epoch": 2.3928,
      "grad_norm": 129.85964965820312,
      "learning_rate": 0.00012024,
      "loss": -117.2409,
      "step": 29910
    },
    {
      "epoch": 2.3936,
      "grad_norm": 61.04650115966797,
      "learning_rate": 0.00012021333333333333,
      "loss": -118.3039,
      "step": 29920
    },
    {
      "epoch": 2.3944,
      "grad_norm": 92.80034637451172,
      "learning_rate": 0.00012018666666666667,
      "loss": -119.1036,
      "step": 29930
    },
    {
      "epoch": 2.3952,
      "grad_norm": 78.3977279663086,
      "learning_rate": 0.00012016,
      "loss": -118.5895,
      "step": 29940
    },
    {
      "epoch": 2.396,
      "grad_norm": 92.37779998779297,
      "learning_rate": 0.00012013333333333334,
      "loss": -119.3822,
      "step": 29950
    },
    {
      "epoch": 2.3968,
      "grad_norm": 81.32545471191406,
      "learning_rate": 0.00012010666666666667,
      "loss": -118.5678,
      "step": 29960
    },
    {
      "epoch": 2.3976,
      "grad_norm": 86.36103820800781,
      "learning_rate": 0.00012008000000000002,
      "loss": -119.2089,
      "step": 29970
    },
    {
      "epoch": 2.3984,
      "grad_norm": 93.96952819824219,
      "learning_rate": 0.00012005333333333333,
      "loss": -118.0525,
      "step": 29980
    },
    {
      "epoch": 2.3992,
      "grad_norm": 89.25093841552734,
      "learning_rate": 0.00012002666666666666,
      "loss": -119.1861,
      "step": 29990
    },
    {
      "epoch": 2.4,
      "grad_norm": 64.86436462402344,
      "learning_rate": 0.00012,
      "loss": -118.2649,
      "step": 30000
    },
    {
      "epoch": 2.4008,
      "grad_norm": 94.27863311767578,
      "learning_rate": 0.00011997333333333333,
      "loss": -116.8552,
      "step": 30010
    },
    {
      "epoch": 2.4016,
      "grad_norm": 98.90596008300781,
      "learning_rate": 0.00011994666666666667,
      "loss": -117.9057,
      "step": 30020
    },
    {
      "epoch": 2.4024,
      "grad_norm": 106.38314819335938,
      "learning_rate": 0.00011992,
      "loss": -118.2507,
      "step": 30030
    },
    {
      "epoch": 2.4032,
      "grad_norm": 125.27104187011719,
      "learning_rate": 0.00011989333333333335,
      "loss": -117.7629,
      "step": 30040
    },
    {
      "epoch": 2.404,
      "grad_norm": 122.19940948486328,
      "learning_rate": 0.00011986666666666669,
      "loss": -119.7627,
      "step": 30050
    },
    {
      "epoch": 2.4048,
      "grad_norm": 54.662452697753906,
      "learning_rate": 0.00011983999999999999,
      "loss": -118.1603,
      "step": 30060
    },
    {
      "epoch": 2.4056,
      "grad_norm": 88.2204360961914,
      "learning_rate": 0.00011981333333333333,
      "loss": -116.753,
      "step": 30070
    },
    {
      "epoch": 2.4064,
      "grad_norm": 75.02003479003906,
      "learning_rate": 0.00011978666666666666,
      "loss": -118.2572,
      "step": 30080
    },
    {
      "epoch": 2.4072,
      "grad_norm": 88.66596984863281,
      "learning_rate": 0.00011976,
      "loss": -118.9749,
      "step": 30090
    },
    {
      "epoch": 2.408,
      "grad_norm": 163.83509826660156,
      "learning_rate": 0.00011973333333333335,
      "loss": -117.0702,
      "step": 30100
    },
    {
      "epoch": 2.4088,
      "grad_norm": 164.69869995117188,
      "learning_rate": 0.00011970666666666668,
      "loss": -117.6601,
      "step": 30110
    },
    {
      "epoch": 2.4096,
      "grad_norm": 161.43600463867188,
      "learning_rate": 0.00011968000000000002,
      "loss": -118.2183,
      "step": 30120
    },
    {
      "epoch": 2.4104,
      "grad_norm": 93.0848617553711,
      "learning_rate": 0.00011965333333333332,
      "loss": -118.4271,
      "step": 30130
    },
    {
      "epoch": 2.4112,
      "grad_norm": 117.66043090820312,
      "learning_rate": 0.00011962666666666666,
      "loss": -118.2875,
      "step": 30140
    },
    {
      "epoch": 2.412,
      "grad_norm": 71.65947723388672,
      "learning_rate": 0.00011960000000000001,
      "loss": -118.3597,
      "step": 30150
    },
    {
      "epoch": 2.4128,
      "grad_norm": 67.14928436279297,
      "learning_rate": 0.00011957333333333334,
      "loss": -118.0011,
      "step": 30160
    },
    {
      "epoch": 2.4136,
      "grad_norm": 90.83196258544922,
      "learning_rate": 0.00011954666666666668,
      "loss": -116.617,
      "step": 30170
    },
    {
      "epoch": 2.4144,
      "grad_norm": 93.89802551269531,
      "learning_rate": 0.00011952000000000001,
      "loss": -117.8397,
      "step": 30180
    },
    {
      "epoch": 2.4152,
      "grad_norm": 109.57425689697266,
      "learning_rate": 0.00011949333333333335,
      "loss": -118.401,
      "step": 30190
    },
    {
      "epoch": 2.416,
      "grad_norm": 123.37857055664062,
      "learning_rate": 0.00011946666666666668,
      "loss": -118.9094,
      "step": 30200
    },
    {
      "epoch": 2.4168,
      "grad_norm": 91.6387939453125,
      "learning_rate": 0.00011944,
      "loss": -118.055,
      "step": 30210
    },
    {
      "epoch": 2.4176,
      "grad_norm": 98.35074615478516,
      "learning_rate": 0.00011941333333333334,
      "loss": -117.205,
      "step": 30220
    },
    {
      "epoch": 2.4184,
      "grad_norm": 927.9063720703125,
      "learning_rate": 0.00011938666666666667,
      "loss": -118.3349,
      "step": 30230
    },
    {
      "epoch": 2.4192,
      "grad_norm": 76.11962890625,
      "learning_rate": 0.00011936000000000001,
      "loss": -118.5175,
      "step": 30240
    },
    {
      "epoch": 2.42,
      "grad_norm": 104.2814712524414,
      "learning_rate": 0.00011933333333333334,
      "loss": -118.9549,
      "step": 30250
    },
    {
      "epoch": 2.4208,
      "grad_norm": 101.74310302734375,
      "learning_rate": 0.00011930666666666668,
      "loss": -117.166,
      "step": 30260
    },
    {
      "epoch": 2.4215999999999998,
      "grad_norm": 120.3274154663086,
      "learning_rate": 0.00011928000000000001,
      "loss": -117.5632,
      "step": 30270
    },
    {
      "epoch": 2.4224,
      "grad_norm": 105.60169219970703,
      "learning_rate": 0.00011925333333333333,
      "loss": -118.2547,
      "step": 30280
    },
    {
      "epoch": 2.4232,
      "grad_norm": 158.9706268310547,
      "learning_rate": 0.00011922666666666667,
      "loss": -118.6733,
      "step": 30290
    },
    {
      "epoch": 2.424,
      "grad_norm": 347.5235290527344,
      "learning_rate": 0.0001192,
      "loss": -117.8138,
      "step": 30300
    },
    {
      "epoch": 2.4248,
      "grad_norm": 147.25347900390625,
      "learning_rate": 0.00011917333333333334,
      "loss": -118.4341,
      "step": 30310
    },
    {
      "epoch": 2.4256,
      "grad_norm": 90.64575958251953,
      "learning_rate": 0.00011914666666666667,
      "loss": -117.4057,
      "step": 30320
    },
    {
      "epoch": 2.4264,
      "grad_norm": 78.77422332763672,
      "learning_rate": 0.00011912000000000001,
      "loss": -118.4306,
      "step": 30330
    },
    {
      "epoch": 2.4272,
      "grad_norm": 70.7650375366211,
      "learning_rate": 0.00011909333333333334,
      "loss": -116.7866,
      "step": 30340
    },
    {
      "epoch": 2.428,
      "grad_norm": 95.12652587890625,
      "learning_rate": 0.00011906666666666668,
      "loss": -118.4935,
      "step": 30350
    },
    {
      "epoch": 2.4288,
      "grad_norm": 71.44123840332031,
      "learning_rate": 0.00011904,
      "loss": -118.7083,
      "step": 30360
    },
    {
      "epoch": 2.4295999999999998,
      "grad_norm": 135.38136291503906,
      "learning_rate": 0.00011901333333333333,
      "loss": -118.6257,
      "step": 30370
    },
    {
      "epoch": 2.4304,
      "grad_norm": 91.0567398071289,
      "learning_rate": 0.00011898666666666667,
      "loss": -117.7225,
      "step": 30380
    },
    {
      "epoch": 2.4312,
      "grad_norm": 87.92182159423828,
      "learning_rate": 0.00011896,
      "loss": -118.6274,
      "step": 30390
    },
    {
      "epoch": 2.432,
      "grad_norm": 77.2396469116211,
      "learning_rate": 0.00011893333333333334,
      "loss": -118.7217,
      "step": 30400
    },
    {
      "epoch": 2.4328,
      "grad_norm": 83.4489974975586,
      "learning_rate": 0.00011890666666666667,
      "loss": -117.0596,
      "step": 30410
    },
    {
      "epoch": 2.4336,
      "grad_norm": 137.1978302001953,
      "learning_rate": 0.00011888000000000001,
      "loss": -117.7989,
      "step": 30420
    },
    {
      "epoch": 2.4344,
      "grad_norm": 159.5749969482422,
      "learning_rate": 0.00011885333333333336,
      "loss": -118.9826,
      "step": 30430
    },
    {
      "epoch": 2.4352,
      "grad_norm": 174.6746368408203,
      "learning_rate": 0.00011882666666666666,
      "loss": -119.7181,
      "step": 30440
    },
    {
      "epoch": 2.436,
      "grad_norm": 73.4671859741211,
      "learning_rate": 0.0001188,
      "loss": -118.5364,
      "step": 30450
    },
    {
      "epoch": 2.4368,
      "grad_norm": 61.49445343017578,
      "learning_rate": 0.00011877333333333333,
      "loss": -117.8542,
      "step": 30460
    },
    {
      "epoch": 2.4375999999999998,
      "grad_norm": 62.39606475830078,
      "learning_rate": 0.00011874666666666667,
      "loss": -117.0784,
      "step": 30470
    },
    {
      "epoch": 2.4384,
      "grad_norm": 64.71727752685547,
      "learning_rate": 0.00011872000000000002,
      "loss": -117.7464,
      "step": 30480
    },
    {
      "epoch": 2.4392,
      "grad_norm": 82.85914611816406,
      "learning_rate": 0.00011869333333333334,
      "loss": -118.4013,
      "step": 30490
    },
    {
      "epoch": 2.44,
      "grad_norm": 85.7271499633789,
      "learning_rate": 0.00011866666666666669,
      "loss": -119.1179,
      "step": 30500
    },
    {
      "epoch": 2.4408,
      "grad_norm": 97.04850006103516,
      "learning_rate": 0.00011863999999999999,
      "loss": -118.1072,
      "step": 30510
    },
    {
      "epoch": 2.4416,
      "grad_norm": 115.60477447509766,
      "learning_rate": 0.00011861333333333333,
      "loss": -118.7456,
      "step": 30520
    },
    {
      "epoch": 2.4424,
      "grad_norm": 104.5499267578125,
      "learning_rate": 0.00011858666666666667,
      "loss": -117.6165,
      "step": 30530
    },
    {
      "epoch": 2.4432,
      "grad_norm": 82.40838623046875,
      "learning_rate": 0.00011856,
      "loss": -116.7295,
      "step": 30540
    },
    {
      "epoch": 2.444,
      "grad_norm": 84.48461151123047,
      "learning_rate": 0.00011853333333333335,
      "loss": -117.1752,
      "step": 30550
    },
    {
      "epoch": 2.4448,
      "grad_norm": 90.09724426269531,
      "learning_rate": 0.00011850666666666667,
      "loss": -118.8881,
      "step": 30560
    },
    {
      "epoch": 2.4455999999999998,
      "grad_norm": 106.39637756347656,
      "learning_rate": 0.00011848000000000002,
      "loss": -117.1172,
      "step": 30570
    },
    {
      "epoch": 2.4464,
      "grad_norm": 118.13802337646484,
      "learning_rate": 0.00011845333333333335,
      "loss": -117.8113,
      "step": 30580
    },
    {
      "epoch": 2.4472,
      "grad_norm": 145.33534240722656,
      "learning_rate": 0.00011842666666666666,
      "loss": -118.5542,
      "step": 30590
    },
    {
      "epoch": 2.448,
      "grad_norm": 162.15672302246094,
      "learning_rate": 0.0001184,
      "loss": -117.2252,
      "step": 30600
    },
    {
      "epoch": 2.4488,
      "grad_norm": 163.76832580566406,
      "learning_rate": 0.00011837333333333333,
      "loss": -118.1971,
      "step": 30610
    },
    {
      "epoch": 2.4496,
      "grad_norm": 108.97322082519531,
      "learning_rate": 0.00011834666666666668,
      "loss": -118.1574,
      "step": 30620
    },
    {
      "epoch": 2.4504,
      "grad_norm": 142.82606506347656,
      "learning_rate": 0.00011832,
      "loss": -116.7474,
      "step": 30630
    },
    {
      "epoch": 2.4512,
      "grad_norm": 102.09991455078125,
      "learning_rate": 0.00011829333333333335,
      "loss": -119.3709,
      "step": 30640
    },
    {
      "epoch": 2.452,
      "grad_norm": 72.60971069335938,
      "learning_rate": 0.00011826666666666668,
      "loss": -119.3219,
      "step": 30650
    },
    {
      "epoch": 2.4528,
      "grad_norm": 133.42483520507812,
      "learning_rate": 0.00011823999999999999,
      "loss": -116.9442,
      "step": 30660
    },
    {
      "epoch": 2.4536,
      "grad_norm": 84.4710693359375,
      "learning_rate": 0.00011821333333333334,
      "loss": -119.6115,
      "step": 30670
    },
    {
      "epoch": 2.4544,
      "grad_norm": 129.26968383789062,
      "learning_rate": 0.00011818666666666666,
      "loss": -118.0457,
      "step": 30680
    },
    {
      "epoch": 2.4552,
      "grad_norm": 67.86947631835938,
      "learning_rate": 0.00011816000000000001,
      "loss": -117.4578,
      "step": 30690
    },
    {
      "epoch": 2.456,
      "grad_norm": 236.05596923828125,
      "learning_rate": 0.00011813333333333334,
      "loss": -118.6671,
      "step": 30700
    },
    {
      "epoch": 2.4568,
      "grad_norm": 95.05546569824219,
      "learning_rate": 0.00011810666666666668,
      "loss": -117.7902,
      "step": 30710
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 141.59228515625,
      "learning_rate": 0.00011808000000000001,
      "loss": -117.8004,
      "step": 30720
    },
    {
      "epoch": 2.4584,
      "grad_norm": 59.130775451660156,
      "learning_rate": 0.00011805333333333335,
      "loss": -119.1858,
      "step": 30730
    },
    {
      "epoch": 2.4592,
      "grad_norm": 68.66715240478516,
      "learning_rate": 0.00011802666666666667,
      "loss": -118.0228,
      "step": 30740
    },
    {
      "epoch": 2.46,
      "grad_norm": 202.26016235351562,
      "learning_rate": 0.000118,
      "loss": -117.6358,
      "step": 30750
    },
    {
      "epoch": 2.4608,
      "grad_norm": 88.4665298461914,
      "learning_rate": 0.00011797333333333334,
      "loss": -119.2203,
      "step": 30760
    },
    {
      "epoch": 2.4616,
      "grad_norm": 209.19857788085938,
      "learning_rate": 0.00011794666666666667,
      "loss": -118.4848,
      "step": 30770
    },
    {
      "epoch": 2.4624,
      "grad_norm": 97.15717315673828,
      "learning_rate": 0.00011792000000000001,
      "loss": -117.3628,
      "step": 30780
    },
    {
      "epoch": 2.4632,
      "grad_norm": 54.60755157470703,
      "learning_rate": 0.00011789333333333334,
      "loss": -118.8237,
      "step": 30790
    },
    {
      "epoch": 2.464,
      "grad_norm": 125.1603012084961,
      "learning_rate": 0.00011786666666666668,
      "loss": -118.3632,
      "step": 30800
    },
    {
      "epoch": 2.4648,
      "grad_norm": 88.24688720703125,
      "learning_rate": 0.00011784,
      "loss": -118.128,
      "step": 30810
    },
    {
      "epoch": 2.4656000000000002,
      "grad_norm": 63.404396057128906,
      "learning_rate": 0.00011781333333333333,
      "loss": -118.4577,
      "step": 30820
    },
    {
      "epoch": 2.4664,
      "grad_norm": 101.53913116455078,
      "learning_rate": 0.00011778666666666667,
      "loss": -118.3181,
      "step": 30830
    },
    {
      "epoch": 2.4672,
      "grad_norm": 116.0042495727539,
      "learning_rate": 0.00011776,
      "loss": -118.8067,
      "step": 30840
    },
    {
      "epoch": 2.468,
      "grad_norm": 92.65814208984375,
      "learning_rate": 0.00011773333333333334,
      "loss": -118.1315,
      "step": 30850
    },
    {
      "epoch": 2.4688,
      "grad_norm": 97.96345520019531,
      "learning_rate": 0.00011770666666666668,
      "loss": -117.0921,
      "step": 30860
    },
    {
      "epoch": 2.4696,
      "grad_norm": 75.65770721435547,
      "learning_rate": 0.00011768000000000001,
      "loss": -118.3038,
      "step": 30870
    },
    {
      "epoch": 2.4704,
      "grad_norm": 103.40968322753906,
      "learning_rate": 0.00011765333333333335,
      "loss": -118.4422,
      "step": 30880
    },
    {
      "epoch": 2.4712,
      "grad_norm": 62.2064208984375,
      "learning_rate": 0.00011762666666666666,
      "loss": -116.8123,
      "step": 30890
    },
    {
      "epoch": 2.472,
      "grad_norm": 139.01991271972656,
      "learning_rate": 0.0001176,
      "loss": -118.0936,
      "step": 30900
    },
    {
      "epoch": 2.4728,
      "grad_norm": 96.82489776611328,
      "learning_rate": 0.00011757333333333334,
      "loss": -117.6058,
      "step": 30910
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 91.43446350097656,
      "learning_rate": 0.00011754666666666667,
      "loss": -118.0545,
      "step": 30920
    },
    {
      "epoch": 2.4744,
      "grad_norm": 58.61317825317383,
      "learning_rate": 0.00011752000000000001,
      "loss": -118.5987,
      "step": 30930
    },
    {
      "epoch": 2.4752,
      "grad_norm": 165.3491973876953,
      "learning_rate": 0.00011749333333333334,
      "loss": -119.0769,
      "step": 30940
    },
    {
      "epoch": 2.476,
      "grad_norm": 92.1824951171875,
      "learning_rate": 0.00011746666666666668,
      "loss": -118.3979,
      "step": 30950
    },
    {
      "epoch": 2.4768,
      "grad_norm": 94.516845703125,
      "learning_rate": 0.00011744000000000001,
      "loss": -118.6624,
      "step": 30960
    },
    {
      "epoch": 2.4776,
      "grad_norm": 65.59305572509766,
      "learning_rate": 0.00011741333333333333,
      "loss": -118.8213,
      "step": 30970
    },
    {
      "epoch": 2.4784,
      "grad_norm": 65.08451843261719,
      "learning_rate": 0.00011738666666666667,
      "loss": -117.7129,
      "step": 30980
    },
    {
      "epoch": 2.4792,
      "grad_norm": 67.42427062988281,
      "learning_rate": 0.00011736,
      "loss": -117.4383,
      "step": 30990
    },
    {
      "epoch": 2.48,
      "grad_norm": 92.97405242919922,
      "learning_rate": 0.00011733333333333334,
      "loss": -118.6466,
      "step": 31000
    },
    {
      "epoch": 2.4808,
      "grad_norm": 84.99873352050781,
      "learning_rate": 0.00011730666666666667,
      "loss": -118.736,
      "step": 31010
    },
    {
      "epoch": 2.4816,
      "grad_norm": 104.52450561523438,
      "learning_rate": 0.00011728000000000002,
      "loss": -117.6904,
      "step": 31020
    },
    {
      "epoch": 2.4824,
      "grad_norm": 77.86183166503906,
      "learning_rate": 0.00011725333333333334,
      "loss": -117.9074,
      "step": 31030
    },
    {
      "epoch": 2.4832,
      "grad_norm": 64.25421142578125,
      "learning_rate": 0.00011722666666666666,
      "loss": -118.3486,
      "step": 31040
    },
    {
      "epoch": 2.484,
      "grad_norm": 111.60289001464844,
      "learning_rate": 0.0001172,
      "loss": -118.6515,
      "step": 31050
    },
    {
      "epoch": 2.4848,
      "grad_norm": 106.5919189453125,
      "learning_rate": 0.00011717333333333333,
      "loss": -118.5036,
      "step": 31060
    },
    {
      "epoch": 2.4856,
      "grad_norm": 91.18128967285156,
      "learning_rate": 0.00011714666666666667,
      "loss": -118.9913,
      "step": 31070
    },
    {
      "epoch": 2.4864,
      "grad_norm": 100.61344909667969,
      "learning_rate": 0.00011712,
      "loss": -118.4577,
      "step": 31080
    },
    {
      "epoch": 2.4872,
      "grad_norm": 100.4765625,
      "learning_rate": 0.00011709333333333335,
      "loss": -117.2037,
      "step": 31090
    },
    {
      "epoch": 2.488,
      "grad_norm": 115.01968383789062,
      "learning_rate": 0.00011706666666666668,
      "loss": -118.2035,
      "step": 31100
    },
    {
      "epoch": 2.4888,
      "grad_norm": 75.82537078857422,
      "learning_rate": 0.00011704000000000002,
      "loss": -119.3804,
      "step": 31110
    },
    {
      "epoch": 2.4896,
      "grad_norm": 91.12541961669922,
      "learning_rate": 0.00011701333333333333,
      "loss": -117.8864,
      "step": 31120
    },
    {
      "epoch": 2.4904,
      "grad_norm": 81.2538833618164,
      "learning_rate": 0.00011698666666666666,
      "loss": -117.2973,
      "step": 31130
    },
    {
      "epoch": 2.4912,
      "grad_norm": 78.97771453857422,
      "learning_rate": 0.00011696,
      "loss": -118.857,
      "step": 31140
    },
    {
      "epoch": 2.492,
      "grad_norm": 130.29196166992188,
      "learning_rate": 0.00011693333333333333,
      "loss": -117.4898,
      "step": 31150
    },
    {
      "epoch": 2.4928,
      "grad_norm": 148.89340209960938,
      "learning_rate": 0.00011690666666666668,
      "loss": -116.9873,
      "step": 31160
    },
    {
      "epoch": 2.4936,
      "grad_norm": 104.10818481445312,
      "learning_rate": 0.00011688,
      "loss": -118.7388,
      "step": 31170
    },
    {
      "epoch": 2.4944,
      "grad_norm": 150.1944122314453,
      "learning_rate": 0.00011685333333333335,
      "loss": -117.1948,
      "step": 31180
    },
    {
      "epoch": 2.4952,
      "grad_norm": 88.8138656616211,
      "learning_rate": 0.00011682666666666666,
      "loss": -118.0609,
      "step": 31190
    },
    {
      "epoch": 2.496,
      "grad_norm": 73.05039978027344,
      "learning_rate": 0.00011679999999999999,
      "loss": -118.902,
      "step": 31200
    },
    {
      "epoch": 2.4968,
      "grad_norm": 97.35606384277344,
      "learning_rate": 0.00011677333333333334,
      "loss": -118.038,
      "step": 31210
    },
    {
      "epoch": 2.4976,
      "grad_norm": 75.09837341308594,
      "learning_rate": 0.00011674666666666666,
      "loss": -117.6629,
      "step": 31220
    },
    {
      "epoch": 2.4984,
      "grad_norm": 64.59944152832031,
      "learning_rate": 0.00011672000000000001,
      "loss": -117.3925,
      "step": 31230
    },
    {
      "epoch": 2.4992,
      "grad_norm": 73.02576446533203,
      "learning_rate": 0.00011669333333333335,
      "loss": -118.008,
      "step": 31240
    },
    {
      "epoch": 2.5,
      "grad_norm": 144.3845977783203,
      "learning_rate": 0.00011666666666666668,
      "loss": -117.7613,
      "step": 31250
    },
    {
      "epoch": 2.5008,
      "grad_norm": 221.2649688720703,
      "learning_rate": 0.00011664000000000002,
      "loss": -117.8722,
      "step": 31260
    },
    {
      "epoch": 2.5016,
      "grad_norm": 91.73442077636719,
      "learning_rate": 0.00011661333333333332,
      "loss": -118.1413,
      "step": 31270
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 66.73656463623047,
      "learning_rate": 0.00011658666666666667,
      "loss": -118.5283,
      "step": 31280
    },
    {
      "epoch": 2.5032,
      "grad_norm": 81.21915435791016,
      "learning_rate": 0.00011656000000000001,
      "loss": -118.6291,
      "step": 31290
    },
    {
      "epoch": 2.504,
      "grad_norm": 72.9283676147461,
      "learning_rate": 0.00011653333333333334,
      "loss": -117.8084,
      "step": 31300
    },
    {
      "epoch": 2.5048,
      "grad_norm": 106.74046325683594,
      "learning_rate": 0.00011650666666666668,
      "loss": -117.3367,
      "step": 31310
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 93.64338684082031,
      "learning_rate": 0.00011648000000000001,
      "loss": -119.1533,
      "step": 31320
    },
    {
      "epoch": 2.5064,
      "grad_norm": 122.34169006347656,
      "learning_rate": 0.00011645333333333335,
      "loss": -117.3905,
      "step": 31330
    },
    {
      "epoch": 2.5072,
      "grad_norm": 79.65998840332031,
      "learning_rate": 0.00011642666666666667,
      "loss": -117.1217,
      "step": 31340
    },
    {
      "epoch": 2.508,
      "grad_norm": 53.2420768737793,
      "learning_rate": 0.0001164,
      "loss": -118.6613,
      "step": 31350
    },
    {
      "epoch": 2.5088,
      "grad_norm": 167.45599365234375,
      "learning_rate": 0.00011637333333333334,
      "loss": -118.1328,
      "step": 31360
    },
    {
      "epoch": 2.5096,
      "grad_norm": 98.53093719482422,
      "learning_rate": 0.00011634666666666667,
      "loss": -117.8035,
      "step": 31370
    },
    {
      "epoch": 2.5103999999999997,
      "grad_norm": 124.71551513671875,
      "learning_rate": 0.00011632000000000001,
      "loss": -118.6346,
      "step": 31380
    },
    {
      "epoch": 2.5112,
      "grad_norm": 79.73600006103516,
      "learning_rate": 0.00011629333333333334,
      "loss": -117.6093,
      "step": 31390
    },
    {
      "epoch": 2.512,
      "grad_norm": 99.99847412109375,
      "learning_rate": 0.00011626666666666668,
      "loss": -118.5172,
      "step": 31400
    },
    {
      "epoch": 2.5128,
      "grad_norm": 80.94486236572266,
      "learning_rate": 0.00011624000000000001,
      "loss": -118.3372,
      "step": 31410
    },
    {
      "epoch": 2.5136,
      "grad_norm": 84.23914337158203,
      "learning_rate": 0.00011621333333333333,
      "loss": -118.3027,
      "step": 31420
    },
    {
      "epoch": 2.5144,
      "grad_norm": 74.25260162353516,
      "learning_rate": 0.00011618666666666667,
      "loss": -117.9394,
      "step": 31430
    },
    {
      "epoch": 2.5152,
      "grad_norm": 108.18704223632812,
      "learning_rate": 0.00011616,
      "loss": -118.3887,
      "step": 31440
    },
    {
      "epoch": 2.516,
      "grad_norm": 72.88740539550781,
      "learning_rate": 0.00011613333333333334,
      "loss": -117.6318,
      "step": 31450
    },
    {
      "epoch": 2.5168,
      "grad_norm": 99.83415985107422,
      "learning_rate": 0.00011610666666666667,
      "loss": -118.6688,
      "step": 31460
    },
    {
      "epoch": 2.5176,
      "grad_norm": 89.05286407470703,
      "learning_rate": 0.00011608000000000001,
      "loss": -117.9228,
      "step": 31470
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 74.27454376220703,
      "learning_rate": 0.00011605333333333334,
      "loss": -118.2194,
      "step": 31480
    },
    {
      "epoch": 2.5192,
      "grad_norm": 96.59369659423828,
      "learning_rate": 0.00011602666666666666,
      "loss": -117.2303,
      "step": 31490
    },
    {
      "epoch": 2.52,
      "grad_norm": 225.4349822998047,
      "learning_rate": 0.000116,
      "loss": -118.7756,
      "step": 31500
    },
    {
      "epoch": 2.5208,
      "grad_norm": 201.65432739257812,
      "learning_rate": 0.00011597333333333333,
      "loss": -117.6842,
      "step": 31510
    },
    {
      "epoch": 2.5216,
      "grad_norm": 112.88023376464844,
      "learning_rate": 0.00011594666666666667,
      "loss": -117.4892,
      "step": 31520
    },
    {
      "epoch": 2.5224,
      "grad_norm": 67.28262329101562,
      "learning_rate": 0.00011592,
      "loss": -118.6006,
      "step": 31530
    },
    {
      "epoch": 2.5232,
      "grad_norm": 70.43031311035156,
      "learning_rate": 0.00011589333333333334,
      "loss": -118.7248,
      "step": 31540
    },
    {
      "epoch": 2.524,
      "grad_norm": 94.568603515625,
      "learning_rate": 0.00011586666666666667,
      "loss": -119.1412,
      "step": 31550
    },
    {
      "epoch": 2.5248,
      "grad_norm": 114.89315032958984,
      "learning_rate": 0.00011584000000000002,
      "loss": -118.3601,
      "step": 31560
    },
    {
      "epoch": 2.5256,
      "grad_norm": 89.73896789550781,
      "learning_rate": 0.00011581333333333333,
      "loss": -118.4896,
      "step": 31570
    },
    {
      "epoch": 2.5263999999999998,
      "grad_norm": 96.85049438476562,
      "learning_rate": 0.00011578666666666666,
      "loss": -118.3247,
      "step": 31580
    },
    {
      "epoch": 2.5272,
      "grad_norm": 112.16083526611328,
      "learning_rate": 0.00011576,
      "loss": -118.6129,
      "step": 31590
    },
    {
      "epoch": 2.528,
      "grad_norm": 71.8438720703125,
      "learning_rate": 0.00011573333333333333,
      "loss": -119.2242,
      "step": 31600
    },
    {
      "epoch": 2.5288,
      "grad_norm": 96.45852661132812,
      "learning_rate": 0.00011570666666666667,
      "loss": -117.4228,
      "step": 31610
    },
    {
      "epoch": 2.5296,
      "grad_norm": 61.462223052978516,
      "learning_rate": 0.00011568000000000002,
      "loss": -119.2077,
      "step": 31620
    },
    {
      "epoch": 2.5304,
      "grad_norm": 118.59464263916016,
      "learning_rate": 0.00011565333333333335,
      "loss": -117.8489,
      "step": 31630
    },
    {
      "epoch": 2.5312,
      "grad_norm": 88.15904998779297,
      "learning_rate": 0.00011562666666666669,
      "loss": -119.7586,
      "step": 31640
    },
    {
      "epoch": 2.532,
      "grad_norm": 69.2691879272461,
      "learning_rate": 0.00011559999999999999,
      "loss": -118.2332,
      "step": 31650
    },
    {
      "epoch": 2.5328,
      "grad_norm": 65.4151611328125,
      "learning_rate": 0.00011557333333333333,
      "loss": -118.1699,
      "step": 31660
    },
    {
      "epoch": 2.5336,
      "grad_norm": 164.42982482910156,
      "learning_rate": 0.00011554666666666668,
      "loss": -118.6453,
      "step": 31670
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 96.49272155761719,
      "learning_rate": 0.00011552,
      "loss": -118.2916,
      "step": 31680
    },
    {
      "epoch": 2.5352,
      "grad_norm": 51.738441467285156,
      "learning_rate": 0.00011549333333333335,
      "loss": -118.5351,
      "step": 31690
    },
    {
      "epoch": 2.536,
      "grad_norm": 93.57695770263672,
      "learning_rate": 0.00011546666666666668,
      "loss": -118.1578,
      "step": 31700
    },
    {
      "epoch": 2.5368,
      "grad_norm": 108.6700439453125,
      "learning_rate": 0.00011544000000000002,
      "loss": -118.3362,
      "step": 31710
    },
    {
      "epoch": 2.5376,
      "grad_norm": 151.97508239746094,
      "learning_rate": 0.00011541333333333334,
      "loss": -116.7092,
      "step": 31720
    },
    {
      "epoch": 2.5384,
      "grad_norm": 107.43751525878906,
      "learning_rate": 0.00011538666666666666,
      "loss": -116.2944,
      "step": 31730
    },
    {
      "epoch": 2.5392,
      "grad_norm": 159.16001892089844,
      "learning_rate": 0.00011536000000000001,
      "loss": -118.5548,
      "step": 31740
    },
    {
      "epoch": 2.54,
      "grad_norm": 96.35707092285156,
      "learning_rate": 0.00011533333333333334,
      "loss": -117.0323,
      "step": 31750
    },
    {
      "epoch": 2.5408,
      "grad_norm": 63.814308166503906,
      "learning_rate": 0.00011530666666666668,
      "loss": -117.6003,
      "step": 31760
    },
    {
      "epoch": 2.5416,
      "grad_norm": 99.516357421875,
      "learning_rate": 0.00011528000000000001,
      "loss": -119.4812,
      "step": 31770
    },
    {
      "epoch": 2.5423999999999998,
      "grad_norm": 106.31878662109375,
      "learning_rate": 0.00011525333333333335,
      "loss": -118.4972,
      "step": 31780
    },
    {
      "epoch": 2.5432,
      "grad_norm": 85.17068481445312,
      "learning_rate": 0.00011522666666666668,
      "loss": -118.6651,
      "step": 31790
    },
    {
      "epoch": 2.544,
      "grad_norm": 112.6193618774414,
      "learning_rate": 0.0001152,
      "loss": -119.0347,
      "step": 31800
    },
    {
      "epoch": 2.5448,
      "grad_norm": 66.45750427246094,
      "learning_rate": 0.00011517333333333334,
      "loss": -117.7444,
      "step": 31810
    },
    {
      "epoch": 2.5456,
      "grad_norm": 86.65895080566406,
      "learning_rate": 0.00011514666666666667,
      "loss": -119.3157,
      "step": 31820
    },
    {
      "epoch": 2.5464,
      "grad_norm": 90.3147201538086,
      "learning_rate": 0.00011512000000000001,
      "loss": -118.8908,
      "step": 31830
    },
    {
      "epoch": 2.5472,
      "grad_norm": 135.43780517578125,
      "learning_rate": 0.00011509333333333334,
      "loss": -119.2628,
      "step": 31840
    },
    {
      "epoch": 2.548,
      "grad_norm": 73.08338928222656,
      "learning_rate": 0.00011506666666666668,
      "loss": -118.5393,
      "step": 31850
    },
    {
      "epoch": 2.5488,
      "grad_norm": 75.14605712890625,
      "learning_rate": 0.00011504000000000001,
      "loss": -119.4318,
      "step": 31860
    },
    {
      "epoch": 2.5496,
      "grad_norm": 69.4354019165039,
      "learning_rate": 0.00011501333333333333,
      "loss": -117.4351,
      "step": 31870
    },
    {
      "epoch": 2.5504,
      "grad_norm": 120.94247436523438,
      "learning_rate": 0.00011498666666666667,
      "loss": -118.479,
      "step": 31880
    },
    {
      "epoch": 2.5512,
      "grad_norm": 82.55838012695312,
      "learning_rate": 0.00011496,
      "loss": -119.2996,
      "step": 31890
    },
    {
      "epoch": 2.552,
      "grad_norm": 80.5944595336914,
      "learning_rate": 0.00011493333333333334,
      "loss": -118.6022,
      "step": 31900
    },
    {
      "epoch": 2.5528,
      "grad_norm": 128.6785125732422,
      "learning_rate": 0.00011490666666666667,
      "loss": -118.5105,
      "step": 31910
    },
    {
      "epoch": 2.5536,
      "grad_norm": 97.75103759765625,
      "learning_rate": 0.00011488000000000001,
      "loss": -118.1844,
      "step": 31920
    },
    {
      "epoch": 2.5544000000000002,
      "grad_norm": 73.59874725341797,
      "learning_rate": 0.00011485333333333334,
      "loss": -119.4471,
      "step": 31930
    },
    {
      "epoch": 2.5552,
      "grad_norm": 114.91873931884766,
      "learning_rate": 0.00011482666666666668,
      "loss": -118.9896,
      "step": 31940
    },
    {
      "epoch": 2.556,
      "grad_norm": 119.97116088867188,
      "learning_rate": 0.0001148,
      "loss": -119.2233,
      "step": 31950
    },
    {
      "epoch": 2.5568,
      "grad_norm": 62.54736328125,
      "learning_rate": 0.00011477333333333333,
      "loss": -117.7601,
      "step": 31960
    },
    {
      "epoch": 2.5576,
      "grad_norm": 99.00440216064453,
      "learning_rate": 0.00011474666666666667,
      "loss": -119.3349,
      "step": 31970
    },
    {
      "epoch": 2.5584,
      "grad_norm": 124.82050323486328,
      "learning_rate": 0.00011472,
      "loss": -119.4802,
      "step": 31980
    },
    {
      "epoch": 2.5592,
      "grad_norm": 82.28312683105469,
      "learning_rate": 0.00011469333333333334,
      "loss": -119.966,
      "step": 31990
    },
    {
      "epoch": 2.56,
      "grad_norm": 125.9153060913086,
      "learning_rate": 0.00011466666666666667,
      "loss": -118.7918,
      "step": 32000
    },
    {
      "epoch": 2.5608,
      "grad_norm": 97.16319274902344,
      "learning_rate": 0.00011464000000000001,
      "loss": -118.5711,
      "step": 32010
    },
    {
      "epoch": 2.5616,
      "grad_norm": 73.7119140625,
      "learning_rate": 0.00011461333333333333,
      "loss": -118.9141,
      "step": 32020
    },
    {
      "epoch": 2.5624000000000002,
      "grad_norm": 94.26293182373047,
      "learning_rate": 0.00011458666666666666,
      "loss": -118.3776,
      "step": 32030
    },
    {
      "epoch": 2.5632,
      "grad_norm": 80.88109588623047,
      "learning_rate": 0.00011456,
      "loss": -118.3837,
      "step": 32040
    },
    {
      "epoch": 2.564,
      "grad_norm": 83.78550720214844,
      "learning_rate": 0.00011453333333333334,
      "loss": -118.4948,
      "step": 32050
    },
    {
      "epoch": 2.5648,
      "grad_norm": 57.515167236328125,
      "learning_rate": 0.00011450666666666667,
      "loss": -118.2814,
      "step": 32060
    },
    {
      "epoch": 2.5656,
      "grad_norm": 131.9075469970703,
      "learning_rate": 0.00011448000000000002,
      "loss": -118.3596,
      "step": 32070
    },
    {
      "epoch": 2.5664,
      "grad_norm": 75.11734771728516,
      "learning_rate": 0.00011445333333333334,
      "loss": -118.8822,
      "step": 32080
    },
    {
      "epoch": 2.5672,
      "grad_norm": 86.57318878173828,
      "learning_rate": 0.00011442666666666669,
      "loss": -119.2728,
      "step": 32090
    },
    {
      "epoch": 2.568,
      "grad_norm": 91.64644622802734,
      "learning_rate": 0.0001144,
      "loss": -119.561,
      "step": 32100
    },
    {
      "epoch": 2.5688,
      "grad_norm": 68.8362045288086,
      "learning_rate": 0.00011437333333333333,
      "loss": -118.9057,
      "step": 32110
    },
    {
      "epoch": 2.5696,
      "grad_norm": 94.71913146972656,
      "learning_rate": 0.00011434666666666667,
      "loss": -118.767,
      "step": 32120
    },
    {
      "epoch": 2.5704000000000002,
      "grad_norm": 86.34557342529297,
      "learning_rate": 0.00011432,
      "loss": -119.0219,
      "step": 32130
    },
    {
      "epoch": 2.5712,
      "grad_norm": 65.39743041992188,
      "learning_rate": 0.00011429333333333335,
      "loss": -119.1498,
      "step": 32140
    },
    {
      "epoch": 2.572,
      "grad_norm": 89.94710540771484,
      "learning_rate": 0.00011426666666666667,
      "loss": -119.7332,
      "step": 32150
    },
    {
      "epoch": 2.5728,
      "grad_norm": 98.36714172363281,
      "learning_rate": 0.00011424000000000002,
      "loss": -118.4421,
      "step": 32160
    },
    {
      "epoch": 2.5736,
      "grad_norm": 91.60086822509766,
      "learning_rate": 0.00011421333333333333,
      "loss": -119.5038,
      "step": 32170
    },
    {
      "epoch": 2.5744,
      "grad_norm": 65.94576263427734,
      "learning_rate": 0.00011418666666666666,
      "loss": -118.8671,
      "step": 32180
    },
    {
      "epoch": 2.5752,
      "grad_norm": 74.20780944824219,
      "learning_rate": 0.00011416,
      "loss": -118.683,
      "step": 32190
    },
    {
      "epoch": 2.576,
      "grad_norm": 134.70144653320312,
      "learning_rate": 0.00011413333333333333,
      "loss": -119.4565,
      "step": 32200
    },
    {
      "epoch": 2.5768,
      "grad_norm": 64.66165924072266,
      "learning_rate": 0.00011410666666666668,
      "loss": -119.1998,
      "step": 32210
    },
    {
      "epoch": 2.5776,
      "grad_norm": 104.27324676513672,
      "learning_rate": 0.00011408,
      "loss": -120.2611,
      "step": 32220
    },
    {
      "epoch": 2.5784000000000002,
      "grad_norm": 73.70091247558594,
      "learning_rate": 0.00011405333333333335,
      "loss": -118.328,
      "step": 32230
    },
    {
      "epoch": 2.5792,
      "grad_norm": 85.99492645263672,
      "learning_rate": 0.00011402666666666668,
      "loss": -118.3595,
      "step": 32240
    },
    {
      "epoch": 2.58,
      "grad_norm": 115.0427474975586,
      "learning_rate": 0.00011399999999999999,
      "loss": -119.6982,
      "step": 32250
    },
    {
      "epoch": 2.5808,
      "grad_norm": 57.84580612182617,
      "learning_rate": 0.00011397333333333334,
      "loss": -119.1058,
      "step": 32260
    },
    {
      "epoch": 2.5816,
      "grad_norm": 122.00836944580078,
      "learning_rate": 0.00011394666666666666,
      "loss": -118.2651,
      "step": 32270
    },
    {
      "epoch": 2.5824,
      "grad_norm": 86.89192199707031,
      "learning_rate": 0.00011392000000000001,
      "loss": -119.3808,
      "step": 32280
    },
    {
      "epoch": 2.5832,
      "grad_norm": 101.57637023925781,
      "learning_rate": 0.00011389333333333334,
      "loss": -119.8028,
      "step": 32290
    },
    {
      "epoch": 2.584,
      "grad_norm": 90.57630157470703,
      "learning_rate": 0.00011386666666666668,
      "loss": -118.5497,
      "step": 32300
    },
    {
      "epoch": 2.5848,
      "grad_norm": 85.20022583007812,
      "learning_rate": 0.00011384000000000001,
      "loss": -118.4638,
      "step": 32310
    },
    {
      "epoch": 2.5856,
      "grad_norm": 86.75064086914062,
      "learning_rate": 0.00011381333333333335,
      "loss": -118.9856,
      "step": 32320
    },
    {
      "epoch": 2.5864000000000003,
      "grad_norm": 74.60994720458984,
      "learning_rate": 0.00011378666666666667,
      "loss": -118.9442,
      "step": 32330
    },
    {
      "epoch": 2.5872,
      "grad_norm": 59.438533782958984,
      "learning_rate": 0.00011376,
      "loss": -119.5992,
      "step": 32340
    },
    {
      "epoch": 2.588,
      "grad_norm": 103.47602844238281,
      "learning_rate": 0.00011373333333333334,
      "loss": -119.4598,
      "step": 32350
    },
    {
      "epoch": 2.5888,
      "grad_norm": 156.8054962158203,
      "learning_rate": 0.00011370666666666667,
      "loss": -118.8495,
      "step": 32360
    },
    {
      "epoch": 2.5896,
      "grad_norm": 113.5849380493164,
      "learning_rate": 0.00011368000000000001,
      "loss": -118.6426,
      "step": 32370
    },
    {
      "epoch": 2.5904,
      "grad_norm": 80.31291961669922,
      "learning_rate": 0.00011365333333333334,
      "loss": -119.4106,
      "step": 32380
    },
    {
      "epoch": 2.5911999999999997,
      "grad_norm": 92.93042755126953,
      "learning_rate": 0.00011362666666666668,
      "loss": -119.6328,
      "step": 32390
    },
    {
      "epoch": 2.592,
      "grad_norm": 76.74051666259766,
      "learning_rate": 0.0001136,
      "loss": -119.1222,
      "step": 32400
    },
    {
      "epoch": 2.5928,
      "grad_norm": 60.335575103759766,
      "learning_rate": 0.00011357333333333333,
      "loss": -119.2285,
      "step": 32410
    },
    {
      "epoch": 2.5936,
      "grad_norm": 100.54649353027344,
      "learning_rate": 0.00011354666666666667,
      "loss": -119.4646,
      "step": 32420
    },
    {
      "epoch": 2.5944000000000003,
      "grad_norm": 95.0815658569336,
      "learning_rate": 0.00011352000000000001,
      "loss": -119.5246,
      "step": 32430
    },
    {
      "epoch": 2.5952,
      "grad_norm": 80.80274200439453,
      "learning_rate": 0.00011349333333333334,
      "loss": -118.528,
      "step": 32440
    },
    {
      "epoch": 2.596,
      "grad_norm": 90.22135925292969,
      "learning_rate": 0.00011346666666666668,
      "loss": -119.3962,
      "step": 32450
    },
    {
      "epoch": 2.5968,
      "grad_norm": 63.30734634399414,
      "learning_rate": 0.00011344000000000001,
      "loss": -118.9325,
      "step": 32460
    },
    {
      "epoch": 2.5976,
      "grad_norm": 114.9659194946289,
      "learning_rate": 0.00011341333333333335,
      "loss": -119.3951,
      "step": 32470
    },
    {
      "epoch": 2.5984,
      "grad_norm": 115.69493103027344,
      "learning_rate": 0.00011338666666666667,
      "loss": -118.7701,
      "step": 32480
    },
    {
      "epoch": 2.5991999999999997,
      "grad_norm": 64.01057434082031,
      "learning_rate": 0.00011336,
      "loss": -119.0222,
      "step": 32490
    },
    {
      "epoch": 2.6,
      "grad_norm": 117.29923248291016,
      "learning_rate": 0.00011333333333333334,
      "loss": -118.4296,
      "step": 32500
    },
    {
      "epoch": 2.6008,
      "grad_norm": 43.57740020751953,
      "learning_rate": 0.00011330666666666667,
      "loss": -118.6526,
      "step": 32510
    },
    {
      "epoch": 2.6016,
      "grad_norm": 67.29446411132812,
      "learning_rate": 0.00011328000000000001,
      "loss": -119.1057,
      "step": 32520
    },
    {
      "epoch": 2.6024000000000003,
      "grad_norm": 71.09201049804688,
      "learning_rate": 0.00011325333333333334,
      "loss": -118.8264,
      "step": 32530
    },
    {
      "epoch": 2.6032,
      "grad_norm": 106.81450653076172,
      "learning_rate": 0.00011322666666666668,
      "loss": -118.7207,
      "step": 32540
    },
    {
      "epoch": 2.604,
      "grad_norm": 75.09819793701172,
      "learning_rate": 0.0001132,
      "loss": -119.455,
      "step": 32550
    },
    {
      "epoch": 2.6048,
      "grad_norm": 88.5291519165039,
      "learning_rate": 0.00011317333333333333,
      "loss": -119.0599,
      "step": 32560
    },
    {
      "epoch": 2.6056,
      "grad_norm": 191.41993713378906,
      "learning_rate": 0.00011314666666666667,
      "loss": -118.9521,
      "step": 32570
    },
    {
      "epoch": 2.6064,
      "grad_norm": 68.12894439697266,
      "learning_rate": 0.00011312,
      "loss": -120.0069,
      "step": 32580
    },
    {
      "epoch": 2.6071999999999997,
      "grad_norm": 75.64402770996094,
      "learning_rate": 0.00011309333333333334,
      "loss": -119.8825,
      "step": 32590
    },
    {
      "epoch": 2.608,
      "grad_norm": 189.77877807617188,
      "learning_rate": 0.00011306666666666667,
      "loss": -119.044,
      "step": 32600
    },
    {
      "epoch": 2.6088,
      "grad_norm": 76.39591979980469,
      "learning_rate": 0.00011304000000000002,
      "loss": -119.2801,
      "step": 32610
    },
    {
      "epoch": 2.6096,
      "grad_norm": 79.92396545410156,
      "learning_rate": 0.00011301333333333334,
      "loss": -118.3,
      "step": 32620
    },
    {
      "epoch": 2.6104000000000003,
      "grad_norm": 112.43734741210938,
      "learning_rate": 0.00011298666666666666,
      "loss": -118.6766,
      "step": 32630
    },
    {
      "epoch": 2.6112,
      "grad_norm": 146.22215270996094,
      "learning_rate": 0.00011296,
      "loss": -119.1279,
      "step": 32640
    },
    {
      "epoch": 2.612,
      "grad_norm": 170.43392944335938,
      "learning_rate": 0.00011293333333333333,
      "loss": -117.8704,
      "step": 32650
    },
    {
      "epoch": 2.6128,
      "grad_norm": 77.7789306640625,
      "learning_rate": 0.00011290666666666667,
      "loss": -118.6332,
      "step": 32660
    },
    {
      "epoch": 2.6136,
      "grad_norm": 61.990047454833984,
      "learning_rate": 0.00011288,
      "loss": -118.3865,
      "step": 32670
    },
    {
      "epoch": 2.6144,
      "grad_norm": 133.3251953125,
      "learning_rate": 0.00011285333333333335,
      "loss": -119.9576,
      "step": 32680
    },
    {
      "epoch": 2.6151999999999997,
      "grad_norm": 99.34827423095703,
      "learning_rate": 0.00011282666666666668,
      "loss": -118.9493,
      "step": 32690
    },
    {
      "epoch": 2.616,
      "grad_norm": 169.6737060546875,
      "learning_rate": 0.00011279999999999999,
      "loss": -119.5831,
      "step": 32700
    },
    {
      "epoch": 2.6168,
      "grad_norm": 86.7364273071289,
      "learning_rate": 0.00011277333333333333,
      "loss": -119.5831,
      "step": 32710
    },
    {
      "epoch": 2.6176,
      "grad_norm": 87.8990249633789,
      "learning_rate": 0.00011274666666666666,
      "loss": -119.6277,
      "step": 32720
    },
    {
      "epoch": 2.6184,
      "grad_norm": 104.48300170898438,
      "learning_rate": 0.00011272,
      "loss": -118.754,
      "step": 32730
    },
    {
      "epoch": 2.6192,
      "grad_norm": 60.82514572143555,
      "learning_rate": 0.00011269333333333333,
      "loss": -118.7412,
      "step": 32740
    },
    {
      "epoch": 2.62,
      "grad_norm": 81.77467346191406,
      "learning_rate": 0.00011266666666666668,
      "loss": -119.6174,
      "step": 32750
    },
    {
      "epoch": 2.6208,
      "grad_norm": 74.9908676147461,
      "learning_rate": 0.00011264,
      "loss": -119.6768,
      "step": 32760
    },
    {
      "epoch": 2.6216,
      "grad_norm": 126.05894470214844,
      "learning_rate": 0.00011261333333333335,
      "loss": -118.8202,
      "step": 32770
    },
    {
      "epoch": 2.6224,
      "grad_norm": 88.09441375732422,
      "learning_rate": 0.00011258666666666666,
      "loss": -119.6138,
      "step": 32780
    },
    {
      "epoch": 2.6231999999999998,
      "grad_norm": 109.53398132324219,
      "learning_rate": 0.00011255999999999999,
      "loss": -119.2216,
      "step": 32790
    },
    {
      "epoch": 2.624,
      "grad_norm": 102.05732727050781,
      "learning_rate": 0.00011253333333333334,
      "loss": -119.2222,
      "step": 32800
    },
    {
      "epoch": 2.6248,
      "grad_norm": 115.76849365234375,
      "learning_rate": 0.00011250666666666666,
      "loss": -118.0234,
      "step": 32810
    },
    {
      "epoch": 2.6256,
      "grad_norm": 64.06829071044922,
      "learning_rate": 0.00011248000000000001,
      "loss": -119.1372,
      "step": 32820
    },
    {
      "epoch": 2.6264,
      "grad_norm": 118.98629760742188,
      "learning_rate": 0.00011245333333333335,
      "loss": -119.7913,
      "step": 32830
    },
    {
      "epoch": 2.6272,
      "grad_norm": 94.49442291259766,
      "learning_rate": 0.00011242666666666668,
      "loss": -120.0372,
      "step": 32840
    },
    {
      "epoch": 2.628,
      "grad_norm": 188.17913818359375,
      "learning_rate": 0.00011240000000000002,
      "loss": -119.0415,
      "step": 32850
    },
    {
      "epoch": 2.6288,
      "grad_norm": 66.87210845947266,
      "learning_rate": 0.00011237333333333334,
      "loss": -119.0259,
      "step": 32860
    },
    {
      "epoch": 2.6296,
      "grad_norm": 117.07305145263672,
      "learning_rate": 0.00011234666666666667,
      "loss": -118.9725,
      "step": 32870
    },
    {
      "epoch": 2.6304,
      "grad_norm": 63.54780960083008,
      "learning_rate": 0.00011232000000000001,
      "loss": -117.9882,
      "step": 32880
    },
    {
      "epoch": 2.6311999999999998,
      "grad_norm": 134.18539428710938,
      "learning_rate": 0.00011229333333333334,
      "loss": -118.7354,
      "step": 32890
    },
    {
      "epoch": 2.632,
      "grad_norm": 75.73540496826172,
      "learning_rate": 0.00011226666666666668,
      "loss": -119.5076,
      "step": 32900
    },
    {
      "epoch": 2.6328,
      "grad_norm": 91.05787658691406,
      "learning_rate": 0.00011224000000000001,
      "loss": -119.0821,
      "step": 32910
    },
    {
      "epoch": 2.6336,
      "grad_norm": 92.35869598388672,
      "learning_rate": 0.00011221333333333335,
      "loss": -118.7695,
      "step": 32920
    },
    {
      "epoch": 2.6344,
      "grad_norm": 88.79669189453125,
      "learning_rate": 0.00011218666666666667,
      "loss": -119.2111,
      "step": 32930
    },
    {
      "epoch": 2.6352,
      "grad_norm": 75.74219512939453,
      "learning_rate": 0.00011216,
      "loss": -118.267,
      "step": 32940
    },
    {
      "epoch": 2.636,
      "grad_norm": 98.270263671875,
      "learning_rate": 0.00011213333333333334,
      "loss": -119.3099,
      "step": 32950
    },
    {
      "epoch": 2.6368,
      "grad_norm": 121.70247650146484,
      "learning_rate": 0.00011210666666666667,
      "loss": -119.232,
      "step": 32960
    },
    {
      "epoch": 2.6376,
      "grad_norm": 108.25696563720703,
      "learning_rate": 0.00011208000000000001,
      "loss": -119.5718,
      "step": 32970
    },
    {
      "epoch": 2.6384,
      "grad_norm": 213.18289184570312,
      "learning_rate": 0.00011205333333333334,
      "loss": -118.4121,
      "step": 32980
    },
    {
      "epoch": 2.6391999999999998,
      "grad_norm": 66.01387786865234,
      "learning_rate": 0.00011202666666666668,
      "loss": -117.6064,
      "step": 32990
    },
    {
      "epoch": 2.64,
      "grad_norm": 78.22095489501953,
      "learning_rate": 0.00011200000000000001,
      "loss": -119.2522,
      "step": 33000
    },
    {
      "epoch": 2.6408,
      "grad_norm": 100.71205139160156,
      "learning_rate": 0.00011197333333333333,
      "loss": -119.0302,
      "step": 33010
    },
    {
      "epoch": 2.6416,
      "grad_norm": 83.33262634277344,
      "learning_rate": 0.00011194666666666667,
      "loss": -119.0154,
      "step": 33020
    },
    {
      "epoch": 2.6424,
      "grad_norm": 75.48119354248047,
      "learning_rate": 0.00011192,
      "loss": -119.0045,
      "step": 33030
    },
    {
      "epoch": 2.6432,
      "grad_norm": 95.76890563964844,
      "learning_rate": 0.00011189333333333334,
      "loss": -119.437,
      "step": 33040
    },
    {
      "epoch": 2.644,
      "grad_norm": 81.60726928710938,
      "learning_rate": 0.00011186666666666667,
      "loss": -118.3417,
      "step": 33050
    },
    {
      "epoch": 2.6448,
      "grad_norm": 129.98471069335938,
      "learning_rate": 0.00011184000000000001,
      "loss": -119.2221,
      "step": 33060
    },
    {
      "epoch": 2.6456,
      "grad_norm": 97.8510971069336,
      "learning_rate": 0.00011181333333333334,
      "loss": -119.4618,
      "step": 33070
    },
    {
      "epoch": 2.6464,
      "grad_norm": 70.86835479736328,
      "learning_rate": 0.00011178666666666666,
      "loss": -119.647,
      "step": 33080
    },
    {
      "epoch": 2.6471999999999998,
      "grad_norm": 58.89265060424805,
      "learning_rate": 0.00011176,
      "loss": -120.633,
      "step": 33090
    },
    {
      "epoch": 2.648,
      "grad_norm": 64.86443328857422,
      "learning_rate": 0.00011173333333333333,
      "loss": -119.6361,
      "step": 33100
    },
    {
      "epoch": 2.6488,
      "grad_norm": 59.96432113647461,
      "learning_rate": 0.00011170666666666667,
      "loss": -119.6306,
      "step": 33110
    },
    {
      "epoch": 2.6496,
      "grad_norm": 103.1097412109375,
      "learning_rate": 0.00011168,
      "loss": -119.5835,
      "step": 33120
    },
    {
      "epoch": 2.6504,
      "grad_norm": 58.86958312988281,
      "learning_rate": 0.00011165333333333334,
      "loss": -117.8187,
      "step": 33130
    },
    {
      "epoch": 2.6512000000000002,
      "grad_norm": 60.37913513183594,
      "learning_rate": 0.00011162666666666667,
      "loss": -118.237,
      "step": 33140
    },
    {
      "epoch": 2.652,
      "grad_norm": 134.45899963378906,
      "learning_rate": 0.00011160000000000002,
      "loss": -118.5435,
      "step": 33150
    },
    {
      "epoch": 2.6528,
      "grad_norm": 73.24703979492188,
      "learning_rate": 0.00011157333333333333,
      "loss": -119.2217,
      "step": 33160
    },
    {
      "epoch": 2.6536,
      "grad_norm": 84.54884338378906,
      "learning_rate": 0.00011154666666666666,
      "loss": -119.0879,
      "step": 33170
    },
    {
      "epoch": 2.6544,
      "grad_norm": 68.0663070678711,
      "learning_rate": 0.00011152,
      "loss": -119.1716,
      "step": 33180
    },
    {
      "epoch": 2.6552,
      "grad_norm": 104.62734985351562,
      "learning_rate": 0.00011149333333333333,
      "loss": -119.3942,
      "step": 33190
    },
    {
      "epoch": 2.656,
      "grad_norm": 77.92829132080078,
      "learning_rate": 0.00011146666666666667,
      "loss": -118.798,
      "step": 33200
    },
    {
      "epoch": 2.6568,
      "grad_norm": 59.12521743774414,
      "learning_rate": 0.00011144000000000002,
      "loss": -119.6802,
      "step": 33210
    },
    {
      "epoch": 2.6576,
      "grad_norm": 95.91898345947266,
      "learning_rate": 0.00011141333333333335,
      "loss": -119.4995,
      "step": 33220
    },
    {
      "epoch": 2.6584,
      "grad_norm": 65.04653930664062,
      "learning_rate": 0.00011138666666666666,
      "loss": -119.5022,
      "step": 33230
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 82.03904724121094,
      "learning_rate": 0.00011135999999999999,
      "loss": -118.4531,
      "step": 33240
    },
    {
      "epoch": 2.66,
      "grad_norm": 52.120601654052734,
      "learning_rate": 0.00011133333333333333,
      "loss": -118.8902,
      "step": 33250
    },
    {
      "epoch": 2.6608,
      "grad_norm": 82.41439819335938,
      "learning_rate": 0.00011130666666666668,
      "loss": -119.4016,
      "step": 33260
    },
    {
      "epoch": 2.6616,
      "grad_norm": 86.42940521240234,
      "learning_rate": 0.00011128,
      "loss": -118.1973,
      "step": 33270
    },
    {
      "epoch": 2.6624,
      "grad_norm": 67.6624755859375,
      "learning_rate": 0.00011125333333333335,
      "loss": -119.7121,
      "step": 33280
    },
    {
      "epoch": 2.6632,
      "grad_norm": 135.56707763671875,
      "learning_rate": 0.00011122666666666668,
      "loss": -118.8531,
      "step": 33290
    },
    {
      "epoch": 2.664,
      "grad_norm": 57.326419830322266,
      "learning_rate": 0.00011120000000000002,
      "loss": -118.3148,
      "step": 33300
    },
    {
      "epoch": 2.6648,
      "grad_norm": 109.39398193359375,
      "learning_rate": 0.00011117333333333334,
      "loss": -118.9514,
      "step": 33310
    },
    {
      "epoch": 2.6656,
      "grad_norm": 73.7738265991211,
      "learning_rate": 0.00011114666666666666,
      "loss": -118.5973,
      "step": 33320
    },
    {
      "epoch": 2.6664,
      "grad_norm": 91.9100570678711,
      "learning_rate": 0.00011112,
      "loss": -119.7608,
      "step": 33330
    },
    {
      "epoch": 2.6672000000000002,
      "grad_norm": 93.9936294555664,
      "learning_rate": 0.00011109333333333334,
      "loss": -119.2314,
      "step": 33340
    },
    {
      "epoch": 2.668,
      "grad_norm": 89.81575012207031,
      "learning_rate": 0.00011106666666666668,
      "loss": -119.4399,
      "step": 33350
    },
    {
      "epoch": 2.6688,
      "grad_norm": 99.21803283691406,
      "learning_rate": 0.00011104000000000001,
      "loss": -118.9384,
      "step": 33360
    },
    {
      "epoch": 2.6696,
      "grad_norm": 75.02284240722656,
      "learning_rate": 0.00011101333333333335,
      "loss": -120.0708,
      "step": 33370
    },
    {
      "epoch": 2.6704,
      "grad_norm": 61.96099853515625,
      "learning_rate": 0.00011098666666666667,
      "loss": -118.9634,
      "step": 33380
    },
    {
      "epoch": 2.6712,
      "grad_norm": 70.32200622558594,
      "learning_rate": 0.00011096,
      "loss": -119.0947,
      "step": 33390
    },
    {
      "epoch": 2.672,
      "grad_norm": 47.5335807800293,
      "learning_rate": 0.00011093333333333334,
      "loss": -119.8372,
      "step": 33400
    },
    {
      "epoch": 2.6728,
      "grad_norm": 112.6529769897461,
      "learning_rate": 0.00011090666666666667,
      "loss": -117.9699,
      "step": 33410
    },
    {
      "epoch": 2.6736,
      "grad_norm": 78.83866119384766,
      "learning_rate": 0.00011088000000000001,
      "loss": -118.9775,
      "step": 33420
    },
    {
      "epoch": 2.6744,
      "grad_norm": 80.68726348876953,
      "learning_rate": 0.00011085333333333334,
      "loss": -119.0627,
      "step": 33430
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 83.65363311767578,
      "learning_rate": 0.00011082666666666668,
      "loss": -119.351,
      "step": 33440
    },
    {
      "epoch": 2.676,
      "grad_norm": 84.9096908569336,
      "learning_rate": 0.00011080000000000001,
      "loss": -119.0889,
      "step": 33450
    },
    {
      "epoch": 2.6768,
      "grad_norm": 64.018310546875,
      "learning_rate": 0.00011077333333333333,
      "loss": -119.4123,
      "step": 33460
    },
    {
      "epoch": 2.6776,
      "grad_norm": 121.33079528808594,
      "learning_rate": 0.00011074666666666667,
      "loss": -118.581,
      "step": 33470
    },
    {
      "epoch": 2.6784,
      "grad_norm": 108.4142837524414,
      "learning_rate": 0.00011072,
      "loss": -118.239,
      "step": 33480
    },
    {
      "epoch": 2.6792,
      "grad_norm": 110.78563690185547,
      "learning_rate": 0.00011069333333333334,
      "loss": -118.7133,
      "step": 33490
    },
    {
      "epoch": 2.68,
      "grad_norm": 65.82524108886719,
      "learning_rate": 0.00011066666666666667,
      "loss": -119.465,
      "step": 33500
    },
    {
      "epoch": 2.6808,
      "grad_norm": 64.0053939819336,
      "learning_rate": 0.00011064000000000001,
      "loss": -120.1159,
      "step": 33510
    },
    {
      "epoch": 2.6816,
      "grad_norm": 131.35618591308594,
      "learning_rate": 0.00011061333333333334,
      "loss": -118.9761,
      "step": 33520
    },
    {
      "epoch": 2.6824,
      "grad_norm": 95.47555541992188,
      "learning_rate": 0.00011058666666666668,
      "loss": -119.3373,
      "step": 33530
    },
    {
      "epoch": 2.6832000000000003,
      "grad_norm": 125.82349395751953,
      "learning_rate": 0.00011056,
      "loss": -118.9655,
      "step": 33540
    },
    {
      "epoch": 2.684,
      "grad_norm": 99.79805755615234,
      "learning_rate": 0.00011053333333333333,
      "loss": -118.4364,
      "step": 33550
    },
    {
      "epoch": 2.6848,
      "grad_norm": 55.695953369140625,
      "learning_rate": 0.00011050666666666667,
      "loss": -118.506,
      "step": 33560
    },
    {
      "epoch": 2.6856,
      "grad_norm": 103.21817016601562,
      "learning_rate": 0.00011048,
      "loss": -119.9982,
      "step": 33570
    },
    {
      "epoch": 2.6864,
      "grad_norm": 81.41861724853516,
      "learning_rate": 0.00011045333333333334,
      "loss": -119.9404,
      "step": 33580
    },
    {
      "epoch": 2.6872,
      "grad_norm": 59.115928649902344,
      "learning_rate": 0.00011042666666666668,
      "loss": -119.0522,
      "step": 33590
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 73.79610443115234,
      "learning_rate": 0.00011040000000000001,
      "loss": -117.9215,
      "step": 33600
    },
    {
      "epoch": 2.6888,
      "grad_norm": 82.91142272949219,
      "learning_rate": 0.00011037333333333333,
      "loss": -119.9207,
      "step": 33610
    },
    {
      "epoch": 2.6896,
      "grad_norm": 92.34237670898438,
      "learning_rate": 0.00011034666666666666,
      "loss": -117.8573,
      "step": 33620
    },
    {
      "epoch": 2.6904,
      "grad_norm": 105.0911636352539,
      "learning_rate": 0.00011032,
      "loss": -119.2485,
      "step": 33630
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 69.23753356933594,
      "learning_rate": 0.00011029333333333334,
      "loss": -119.2315,
      "step": 33640
    },
    {
      "epoch": 2.692,
      "grad_norm": 79.55892181396484,
      "learning_rate": 0.00011026666666666667,
      "loss": -118.5659,
      "step": 33650
    },
    {
      "epoch": 2.6928,
      "grad_norm": 125.23723602294922,
      "learning_rate": 0.00011024000000000002,
      "loss": -119.4725,
      "step": 33660
    },
    {
      "epoch": 2.6936,
      "grad_norm": 91.62594604492188,
      "learning_rate": 0.00011021333333333334,
      "loss": -119.8072,
      "step": 33670
    },
    {
      "epoch": 2.6944,
      "grad_norm": 70.27098846435547,
      "learning_rate": 0.00011018666666666669,
      "loss": -119.3385,
      "step": 33680
    },
    {
      "epoch": 2.6952,
      "grad_norm": 142.8961944580078,
      "learning_rate": 0.00011016,
      "loss": -119.7598,
      "step": 33690
    },
    {
      "epoch": 2.6959999999999997,
      "grad_norm": 53.315155029296875,
      "learning_rate": 0.00011013333333333333,
      "loss": -119.5823,
      "step": 33700
    },
    {
      "epoch": 2.6968,
      "grad_norm": 84.37427520751953,
      "learning_rate": 0.00011010666666666667,
      "loss": -119.4652,
      "step": 33710
    },
    {
      "epoch": 2.6976,
      "grad_norm": 77.27253723144531,
      "learning_rate": 0.00011008,
      "loss": -119.108,
      "step": 33720
    },
    {
      "epoch": 2.6984,
      "grad_norm": 85.84280395507812,
      "learning_rate": 0.00011005333333333335,
      "loss": -119.5964,
      "step": 33730
    },
    {
      "epoch": 2.6992000000000003,
      "grad_norm": 95.1036148071289,
      "learning_rate": 0.00011002666666666667,
      "loss": -119.515,
      "step": 33740
    },
    {
      "epoch": 2.7,
      "grad_norm": 175.9142303466797,
      "learning_rate": 0.00011000000000000002,
      "loss": -118.9907,
      "step": 33750
    },
    {
      "epoch": 2.7008,
      "grad_norm": 254.33509826660156,
      "learning_rate": 0.00010997333333333333,
      "loss": -118.2538,
      "step": 33760
    },
    {
      "epoch": 2.7016,
      "grad_norm": 111.2427749633789,
      "learning_rate": 0.00010994666666666666,
      "loss": -119.2977,
      "step": 33770
    },
    {
      "epoch": 2.7024,
      "grad_norm": 121.58106231689453,
      "learning_rate": 0.00010992,
      "loss": -119.2404,
      "step": 33780
    },
    {
      "epoch": 2.7032,
      "grad_norm": 139.22244262695312,
      "learning_rate": 0.00010989333333333333,
      "loss": -119.6911,
      "step": 33790
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 74.54202270507812,
      "learning_rate": 0.00010986666666666668,
      "loss": -118.905,
      "step": 33800
    },
    {
      "epoch": 2.7048,
      "grad_norm": 104.32646179199219,
      "learning_rate": 0.00010984,
      "loss": -119.6286,
      "step": 33810
    },
    {
      "epoch": 2.7056,
      "grad_norm": 77.22782897949219,
      "learning_rate": 0.00010981333333333335,
      "loss": -118.9527,
      "step": 33820
    },
    {
      "epoch": 2.7064,
      "grad_norm": 97.91090393066406,
      "learning_rate": 0.00010978666666666668,
      "loss": -119.4061,
      "step": 33830
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 86.31831359863281,
      "learning_rate": 0.00010975999999999999,
      "loss": -119.2521,
      "step": 33840
    },
    {
      "epoch": 2.708,
      "grad_norm": 135.1433868408203,
      "learning_rate": 0.00010973333333333334,
      "loss": -120.1733,
      "step": 33850
    },
    {
      "epoch": 2.7088,
      "grad_norm": 81.73407745361328,
      "learning_rate": 0.00010970666666666666,
      "loss": -118.9032,
      "step": 33860
    },
    {
      "epoch": 2.7096,
      "grad_norm": 2729.547607421875,
      "learning_rate": 0.00010968000000000001,
      "loss": -99.199,
      "step": 33870
    },
    {
      "epoch": 2.7104,
      "grad_norm": 131.5724334716797,
      "learning_rate": 0.00010965333333333334,
      "loss": -76.0074,
      "step": 33880
    },
    {
      "epoch": 2.7112,
      "grad_norm": 132.87940979003906,
      "learning_rate": 0.00010962666666666668,
      "loss": -88.3995,
      "step": 33890
    },
    {
      "epoch": 2.7119999999999997,
      "grad_norm": 115.20515441894531,
      "learning_rate": 0.00010960000000000001,
      "loss": -100.0708,
      "step": 33900
    },
    {
      "epoch": 2.7128,
      "grad_norm": 70.95479583740234,
      "learning_rate": 0.00010957333333333332,
      "loss": -103.9028,
      "step": 33910
    },
    {
      "epoch": 2.7136,
      "grad_norm": 29.472116470336914,
      "learning_rate": 0.00010954666666666667,
      "loss": -106.3664,
      "step": 33920
    },
    {
      "epoch": 2.7144,
      "grad_norm": 26.431062698364258,
      "learning_rate": 0.00010952,
      "loss": -107.5845,
      "step": 33930
    },
    {
      "epoch": 2.7152,
      "grad_norm": 15.734747886657715,
      "learning_rate": 0.00010949333333333334,
      "loss": -109.0358,
      "step": 33940
    },
    {
      "epoch": 2.716,
      "grad_norm": 23.97408103942871,
      "learning_rate": 0.00010946666666666667,
      "loss": -108.3206,
      "step": 33950
    },
    {
      "epoch": 2.7168,
      "grad_norm": 94.60697174072266,
      "learning_rate": 0.00010944000000000001,
      "loss": -109.1396,
      "step": 33960
    },
    {
      "epoch": 2.7176,
      "grad_norm": 43.625831604003906,
      "learning_rate": 0.00010941333333333335,
      "loss": -108.7033,
      "step": 33970
    },
    {
      "epoch": 2.7184,
      "grad_norm": 13.369926452636719,
      "learning_rate": 0.00010938666666666668,
      "loss": -109.5601,
      "step": 33980
    },
    {
      "epoch": 2.7192,
      "grad_norm": 21.05428695678711,
      "learning_rate": 0.00010936,
      "loss": -111.0182,
      "step": 33990
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 38.149024963378906,
      "learning_rate": 0.00010933333333333333,
      "loss": -110.2617,
      "step": 34000
    },
    {
      "epoch": 2.7208,
      "grad_norm": 11.89834213256836,
      "learning_rate": 0.00010930666666666667,
      "loss": -109.8893,
      "step": 34010
    },
    {
      "epoch": 2.7216,
      "grad_norm": 44.51805877685547,
      "learning_rate": 0.00010928000000000001,
      "loss": -110.0383,
      "step": 34020
    },
    {
      "epoch": 2.7224,
      "grad_norm": 35.454185485839844,
      "learning_rate": 0.00010925333333333334,
      "loss": -111.4612,
      "step": 34030
    },
    {
      "epoch": 2.7232,
      "grad_norm": 46.6077880859375,
      "learning_rate": 0.00010922666666666668,
      "loss": -111.656,
      "step": 34040
    },
    {
      "epoch": 2.724,
      "grad_norm": 33.07834243774414,
      "learning_rate": 0.00010920000000000001,
      "loss": -110.3549,
      "step": 34050
    },
    {
      "epoch": 2.7248,
      "grad_norm": 18.20186424255371,
      "learning_rate": 0.00010917333333333333,
      "loss": -111.2657,
      "step": 34060
    },
    {
      "epoch": 2.7256,
      "grad_norm": 24.193801879882812,
      "learning_rate": 0.00010914666666666667,
      "loss": -112.104,
      "step": 34070
    },
    {
      "epoch": 2.7264,
      "grad_norm": 19.19391441345215,
      "learning_rate": 0.00010912,
      "loss": -111.1741,
      "step": 34080
    },
    {
      "epoch": 2.7272,
      "grad_norm": 27.078784942626953,
      "learning_rate": 0.00010909333333333334,
      "loss": -111.2475,
      "step": 34090
    },
    {
      "epoch": 2.7279999999999998,
      "grad_norm": 28.85565757751465,
      "learning_rate": 0.00010906666666666667,
      "loss": -111.6714,
      "step": 34100
    },
    {
      "epoch": 2.7288,
      "grad_norm": 18.395566940307617,
      "learning_rate": 0.00010904000000000001,
      "loss": -111.3604,
      "step": 34110
    },
    {
      "epoch": 2.7296,
      "grad_norm": 20.944189071655273,
      "learning_rate": 0.00010901333333333334,
      "loss": -111.2217,
      "step": 34120
    },
    {
      "epoch": 2.7304,
      "grad_norm": 22.582284927368164,
      "learning_rate": 0.00010898666666666668,
      "loss": -111.0955,
      "step": 34130
    },
    {
      "epoch": 2.7312,
      "grad_norm": 25.449249267578125,
      "learning_rate": 0.00010896,
      "loss": -110.387,
      "step": 34140
    },
    {
      "epoch": 2.732,
      "grad_norm": 19.780122756958008,
      "learning_rate": 0.00010893333333333333,
      "loss": -110.9566,
      "step": 34150
    },
    {
      "epoch": 2.7328,
      "grad_norm": 22.613859176635742,
      "learning_rate": 0.00010890666666666667,
      "loss": -112.0017,
      "step": 34160
    },
    {
      "epoch": 2.7336,
      "grad_norm": 14.662479400634766,
      "learning_rate": 0.00010888,
      "loss": -110.8215,
      "step": 34170
    },
    {
      "epoch": 2.7344,
      "grad_norm": 23.34748649597168,
      "learning_rate": 0.00010885333333333334,
      "loss": -111.9879,
      "step": 34180
    },
    {
      "epoch": 2.7352,
      "grad_norm": 22.00640106201172,
      "learning_rate": 0.00010882666666666667,
      "loss": -111.983,
      "step": 34190
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 17.551666259765625,
      "learning_rate": 0.00010880000000000002,
      "loss": -110.6542,
      "step": 34200
    },
    {
      "epoch": 2.7368,
      "grad_norm": 22.9989070892334,
      "learning_rate": 0.00010877333333333334,
      "loss": -111.5943,
      "step": 34210
    },
    {
      "epoch": 2.7376,
      "grad_norm": 21.1157169342041,
      "learning_rate": 0.00010874666666666666,
      "loss": -111.4196,
      "step": 34220
    },
    {
      "epoch": 2.7384,
      "grad_norm": 26.126983642578125,
      "learning_rate": 0.00010872,
      "loss": -111.1962,
      "step": 34230
    },
    {
      "epoch": 2.7392,
      "grad_norm": 45.62685012817383,
      "learning_rate": 0.00010869333333333333,
      "loss": -112.3755,
      "step": 34240
    },
    {
      "epoch": 2.74,
      "grad_norm": 23.106664657592773,
      "learning_rate": 0.00010866666666666667,
      "loss": -111.7504,
      "step": 34250
    },
    {
      "epoch": 2.7408,
      "grad_norm": 26.57839584350586,
      "learning_rate": 0.00010864,
      "loss": -111.1025,
      "step": 34260
    },
    {
      "epoch": 2.7416,
      "grad_norm": 34.51123046875,
      "learning_rate": 0.00010861333333333335,
      "loss": -111.2275,
      "step": 34270
    },
    {
      "epoch": 2.7424,
      "grad_norm": 26.493616104125977,
      "learning_rate": 0.00010858666666666668,
      "loss": -111.8047,
      "step": 34280
    },
    {
      "epoch": 2.7432,
      "grad_norm": 25.21246910095215,
      "learning_rate": 0.00010855999999999999,
      "loss": -111.5796,
      "step": 34290
    },
    {
      "epoch": 2.7439999999999998,
      "grad_norm": 55.13578796386719,
      "learning_rate": 0.00010853333333333333,
      "loss": -110.88,
      "step": 34300
    },
    {
      "epoch": 2.7448,
      "grad_norm": 19.979631423950195,
      "learning_rate": 0.00010850666666666666,
      "loss": -110.9951,
      "step": 34310
    },
    {
      "epoch": 2.7456,
      "grad_norm": 20.257722854614258,
      "learning_rate": 0.00010848,
      "loss": -111.2377,
      "step": 34320
    },
    {
      "epoch": 2.7464,
      "grad_norm": 43.98117446899414,
      "learning_rate": 0.00010845333333333333,
      "loss": -110.3852,
      "step": 34330
    },
    {
      "epoch": 2.7472,
      "grad_norm": 27.00227928161621,
      "learning_rate": 0.00010842666666666668,
      "loss": -111.5482,
      "step": 34340
    },
    {
      "epoch": 2.748,
      "grad_norm": 19.765047073364258,
      "learning_rate": 0.00010840000000000002,
      "loss": -112.058,
      "step": 34350
    },
    {
      "epoch": 2.7488,
      "grad_norm": 15.686135292053223,
      "learning_rate": 0.00010837333333333335,
      "loss": -110.0563,
      "step": 34360
    },
    {
      "epoch": 2.7496,
      "grad_norm": 102.54452514648438,
      "learning_rate": 0.00010834666666666666,
      "loss": -111.1957,
      "step": 34370
    },
    {
      "epoch": 2.7504,
      "grad_norm": 13.242594718933105,
      "learning_rate": 0.00010831999999999999,
      "loss": -111.3133,
      "step": 34380
    },
    {
      "epoch": 2.7512,
      "grad_norm": 30.88250160217285,
      "learning_rate": 0.00010829333333333334,
      "loss": -109.8191,
      "step": 34390
    },
    {
      "epoch": 2.752,
      "grad_norm": 29.11227035522461,
      "learning_rate": 0.00010826666666666668,
      "loss": -112.3963,
      "step": 34400
    },
    {
      "epoch": 2.7528,
      "grad_norm": 20.473365783691406,
      "learning_rate": 0.00010824000000000001,
      "loss": -112.0349,
      "step": 34410
    },
    {
      "epoch": 2.7536,
      "grad_norm": 16.396020889282227,
      "learning_rate": 0.00010821333333333335,
      "loss": -109.7895,
      "step": 34420
    },
    {
      "epoch": 2.7544,
      "grad_norm": 35.07292556762695,
      "learning_rate": 0.00010818666666666668,
      "loss": -111.3743,
      "step": 34430
    },
    {
      "epoch": 2.7552,
      "grad_norm": 19.311016082763672,
      "learning_rate": 0.00010816,
      "loss": -110.4499,
      "step": 34440
    },
    {
      "epoch": 2.7560000000000002,
      "grad_norm": 16.772830963134766,
      "learning_rate": 0.00010813333333333334,
      "loss": -111.5965,
      "step": 34450
    },
    {
      "epoch": 2.7568,
      "grad_norm": 26.987503051757812,
      "learning_rate": 0.00010810666666666667,
      "loss": -111.1036,
      "step": 34460
    },
    {
      "epoch": 2.7576,
      "grad_norm": 30.459928512573242,
      "learning_rate": 0.00010808000000000001,
      "loss": -110.7903,
      "step": 34470
    },
    {
      "epoch": 2.7584,
      "grad_norm": 21.791011810302734,
      "learning_rate": 0.00010805333333333334,
      "loss": -110.43,
      "step": 34480
    },
    {
      "epoch": 2.7592,
      "grad_norm": 14.237771987915039,
      "learning_rate": 0.00010802666666666668,
      "loss": -110.5256,
      "step": 34490
    },
    {
      "epoch": 2.76,
      "grad_norm": 75.09526824951172,
      "learning_rate": 0.00010800000000000001,
      "loss": -111.9079,
      "step": 34500
    },
    {
      "epoch": 2.7608,
      "grad_norm": 22.22538185119629,
      "learning_rate": 0.00010797333333333335,
      "loss": -110.9612,
      "step": 34510
    },
    {
      "epoch": 2.7616,
      "grad_norm": 17.291683197021484,
      "learning_rate": 0.00010794666666666667,
      "loss": -111.0518,
      "step": 34520
    },
    {
      "epoch": 2.7624,
      "grad_norm": 32.95223617553711,
      "learning_rate": 0.00010792,
      "loss": -111.4459,
      "step": 34530
    },
    {
      "epoch": 2.7632,
      "grad_norm": 18.644317626953125,
      "learning_rate": 0.00010789333333333334,
      "loss": -110.6906,
      "step": 34540
    },
    {
      "epoch": 2.7640000000000002,
      "grad_norm": 16.554630279541016,
      "learning_rate": 0.00010786666666666667,
      "loss": -111.8519,
      "step": 34550
    },
    {
      "epoch": 2.7648,
      "grad_norm": 16.323274612426758,
      "learning_rate": 0.00010784000000000001,
      "loss": -110.7875,
      "step": 34560
    },
    {
      "epoch": 2.7656,
      "grad_norm": 131.72943115234375,
      "learning_rate": 0.00010781333333333334,
      "loss": -111.2357,
      "step": 34570
    },
    {
      "epoch": 2.7664,
      "grad_norm": 14.795731544494629,
      "learning_rate": 0.00010778666666666668,
      "loss": -111.1329,
      "step": 34580
    },
    {
      "epoch": 2.7672,
      "grad_norm": 19.777103424072266,
      "learning_rate": 0.00010776,
      "loss": -112.2603,
      "step": 34590
    },
    {
      "epoch": 2.768,
      "grad_norm": 17.15874671936035,
      "learning_rate": 0.00010773333333333333,
      "loss": -110.2588,
      "step": 34600
    },
    {
      "epoch": 2.7688,
      "grad_norm": 28.74209976196289,
      "learning_rate": 0.00010770666666666667,
      "loss": -111.7767,
      "step": 34610
    },
    {
      "epoch": 2.7696,
      "grad_norm": 23.490345001220703,
      "learning_rate": 0.00010768,
      "loss": -110.2186,
      "step": 34620
    },
    {
      "epoch": 2.7704,
      "grad_norm": 21.795883178710938,
      "learning_rate": 0.00010765333333333334,
      "loss": -111.7026,
      "step": 34630
    },
    {
      "epoch": 2.7712,
      "grad_norm": 26.675617218017578,
      "learning_rate": 0.00010762666666666667,
      "loss": -110.9378,
      "step": 34640
    },
    {
      "epoch": 2.7720000000000002,
      "grad_norm": 53.058780670166016,
      "learning_rate": 0.00010760000000000001,
      "loss": -112.8971,
      "step": 34650
    },
    {
      "epoch": 2.7728,
      "grad_norm": 40.52642822265625,
      "learning_rate": 0.00010757333333333334,
      "loss": -110.957,
      "step": 34660
    },
    {
      "epoch": 2.7736,
      "grad_norm": 24.88473129272461,
      "learning_rate": 0.00010754666666666666,
      "loss": -111.3032,
      "step": 34670
    },
    {
      "epoch": 2.7744,
      "grad_norm": 51.344581604003906,
      "learning_rate": 0.00010752,
      "loss": -111.6171,
      "step": 34680
    },
    {
      "epoch": 2.7752,
      "grad_norm": 26.17715835571289,
      "learning_rate": 0.00010749333333333333,
      "loss": -111.8042,
      "step": 34690
    },
    {
      "epoch": 2.776,
      "grad_norm": 21.07903289794922,
      "learning_rate": 0.00010746666666666667,
      "loss": -110.622,
      "step": 34700
    },
    {
      "epoch": 2.7768,
      "grad_norm": 25.17034912109375,
      "learning_rate": 0.00010744,
      "loss": -111.1551,
      "step": 34710
    },
    {
      "epoch": 2.7776,
      "grad_norm": 25.8018741607666,
      "learning_rate": 0.00010741333333333334,
      "loss": -111.2749,
      "step": 34720
    },
    {
      "epoch": 2.7784,
      "grad_norm": 44.966373443603516,
      "learning_rate": 0.00010738666666666669,
      "loss": -109.7082,
      "step": 34730
    },
    {
      "epoch": 2.7792,
      "grad_norm": 31.268383026123047,
      "learning_rate": 0.00010736000000000002,
      "loss": -111.3894,
      "step": 34740
    },
    {
      "epoch": 2.7800000000000002,
      "grad_norm": 25.396076202392578,
      "learning_rate": 0.00010733333333333333,
      "loss": -111.8672,
      "step": 34750
    },
    {
      "epoch": 2.7808,
      "grad_norm": 16.543434143066406,
      "learning_rate": 0.00010730666666666666,
      "loss": -111.9272,
      "step": 34760
    },
    {
      "epoch": 2.7816,
      "grad_norm": 47.22941207885742,
      "learning_rate": 0.00010728,
      "loss": -111.4519,
      "step": 34770
    },
    {
      "epoch": 2.7824,
      "grad_norm": 20.411996841430664,
      "learning_rate": 0.00010725333333333335,
      "loss": -111.8733,
      "step": 34780
    },
    {
      "epoch": 2.7832,
      "grad_norm": 41.98002624511719,
      "learning_rate": 0.00010722666666666667,
      "loss": -111.4236,
      "step": 34790
    },
    {
      "epoch": 2.784,
      "grad_norm": 40.51073455810547,
      "learning_rate": 0.00010720000000000002,
      "loss": -110.6062,
      "step": 34800
    },
    {
      "epoch": 2.7848,
      "grad_norm": 38.673789978027344,
      "learning_rate": 0.00010717333333333335,
      "loss": -111.3129,
      "step": 34810
    },
    {
      "epoch": 2.7856,
      "grad_norm": 25.03510284423828,
      "learning_rate": 0.00010714666666666666,
      "loss": -111.0814,
      "step": 34820
    },
    {
      "epoch": 2.7864,
      "grad_norm": 22.985980987548828,
      "learning_rate": 0.00010712,
      "loss": -111.5998,
      "step": 34830
    },
    {
      "epoch": 2.7872,
      "grad_norm": 32.54501724243164,
      "learning_rate": 0.00010709333333333333,
      "loss": -111.5134,
      "step": 34840
    },
    {
      "epoch": 2.7880000000000003,
      "grad_norm": 24.305862426757812,
      "learning_rate": 0.00010706666666666668,
      "loss": -112.0117,
      "step": 34850
    },
    {
      "epoch": 2.7888,
      "grad_norm": 14.525360107421875,
      "learning_rate": 0.00010704,
      "loss": -111.5722,
      "step": 34860
    },
    {
      "epoch": 2.7896,
      "grad_norm": 55.66220474243164,
      "learning_rate": 0.00010701333333333335,
      "loss": -111.0711,
      "step": 34870
    },
    {
      "epoch": 2.7904,
      "grad_norm": 35.87712097167969,
      "learning_rate": 0.00010698666666666668,
      "loss": -111.3905,
      "step": 34880
    },
    {
      "epoch": 2.7912,
      "grad_norm": 26.300395965576172,
      "learning_rate": 0.00010696000000000002,
      "loss": -110.6896,
      "step": 34890
    },
    {
      "epoch": 2.792,
      "grad_norm": 15.901320457458496,
      "learning_rate": 0.00010693333333333333,
      "loss": -112.0512,
      "step": 34900
    },
    {
      "epoch": 2.7927999999999997,
      "grad_norm": 20.986967086791992,
      "learning_rate": 0.00010690666666666666,
      "loss": -112.0725,
      "step": 34910
    },
    {
      "epoch": 2.7936,
      "grad_norm": 15.349853515625,
      "learning_rate": 0.00010688,
      "loss": -112.7314,
      "step": 34920
    },
    {
      "epoch": 2.7944,
      "grad_norm": 52.691749572753906,
      "learning_rate": 0.00010685333333333334,
      "loss": -111.1206,
      "step": 34930
    },
    {
      "epoch": 2.7952,
      "grad_norm": 33.18693542480469,
      "learning_rate": 0.00010682666666666668,
      "loss": -112.0036,
      "step": 34940
    },
    {
      "epoch": 2.7960000000000003,
      "grad_norm": 19.594715118408203,
      "learning_rate": 0.00010680000000000001,
      "loss": -111.5414,
      "step": 34950
    },
    {
      "epoch": 2.7968,
      "grad_norm": 114.5340805053711,
      "learning_rate": 0.00010677333333333335,
      "loss": -111.5453,
      "step": 34960
    },
    {
      "epoch": 2.7976,
      "grad_norm": 34.50437927246094,
      "learning_rate": 0.00010674666666666667,
      "loss": -111.0151,
      "step": 34970
    },
    {
      "epoch": 2.7984,
      "grad_norm": 23.7757625579834,
      "learning_rate": 0.00010672,
      "loss": -111.4103,
      "step": 34980
    },
    {
      "epoch": 2.7992,
      "grad_norm": 14.744833946228027,
      "learning_rate": 0.00010669333333333334,
      "loss": -111.3594,
      "step": 34990
    },
    {
      "epoch": 2.8,
      "grad_norm": 361.8949279785156,
      "learning_rate": 0.00010666666666666667,
      "loss": -111.0399,
      "step": 35000
    },
    {
      "epoch": 2.8007999999999997,
      "grad_norm": 33.91015625,
      "learning_rate": 0.00010664000000000001,
      "loss": -111.5638,
      "step": 35010
    },
    {
      "epoch": 2.8016,
      "grad_norm": 19.61309242248535,
      "learning_rate": 0.00010661333333333334,
      "loss": -110.6169,
      "step": 35020
    },
    {
      "epoch": 2.8024,
      "grad_norm": 28.71331214904785,
      "learning_rate": 0.00010658666666666668,
      "loss": -111.2037,
      "step": 35030
    },
    {
      "epoch": 2.8032,
      "grad_norm": 51.76166915893555,
      "learning_rate": 0.00010656000000000001,
      "loss": -111.4971,
      "step": 35040
    },
    {
      "epoch": 2.8040000000000003,
      "grad_norm": 45.028018951416016,
      "learning_rate": 0.00010653333333333333,
      "loss": -111.3753,
      "step": 35050
    },
    {
      "epoch": 2.8048,
      "grad_norm": 29.53013038635254,
      "learning_rate": 0.00010650666666666667,
      "loss": -111.6685,
      "step": 35060
    },
    {
      "epoch": 2.8056,
      "grad_norm": 32.847530364990234,
      "learning_rate": 0.00010648,
      "loss": -111.2336,
      "step": 35070
    },
    {
      "epoch": 2.8064,
      "grad_norm": 17.821521759033203,
      "learning_rate": 0.00010645333333333334,
      "loss": -110.4915,
      "step": 35080
    },
    {
      "epoch": 2.8072,
      "grad_norm": 30.518251419067383,
      "learning_rate": 0.00010642666666666667,
      "loss": -111.9748,
      "step": 35090
    },
    {
      "epoch": 2.808,
      "grad_norm": 21.61161994934082,
      "learning_rate": 0.00010640000000000001,
      "loss": -111.2155,
      "step": 35100
    },
    {
      "epoch": 2.8087999999999997,
      "grad_norm": 22.381919860839844,
      "learning_rate": 0.00010637333333333334,
      "loss": -112.0612,
      "step": 35110
    },
    {
      "epoch": 2.8096,
      "grad_norm": 26.793928146362305,
      "learning_rate": 0.00010634666666666666,
      "loss": -111.6421,
      "step": 35120
    },
    {
      "epoch": 2.8104,
      "grad_norm": 45.076236724853516,
      "learning_rate": 0.00010632,
      "loss": -111.727,
      "step": 35130
    },
    {
      "epoch": 2.8112,
      "grad_norm": 18.455284118652344,
      "learning_rate": 0.00010629333333333333,
      "loss": -112.0628,
      "step": 35140
    },
    {
      "epoch": 2.8120000000000003,
      "grad_norm": 42.17917251586914,
      "learning_rate": 0.00010626666666666667,
      "loss": -111.1169,
      "step": 35150
    },
    {
      "epoch": 2.8128,
      "grad_norm": 23.353134155273438,
      "learning_rate": 0.00010624000000000001,
      "loss": -111.5351,
      "step": 35160
    },
    {
      "epoch": 2.8136,
      "grad_norm": 38.573020935058594,
      "learning_rate": 0.00010621333333333334,
      "loss": -111.0549,
      "step": 35170
    },
    {
      "epoch": 2.8144,
      "grad_norm": 42.55607604980469,
      "learning_rate": 0.00010618666666666668,
      "loss": -110.4732,
      "step": 35180
    },
    {
      "epoch": 2.8152,
      "grad_norm": 34.43666076660156,
      "learning_rate": 0.00010616000000000001,
      "loss": -111.9563,
      "step": 35190
    },
    {
      "epoch": 2.816,
      "grad_norm": 21.065471649169922,
      "learning_rate": 0.00010613333333333333,
      "loss": -110.9379,
      "step": 35200
    },
    {
      "epoch": 2.8167999999999997,
      "grad_norm": 21.499248504638672,
      "learning_rate": 0.00010610666666666667,
      "loss": -110.7375,
      "step": 35210
    },
    {
      "epoch": 2.8176,
      "grad_norm": 34.20903396606445,
      "learning_rate": 0.00010608,
      "loss": -110.501,
      "step": 35220
    },
    {
      "epoch": 2.8184,
      "grad_norm": 28.787046432495117,
      "learning_rate": 0.00010605333333333334,
      "loss": -110.6059,
      "step": 35230
    },
    {
      "epoch": 2.8192,
      "grad_norm": 53.514503479003906,
      "learning_rate": 0.00010602666666666667,
      "loss": -112.2442,
      "step": 35240
    },
    {
      "epoch": 2.82,
      "grad_norm": 306.1409606933594,
      "learning_rate": 0.00010600000000000002,
      "loss": -111.1281,
      "step": 35250
    },
    {
      "epoch": 2.8208,
      "grad_norm": 23.786890029907227,
      "learning_rate": 0.00010597333333333334,
      "loss": -110.7635,
      "step": 35260
    },
    {
      "epoch": 2.8216,
      "grad_norm": 32.90636444091797,
      "learning_rate": 0.00010594666666666666,
      "loss": -111.0747,
      "step": 35270
    },
    {
      "epoch": 2.8224,
      "grad_norm": 20.040462493896484,
      "learning_rate": 0.00010592,
      "loss": -110.8773,
      "step": 35280
    },
    {
      "epoch": 2.8232,
      "grad_norm": 39.21567153930664,
      "learning_rate": 0.00010589333333333333,
      "loss": -111.4579,
      "step": 35290
    },
    {
      "epoch": 2.824,
      "grad_norm": 18.104862213134766,
      "learning_rate": 0.00010586666666666667,
      "loss": -111.8632,
      "step": 35300
    },
    {
      "epoch": 2.8247999999999998,
      "grad_norm": 22.525617599487305,
      "learning_rate": 0.00010584,
      "loss": -111.9985,
      "step": 35310
    },
    {
      "epoch": 2.8256,
      "grad_norm": 30.806537628173828,
      "learning_rate": 0.00010581333333333335,
      "loss": -110.669,
      "step": 35320
    },
    {
      "epoch": 2.8264,
      "grad_norm": 26.368810653686523,
      "learning_rate": 0.00010578666666666667,
      "loss": -110.9792,
      "step": 35330
    },
    {
      "epoch": 2.8272,
      "grad_norm": 28.63733673095703,
      "learning_rate": 0.00010576000000000002,
      "loss": -111.9855,
      "step": 35340
    },
    {
      "epoch": 2.828,
      "grad_norm": 17.779882431030273,
      "learning_rate": 0.00010573333333333333,
      "loss": -111.1544,
      "step": 35350
    },
    {
      "epoch": 2.8288,
      "grad_norm": 21.141338348388672,
      "learning_rate": 0.00010570666666666666,
      "loss": -110.0757,
      "step": 35360
    },
    {
      "epoch": 2.8296,
      "grad_norm": 43.049835205078125,
      "learning_rate": 0.00010568,
      "loss": -111.0577,
      "step": 35370
    },
    {
      "epoch": 2.8304,
      "grad_norm": 24.40541648864746,
      "learning_rate": 0.00010565333333333333,
      "loss": -111.1112,
      "step": 35380
    },
    {
      "epoch": 2.8312,
      "grad_norm": 27.314910888671875,
      "learning_rate": 0.00010562666666666668,
      "loss": -110.8947,
      "step": 35390
    },
    {
      "epoch": 2.832,
      "grad_norm": 46.3880500793457,
      "learning_rate": 0.0001056,
      "loss": -111.3898,
      "step": 35400
    },
    {
      "epoch": 2.8327999999999998,
      "grad_norm": 27.45153045654297,
      "learning_rate": 0.00010557333333333335,
      "loss": -111.1497,
      "step": 35410
    },
    {
      "epoch": 2.8336,
      "grad_norm": 118.78169250488281,
      "learning_rate": 0.00010554666666666668,
      "loss": -110.8526,
      "step": 35420
    },
    {
      "epoch": 2.8344,
      "grad_norm": 40.348934173583984,
      "learning_rate": 0.00010551999999999999,
      "loss": -111.4219,
      "step": 35430
    },
    {
      "epoch": 2.8352,
      "grad_norm": 20.21917152404785,
      "learning_rate": 0.00010549333333333334,
      "loss": -112.0093,
      "step": 35440
    },
    {
      "epoch": 2.836,
      "grad_norm": 22.520532608032227,
      "learning_rate": 0.00010546666666666666,
      "loss": -111.7527,
      "step": 35450
    },
    {
      "epoch": 2.8368,
      "grad_norm": 29.511457443237305,
      "learning_rate": 0.00010544000000000001,
      "loss": -112.3209,
      "step": 35460
    },
    {
      "epoch": 2.8376,
      "grad_norm": 28.279993057250977,
      "learning_rate": 0.00010541333333333334,
      "loss": -111.2135,
      "step": 35470
    },
    {
      "epoch": 2.8384,
      "grad_norm": 80.2254638671875,
      "learning_rate": 0.00010538666666666668,
      "loss": -111.505,
      "step": 35480
    },
    {
      "epoch": 2.8392,
      "grad_norm": 40.35935974121094,
      "learning_rate": 0.00010536000000000001,
      "loss": -111.9343,
      "step": 35490
    },
    {
      "epoch": 2.84,
      "grad_norm": 17.965499877929688,
      "learning_rate": 0.00010533333333333332,
      "loss": -111.9582,
      "step": 35500
    },
    {
      "epoch": 2.8407999999999998,
      "grad_norm": 20.40177345275879,
      "learning_rate": 0.00010530666666666667,
      "loss": -111.7575,
      "step": 35510
    },
    {
      "epoch": 2.8416,
      "grad_norm": 55.71199417114258,
      "learning_rate": 0.00010528,
      "loss": -110.9935,
      "step": 35520
    },
    {
      "epoch": 2.8424,
      "grad_norm": 65.63511657714844,
      "learning_rate": 0.00010525333333333334,
      "loss": -111.7089,
      "step": 35530
    },
    {
      "epoch": 2.8432,
      "grad_norm": 22.773677825927734,
      "learning_rate": 0.00010522666666666668,
      "loss": -112.2104,
      "step": 35540
    },
    {
      "epoch": 2.844,
      "grad_norm": 30.78895378112793,
      "learning_rate": 0.00010520000000000001,
      "loss": -111.2107,
      "step": 35550
    },
    {
      "epoch": 2.8448,
      "grad_norm": 23.811620712280273,
      "learning_rate": 0.00010517333333333335,
      "loss": -111.2279,
      "step": 35560
    },
    {
      "epoch": 2.8456,
      "grad_norm": 36.515220642089844,
      "learning_rate": 0.00010514666666666668,
      "loss": -111.7166,
      "step": 35570
    },
    {
      "epoch": 2.8464,
      "grad_norm": 170.7016143798828,
      "learning_rate": 0.00010512,
      "loss": -111.852,
      "step": 35580
    },
    {
      "epoch": 2.8472,
      "grad_norm": 42.645145416259766,
      "learning_rate": 0.00010509333333333334,
      "loss": -111.1117,
      "step": 35590
    },
    {
      "epoch": 2.848,
      "grad_norm": 20.22212028503418,
      "learning_rate": 0.00010506666666666667,
      "loss": -110.8817,
      "step": 35600
    },
    {
      "epoch": 2.8487999999999998,
      "grad_norm": 28.015239715576172,
      "learning_rate": 0.00010504000000000001,
      "loss": -111.9946,
      "step": 35610
    },
    {
      "epoch": 2.8496,
      "grad_norm": 38.04438018798828,
      "learning_rate": 0.00010501333333333334,
      "loss": -110.6818,
      "step": 35620
    },
    {
      "epoch": 2.8504,
      "grad_norm": 65.71282196044922,
      "learning_rate": 0.00010498666666666668,
      "loss": -111.5545,
      "step": 35630
    },
    {
      "epoch": 2.8512,
      "grad_norm": 88.3174819946289,
      "learning_rate": 0.00010496000000000001,
      "loss": -111.2445,
      "step": 35640
    },
    {
      "epoch": 2.852,
      "grad_norm": 34.95121765136719,
      "learning_rate": 0.00010493333333333333,
      "loss": -111.5382,
      "step": 35650
    },
    {
      "epoch": 2.8528000000000002,
      "grad_norm": 21.701923370361328,
      "learning_rate": 0.00010490666666666667,
      "loss": -111.3343,
      "step": 35660
    },
    {
      "epoch": 2.8536,
      "grad_norm": 22.854354858398438,
      "learning_rate": 0.00010488,
      "loss": -112.3939,
      "step": 35670
    },
    {
      "epoch": 2.8544,
      "grad_norm": 29.158714294433594,
      "learning_rate": 0.00010485333333333334,
      "loss": -111.7456,
      "step": 35680
    },
    {
      "epoch": 2.8552,
      "grad_norm": 360.2548828125,
      "learning_rate": 0.00010482666666666667,
      "loss": -111.7421,
      "step": 35690
    },
    {
      "epoch": 2.856,
      "grad_norm": 23.210363388061523,
      "learning_rate": 0.00010480000000000001,
      "loss": -111.422,
      "step": 35700
    },
    {
      "epoch": 2.8568,
      "grad_norm": 18.8972110748291,
      "learning_rate": 0.00010477333333333334,
      "loss": -111.6007,
      "step": 35710
    },
    {
      "epoch": 2.8576,
      "grad_norm": 77.13534545898438,
      "learning_rate": 0.00010474666666666668,
      "loss": -111.7142,
      "step": 35720
    },
    {
      "epoch": 2.8584,
      "grad_norm": 29.969173431396484,
      "learning_rate": 0.00010472,
      "loss": -112.1112,
      "step": 35730
    },
    {
      "epoch": 2.8592,
      "grad_norm": 62.20564651489258,
      "learning_rate": 0.00010469333333333333,
      "loss": -111.2137,
      "step": 35740
    },
    {
      "epoch": 2.86,
      "grad_norm": 117.16817474365234,
      "learning_rate": 0.00010466666666666667,
      "loss": -111.2874,
      "step": 35750
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 22.240922927856445,
      "learning_rate": 0.00010464,
      "loss": -111.7585,
      "step": 35760
    },
    {
      "epoch": 2.8616,
      "grad_norm": 322.20062255859375,
      "learning_rate": 0.00010461333333333334,
      "loss": -111.7096,
      "step": 35770
    },
    {
      "epoch": 2.8624,
      "grad_norm": 45.94334030151367,
      "learning_rate": 0.00010458666666666667,
      "loss": -111.4325,
      "step": 35780
    },
    {
      "epoch": 2.8632,
      "grad_norm": 54.97059631347656,
      "learning_rate": 0.00010456000000000002,
      "loss": -111.5405,
      "step": 35790
    },
    {
      "epoch": 2.864,
      "grad_norm": 22.475391387939453,
      "learning_rate": 0.00010453333333333333,
      "loss": -111.2747,
      "step": 35800
    },
    {
      "epoch": 2.8648,
      "grad_norm": 26.307336807250977,
      "learning_rate": 0.00010450666666666666,
      "loss": -111.5187,
      "step": 35810
    },
    {
      "epoch": 2.8656,
      "grad_norm": 51.4494514465332,
      "learning_rate": 0.00010448,
      "loss": -110.7035,
      "step": 35820
    },
    {
      "epoch": 2.8664,
      "grad_norm": 26.876523971557617,
      "learning_rate": 0.00010445333333333333,
      "loss": -111.9191,
      "step": 35830
    },
    {
      "epoch": 2.8672,
      "grad_norm": 27.49065399169922,
      "learning_rate": 0.00010442666666666667,
      "loss": -110.5611,
      "step": 35840
    },
    {
      "epoch": 2.868,
      "grad_norm": 24.88068389892578,
      "learning_rate": 0.0001044,
      "loss": -112.6564,
      "step": 35850
    },
    {
      "epoch": 2.8688000000000002,
      "grad_norm": 53.52610397338867,
      "learning_rate": 0.00010437333333333335,
      "loss": -111.5301,
      "step": 35860
    },
    {
      "epoch": 2.8696,
      "grad_norm": 52.3737907409668,
      "learning_rate": 0.00010434666666666668,
      "loss": -111.9323,
      "step": 35870
    },
    {
      "epoch": 2.8704,
      "grad_norm": 42.09944152832031,
      "learning_rate": 0.00010431999999999999,
      "loss": -111.8989,
      "step": 35880
    },
    {
      "epoch": 2.8712,
      "grad_norm": 25.80337142944336,
      "learning_rate": 0.00010429333333333333,
      "loss": -112.017,
      "step": 35890
    },
    {
      "epoch": 2.872,
      "grad_norm": 28.3740234375,
      "learning_rate": 0.00010426666666666666,
      "loss": -111.1589,
      "step": 35900
    },
    {
      "epoch": 2.8728,
      "grad_norm": 29.733047485351562,
      "learning_rate": 0.00010424,
      "loss": -111.5557,
      "step": 35910
    },
    {
      "epoch": 2.8736,
      "grad_norm": 33.83186340332031,
      "learning_rate": 0.00010421333333333333,
      "loss": -110.1185,
      "step": 35920
    },
    {
      "epoch": 2.8744,
      "grad_norm": 21.697019577026367,
      "learning_rate": 0.00010418666666666668,
      "loss": -112.1509,
      "step": 35930
    },
    {
      "epoch": 2.8752,
      "grad_norm": 231.97520446777344,
      "learning_rate": 0.00010416000000000002,
      "loss": -111.9022,
      "step": 35940
    },
    {
      "epoch": 2.876,
      "grad_norm": 35.33319091796875,
      "learning_rate": 0.00010413333333333335,
      "loss": -110.4597,
      "step": 35950
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 27.24944496154785,
      "learning_rate": 0.00010410666666666666,
      "loss": -111.4822,
      "step": 35960
    },
    {
      "epoch": 2.8776,
      "grad_norm": 50.66510009765625,
      "learning_rate": 0.00010408,
      "loss": -112.221,
      "step": 35970
    },
    {
      "epoch": 2.8784,
      "grad_norm": 35.20659255981445,
      "learning_rate": 0.00010405333333333334,
      "loss": -111.1494,
      "step": 35980
    },
    {
      "epoch": 2.8792,
      "grad_norm": 24.097694396972656,
      "learning_rate": 0.00010402666666666668,
      "loss": -111.5826,
      "step": 35990
    },
    {
      "epoch": 2.88,
      "grad_norm": 17.754756927490234,
      "learning_rate": 0.00010400000000000001,
      "loss": -111.233,
      "step": 36000
    },
    {
      "epoch": 2.8808,
      "grad_norm": 23.53445053100586,
      "learning_rate": 0.00010397333333333335,
      "loss": -110.8674,
      "step": 36010
    },
    {
      "epoch": 2.8816,
      "grad_norm": 24.713211059570312,
      "learning_rate": 0.00010394666666666668,
      "loss": -111.6112,
      "step": 36020
    },
    {
      "epoch": 2.8824,
      "grad_norm": 116.49128723144531,
      "learning_rate": 0.00010392,
      "loss": -110.3521,
      "step": 36030
    },
    {
      "epoch": 2.8832,
      "grad_norm": 27.26331329345703,
      "learning_rate": 0.00010389333333333334,
      "loss": -111.6627,
      "step": 36040
    },
    {
      "epoch": 2.884,
      "grad_norm": 37.88353729248047,
      "learning_rate": 0.00010386666666666667,
      "loss": -111.4018,
      "step": 36050
    },
    {
      "epoch": 2.8848000000000003,
      "grad_norm": 246.4962921142578,
      "learning_rate": 0.00010384000000000001,
      "loss": -111.2079,
      "step": 36060
    },
    {
      "epoch": 2.8856,
      "grad_norm": 21.797819137573242,
      "learning_rate": 0.00010381333333333334,
      "loss": -111.7229,
      "step": 36070
    },
    {
      "epoch": 2.8864,
      "grad_norm": 51.79318618774414,
      "learning_rate": 0.00010378666666666668,
      "loss": -111.1759,
      "step": 36080
    },
    {
      "epoch": 2.8872,
      "grad_norm": 31.106204986572266,
      "learning_rate": 0.00010376000000000001,
      "loss": -110.7761,
      "step": 36090
    },
    {
      "epoch": 2.888,
      "grad_norm": 126.16290283203125,
      "learning_rate": 0.00010373333333333335,
      "loss": -111.6876,
      "step": 36100
    },
    {
      "epoch": 2.8888,
      "grad_norm": 15.641789436340332,
      "learning_rate": 0.00010370666666666667,
      "loss": -111.4629,
      "step": 36110
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 27.46749496459961,
      "learning_rate": 0.00010368,
      "loss": -110.8646,
      "step": 36120
    },
    {
      "epoch": 2.8904,
      "grad_norm": 31.880144119262695,
      "learning_rate": 0.00010365333333333334,
      "loss": -110.3029,
      "step": 36130
    },
    {
      "epoch": 2.8912,
      "grad_norm": 36.6413459777832,
      "learning_rate": 0.00010362666666666667,
      "loss": -111.6939,
      "step": 36140
    },
    {
      "epoch": 2.892,
      "grad_norm": 118.75595092773438,
      "learning_rate": 0.00010360000000000001,
      "loss": -110.7228,
      "step": 36150
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 25.920211791992188,
      "learning_rate": 0.00010357333333333334,
      "loss": -110.3475,
      "step": 36160
    },
    {
      "epoch": 2.8936,
      "grad_norm": 58.60750961303711,
      "learning_rate": 0.00010354666666666668,
      "loss": -111.962,
      "step": 36170
    },
    {
      "epoch": 2.8944,
      "grad_norm": 44.5653076171875,
      "learning_rate": 0.00010352,
      "loss": -110.9483,
      "step": 36180
    },
    {
      "epoch": 2.8952,
      "grad_norm": 28.16143035888672,
      "learning_rate": 0.00010349333333333333,
      "loss": -112.4409,
      "step": 36190
    },
    {
      "epoch": 2.896,
      "grad_norm": 32.91048049926758,
      "learning_rate": 0.00010346666666666667,
      "loss": -111.3887,
      "step": 36200
    },
    {
      "epoch": 2.8968,
      "grad_norm": 40.19087219238281,
      "learning_rate": 0.00010344,
      "loss": -112.0655,
      "step": 36210
    },
    {
      "epoch": 2.8975999999999997,
      "grad_norm": 33.69926071166992,
      "learning_rate": 0.00010341333333333334,
      "loss": -110.6157,
      "step": 36220
    },
    {
      "epoch": 2.8984,
      "grad_norm": 48.99094772338867,
      "learning_rate": 0.00010338666666666667,
      "loss": -112.491,
      "step": 36230
    },
    {
      "epoch": 2.8992,
      "grad_norm": 52.17811584472656,
      "learning_rate": 0.00010336000000000001,
      "loss": -111.4355,
      "step": 36240
    },
    {
      "epoch": 2.9,
      "grad_norm": 34.23202896118164,
      "learning_rate": 0.00010333333333333334,
      "loss": -111.4424,
      "step": 36250
    },
    {
      "epoch": 2.9008000000000003,
      "grad_norm": 84.8005599975586,
      "learning_rate": 0.00010330666666666666,
      "loss": -111.0911,
      "step": 36260
    },
    {
      "epoch": 2.9016,
      "grad_norm": 28.214447021484375,
      "learning_rate": 0.00010328,
      "loss": -111.7423,
      "step": 36270
    },
    {
      "epoch": 2.9024,
      "grad_norm": 25.699756622314453,
      "learning_rate": 0.00010325333333333333,
      "loss": -111.8015,
      "step": 36280
    },
    {
      "epoch": 2.9032,
      "grad_norm": 32.20823287963867,
      "learning_rate": 0.00010322666666666667,
      "loss": -110.3136,
      "step": 36290
    },
    {
      "epoch": 2.904,
      "grad_norm": 34.653472900390625,
      "learning_rate": 0.0001032,
      "loss": -111.0245,
      "step": 36300
    },
    {
      "epoch": 2.9048,
      "grad_norm": 122.35391235351562,
      "learning_rate": 0.00010317333333333334,
      "loss": -111.4536,
      "step": 36310
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 18.73907470703125,
      "learning_rate": 0.00010314666666666669,
      "loss": -112.2415,
      "step": 36320
    },
    {
      "epoch": 2.9064,
      "grad_norm": 51.47618865966797,
      "learning_rate": 0.00010311999999999999,
      "loss": -109.9944,
      "step": 36330
    },
    {
      "epoch": 2.9072,
      "grad_norm": 34.416847229003906,
      "learning_rate": 0.00010309333333333333,
      "loss": -112.0502,
      "step": 36340
    },
    {
      "epoch": 2.908,
      "grad_norm": 25.729089736938477,
      "learning_rate": 0.00010306666666666666,
      "loss": -110.9139,
      "step": 36350
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 32.46250534057617,
      "learning_rate": 0.00010304,
      "loss": -111.7528,
      "step": 36360
    },
    {
      "epoch": 2.9096,
      "grad_norm": 45.374874114990234,
      "learning_rate": 0.00010301333333333335,
      "loss": -111.8706,
      "step": 36370
    },
    {
      "epoch": 2.9104,
      "grad_norm": 37.27233123779297,
      "learning_rate": 0.00010298666666666667,
      "loss": -112.4048,
      "step": 36380
    },
    {
      "epoch": 2.9112,
      "grad_norm": 146.1148223876953,
      "learning_rate": 0.00010296000000000002,
      "loss": -111.6207,
      "step": 36390
    },
    {
      "epoch": 2.912,
      "grad_norm": 37.901485443115234,
      "learning_rate": 0.00010293333333333335,
      "loss": -111.9618,
      "step": 36400
    },
    {
      "epoch": 2.9128,
      "grad_norm": 88.8521499633789,
      "learning_rate": 0.00010290666666666666,
      "loss": -112.1534,
      "step": 36410
    },
    {
      "epoch": 2.9135999999999997,
      "grad_norm": 907.7966918945312,
      "learning_rate": 0.00010288,
      "loss": -112.1917,
      "step": 36420
    },
    {
      "epoch": 2.9144,
      "grad_norm": 25.702335357666016,
      "learning_rate": 0.00010285333333333333,
      "loss": -110.9763,
      "step": 36430
    },
    {
      "epoch": 2.9152,
      "grad_norm": 97.51139068603516,
      "learning_rate": 0.00010282666666666668,
      "loss": -110.7237,
      "step": 36440
    },
    {
      "epoch": 2.916,
      "grad_norm": 59.12751388549805,
      "learning_rate": 0.0001028,
      "loss": -111.5471,
      "step": 36450
    },
    {
      "epoch": 2.9168,
      "grad_norm": 52.30449676513672,
      "learning_rate": 0.00010277333333333335,
      "loss": -110.6139,
      "step": 36460
    },
    {
      "epoch": 2.9176,
      "grad_norm": 33.65835952758789,
      "learning_rate": 0.00010274666666666668,
      "loss": -111.2334,
      "step": 36470
    },
    {
      "epoch": 2.9184,
      "grad_norm": 46.26515579223633,
      "learning_rate": 0.00010271999999999999,
      "loss": -110.9746,
      "step": 36480
    },
    {
      "epoch": 2.9192,
      "grad_norm": 110.72534942626953,
      "learning_rate": 0.00010269333333333333,
      "loss": -111.2576,
      "step": 36490
    },
    {
      "epoch": 2.92,
      "grad_norm": 117.40991973876953,
      "learning_rate": 0.00010266666666666666,
      "loss": -111.4194,
      "step": 36500
    },
    {
      "epoch": 2.9208,
      "grad_norm": 46.954925537109375,
      "learning_rate": 0.00010264,
      "loss": -111.1584,
      "step": 36510
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 159.18878173828125,
      "learning_rate": 0.00010261333333333334,
      "loss": -111.6318,
      "step": 36520
    },
    {
      "epoch": 2.9224,
      "grad_norm": 88.41761779785156,
      "learning_rate": 0.00010258666666666668,
      "loss": -111.7123,
      "step": 36530
    },
    {
      "epoch": 2.9232,
      "grad_norm": 62.00937271118164,
      "learning_rate": 0.00010256000000000001,
      "loss": -111.6839,
      "step": 36540
    },
    {
      "epoch": 2.924,
      "grad_norm": 198.18637084960938,
      "learning_rate": 0.00010253333333333335,
      "loss": -112.1308,
      "step": 36550
    },
    {
      "epoch": 2.9248,
      "grad_norm": 54.750282287597656,
      "learning_rate": 0.00010250666666666667,
      "loss": -110.801,
      "step": 36560
    },
    {
      "epoch": 2.9256,
      "grad_norm": 68.4354248046875,
      "learning_rate": 0.00010248,
      "loss": -111.262,
      "step": 36570
    },
    {
      "epoch": 2.9264,
      "grad_norm": 26.905893325805664,
      "learning_rate": 0.00010245333333333334,
      "loss": -111.3741,
      "step": 36580
    },
    {
      "epoch": 2.9272,
      "grad_norm": 32.53910446166992,
      "learning_rate": 0.00010242666666666667,
      "loss": -111.0048,
      "step": 36590
    },
    {
      "epoch": 2.928,
      "grad_norm": 28.444568634033203,
      "learning_rate": 0.00010240000000000001,
      "loss": -110.9549,
      "step": 36600
    },
    {
      "epoch": 2.9288,
      "grad_norm": 19.012744903564453,
      "learning_rate": 0.00010237333333333334,
      "loss": -112.0692,
      "step": 36610
    },
    {
      "epoch": 2.9295999999999998,
      "grad_norm": 59.26090621948242,
      "learning_rate": 0.00010234666666666668,
      "loss": -111.0833,
      "step": 36620
    },
    {
      "epoch": 2.9304,
      "grad_norm": 125.85365295410156,
      "learning_rate": 0.00010232000000000001,
      "loss": -110.7598,
      "step": 36630
    },
    {
      "epoch": 2.9312,
      "grad_norm": 25.978260040283203,
      "learning_rate": 0.00010229333333333333,
      "loss": -111.2728,
      "step": 36640
    },
    {
      "epoch": 2.932,
      "grad_norm": 30.049081802368164,
      "learning_rate": 0.00010226666666666667,
      "loss": -111.5669,
      "step": 36650
    },
    {
      "epoch": 2.9328,
      "grad_norm": 18.770105361938477,
      "learning_rate": 0.00010224,
      "loss": -111.1029,
      "step": 36660
    },
    {
      "epoch": 2.9336,
      "grad_norm": 55.961971282958984,
      "learning_rate": 0.00010221333333333334,
      "loss": -111.3551,
      "step": 36670
    },
    {
      "epoch": 2.9344,
      "grad_norm": 81.35639190673828,
      "learning_rate": 0.00010218666666666667,
      "loss": -111.4468,
      "step": 36680
    },
    {
      "epoch": 2.9352,
      "grad_norm": 33.388675689697266,
      "learning_rate": 0.00010216000000000001,
      "loss": -111.0155,
      "step": 36690
    },
    {
      "epoch": 2.936,
      "grad_norm": 52.149112701416016,
      "learning_rate": 0.00010213333333333335,
      "loss": -111.7157,
      "step": 36700
    },
    {
      "epoch": 2.9368,
      "grad_norm": 37.02741241455078,
      "learning_rate": 0.00010210666666666666,
      "loss": -111.0139,
      "step": 36710
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 66.69815826416016,
      "learning_rate": 0.00010208,
      "loss": -111.1704,
      "step": 36720
    },
    {
      "epoch": 2.9384,
      "grad_norm": 38.15869903564453,
      "learning_rate": 0.00010205333333333333,
      "loss": -109.8235,
      "step": 36730
    },
    {
      "epoch": 2.9392,
      "grad_norm": 60.99390411376953,
      "learning_rate": 0.00010202666666666667,
      "loss": -111.1699,
      "step": 36740
    },
    {
      "epoch": 2.94,
      "grad_norm": 25.978715896606445,
      "learning_rate": 0.00010200000000000001,
      "loss": -111.2401,
      "step": 36750
    },
    {
      "epoch": 2.9408,
      "grad_norm": 38.905601501464844,
      "learning_rate": 0.00010197333333333334,
      "loss": -111.2082,
      "step": 36760
    },
    {
      "epoch": 2.9416,
      "grad_norm": 54.64927291870117,
      "learning_rate": 0.00010194666666666668,
      "loss": -110.7485,
      "step": 36770
    },
    {
      "epoch": 2.9424,
      "grad_norm": 37.34662628173828,
      "learning_rate": 0.00010192000000000001,
      "loss": -110.8492,
      "step": 36780
    },
    {
      "epoch": 2.9432,
      "grad_norm": 500.45733642578125,
      "learning_rate": 0.00010189333333333333,
      "loss": -111.3144,
      "step": 36790
    },
    {
      "epoch": 2.944,
      "grad_norm": 54.38663101196289,
      "learning_rate": 0.00010186666666666667,
      "loss": -111.7824,
      "step": 36800
    },
    {
      "epoch": 2.9448,
      "grad_norm": 32.894676208496094,
      "learning_rate": 0.00010184,
      "loss": -111.1826,
      "step": 36810
    },
    {
      "epoch": 2.9455999999999998,
      "grad_norm": 53.517520904541016,
      "learning_rate": 0.00010181333333333334,
      "loss": -111.5216,
      "step": 36820
    },
    {
      "epoch": 2.9464,
      "grad_norm": 34.05930709838867,
      "learning_rate": 0.00010178666666666667,
      "loss": -110.0056,
      "step": 36830
    },
    {
      "epoch": 2.9472,
      "grad_norm": 33.061466217041016,
      "learning_rate": 0.00010176000000000002,
      "loss": -111.412,
      "step": 36840
    },
    {
      "epoch": 2.948,
      "grad_norm": 43.760459899902344,
      "learning_rate": 0.00010173333333333334,
      "loss": -112.281,
      "step": 36850
    },
    {
      "epoch": 2.9488,
      "grad_norm": 41.87601089477539,
      "learning_rate": 0.00010170666666666666,
      "loss": -111.6116,
      "step": 36860
    },
    {
      "epoch": 2.9496,
      "grad_norm": 19.969833374023438,
      "learning_rate": 0.00010168,
      "loss": -111.5791,
      "step": 36870
    },
    {
      "epoch": 2.9504,
      "grad_norm": 41.440547943115234,
      "learning_rate": 0.00010165333333333333,
      "loss": -111.3763,
      "step": 36880
    },
    {
      "epoch": 2.9512,
      "grad_norm": 40.37171936035156,
      "learning_rate": 0.00010162666666666667,
      "loss": -111.0234,
      "step": 36890
    },
    {
      "epoch": 2.952,
      "grad_norm": 114.2112808227539,
      "learning_rate": 0.0001016,
      "loss": -110.8247,
      "step": 36900
    },
    {
      "epoch": 2.9528,
      "grad_norm": 130.39244079589844,
      "learning_rate": 0.00010157333333333335,
      "loss": -110.3633,
      "step": 36910
    },
    {
      "epoch": 2.9536,
      "grad_norm": 53.83843994140625,
      "learning_rate": 0.00010154666666666667,
      "loss": -110.3809,
      "step": 36920
    },
    {
      "epoch": 2.9544,
      "grad_norm": 122.14945983886719,
      "learning_rate": 0.00010152000000000002,
      "loss": -110.8543,
      "step": 36930
    },
    {
      "epoch": 2.9552,
      "grad_norm": 46.36836242675781,
      "learning_rate": 0.00010149333333333333,
      "loss": -111.2715,
      "step": 36940
    },
    {
      "epoch": 2.956,
      "grad_norm": 268.8088073730469,
      "learning_rate": 0.00010146666666666666,
      "loss": -110.2639,
      "step": 36950
    },
    {
      "epoch": 2.9568,
      "grad_norm": 149.2962188720703,
      "learning_rate": 0.00010144,
      "loss": -111.2345,
      "step": 36960
    },
    {
      "epoch": 2.9576000000000002,
      "grad_norm": 85.92003631591797,
      "learning_rate": 0.00010141333333333333,
      "loss": -110.9045,
      "step": 36970
    },
    {
      "epoch": 2.9584,
      "grad_norm": 47.52537155151367,
      "learning_rate": 0.00010138666666666668,
      "loss": -109.9669,
      "step": 36980
    },
    {
      "epoch": 2.9592,
      "grad_norm": 51.2007942199707,
      "learning_rate": 0.00010136,
      "loss": -112.5142,
      "step": 36990
    },
    {
      "epoch": 2.96,
      "grad_norm": 42.481689453125,
      "learning_rate": 0.00010133333333333335,
      "loss": -112.2369,
      "step": 37000
    },
    {
      "epoch": 2.9608,
      "grad_norm": 82.43087768554688,
      "learning_rate": 0.00010130666666666666,
      "loss": -111.2183,
      "step": 37010
    },
    {
      "epoch": 2.9616,
      "grad_norm": 41.61753463745117,
      "learning_rate": 0.00010127999999999999,
      "loss": -111.006,
      "step": 37020
    },
    {
      "epoch": 2.9624,
      "grad_norm": 219.1885528564453,
      "learning_rate": 0.00010125333333333334,
      "loss": -110.8015,
      "step": 37030
    },
    {
      "epoch": 2.9632,
      "grad_norm": 30.404550552368164,
      "learning_rate": 0.00010122666666666666,
      "loss": -111.001,
      "step": 37040
    },
    {
      "epoch": 2.964,
      "grad_norm": 38.405250549316406,
      "learning_rate": 0.00010120000000000001,
      "loss": -110.7812,
      "step": 37050
    },
    {
      "epoch": 2.9648,
      "grad_norm": 30.741077423095703,
      "learning_rate": 0.00010117333333333334,
      "loss": -110.9381,
      "step": 37060
    },
    {
      "epoch": 2.9656000000000002,
      "grad_norm": 66.5014877319336,
      "learning_rate": 0.00010114666666666668,
      "loss": -111.9926,
      "step": 37070
    },
    {
      "epoch": 2.9664,
      "grad_norm": 95.43314361572266,
      "learning_rate": 0.00010112000000000002,
      "loss": -111.0568,
      "step": 37080
    },
    {
      "epoch": 2.9672,
      "grad_norm": 68.90599822998047,
      "learning_rate": 0.00010109333333333332,
      "loss": -110.5776,
      "step": 37090
    },
    {
      "epoch": 2.968,
      "grad_norm": 92.68083190917969,
      "learning_rate": 0.00010106666666666667,
      "loss": -110.6242,
      "step": 37100
    },
    {
      "epoch": 2.9688,
      "grad_norm": 51.12502670288086,
      "learning_rate": 0.00010104,
      "loss": -110.6119,
      "step": 37110
    },
    {
      "epoch": 2.9696,
      "grad_norm": 239.92062377929688,
      "learning_rate": 0.00010101333333333334,
      "loss": -111.8062,
      "step": 37120
    },
    {
      "epoch": 2.9704,
      "grad_norm": 38.19293212890625,
      "learning_rate": 0.00010098666666666668,
      "loss": -111.4602,
      "step": 37130
    },
    {
      "epoch": 2.9712,
      "grad_norm": 42.99444580078125,
      "learning_rate": 0.00010096000000000001,
      "loss": -111.4203,
      "step": 37140
    },
    {
      "epoch": 2.972,
      "grad_norm": 34.006324768066406,
      "learning_rate": 0.00010093333333333335,
      "loss": -110.8429,
      "step": 37150
    },
    {
      "epoch": 2.9728,
      "grad_norm": 52.742130279541016,
      "learning_rate": 0.00010090666666666665,
      "loss": -110.232,
      "step": 37160
    },
    {
      "epoch": 2.9736000000000002,
      "grad_norm": 64.330078125,
      "learning_rate": 0.00010088,
      "loss": -111.0199,
      "step": 37170
    },
    {
      "epoch": 2.9744,
      "grad_norm": 40.3159065246582,
      "learning_rate": 0.00010085333333333334,
      "loss": -111.2873,
      "step": 37180
    },
    {
      "epoch": 2.9752,
      "grad_norm": 133.56607055664062,
      "learning_rate": 0.00010082666666666667,
      "loss": -111.1627,
      "step": 37190
    },
    {
      "epoch": 2.976,
      "grad_norm": 69.4279556274414,
      "learning_rate": 0.00010080000000000001,
      "loss": -110.4467,
      "step": 37200
    },
    {
      "epoch": 2.9768,
      "grad_norm": 47.09731674194336,
      "learning_rate": 0.00010077333333333334,
      "loss": -111.4103,
      "step": 37210
    },
    {
      "epoch": 2.9776,
      "grad_norm": 65.41847229003906,
      "learning_rate": 0.00010074666666666668,
      "loss": -110.9328,
      "step": 37220
    },
    {
      "epoch": 2.9784,
      "grad_norm": 258.1713562011719,
      "learning_rate": 0.00010072000000000001,
      "loss": -111.3412,
      "step": 37230
    },
    {
      "epoch": 2.9792,
      "grad_norm": 180.3016357421875,
      "learning_rate": 0.00010069333333333333,
      "loss": -111.34,
      "step": 37240
    },
    {
      "epoch": 2.98,
      "grad_norm": 108.67158508300781,
      "learning_rate": 0.00010066666666666667,
      "loss": -111.275,
      "step": 37250
    },
    {
      "epoch": 2.9808,
      "grad_norm": 27.188323974609375,
      "learning_rate": 0.00010064,
      "loss": -111.7191,
      "step": 37260
    },
    {
      "epoch": 2.9816000000000003,
      "grad_norm": 43.04391860961914,
      "learning_rate": 0.00010061333333333334,
      "loss": -111.9725,
      "step": 37270
    },
    {
      "epoch": 2.9824,
      "grad_norm": 59.366676330566406,
      "learning_rate": 0.00010058666666666667,
      "loss": -110.3077,
      "step": 37280
    },
    {
      "epoch": 2.9832,
      "grad_norm": 49.75592041015625,
      "learning_rate": 0.00010056000000000001,
      "loss": -111.1204,
      "step": 37290
    },
    {
      "epoch": 2.984,
      "grad_norm": 45.64471435546875,
      "learning_rate": 0.00010053333333333334,
      "loss": -110.4374,
      "step": 37300
    },
    {
      "epoch": 2.9848,
      "grad_norm": 38.999000549316406,
      "learning_rate": 0.00010050666666666668,
      "loss": -111.0365,
      "step": 37310
    },
    {
      "epoch": 2.9856,
      "grad_norm": 88.1368408203125,
      "learning_rate": 0.00010048,
      "loss": -112.0049,
      "step": 37320
    },
    {
      "epoch": 2.9864,
      "grad_norm": 46.98225402832031,
      "learning_rate": 0.00010045333333333333,
      "loss": -111.6293,
      "step": 37330
    },
    {
      "epoch": 2.9872,
      "grad_norm": 30.635955810546875,
      "learning_rate": 0.00010042666666666667,
      "loss": -111.1988,
      "step": 37340
    },
    {
      "epoch": 2.988,
      "grad_norm": 50.793670654296875,
      "learning_rate": 0.0001004,
      "loss": -111.5117,
      "step": 37350
    },
    {
      "epoch": 2.9888,
      "grad_norm": 38.316650390625,
      "learning_rate": 0.00010037333333333334,
      "loss": -111.723,
      "step": 37360
    },
    {
      "epoch": 2.9896000000000003,
      "grad_norm": 29.474390029907227,
      "learning_rate": 0.00010034666666666667,
      "loss": -111.6523,
      "step": 37370
    },
    {
      "epoch": 2.9904,
      "grad_norm": 72.33367156982422,
      "learning_rate": 0.00010032000000000002,
      "loss": -110.7632,
      "step": 37380
    },
    {
      "epoch": 2.9912,
      "grad_norm": 56.31361389160156,
      "learning_rate": 0.00010029333333333333,
      "loss": -111.7177,
      "step": 37390
    },
    {
      "epoch": 2.992,
      "grad_norm": 81.36941528320312,
      "learning_rate": 0.00010026666666666666,
      "loss": -111.0749,
      "step": 37400
    },
    {
      "epoch": 2.9928,
      "grad_norm": 81.3812484741211,
      "learning_rate": 0.00010024,
      "loss": -111.3495,
      "step": 37410
    },
    {
      "epoch": 2.9936,
      "grad_norm": 52.099300384521484,
      "learning_rate": 0.00010021333333333333,
      "loss": -110.6237,
      "step": 37420
    },
    {
      "epoch": 2.9943999999999997,
      "grad_norm": 34.31340408325195,
      "learning_rate": 0.00010018666666666667,
      "loss": -110.6653,
      "step": 37430
    },
    {
      "epoch": 2.9952,
      "grad_norm": 48.25419998168945,
      "learning_rate": 0.00010016,
      "loss": -110.3063,
      "step": 37440
    },
    {
      "epoch": 2.996,
      "grad_norm": 25.90799903869629,
      "learning_rate": 0.00010013333333333335,
      "loss": -111.3302,
      "step": 37450
    },
    {
      "epoch": 2.9968,
      "grad_norm": 31.34112548828125,
      "learning_rate": 0.00010010666666666669,
      "loss": -111.2476,
      "step": 37460
    },
    {
      "epoch": 2.9976000000000003,
      "grad_norm": 38.92439270019531,
      "learning_rate": 0.00010007999999999999,
      "loss": -111.6155,
      "step": 37470
    },
    {
      "epoch": 2.9984,
      "grad_norm": 44.60199737548828,
      "learning_rate": 0.00010005333333333333,
      "loss": -111.0897,
      "step": 37480
    },
    {
      "epoch": 2.9992,
      "grad_norm": 29.482669830322266,
      "learning_rate": 0.00010002666666666666,
      "loss": -111.3039,
      "step": 37490
    },
    {
      "epoch": 3.0,
      "grad_norm": 36.68775939941406,
      "learning_rate": 0.0001,
      "loss": -110.3845,
      "step": 37500
    },
    {
      "epoch": 3.0008,
      "grad_norm": 95.37371826171875,
      "learning_rate": 9.997333333333335e-05,
      "loss": -111.1108,
      "step": 37510
    },
    {
      "epoch": 3.0016,
      "grad_norm": 28.974279403686523,
      "learning_rate": 9.994666666666666e-05,
      "loss": -111.1245,
      "step": 37520
    },
    {
      "epoch": 3.0024,
      "grad_norm": 81.7844009399414,
      "learning_rate": 9.992e-05,
      "loss": -110.5207,
      "step": 37530
    },
    {
      "epoch": 3.0032,
      "grad_norm": 29.204280853271484,
      "learning_rate": 9.989333333333333e-05,
      "loss": -110.728,
      "step": 37540
    },
    {
      "epoch": 3.004,
      "grad_norm": 45.685768127441406,
      "learning_rate": 9.986666666666668e-05,
      "loss": -111.7371,
      "step": 37550
    },
    {
      "epoch": 3.0048,
      "grad_norm": 132.2301788330078,
      "learning_rate": 9.984e-05,
      "loss": -111.9544,
      "step": 37560
    },
    {
      "epoch": 3.0056,
      "grad_norm": 45.66739273071289,
      "learning_rate": 9.981333333333334e-05,
      "loss": -111.8765,
      "step": 37570
    },
    {
      "epoch": 3.0064,
      "grad_norm": 46.90226745605469,
      "learning_rate": 9.978666666666668e-05,
      "loss": -111.2815,
      "step": 37580
    },
    {
      "epoch": 3.0072,
      "grad_norm": 67.65139770507812,
      "learning_rate": 9.976000000000001e-05,
      "loss": -111.1706,
      "step": 37590
    },
    {
      "epoch": 3.008,
      "grad_norm": 143.2154083251953,
      "learning_rate": 9.973333333333334e-05,
      "loss": -110.8697,
      "step": 37600
    },
    {
      "epoch": 3.0088,
      "grad_norm": 41.104129791259766,
      "learning_rate": 9.970666666666667e-05,
      "loss": -110.3791,
      "step": 37610
    },
    {
      "epoch": 3.0096,
      "grad_norm": 56.133609771728516,
      "learning_rate": 9.968000000000001e-05,
      "loss": -110.9073,
      "step": 37620
    },
    {
      "epoch": 3.0104,
      "grad_norm": 56.40702438354492,
      "learning_rate": 9.965333333333334e-05,
      "loss": -110.7818,
      "step": 37630
    },
    {
      "epoch": 3.0112,
      "grad_norm": 54.52042007446289,
      "learning_rate": 9.962666666666667e-05,
      "loss": -111.4556,
      "step": 37640
    },
    {
      "epoch": 3.012,
      "grad_norm": 44.32773208618164,
      "learning_rate": 9.960000000000001e-05,
      "loss": -111.1676,
      "step": 37650
    },
    {
      "epoch": 3.0128,
      "grad_norm": 36.22444534301758,
      "learning_rate": 9.957333333333334e-05,
      "loss": -110.4947,
      "step": 37660
    },
    {
      "epoch": 3.0136,
      "grad_norm": 55.608306884765625,
      "learning_rate": 9.954666666666668e-05,
      "loss": -110.9757,
      "step": 37670
    },
    {
      "epoch": 3.0144,
      "grad_norm": 256.64007568359375,
      "learning_rate": 9.952e-05,
      "loss": -111.2374,
      "step": 37680
    },
    {
      "epoch": 3.0152,
      "grad_norm": 35.167762756347656,
      "learning_rate": 9.949333333333334e-05,
      "loss": -111.4961,
      "step": 37690
    },
    {
      "epoch": 3.016,
      "grad_norm": 67.85919952392578,
      "learning_rate": 9.946666666666668e-05,
      "loss": -110.7078,
      "step": 37700
    },
    {
      "epoch": 3.0168,
      "grad_norm": 33.33626174926758,
      "learning_rate": 9.944e-05,
      "loss": -109.9656,
      "step": 37710
    },
    {
      "epoch": 3.0176,
      "grad_norm": 126.8753662109375,
      "learning_rate": 9.941333333333334e-05,
      "loss": -111.2432,
      "step": 37720
    },
    {
      "epoch": 3.0184,
      "grad_norm": 60.43584442138672,
      "learning_rate": 9.938666666666667e-05,
      "loss": -111.2358,
      "step": 37730
    },
    {
      "epoch": 3.0192,
      "grad_norm": 44.84970474243164,
      "learning_rate": 9.936000000000001e-05,
      "loss": -110.8251,
      "step": 37740
    },
    {
      "epoch": 3.02,
      "grad_norm": 124.02538299560547,
      "learning_rate": 9.933333333333334e-05,
      "loss": -112.0517,
      "step": 37750
    },
    {
      "epoch": 3.0208,
      "grad_norm": 57.74968719482422,
      "learning_rate": 9.930666666666667e-05,
      "loss": -111.33,
      "step": 37760
    },
    {
      "epoch": 3.0216,
      "grad_norm": 37.03621292114258,
      "learning_rate": 9.928000000000001e-05,
      "loss": -110.9761,
      "step": 37770
    },
    {
      "epoch": 3.0224,
      "grad_norm": 19.53929328918457,
      "learning_rate": 9.925333333333334e-05,
      "loss": -109.7533,
      "step": 37780
    },
    {
      "epoch": 3.0232,
      "grad_norm": 28.97230339050293,
      "learning_rate": 9.922666666666667e-05,
      "loss": -110.6685,
      "step": 37790
    },
    {
      "epoch": 3.024,
      "grad_norm": 40.20313262939453,
      "learning_rate": 9.92e-05,
      "loss": -110.1799,
      "step": 37800
    },
    {
      "epoch": 3.0248,
      "grad_norm": 34.2642936706543,
      "learning_rate": 9.917333333333334e-05,
      "loss": -111.7681,
      "step": 37810
    },
    {
      "epoch": 3.0256,
      "grad_norm": 147.14332580566406,
      "learning_rate": 9.914666666666667e-05,
      "loss": -110.2435,
      "step": 37820
    },
    {
      "epoch": 3.0264,
      "grad_norm": 47.9755744934082,
      "learning_rate": 9.912e-05,
      "loss": -111.1605,
      "step": 37830
    },
    {
      "epoch": 3.0272,
      "grad_norm": 105.52275085449219,
      "learning_rate": 9.909333333333334e-05,
      "loss": -111.3692,
      "step": 37840
    },
    {
      "epoch": 3.028,
      "grad_norm": 35.461490631103516,
      "learning_rate": 9.906666666666667e-05,
      "loss": -110.8798,
      "step": 37850
    },
    {
      "epoch": 3.0288,
      "grad_norm": 57.609867095947266,
      "learning_rate": 9.904e-05,
      "loss": -111.6293,
      "step": 37860
    },
    {
      "epoch": 3.0296,
      "grad_norm": 74.3583984375,
      "learning_rate": 9.901333333333333e-05,
      "loss": -110.5258,
      "step": 37870
    },
    {
      "epoch": 3.0304,
      "grad_norm": 42.39530563354492,
      "learning_rate": 9.898666666666667e-05,
      "loss": -110.9692,
      "step": 37880
    },
    {
      "epoch": 3.0312,
      "grad_norm": 35.44138717651367,
      "learning_rate": 9.896000000000001e-05,
      "loss": -111.6898,
      "step": 37890
    },
    {
      "epoch": 3.032,
      "grad_norm": 102.29434204101562,
      "learning_rate": 9.893333333333333e-05,
      "loss": -110.2934,
      "step": 37900
    },
    {
      "epoch": 3.0328,
      "grad_norm": 43.97637176513672,
      "learning_rate": 9.890666666666667e-05,
      "loss": -111.6653,
      "step": 37910
    },
    {
      "epoch": 3.0336,
      "grad_norm": 28.68475914001465,
      "learning_rate": 9.888e-05,
      "loss": -111.4304,
      "step": 37920
    },
    {
      "epoch": 3.0344,
      "grad_norm": 123.27152252197266,
      "learning_rate": 9.885333333333334e-05,
      "loss": -110.7871,
      "step": 37930
    },
    {
      "epoch": 3.0352,
      "grad_norm": 218.94674682617188,
      "learning_rate": 9.882666666666667e-05,
      "loss": -111.4643,
      "step": 37940
    },
    {
      "epoch": 3.036,
      "grad_norm": 36.47709655761719,
      "learning_rate": 9.88e-05,
      "loss": -111.3103,
      "step": 37950
    },
    {
      "epoch": 3.0368,
      "grad_norm": 58.24055480957031,
      "learning_rate": 9.877333333333335e-05,
      "loss": -112.3792,
      "step": 37960
    },
    {
      "epoch": 3.0376,
      "grad_norm": 46.497135162353516,
      "learning_rate": 9.874666666666667e-05,
      "loss": -111.0512,
      "step": 37970
    },
    {
      "epoch": 3.0384,
      "grad_norm": 123.67621612548828,
      "learning_rate": 9.872e-05,
      "loss": -112.6719,
      "step": 37980
    },
    {
      "epoch": 3.0392,
      "grad_norm": 29.693639755249023,
      "learning_rate": 9.869333333333333e-05,
      "loss": -110.8122,
      "step": 37990
    },
    {
      "epoch": 3.04,
      "grad_norm": 101.18314361572266,
      "learning_rate": 9.866666666666668e-05,
      "loss": -111.9502,
      "step": 38000
    },
    {
      "epoch": 3.0408,
      "grad_norm": 49.102542877197266,
      "learning_rate": 9.864e-05,
      "loss": -111.2314,
      "step": 38010
    },
    {
      "epoch": 3.0416,
      "grad_norm": 46.689022064208984,
      "learning_rate": 9.861333333333333e-05,
      "loss": -111.1126,
      "step": 38020
    },
    {
      "epoch": 3.0424,
      "grad_norm": 43.81745910644531,
      "learning_rate": 9.858666666666668e-05,
      "loss": -111.5707,
      "step": 38030
    },
    {
      "epoch": 3.0432,
      "grad_norm": 136.80670166015625,
      "learning_rate": 9.856e-05,
      "loss": -110.7009,
      "step": 38040
    },
    {
      "epoch": 3.044,
      "grad_norm": 109.22090911865234,
      "learning_rate": 9.853333333333333e-05,
      "loss": -110.4643,
      "step": 38050
    },
    {
      "epoch": 3.0448,
      "grad_norm": 24.101741790771484,
      "learning_rate": 9.850666666666666e-05,
      "loss": -110.8368,
      "step": 38060
    },
    {
      "epoch": 3.0456,
      "grad_norm": 57.95691680908203,
      "learning_rate": 9.848e-05,
      "loss": -111.5496,
      "step": 38070
    },
    {
      "epoch": 3.0464,
      "grad_norm": 34.91478729248047,
      "learning_rate": 9.845333333333335e-05,
      "loss": -111.7939,
      "step": 38080
    },
    {
      "epoch": 3.0472,
      "grad_norm": 219.10081481933594,
      "learning_rate": 9.842666666666666e-05,
      "loss": -111.3256,
      "step": 38090
    },
    {
      "epoch": 3.048,
      "grad_norm": 151.0727081298828,
      "learning_rate": 9.84e-05,
      "loss": -111.4084,
      "step": 38100
    },
    {
      "epoch": 3.0488,
      "grad_norm": 49.699222564697266,
      "learning_rate": 9.837333333333334e-05,
      "loss": -110.8536,
      "step": 38110
    },
    {
      "epoch": 3.0496,
      "grad_norm": 58.39192581176758,
      "learning_rate": 9.834666666666668e-05,
      "loss": -111.2754,
      "step": 38120
    },
    {
      "epoch": 3.0504,
      "grad_norm": 231.58792114257812,
      "learning_rate": 9.832000000000001e-05,
      "loss": -111.4067,
      "step": 38130
    },
    {
      "epoch": 3.0512,
      "grad_norm": 126.91016387939453,
      "learning_rate": 9.829333333333334e-05,
      "loss": -111.4377,
      "step": 38140
    },
    {
      "epoch": 3.052,
      "grad_norm": 64.2179183959961,
      "learning_rate": 9.826666666666668e-05,
      "loss": -112.0429,
      "step": 38150
    },
    {
      "epoch": 3.0528,
      "grad_norm": 63.533145904541016,
      "learning_rate": 9.824000000000001e-05,
      "loss": -110.9119,
      "step": 38160
    },
    {
      "epoch": 3.0536,
      "grad_norm": 25.946435928344727,
      "learning_rate": 9.821333333333334e-05,
      "loss": -110.7937,
      "step": 38170
    },
    {
      "epoch": 3.0544,
      "grad_norm": 47.8931884765625,
      "learning_rate": 9.818666666666667e-05,
      "loss": -111.3113,
      "step": 38180
    },
    {
      "epoch": 3.0552,
      "grad_norm": 77.6565170288086,
      "learning_rate": 9.816000000000001e-05,
      "loss": -111.9535,
      "step": 38190
    },
    {
      "epoch": 3.056,
      "grad_norm": 30.849966049194336,
      "learning_rate": 9.813333333333334e-05,
      "loss": -110.598,
      "step": 38200
    },
    {
      "epoch": 3.0568,
      "grad_norm": 59.97064971923828,
      "learning_rate": 9.810666666666667e-05,
      "loss": -111.3325,
      "step": 38210
    },
    {
      "epoch": 3.0576,
      "grad_norm": 67.44520568847656,
      "learning_rate": 9.808000000000001e-05,
      "loss": -111.6595,
      "step": 38220
    },
    {
      "epoch": 3.0584,
      "grad_norm": 26.48786163330078,
      "learning_rate": 9.805333333333334e-05,
      "loss": -110.7091,
      "step": 38230
    },
    {
      "epoch": 3.0592,
      "grad_norm": 31.168317794799805,
      "learning_rate": 9.802666666666667e-05,
      "loss": -111.4431,
      "step": 38240
    },
    {
      "epoch": 3.06,
      "grad_norm": 23.086599349975586,
      "learning_rate": 9.8e-05,
      "loss": -111.2137,
      "step": 38250
    },
    {
      "epoch": 3.0608,
      "grad_norm": 84.00971984863281,
      "learning_rate": 9.797333333333334e-05,
      "loss": -111.3623,
      "step": 38260
    },
    {
      "epoch": 3.0616,
      "grad_norm": 147.87391662597656,
      "learning_rate": 9.794666666666668e-05,
      "loss": -111.4155,
      "step": 38270
    },
    {
      "epoch": 3.0624,
      "grad_norm": 34.10130310058594,
      "learning_rate": 9.792e-05,
      "loss": -110.6869,
      "step": 38280
    },
    {
      "epoch": 3.0632,
      "grad_norm": 59.825992584228516,
      "learning_rate": 9.789333333333334e-05,
      "loss": -110.9099,
      "step": 38290
    },
    {
      "epoch": 3.064,
      "grad_norm": 34.4818115234375,
      "learning_rate": 9.786666666666667e-05,
      "loss": -110.7921,
      "step": 38300
    },
    {
      "epoch": 3.0648,
      "grad_norm": 46.5951042175293,
      "learning_rate": 9.784000000000001e-05,
      "loss": -110.8462,
      "step": 38310
    },
    {
      "epoch": 3.0656,
      "grad_norm": 69.95519256591797,
      "learning_rate": 9.781333333333334e-05,
      "loss": -109.8911,
      "step": 38320
    },
    {
      "epoch": 3.0664,
      "grad_norm": 53.10207748413086,
      "learning_rate": 9.778666666666667e-05,
      "loss": -110.3585,
      "step": 38330
    },
    {
      "epoch": 3.0672,
      "grad_norm": 115.49017333984375,
      "learning_rate": 9.776000000000001e-05,
      "loss": -109.9997,
      "step": 38340
    },
    {
      "epoch": 3.068,
      "grad_norm": 100.41004180908203,
      "learning_rate": 9.773333333333334e-05,
      "loss": -111.5327,
      "step": 38350
    },
    {
      "epoch": 3.0688,
      "grad_norm": 129.58688354492188,
      "learning_rate": 9.770666666666667e-05,
      "loss": -111.71,
      "step": 38360
    },
    {
      "epoch": 3.0696,
      "grad_norm": 69.57706451416016,
      "learning_rate": 9.768e-05,
      "loss": -110.7757,
      "step": 38370
    },
    {
      "epoch": 3.0704,
      "grad_norm": 99.4894027709961,
      "learning_rate": 9.765333333333334e-05,
      "loss": -111.0552,
      "step": 38380
    },
    {
      "epoch": 3.0712,
      "grad_norm": 77.51814270019531,
      "learning_rate": 9.762666666666667e-05,
      "loss": -111.2815,
      "step": 38390
    },
    {
      "epoch": 3.072,
      "grad_norm": 27.052379608154297,
      "learning_rate": 9.76e-05,
      "loss": -111.4399,
      "step": 38400
    },
    {
      "epoch": 3.0728,
      "grad_norm": 22.772741317749023,
      "learning_rate": 9.757333333333334e-05,
      "loss": -111.7213,
      "step": 38410
    },
    {
      "epoch": 3.0736,
      "grad_norm": 35.74822998046875,
      "learning_rate": 9.754666666666667e-05,
      "loss": -111.1595,
      "step": 38420
    },
    {
      "epoch": 3.0744,
      "grad_norm": 52.475929260253906,
      "learning_rate": 9.752e-05,
      "loss": -112.0262,
      "step": 38430
    },
    {
      "epoch": 3.0752,
      "grad_norm": 68.30986785888672,
      "learning_rate": 9.749333333333333e-05,
      "loss": -111.0149,
      "step": 38440
    },
    {
      "epoch": 3.076,
      "grad_norm": 295.1790466308594,
      "learning_rate": 9.746666666666667e-05,
      "loss": -110.4527,
      "step": 38450
    },
    {
      "epoch": 3.0768,
      "grad_norm": 33.46072006225586,
      "learning_rate": 9.744000000000002e-05,
      "loss": -110.799,
      "step": 38460
    },
    {
      "epoch": 3.0776,
      "grad_norm": 29.052949905395508,
      "learning_rate": 9.741333333333333e-05,
      "loss": -111.572,
      "step": 38470
    },
    {
      "epoch": 3.0784,
      "grad_norm": 309.3841552734375,
      "learning_rate": 9.738666666666667e-05,
      "loss": -111.1683,
      "step": 38480
    },
    {
      "epoch": 3.0792,
      "grad_norm": 51.04974365234375,
      "learning_rate": 9.736e-05,
      "loss": -111.3239,
      "step": 38490
    },
    {
      "epoch": 3.08,
      "grad_norm": 79.78987884521484,
      "learning_rate": 9.733333333333335e-05,
      "loss": -111.7662,
      "step": 38500
    },
    {
      "epoch": 3.0808,
      "grad_norm": 29.786113739013672,
      "learning_rate": 9.730666666666667e-05,
      "loss": -110.8755,
      "step": 38510
    },
    {
      "epoch": 3.0816,
      "grad_norm": 63.700721740722656,
      "learning_rate": 9.728e-05,
      "loss": -111.1171,
      "step": 38520
    },
    {
      "epoch": 3.0824,
      "grad_norm": 18.520835876464844,
      "learning_rate": 9.725333333333335e-05,
      "loss": -110.8477,
      "step": 38530
    },
    {
      "epoch": 3.0832,
      "grad_norm": 31.5532283782959,
      "learning_rate": 9.722666666666666e-05,
      "loss": -111.1478,
      "step": 38540
    },
    {
      "epoch": 3.084,
      "grad_norm": 86.61561584472656,
      "learning_rate": 9.72e-05,
      "loss": -111.201,
      "step": 38550
    },
    {
      "epoch": 3.0848,
      "grad_norm": 305.8714904785156,
      "learning_rate": 9.717333333333333e-05,
      "loss": -111.698,
      "step": 38560
    },
    {
      "epoch": 3.0856,
      "grad_norm": 57.058815002441406,
      "learning_rate": 9.714666666666668e-05,
      "loss": -110.8859,
      "step": 38570
    },
    {
      "epoch": 3.0864,
      "grad_norm": 36.48229217529297,
      "learning_rate": 9.712e-05,
      "loss": -111.156,
      "step": 38580
    },
    {
      "epoch": 3.0872,
      "grad_norm": 52.302574157714844,
      "learning_rate": 9.709333333333333e-05,
      "loss": -110.752,
      "step": 38590
    },
    {
      "epoch": 3.088,
      "grad_norm": 116.41546630859375,
      "learning_rate": 9.706666666666668e-05,
      "loss": -111.5992,
      "step": 38600
    },
    {
      "epoch": 3.0888,
      "grad_norm": 25.630897521972656,
      "learning_rate": 9.704e-05,
      "loss": -111.2971,
      "step": 38610
    },
    {
      "epoch": 3.0896,
      "grad_norm": 103.01065826416016,
      "learning_rate": 9.701333333333334e-05,
      "loss": -111.1616,
      "step": 38620
    },
    {
      "epoch": 3.0904,
      "grad_norm": 74.0114974975586,
      "learning_rate": 9.698666666666666e-05,
      "loss": -110.8303,
      "step": 38630
    },
    {
      "epoch": 3.0912,
      "grad_norm": 28.80263328552246,
      "learning_rate": 9.696000000000001e-05,
      "loss": -111.1324,
      "step": 38640
    },
    {
      "epoch": 3.092,
      "grad_norm": 41.3414192199707,
      "learning_rate": 9.693333333333335e-05,
      "loss": -110.7269,
      "step": 38650
    },
    {
      "epoch": 3.0928,
      "grad_norm": 65.37491607666016,
      "learning_rate": 9.690666666666666e-05,
      "loss": -112.254,
      "step": 38660
    },
    {
      "epoch": 3.0936,
      "grad_norm": 77.06098937988281,
      "learning_rate": 9.688000000000001e-05,
      "loss": -112.1836,
      "step": 38670
    },
    {
      "epoch": 3.0944,
      "grad_norm": 57.95897674560547,
      "learning_rate": 9.685333333333334e-05,
      "loss": -111.71,
      "step": 38680
    },
    {
      "epoch": 3.0952,
      "grad_norm": 154.49156188964844,
      "learning_rate": 9.682666666666668e-05,
      "loss": -110.4621,
      "step": 38690
    },
    {
      "epoch": 3.096,
      "grad_norm": 38.88018798828125,
      "learning_rate": 9.680000000000001e-05,
      "loss": -111.4747,
      "step": 38700
    },
    {
      "epoch": 3.0968,
      "grad_norm": 39.30547332763672,
      "learning_rate": 9.677333333333334e-05,
      "loss": -110.8411,
      "step": 38710
    },
    {
      "epoch": 3.0976,
      "grad_norm": 50.86322021484375,
      "learning_rate": 9.674666666666668e-05,
      "loss": -110.7355,
      "step": 38720
    },
    {
      "epoch": 3.0984,
      "grad_norm": 54.41282272338867,
      "learning_rate": 9.672e-05,
      "loss": -111.0939,
      "step": 38730
    },
    {
      "epoch": 3.0992,
      "grad_norm": 38.044368743896484,
      "learning_rate": 9.669333333333334e-05,
      "loss": -110.2048,
      "step": 38740
    },
    {
      "epoch": 3.1,
      "grad_norm": 57.954410552978516,
      "learning_rate": 9.666666666666667e-05,
      "loss": -111.4752,
      "step": 38750
    },
    {
      "epoch": 3.1008,
      "grad_norm": 47.680015563964844,
      "learning_rate": 9.664000000000001e-05,
      "loss": -110.61,
      "step": 38760
    },
    {
      "epoch": 3.1016,
      "grad_norm": 38.99009323120117,
      "learning_rate": 9.661333333333334e-05,
      "loss": -110.7366,
      "step": 38770
    },
    {
      "epoch": 3.1024,
      "grad_norm": 32.59942626953125,
      "learning_rate": 9.658666666666667e-05,
      "loss": -110.5602,
      "step": 38780
    },
    {
      "epoch": 3.1032,
      "grad_norm": 51.828033447265625,
      "learning_rate": 9.656000000000001e-05,
      "loss": -111.2307,
      "step": 38790
    },
    {
      "epoch": 3.104,
      "grad_norm": 135.0312957763672,
      "learning_rate": 9.653333333333334e-05,
      "loss": -110.8828,
      "step": 38800
    },
    {
      "epoch": 3.1048,
      "grad_norm": 45.80848693847656,
      "learning_rate": 9.650666666666667e-05,
      "loss": -110.5742,
      "step": 38810
    },
    {
      "epoch": 3.1056,
      "grad_norm": 134.46737670898438,
      "learning_rate": 9.648e-05,
      "loss": -111.1751,
      "step": 38820
    },
    {
      "epoch": 3.1064,
      "grad_norm": 42.5806884765625,
      "learning_rate": 9.645333333333334e-05,
      "loss": -109.9869,
      "step": 38830
    },
    {
      "epoch": 3.1072,
      "grad_norm": 75.83790588378906,
      "learning_rate": 9.642666666666667e-05,
      "loss": -110.5877,
      "step": 38840
    },
    {
      "epoch": 3.108,
      "grad_norm": 53.285030364990234,
      "learning_rate": 9.64e-05,
      "loss": -111.0679,
      "step": 38850
    },
    {
      "epoch": 3.1088,
      "grad_norm": 27.44757652282715,
      "learning_rate": 9.637333333333334e-05,
      "loss": -110.7028,
      "step": 38860
    },
    {
      "epoch": 3.1096,
      "grad_norm": 99.09564971923828,
      "learning_rate": 9.634666666666667e-05,
      "loss": -111.0471,
      "step": 38870
    },
    {
      "epoch": 3.1104,
      "grad_norm": 96.61271667480469,
      "learning_rate": 9.632e-05,
      "loss": -110.9598,
      "step": 38880
    },
    {
      "epoch": 3.1112,
      "grad_norm": 85.96617126464844,
      "learning_rate": 9.629333333333334e-05,
      "loss": -110.9636,
      "step": 38890
    },
    {
      "epoch": 3.112,
      "grad_norm": 60.96266174316406,
      "learning_rate": 9.626666666666667e-05,
      "loss": -111.3467,
      "step": 38900
    },
    {
      "epoch": 3.1128,
      "grad_norm": 56.24973678588867,
      "learning_rate": 9.624000000000001e-05,
      "loss": -110.4521,
      "step": 38910
    },
    {
      "epoch": 3.1136,
      "grad_norm": 81.2218246459961,
      "learning_rate": 9.621333333333333e-05,
      "loss": -111.9771,
      "step": 38920
    },
    {
      "epoch": 3.1144,
      "grad_norm": 101.47737121582031,
      "learning_rate": 9.618666666666667e-05,
      "loss": -111.1027,
      "step": 38930
    },
    {
      "epoch": 3.1152,
      "grad_norm": 26.142972946166992,
      "learning_rate": 9.616e-05,
      "loss": -111.7839,
      "step": 38940
    },
    {
      "epoch": 3.116,
      "grad_norm": 23.846269607543945,
      "learning_rate": 9.613333333333334e-05,
      "loss": -110.0917,
      "step": 38950
    },
    {
      "epoch": 3.1168,
      "grad_norm": 47.14511489868164,
      "learning_rate": 9.610666666666667e-05,
      "loss": -110.5953,
      "step": 38960
    },
    {
      "epoch": 3.1176,
      "grad_norm": 33.31661605834961,
      "learning_rate": 9.608e-05,
      "loss": -110.7416,
      "step": 38970
    },
    {
      "epoch": 3.1184,
      "grad_norm": 83.1703872680664,
      "learning_rate": 9.605333333333334e-05,
      "loss": -110.7826,
      "step": 38980
    },
    {
      "epoch": 3.1192,
      "grad_norm": 44.06307601928711,
      "learning_rate": 9.602666666666667e-05,
      "loss": -110.4949,
      "step": 38990
    },
    {
      "epoch": 3.12,
      "grad_norm": 147.610107421875,
      "learning_rate": 9.6e-05,
      "loss": -111.9907,
      "step": 39000
    },
    {
      "epoch": 3.1208,
      "grad_norm": 47.90149688720703,
      "learning_rate": 9.597333333333333e-05,
      "loss": -110.3497,
      "step": 39010
    },
    {
      "epoch": 3.1216,
      "grad_norm": 77.43050384521484,
      "learning_rate": 9.594666666666667e-05,
      "loss": -110.8841,
      "step": 39020
    },
    {
      "epoch": 3.1224,
      "grad_norm": 118.54711151123047,
      "learning_rate": 9.592e-05,
      "loss": -111.48,
      "step": 39030
    },
    {
      "epoch": 3.1232,
      "grad_norm": 43.28458786010742,
      "learning_rate": 9.589333333333333e-05,
      "loss": -111.4805,
      "step": 39040
    },
    {
      "epoch": 3.124,
      "grad_norm": 152.54661560058594,
      "learning_rate": 9.586666666666667e-05,
      "loss": -110.6309,
      "step": 39050
    },
    {
      "epoch": 3.1248,
      "grad_norm": 59.03736114501953,
      "learning_rate": 9.584e-05,
      "loss": -111.4009,
      "step": 39060
    },
    {
      "epoch": 3.1256,
      "grad_norm": 25.95008087158203,
      "learning_rate": 9.581333333333333e-05,
      "loss": -111.3726,
      "step": 39070
    },
    {
      "epoch": 3.1264,
      "grad_norm": 46.840576171875,
      "learning_rate": 9.578666666666668e-05,
      "loss": -110.0107,
      "step": 39080
    },
    {
      "epoch": 3.1272,
      "grad_norm": 62.542781829833984,
      "learning_rate": 9.576e-05,
      "loss": -110.9766,
      "step": 39090
    },
    {
      "epoch": 3.128,
      "grad_norm": 46.40151596069336,
      "learning_rate": 9.573333333333335e-05,
      "loss": -110.6203,
      "step": 39100
    },
    {
      "epoch": 3.1288,
      "grad_norm": 58.298213958740234,
      "learning_rate": 9.570666666666666e-05,
      "loss": -110.9826,
      "step": 39110
    },
    {
      "epoch": 3.1296,
      "grad_norm": 50.99416732788086,
      "learning_rate": 9.568e-05,
      "loss": -111.9773,
      "step": 39120
    },
    {
      "epoch": 3.1304,
      "grad_norm": 33.494319915771484,
      "learning_rate": 9.565333333333333e-05,
      "loss": -110.6458,
      "step": 39130
    },
    {
      "epoch": 3.1312,
      "grad_norm": 36.952247619628906,
      "learning_rate": 9.562666666666668e-05,
      "loss": -111.384,
      "step": 39140
    },
    {
      "epoch": 3.132,
      "grad_norm": 34.608673095703125,
      "learning_rate": 9.56e-05,
      "loss": -110.9804,
      "step": 39150
    },
    {
      "epoch": 3.1328,
      "grad_norm": 41.574363708496094,
      "learning_rate": 9.557333333333334e-05,
      "loss": -110.8738,
      "step": 39160
    },
    {
      "epoch": 3.1336,
      "grad_norm": 51.701507568359375,
      "learning_rate": 9.554666666666668e-05,
      "loss": -110.3002,
      "step": 39170
    },
    {
      "epoch": 3.1344,
      "grad_norm": 61.143253326416016,
      "learning_rate": 9.552000000000001e-05,
      "loss": -110.8168,
      "step": 39180
    },
    {
      "epoch": 3.1352,
      "grad_norm": 118.39191436767578,
      "learning_rate": 9.549333333333334e-05,
      "loss": -111.1832,
      "step": 39190
    },
    {
      "epoch": 3.136,
      "grad_norm": 42.27543258666992,
      "learning_rate": 9.546666666666667e-05,
      "loss": -110.7217,
      "step": 39200
    },
    {
      "epoch": 3.1368,
      "grad_norm": 103.29440307617188,
      "learning_rate": 9.544000000000001e-05,
      "loss": -110.6926,
      "step": 39210
    },
    {
      "epoch": 3.1376,
      "grad_norm": 39.278316497802734,
      "learning_rate": 9.541333333333334e-05,
      "loss": -110.4959,
      "step": 39220
    },
    {
      "epoch": 3.1384,
      "grad_norm": 28.1843204498291,
      "learning_rate": 9.538666666666667e-05,
      "loss": -110.6583,
      "step": 39230
    },
    {
      "epoch": 3.1391999999999998,
      "grad_norm": 80.97937774658203,
      "learning_rate": 9.536000000000001e-05,
      "loss": -111.2371,
      "step": 39240
    },
    {
      "epoch": 3.14,
      "grad_norm": 63.28414535522461,
      "learning_rate": 9.533333333333334e-05,
      "loss": -111.9426,
      "step": 39250
    },
    {
      "epoch": 3.1408,
      "grad_norm": 106.94801330566406,
      "learning_rate": 9.530666666666667e-05,
      "loss": -110.4628,
      "step": 39260
    },
    {
      "epoch": 3.1416,
      "grad_norm": 35.99906921386719,
      "learning_rate": 9.528000000000001e-05,
      "loss": -110.3445,
      "step": 39270
    },
    {
      "epoch": 3.1424,
      "grad_norm": 205.47012329101562,
      "learning_rate": 9.525333333333334e-05,
      "loss": -110.5047,
      "step": 39280
    },
    {
      "epoch": 3.1432,
      "grad_norm": 29.774017333984375,
      "learning_rate": 9.522666666666668e-05,
      "loss": -110.997,
      "step": 39290
    },
    {
      "epoch": 3.144,
      "grad_norm": 21.585128784179688,
      "learning_rate": 9.52e-05,
      "loss": -111.2657,
      "step": 39300
    },
    {
      "epoch": 3.1448,
      "grad_norm": 61.57889175415039,
      "learning_rate": 9.517333333333334e-05,
      "loss": -110.3557,
      "step": 39310
    },
    {
      "epoch": 3.1456,
      "grad_norm": 44.38454818725586,
      "learning_rate": 9.514666666666667e-05,
      "loss": -110.3523,
      "step": 39320
    },
    {
      "epoch": 3.1464,
      "grad_norm": 46.01062774658203,
      "learning_rate": 9.512000000000001e-05,
      "loss": -111.2516,
      "step": 39330
    },
    {
      "epoch": 3.1471999999999998,
      "grad_norm": 38.07740020751953,
      "learning_rate": 9.509333333333334e-05,
      "loss": -111.1107,
      "step": 39340
    },
    {
      "epoch": 3.148,
      "grad_norm": 100.36067962646484,
      "learning_rate": 9.506666666666667e-05,
      "loss": -111.104,
      "step": 39350
    },
    {
      "epoch": 3.1488,
      "grad_norm": 35.936283111572266,
      "learning_rate": 9.504000000000001e-05,
      "loss": -111.1157,
      "step": 39360
    },
    {
      "epoch": 3.1496,
      "grad_norm": 41.28044891357422,
      "learning_rate": 9.501333333333334e-05,
      "loss": -111.6547,
      "step": 39370
    },
    {
      "epoch": 3.1504,
      "grad_norm": 45.84407043457031,
      "learning_rate": 9.498666666666667e-05,
      "loss": -111.1709,
      "step": 39380
    },
    {
      "epoch": 3.1512000000000002,
      "grad_norm": 61.93018341064453,
      "learning_rate": 9.496e-05,
      "loss": -110.7765,
      "step": 39390
    },
    {
      "epoch": 3.152,
      "grad_norm": 43.49733352661133,
      "learning_rate": 9.493333333333334e-05,
      "loss": -111.904,
      "step": 39400
    },
    {
      "epoch": 3.1528,
      "grad_norm": 74.33047485351562,
      "learning_rate": 9.490666666666667e-05,
      "loss": -111.3432,
      "step": 39410
    },
    {
      "epoch": 3.1536,
      "grad_norm": 30.909425735473633,
      "learning_rate": 9.488e-05,
      "loss": -110.6234,
      "step": 39420
    },
    {
      "epoch": 3.1544,
      "grad_norm": 57.3156623840332,
      "learning_rate": 9.485333333333334e-05,
      "loss": -111.4822,
      "step": 39430
    },
    {
      "epoch": 3.1552,
      "grad_norm": 98.63298797607422,
      "learning_rate": 9.482666666666667e-05,
      "loss": -111.109,
      "step": 39440
    },
    {
      "epoch": 3.156,
      "grad_norm": 62.78199005126953,
      "learning_rate": 9.48e-05,
      "loss": -111.0613,
      "step": 39450
    },
    {
      "epoch": 3.1568,
      "grad_norm": 27.067626953125,
      "learning_rate": 9.477333333333333e-05,
      "loss": -110.2579,
      "step": 39460
    },
    {
      "epoch": 3.1576,
      "grad_norm": 178.57887268066406,
      "learning_rate": 9.474666666666667e-05,
      "loss": -111.9173,
      "step": 39470
    },
    {
      "epoch": 3.1584,
      "grad_norm": 86.65716552734375,
      "learning_rate": 9.472000000000001e-05,
      "loss": -111.2442,
      "step": 39480
    },
    {
      "epoch": 3.1592000000000002,
      "grad_norm": 43.2200927734375,
      "learning_rate": 9.469333333333333e-05,
      "loss": -111.0824,
      "step": 39490
    },
    {
      "epoch": 3.16,
      "grad_norm": 172.88223266601562,
      "learning_rate": 9.466666666666667e-05,
      "loss": -111.4433,
      "step": 39500
    },
    {
      "epoch": 3.1608,
      "grad_norm": 444.0144958496094,
      "learning_rate": 9.464e-05,
      "loss": -112.0314,
      "step": 39510
    },
    {
      "epoch": 3.1616,
      "grad_norm": 121.90464782714844,
      "learning_rate": 9.461333333333334e-05,
      "loss": -111.4573,
      "step": 39520
    },
    {
      "epoch": 3.1624,
      "grad_norm": 79.00241088867188,
      "learning_rate": 9.458666666666667e-05,
      "loss": -111.6018,
      "step": 39530
    },
    {
      "epoch": 3.1632,
      "grad_norm": 47.672977447509766,
      "learning_rate": 9.456e-05,
      "loss": -112.2254,
      "step": 39540
    },
    {
      "epoch": 3.164,
      "grad_norm": 27.169031143188477,
      "learning_rate": 9.453333333333335e-05,
      "loss": -110.7758,
      "step": 39550
    },
    {
      "epoch": 3.1648,
      "grad_norm": 62.72465896606445,
      "learning_rate": 9.450666666666667e-05,
      "loss": -110.9938,
      "step": 39560
    },
    {
      "epoch": 3.1656,
      "grad_norm": 33.252010345458984,
      "learning_rate": 9.448e-05,
      "loss": -110.7261,
      "step": 39570
    },
    {
      "epoch": 3.1664,
      "grad_norm": 63.377342224121094,
      "learning_rate": 9.445333333333333e-05,
      "loss": -111.57,
      "step": 39580
    },
    {
      "epoch": 3.1672,
      "grad_norm": 27.502899169921875,
      "learning_rate": 9.442666666666668e-05,
      "loss": -110.7299,
      "step": 39590
    },
    {
      "epoch": 3.168,
      "grad_norm": 36.03207778930664,
      "learning_rate": 9.44e-05,
      "loss": -109.9724,
      "step": 39600
    },
    {
      "epoch": 3.1688,
      "grad_norm": 70.03367614746094,
      "learning_rate": 9.437333333333333e-05,
      "loss": -111.6853,
      "step": 39610
    },
    {
      "epoch": 3.1696,
      "grad_norm": 39.59282684326172,
      "learning_rate": 9.434666666666668e-05,
      "loss": -110.6541,
      "step": 39620
    },
    {
      "epoch": 3.1704,
      "grad_norm": 27.764488220214844,
      "learning_rate": 9.432e-05,
      "loss": -111.1043,
      "step": 39630
    },
    {
      "epoch": 3.1712,
      "grad_norm": 122.19181823730469,
      "learning_rate": 9.429333333333333e-05,
      "loss": -112.1205,
      "step": 39640
    },
    {
      "epoch": 3.172,
      "grad_norm": 136.10084533691406,
      "learning_rate": 9.426666666666666e-05,
      "loss": -110.9651,
      "step": 39650
    },
    {
      "epoch": 3.1728,
      "grad_norm": 47.591026306152344,
      "learning_rate": 9.424e-05,
      "loss": -111.0467,
      "step": 39660
    },
    {
      "epoch": 3.1736,
      "grad_norm": 56.36812973022461,
      "learning_rate": 9.421333333333335e-05,
      "loss": -111.5928,
      "step": 39670
    },
    {
      "epoch": 3.1744,
      "grad_norm": 36.528255462646484,
      "learning_rate": 9.418666666666666e-05,
      "loss": -111.2576,
      "step": 39680
    },
    {
      "epoch": 3.1752,
      "grad_norm": 28.4790096282959,
      "learning_rate": 9.416e-05,
      "loss": -110.1566,
      "step": 39690
    },
    {
      "epoch": 3.176,
      "grad_norm": 91.6012954711914,
      "learning_rate": 9.413333333333334e-05,
      "loss": -110.5205,
      "step": 39700
    },
    {
      "epoch": 3.1768,
      "grad_norm": 38.65329360961914,
      "learning_rate": 9.410666666666668e-05,
      "loss": -110.4788,
      "step": 39710
    },
    {
      "epoch": 3.1776,
      "grad_norm": 33.447471618652344,
      "learning_rate": 9.408000000000001e-05,
      "loss": -110.7787,
      "step": 39720
    },
    {
      "epoch": 3.1784,
      "grad_norm": 53.84856033325195,
      "learning_rate": 9.405333333333334e-05,
      "loss": -111.2442,
      "step": 39730
    },
    {
      "epoch": 3.1792,
      "grad_norm": 55.00431442260742,
      "learning_rate": 9.402666666666668e-05,
      "loss": -110.3802,
      "step": 39740
    },
    {
      "epoch": 3.18,
      "grad_norm": 52.71109390258789,
      "learning_rate": 9.4e-05,
      "loss": -111.1137,
      "step": 39750
    },
    {
      "epoch": 3.1808,
      "grad_norm": 32.575401306152344,
      "learning_rate": 9.397333333333334e-05,
      "loss": -111.5588,
      "step": 39760
    },
    {
      "epoch": 3.1816,
      "grad_norm": 38.16642379760742,
      "learning_rate": 9.394666666666667e-05,
      "loss": -109.9975,
      "step": 39770
    },
    {
      "epoch": 3.1824,
      "grad_norm": 53.239925384521484,
      "learning_rate": 9.392000000000001e-05,
      "loss": -110.8916,
      "step": 39780
    },
    {
      "epoch": 3.1832,
      "grad_norm": 31.900924682617188,
      "learning_rate": 9.389333333333334e-05,
      "loss": -109.9703,
      "step": 39790
    },
    {
      "epoch": 3.184,
      "grad_norm": 52.652462005615234,
      "learning_rate": 9.386666666666667e-05,
      "loss": -111.5545,
      "step": 39800
    },
    {
      "epoch": 3.1848,
      "grad_norm": 29.132680892944336,
      "learning_rate": 9.384000000000001e-05,
      "loss": -111.5126,
      "step": 39810
    },
    {
      "epoch": 3.1856,
      "grad_norm": 96.61527252197266,
      "learning_rate": 9.381333333333334e-05,
      "loss": -111.3215,
      "step": 39820
    },
    {
      "epoch": 3.1864,
      "grad_norm": 35.64957809448242,
      "learning_rate": 9.378666666666667e-05,
      "loss": -109.9086,
      "step": 39830
    },
    {
      "epoch": 3.1872,
      "grad_norm": 40.79543685913086,
      "learning_rate": 9.376e-05,
      "loss": -110.9171,
      "step": 39840
    },
    {
      "epoch": 3.188,
      "grad_norm": 37.55439758300781,
      "learning_rate": 9.373333333333334e-05,
      "loss": -111.4326,
      "step": 39850
    },
    {
      "epoch": 3.1888,
      "grad_norm": 82.00119018554688,
      "learning_rate": 9.370666666666668e-05,
      "loss": -111.3224,
      "step": 39860
    },
    {
      "epoch": 3.1896,
      "grad_norm": 51.905033111572266,
      "learning_rate": 9.368e-05,
      "loss": -111.4333,
      "step": 39870
    },
    {
      "epoch": 3.1904,
      "grad_norm": 60.69802474975586,
      "learning_rate": 9.365333333333334e-05,
      "loss": -111.0424,
      "step": 39880
    },
    {
      "epoch": 3.1912,
      "grad_norm": 57.30080032348633,
      "learning_rate": 9.362666666666667e-05,
      "loss": -111.8281,
      "step": 39890
    },
    {
      "epoch": 3.192,
      "grad_norm": 44.18912887573242,
      "learning_rate": 9.360000000000001e-05,
      "loss": -111.1306,
      "step": 39900
    },
    {
      "epoch": 3.1928,
      "grad_norm": 27.51013946533203,
      "learning_rate": 9.357333333333334e-05,
      "loss": -111.346,
      "step": 39910
    },
    {
      "epoch": 3.1936,
      "grad_norm": 63.91901397705078,
      "learning_rate": 9.354666666666667e-05,
      "loss": -110.4008,
      "step": 39920
    },
    {
      "epoch": 3.1944,
      "grad_norm": 71.36833953857422,
      "learning_rate": 9.352000000000001e-05,
      "loss": -111.7217,
      "step": 39930
    },
    {
      "epoch": 3.1952,
      "grad_norm": 52.99895095825195,
      "learning_rate": 9.349333333333333e-05,
      "loss": -110.4667,
      "step": 39940
    },
    {
      "epoch": 3.196,
      "grad_norm": 65.5443344116211,
      "learning_rate": 9.346666666666667e-05,
      "loss": -111.1843,
      "step": 39950
    },
    {
      "epoch": 3.1968,
      "grad_norm": 34.62192916870117,
      "learning_rate": 9.344e-05,
      "loss": -111.2276,
      "step": 39960
    },
    {
      "epoch": 3.1976,
      "grad_norm": 99.53007507324219,
      "learning_rate": 9.341333333333334e-05,
      "loss": -111.8765,
      "step": 39970
    },
    {
      "epoch": 3.1984,
      "grad_norm": 37.85161209106445,
      "learning_rate": 9.338666666666667e-05,
      "loss": -111.5336,
      "step": 39980
    },
    {
      "epoch": 3.1992,
      "grad_norm": 77.062255859375,
      "learning_rate": 9.336e-05,
      "loss": -110.991,
      "step": 39990
    },
    {
      "epoch": 3.2,
      "grad_norm": 40.9668083190918,
      "learning_rate": 9.333333333333334e-05,
      "loss": -110.6661,
      "step": 40000
    },
    {
      "epoch": 3.2008,
      "grad_norm": 26.93506622314453,
      "learning_rate": 9.330666666666667e-05,
      "loss": -110.7346,
      "step": 40010
    },
    {
      "epoch": 3.2016,
      "grad_norm": 44.3409309387207,
      "learning_rate": 9.328e-05,
      "loss": -111.0101,
      "step": 40020
    },
    {
      "epoch": 3.2024,
      "grad_norm": 27.492332458496094,
      "learning_rate": 9.325333333333333e-05,
      "loss": -110.3438,
      "step": 40030
    },
    {
      "epoch": 3.2032,
      "grad_norm": 176.779541015625,
      "learning_rate": 9.322666666666667e-05,
      "loss": -112.0432,
      "step": 40040
    },
    {
      "epoch": 3.204,
      "grad_norm": 29.288488388061523,
      "learning_rate": 9.320000000000002e-05,
      "loss": -111.6653,
      "step": 40050
    },
    {
      "epoch": 3.2048,
      "grad_norm": 91.3506088256836,
      "learning_rate": 9.317333333333333e-05,
      "loss": -110.0735,
      "step": 40060
    },
    {
      "epoch": 3.2056,
      "grad_norm": 80.35735321044922,
      "learning_rate": 9.314666666666667e-05,
      "loss": -110.9371,
      "step": 40070
    },
    {
      "epoch": 3.2064,
      "grad_norm": 85.79376983642578,
      "learning_rate": 9.312e-05,
      "loss": -111.1301,
      "step": 40080
    },
    {
      "epoch": 3.2072,
      "grad_norm": 74.42166900634766,
      "learning_rate": 9.309333333333333e-05,
      "loss": -110.8367,
      "step": 40090
    },
    {
      "epoch": 3.208,
      "grad_norm": 58.67314910888672,
      "learning_rate": 9.306666666666667e-05,
      "loss": -111.2119,
      "step": 40100
    },
    {
      "epoch": 3.2088,
      "grad_norm": 65.21830749511719,
      "learning_rate": 9.304e-05,
      "loss": -111.166,
      "step": 40110
    },
    {
      "epoch": 3.2096,
      "grad_norm": 45.34177017211914,
      "learning_rate": 9.301333333333335e-05,
      "loss": -110.4614,
      "step": 40120
    },
    {
      "epoch": 3.2104,
      "grad_norm": 48.41992950439453,
      "learning_rate": 9.298666666666666e-05,
      "loss": -111.6519,
      "step": 40130
    },
    {
      "epoch": 3.2112,
      "grad_norm": 55.232566833496094,
      "learning_rate": 9.296e-05,
      "loss": -111.2542,
      "step": 40140
    },
    {
      "epoch": 3.212,
      "grad_norm": 97.50994873046875,
      "learning_rate": 9.293333333333333e-05,
      "loss": -110.9684,
      "step": 40150
    },
    {
      "epoch": 3.2128,
      "grad_norm": 120.47952270507812,
      "learning_rate": 9.290666666666668e-05,
      "loss": -111.1508,
      "step": 40160
    },
    {
      "epoch": 3.2136,
      "grad_norm": 29.221981048583984,
      "learning_rate": 9.288e-05,
      "loss": -110.4548,
      "step": 40170
    },
    {
      "epoch": 3.2144,
      "grad_norm": 51.51927947998047,
      "learning_rate": 9.285333333333333e-05,
      "loss": -111.1339,
      "step": 40180
    },
    {
      "epoch": 3.2152,
      "grad_norm": 106.09104919433594,
      "learning_rate": 9.282666666666668e-05,
      "loss": -111.186,
      "step": 40190
    },
    {
      "epoch": 3.216,
      "grad_norm": 46.07292556762695,
      "learning_rate": 9.28e-05,
      "loss": -111.13,
      "step": 40200
    },
    {
      "epoch": 3.2168,
      "grad_norm": 38.44996643066406,
      "learning_rate": 9.277333333333334e-05,
      "loss": -111.307,
      "step": 40210
    },
    {
      "epoch": 3.2176,
      "grad_norm": 44.88380813598633,
      "learning_rate": 9.274666666666666e-05,
      "loss": -110.3212,
      "step": 40220
    },
    {
      "epoch": 3.2184,
      "grad_norm": 117.55572509765625,
      "learning_rate": 9.272e-05,
      "loss": -111.226,
      "step": 40230
    },
    {
      "epoch": 3.2192,
      "grad_norm": 54.34286117553711,
      "learning_rate": 9.269333333333335e-05,
      "loss": -110.8074,
      "step": 40240
    },
    {
      "epoch": 3.22,
      "grad_norm": 42.61797332763672,
      "learning_rate": 9.266666666666666e-05,
      "loss": -111.6926,
      "step": 40250
    },
    {
      "epoch": 3.2208,
      "grad_norm": 27.150846481323242,
      "learning_rate": 9.264000000000001e-05,
      "loss": -111.1858,
      "step": 40260
    },
    {
      "epoch": 3.2216,
      "grad_norm": 56.91661071777344,
      "learning_rate": 9.261333333333334e-05,
      "loss": -110.8104,
      "step": 40270
    },
    {
      "epoch": 3.2224,
      "grad_norm": 295.23907470703125,
      "learning_rate": 9.258666666666667e-05,
      "loss": -110.1202,
      "step": 40280
    },
    {
      "epoch": 3.2232,
      "grad_norm": 30.16054916381836,
      "learning_rate": 9.256000000000001e-05,
      "loss": -110.2715,
      "step": 40290
    },
    {
      "epoch": 3.224,
      "grad_norm": 57.97412872314453,
      "learning_rate": 9.253333333333334e-05,
      "loss": -110.4698,
      "step": 40300
    },
    {
      "epoch": 3.2248,
      "grad_norm": 197.5537567138672,
      "learning_rate": 9.250666666666668e-05,
      "loss": -109.817,
      "step": 40310
    },
    {
      "epoch": 3.2256,
      "grad_norm": 81.19274139404297,
      "learning_rate": 9.248e-05,
      "loss": -111.2599,
      "step": 40320
    },
    {
      "epoch": 3.2264,
      "grad_norm": 34.222145080566406,
      "learning_rate": 9.245333333333334e-05,
      "loss": -110.4033,
      "step": 40330
    },
    {
      "epoch": 3.2272,
      "grad_norm": 37.20420837402344,
      "learning_rate": 9.242666666666667e-05,
      "loss": -111.2337,
      "step": 40340
    },
    {
      "epoch": 3.228,
      "grad_norm": 41.54859924316406,
      "learning_rate": 9.240000000000001e-05,
      "loss": -110.3376,
      "step": 40350
    },
    {
      "epoch": 3.2288,
      "grad_norm": 22.170719146728516,
      "learning_rate": 9.237333333333334e-05,
      "loss": -109.6704,
      "step": 40360
    },
    {
      "epoch": 3.2296,
      "grad_norm": 63.84542465209961,
      "learning_rate": 9.234666666666667e-05,
      "loss": -110.6493,
      "step": 40370
    },
    {
      "epoch": 3.2304,
      "grad_norm": 32.545257568359375,
      "learning_rate": 9.232000000000001e-05,
      "loss": -110.6354,
      "step": 40380
    },
    {
      "epoch": 3.2312,
      "grad_norm": 44.72141647338867,
      "learning_rate": 9.229333333333334e-05,
      "loss": -112.1336,
      "step": 40390
    },
    {
      "epoch": 3.232,
      "grad_norm": 292.1908264160156,
      "learning_rate": 9.226666666666667e-05,
      "loss": -110.407,
      "step": 40400
    },
    {
      "epoch": 3.2328,
      "grad_norm": 63.536888122558594,
      "learning_rate": 9.224e-05,
      "loss": -110.4248,
      "step": 40410
    },
    {
      "epoch": 3.2336,
      "grad_norm": 90.30691528320312,
      "learning_rate": 9.221333333333334e-05,
      "loss": -110.7465,
      "step": 40420
    },
    {
      "epoch": 3.2344,
      "grad_norm": 182.4364471435547,
      "learning_rate": 9.218666666666667e-05,
      "loss": -111.1348,
      "step": 40430
    },
    {
      "epoch": 3.2352,
      "grad_norm": 145.89263916015625,
      "learning_rate": 9.216e-05,
      "loss": -110.8621,
      "step": 40440
    },
    {
      "epoch": 3.2359999999999998,
      "grad_norm": 115.56336212158203,
      "learning_rate": 9.213333333333334e-05,
      "loss": -110.5932,
      "step": 40450
    },
    {
      "epoch": 3.2368,
      "grad_norm": 115.00843048095703,
      "learning_rate": 9.210666666666667e-05,
      "loss": -111.0532,
      "step": 40460
    },
    {
      "epoch": 3.2376,
      "grad_norm": 37.18819046020508,
      "learning_rate": 9.208e-05,
      "loss": -111.0779,
      "step": 40470
    },
    {
      "epoch": 3.2384,
      "grad_norm": 36.77645492553711,
      "learning_rate": 9.205333333333334e-05,
      "loss": -111.6196,
      "step": 40480
    },
    {
      "epoch": 3.2392,
      "grad_norm": 349.1277160644531,
      "learning_rate": 9.202666666666667e-05,
      "loss": -110.9342,
      "step": 40490
    },
    {
      "epoch": 3.24,
      "grad_norm": 125.34624481201172,
      "learning_rate": 9.200000000000001e-05,
      "loss": -112.0236,
      "step": 40500
    },
    {
      "epoch": 3.2408,
      "grad_norm": 74.16040802001953,
      "learning_rate": 9.197333333333333e-05,
      "loss": -111.427,
      "step": 40510
    },
    {
      "epoch": 3.2416,
      "grad_norm": 53.700714111328125,
      "learning_rate": 9.194666666666667e-05,
      "loss": -110.7716,
      "step": 40520
    },
    {
      "epoch": 3.2424,
      "grad_norm": 49.07789611816406,
      "learning_rate": 9.192e-05,
      "loss": -111.059,
      "step": 40530
    },
    {
      "epoch": 3.2432,
      "grad_norm": 26.250186920166016,
      "learning_rate": 9.189333333333334e-05,
      "loss": -111.7108,
      "step": 40540
    },
    {
      "epoch": 3.2439999999999998,
      "grad_norm": 61.00194549560547,
      "learning_rate": 9.186666666666667e-05,
      "loss": -110.7027,
      "step": 40550
    },
    {
      "epoch": 3.2448,
      "grad_norm": 46.715389251708984,
      "learning_rate": 9.184e-05,
      "loss": -111.4952,
      "step": 40560
    },
    {
      "epoch": 3.2456,
      "grad_norm": 68.6522216796875,
      "learning_rate": 9.181333333333334e-05,
      "loss": -111.3013,
      "step": 40570
    },
    {
      "epoch": 3.2464,
      "grad_norm": 31.514755249023438,
      "learning_rate": 9.178666666666667e-05,
      "loss": -111.9368,
      "step": 40580
    },
    {
      "epoch": 3.2472,
      "grad_norm": 29.11775779724121,
      "learning_rate": 9.176e-05,
      "loss": -110.1426,
      "step": 40590
    },
    {
      "epoch": 3.248,
      "grad_norm": 20.33253288269043,
      "learning_rate": 9.173333333333333e-05,
      "loss": -110.7705,
      "step": 40600
    },
    {
      "epoch": 3.2488,
      "grad_norm": 46.192039489746094,
      "learning_rate": 9.170666666666667e-05,
      "loss": -110.502,
      "step": 40610
    },
    {
      "epoch": 3.2496,
      "grad_norm": 268.54693603515625,
      "learning_rate": 9.168e-05,
      "loss": -110.6224,
      "step": 40620
    },
    {
      "epoch": 3.2504,
      "grad_norm": 32.054222106933594,
      "learning_rate": 9.165333333333333e-05,
      "loss": -111.3226,
      "step": 40630
    },
    {
      "epoch": 3.2512,
      "grad_norm": 24.681058883666992,
      "learning_rate": 9.162666666666667e-05,
      "loss": -110.9322,
      "step": 40640
    },
    {
      "epoch": 3.252,
      "grad_norm": 42.71052932739258,
      "learning_rate": 9.16e-05,
      "loss": -111.816,
      "step": 40650
    },
    {
      "epoch": 3.2528,
      "grad_norm": 68.24463653564453,
      "learning_rate": 9.157333333333333e-05,
      "loss": -111.6388,
      "step": 40660
    },
    {
      "epoch": 3.2536,
      "grad_norm": 56.51818084716797,
      "learning_rate": 9.154666666666668e-05,
      "loss": -110.5947,
      "step": 40670
    },
    {
      "epoch": 3.2544,
      "grad_norm": 73.85623931884766,
      "learning_rate": 9.152e-05,
      "loss": -111.3606,
      "step": 40680
    },
    {
      "epoch": 3.2552,
      "grad_norm": 30.414684295654297,
      "learning_rate": 9.149333333333335e-05,
      "loss": -110.6404,
      "step": 40690
    },
    {
      "epoch": 3.2560000000000002,
      "grad_norm": 30.9872989654541,
      "learning_rate": 9.146666666666666e-05,
      "loss": -110.2728,
      "step": 40700
    },
    {
      "epoch": 3.2568,
      "grad_norm": 68.05648803710938,
      "learning_rate": 9.144e-05,
      "loss": -111.9756,
      "step": 40710
    },
    {
      "epoch": 3.2576,
      "grad_norm": 138.84523010253906,
      "learning_rate": 9.141333333333333e-05,
      "loss": -111.0049,
      "step": 40720
    },
    {
      "epoch": 3.2584,
      "grad_norm": 56.92473602294922,
      "learning_rate": 9.138666666666668e-05,
      "loss": -111.4337,
      "step": 40730
    },
    {
      "epoch": 3.2592,
      "grad_norm": 114.93531799316406,
      "learning_rate": 9.136e-05,
      "loss": -111.2737,
      "step": 40740
    },
    {
      "epoch": 3.26,
      "grad_norm": 79.0993881225586,
      "learning_rate": 9.133333333333334e-05,
      "loss": -111.5099,
      "step": 40750
    },
    {
      "epoch": 3.2608,
      "grad_norm": 27.275476455688477,
      "learning_rate": 9.130666666666668e-05,
      "loss": -111.3117,
      "step": 40760
    },
    {
      "epoch": 3.2616,
      "grad_norm": 55.97243118286133,
      "learning_rate": 9.128e-05,
      "loss": -111.5104,
      "step": 40770
    },
    {
      "epoch": 3.2624,
      "grad_norm": 26.408740997314453,
      "learning_rate": 9.125333333333334e-05,
      "loss": -110.9279,
      "step": 40780
    },
    {
      "epoch": 3.2632,
      "grad_norm": 42.83273696899414,
      "learning_rate": 9.122666666666667e-05,
      "loss": -110.9312,
      "step": 40790
    },
    {
      "epoch": 3.2640000000000002,
      "grad_norm": 36.8818244934082,
      "learning_rate": 9.120000000000001e-05,
      "loss": -110.1594,
      "step": 40800
    },
    {
      "epoch": 3.2648,
      "grad_norm": 37.9631462097168,
      "learning_rate": 9.117333333333334e-05,
      "loss": -110.9479,
      "step": 40810
    },
    {
      "epoch": 3.2656,
      "grad_norm": 91.2662582397461,
      "learning_rate": 9.114666666666667e-05,
      "loss": -111.0432,
      "step": 40820
    },
    {
      "epoch": 3.2664,
      "grad_norm": 80.78758239746094,
      "learning_rate": 9.112000000000001e-05,
      "loss": -110.5662,
      "step": 40830
    },
    {
      "epoch": 3.2672,
      "grad_norm": 53.86335372924805,
      "learning_rate": 9.109333333333334e-05,
      "loss": -110.9097,
      "step": 40840
    },
    {
      "epoch": 3.268,
      "grad_norm": 35.307003021240234,
      "learning_rate": 9.106666666666667e-05,
      "loss": -110.2731,
      "step": 40850
    },
    {
      "epoch": 3.2688,
      "grad_norm": 31.753135681152344,
      "learning_rate": 9.104000000000001e-05,
      "loss": -111.1323,
      "step": 40860
    },
    {
      "epoch": 3.2696,
      "grad_norm": 47.919158935546875,
      "learning_rate": 9.101333333333334e-05,
      "loss": -111.6795,
      "step": 40870
    },
    {
      "epoch": 3.2704,
      "grad_norm": 20.144073486328125,
      "learning_rate": 9.098666666666668e-05,
      "loss": -111.1766,
      "step": 40880
    },
    {
      "epoch": 3.2712,
      "grad_norm": 34.21714782714844,
      "learning_rate": 9.096e-05,
      "loss": -110.699,
      "step": 40890
    },
    {
      "epoch": 3.2720000000000002,
      "grad_norm": 79.09639739990234,
      "learning_rate": 9.093333333333334e-05,
      "loss": -111.2326,
      "step": 40900
    },
    {
      "epoch": 3.2728,
      "grad_norm": 74.79586791992188,
      "learning_rate": 9.090666666666667e-05,
      "loss": -111.6024,
      "step": 40910
    },
    {
      "epoch": 3.2736,
      "grad_norm": 50.4527587890625,
      "learning_rate": 9.088000000000001e-05,
      "loss": -111.3632,
      "step": 40920
    },
    {
      "epoch": 3.2744,
      "grad_norm": 45.4493522644043,
      "learning_rate": 9.085333333333334e-05,
      "loss": -110.8018,
      "step": 40930
    },
    {
      "epoch": 3.2752,
      "grad_norm": 136.9788360595703,
      "learning_rate": 9.082666666666667e-05,
      "loss": -111.383,
      "step": 40940
    },
    {
      "epoch": 3.276,
      "grad_norm": 53.598350524902344,
      "learning_rate": 9.080000000000001e-05,
      "loss": -110.8668,
      "step": 40950
    },
    {
      "epoch": 3.2768,
      "grad_norm": 55.536251068115234,
      "learning_rate": 9.077333333333333e-05,
      "loss": -111.6266,
      "step": 40960
    },
    {
      "epoch": 3.2776,
      "grad_norm": 125.54341888427734,
      "learning_rate": 9.074666666666667e-05,
      "loss": -110.4324,
      "step": 40970
    },
    {
      "epoch": 3.2784,
      "grad_norm": 47.44660186767578,
      "learning_rate": 9.072e-05,
      "loss": -110.9698,
      "step": 40980
    },
    {
      "epoch": 3.2792,
      "grad_norm": 74.81222534179688,
      "learning_rate": 9.069333333333334e-05,
      "loss": -110.8358,
      "step": 40990
    },
    {
      "epoch": 3.2800000000000002,
      "grad_norm": 77.11797332763672,
      "learning_rate": 9.066666666666667e-05,
      "loss": -110.7138,
      "step": 41000
    },
    {
      "epoch": 3.2808,
      "grad_norm": 66.942138671875,
      "learning_rate": 9.064e-05,
      "loss": -110.6265,
      "step": 41010
    },
    {
      "epoch": 3.2816,
      "grad_norm": 33.60414505004883,
      "learning_rate": 9.061333333333334e-05,
      "loss": -111.0458,
      "step": 41020
    },
    {
      "epoch": 3.2824,
      "grad_norm": 29.52466583251953,
      "learning_rate": 9.058666666666667e-05,
      "loss": -110.8327,
      "step": 41030
    },
    {
      "epoch": 3.2832,
      "grad_norm": 34.414581298828125,
      "learning_rate": 9.056e-05,
      "loss": -110.5301,
      "step": 41040
    },
    {
      "epoch": 3.284,
      "grad_norm": 33.043800354003906,
      "learning_rate": 9.053333333333334e-05,
      "loss": -109.9689,
      "step": 41050
    },
    {
      "epoch": 3.2848,
      "grad_norm": 166.96710205078125,
      "learning_rate": 9.050666666666667e-05,
      "loss": -111.2279,
      "step": 41060
    },
    {
      "epoch": 3.2856,
      "grad_norm": 477.26513671875,
      "learning_rate": 9.048000000000001e-05,
      "loss": -110.6519,
      "step": 41070
    },
    {
      "epoch": 3.2864,
      "grad_norm": 47.14512252807617,
      "learning_rate": 9.045333333333333e-05,
      "loss": -111.7736,
      "step": 41080
    },
    {
      "epoch": 3.2872,
      "grad_norm": 93.93798828125,
      "learning_rate": 9.042666666666667e-05,
      "loss": -112.1656,
      "step": 41090
    },
    {
      "epoch": 3.288,
      "grad_norm": 29.9914608001709,
      "learning_rate": 9.04e-05,
      "loss": -111.452,
      "step": 41100
    },
    {
      "epoch": 3.2888,
      "grad_norm": 41.702667236328125,
      "learning_rate": 9.037333333333334e-05,
      "loss": -110.696,
      "step": 41110
    },
    {
      "epoch": 3.2896,
      "grad_norm": 153.33834838867188,
      "learning_rate": 9.034666666666667e-05,
      "loss": -111.6964,
      "step": 41120
    },
    {
      "epoch": 3.2904,
      "grad_norm": 114.12540435791016,
      "learning_rate": 9.032e-05,
      "loss": -110.6263,
      "step": 41130
    },
    {
      "epoch": 3.2912,
      "grad_norm": 25.647600173950195,
      "learning_rate": 9.029333333333335e-05,
      "loss": -111.5214,
      "step": 41140
    },
    {
      "epoch": 3.292,
      "grad_norm": 90.97151184082031,
      "learning_rate": 9.026666666666666e-05,
      "loss": -110.7079,
      "step": 41150
    },
    {
      "epoch": 3.2928,
      "grad_norm": 24.935503005981445,
      "learning_rate": 9.024e-05,
      "loss": -111.0216,
      "step": 41160
    },
    {
      "epoch": 3.2936,
      "grad_norm": 30.583667755126953,
      "learning_rate": 9.021333333333333e-05,
      "loss": -111.9455,
      "step": 41170
    },
    {
      "epoch": 3.2944,
      "grad_norm": 23.905963897705078,
      "learning_rate": 9.018666666666668e-05,
      "loss": -111.3989,
      "step": 41180
    },
    {
      "epoch": 3.2952,
      "grad_norm": 55.36496353149414,
      "learning_rate": 9.016e-05,
      "loss": -111.11,
      "step": 41190
    },
    {
      "epoch": 3.296,
      "grad_norm": 33.2283935546875,
      "learning_rate": 9.013333333333333e-05,
      "loss": -111.016,
      "step": 41200
    },
    {
      "epoch": 3.2968,
      "grad_norm": 104.8092041015625,
      "learning_rate": 9.010666666666668e-05,
      "loss": -110.4851,
      "step": 41210
    },
    {
      "epoch": 3.2976,
      "grad_norm": 61.76145935058594,
      "learning_rate": 9.008e-05,
      "loss": -111.0348,
      "step": 41220
    },
    {
      "epoch": 3.2984,
      "grad_norm": 78.2042007446289,
      "learning_rate": 9.005333333333333e-05,
      "loss": -110.3642,
      "step": 41230
    },
    {
      "epoch": 3.2992,
      "grad_norm": 100.15028381347656,
      "learning_rate": 9.002666666666668e-05,
      "loss": -111.4759,
      "step": 41240
    },
    {
      "epoch": 3.3,
      "grad_norm": 34.837982177734375,
      "learning_rate": 9e-05,
      "loss": -111.1648,
      "step": 41250
    },
    {
      "epoch": 3.3008,
      "grad_norm": 92.025390625,
      "learning_rate": 8.997333333333335e-05,
      "loss": -110.314,
      "step": 41260
    },
    {
      "epoch": 3.3016,
      "grad_norm": 117.2064437866211,
      "learning_rate": 8.994666666666666e-05,
      "loss": -111.1416,
      "step": 41270
    },
    {
      "epoch": 3.3024,
      "grad_norm": 42.095680236816406,
      "learning_rate": 8.992e-05,
      "loss": -111.1818,
      "step": 41280
    },
    {
      "epoch": 3.3032,
      "grad_norm": 41.873905181884766,
      "learning_rate": 8.989333333333334e-05,
      "loss": -110.484,
      "step": 41290
    },
    {
      "epoch": 3.304,
      "grad_norm": 46.360782623291016,
      "learning_rate": 8.986666666666666e-05,
      "loss": -111.4168,
      "step": 41300
    },
    {
      "epoch": 3.3048,
      "grad_norm": 210.23048400878906,
      "learning_rate": 8.984000000000001e-05,
      "loss": -111.4042,
      "step": 41310
    },
    {
      "epoch": 3.3056,
      "grad_norm": 44.38370895385742,
      "learning_rate": 8.981333333333334e-05,
      "loss": -111.1945,
      "step": 41320
    },
    {
      "epoch": 3.3064,
      "grad_norm": 126.63780212402344,
      "learning_rate": 8.978666666666668e-05,
      "loss": -110.7074,
      "step": 41330
    },
    {
      "epoch": 3.3072,
      "grad_norm": 40.29508590698242,
      "learning_rate": 8.976e-05,
      "loss": -110.986,
      "step": 41340
    },
    {
      "epoch": 3.308,
      "grad_norm": 37.698486328125,
      "learning_rate": 8.973333333333334e-05,
      "loss": -111.2669,
      "step": 41350
    },
    {
      "epoch": 3.3088,
      "grad_norm": 33.903602600097656,
      "learning_rate": 8.970666666666667e-05,
      "loss": -111.2931,
      "step": 41360
    },
    {
      "epoch": 3.3096,
      "grad_norm": 123.28160095214844,
      "learning_rate": 8.968000000000001e-05,
      "loss": -111.2217,
      "step": 41370
    },
    {
      "epoch": 3.3104,
      "grad_norm": 141.13217163085938,
      "learning_rate": 8.965333333333334e-05,
      "loss": -111.8452,
      "step": 41380
    },
    {
      "epoch": 3.3112,
      "grad_norm": 55.558963775634766,
      "learning_rate": 8.962666666666667e-05,
      "loss": -110.9722,
      "step": 41390
    },
    {
      "epoch": 3.312,
      "grad_norm": 32.70660400390625,
      "learning_rate": 8.960000000000001e-05,
      "loss": -111.1859,
      "step": 41400
    },
    {
      "epoch": 3.3128,
      "grad_norm": 63.18885803222656,
      "learning_rate": 8.957333333333334e-05,
      "loss": -111.0096,
      "step": 41410
    },
    {
      "epoch": 3.3136,
      "grad_norm": 141.67523193359375,
      "learning_rate": 8.954666666666667e-05,
      "loss": -111.6373,
      "step": 41420
    },
    {
      "epoch": 3.3144,
      "grad_norm": 34.29233932495117,
      "learning_rate": 8.952000000000001e-05,
      "loss": -111.6336,
      "step": 41430
    },
    {
      "epoch": 3.3152,
      "grad_norm": 43.332515716552734,
      "learning_rate": 8.949333333333334e-05,
      "loss": -111.1288,
      "step": 41440
    },
    {
      "epoch": 3.316,
      "grad_norm": 32.5153694152832,
      "learning_rate": 8.946666666666668e-05,
      "loss": -111.3391,
      "step": 41450
    },
    {
      "epoch": 3.3168,
      "grad_norm": 37.606109619140625,
      "learning_rate": 8.944e-05,
      "loss": -111.6866,
      "step": 41460
    },
    {
      "epoch": 3.3176,
      "grad_norm": 69.57290649414062,
      "learning_rate": 8.941333333333334e-05,
      "loss": -111.5461,
      "step": 41470
    },
    {
      "epoch": 3.3184,
      "grad_norm": 175.6911163330078,
      "learning_rate": 8.938666666666667e-05,
      "loss": -111.3203,
      "step": 41480
    },
    {
      "epoch": 3.3192,
      "grad_norm": 126.04901885986328,
      "learning_rate": 8.936e-05,
      "loss": -111.3836,
      "step": 41490
    },
    {
      "epoch": 3.32,
      "grad_norm": 64.44134521484375,
      "learning_rate": 8.933333333333334e-05,
      "loss": -110.9111,
      "step": 41500
    },
    {
      "epoch": 3.3208,
      "grad_norm": 68.92462158203125,
      "learning_rate": 8.930666666666667e-05,
      "loss": -112.2446,
      "step": 41510
    },
    {
      "epoch": 3.3216,
      "grad_norm": 29.447296142578125,
      "learning_rate": 8.928000000000001e-05,
      "loss": -111.1467,
      "step": 41520
    },
    {
      "epoch": 3.3224,
      "grad_norm": 136.3492889404297,
      "learning_rate": 8.925333333333333e-05,
      "loss": -111.9729,
      "step": 41530
    },
    {
      "epoch": 3.3232,
      "grad_norm": 31.71725082397461,
      "learning_rate": 8.922666666666667e-05,
      "loss": -110.7792,
      "step": 41540
    },
    {
      "epoch": 3.324,
      "grad_norm": 63.176753997802734,
      "learning_rate": 8.92e-05,
      "loss": -111.2493,
      "step": 41550
    },
    {
      "epoch": 3.3247999999999998,
      "grad_norm": 59.067569732666016,
      "learning_rate": 8.917333333333334e-05,
      "loss": -111.329,
      "step": 41560
    },
    {
      "epoch": 3.3256,
      "grad_norm": 21.047513961791992,
      "learning_rate": 8.914666666666667e-05,
      "loss": -111.2787,
      "step": 41570
    },
    {
      "epoch": 3.3264,
      "grad_norm": 209.66969299316406,
      "learning_rate": 8.912e-05,
      "loss": -111.6177,
      "step": 41580
    },
    {
      "epoch": 3.3272,
      "grad_norm": 34.9870719909668,
      "learning_rate": 8.909333333333334e-05,
      "loss": -111.0054,
      "step": 41590
    },
    {
      "epoch": 3.328,
      "grad_norm": 175.61380004882812,
      "learning_rate": 8.906666666666667e-05,
      "loss": -111.1432,
      "step": 41600
    },
    {
      "epoch": 3.3288,
      "grad_norm": 26.664148330688477,
      "learning_rate": 8.904e-05,
      "loss": -110.7367,
      "step": 41610
    },
    {
      "epoch": 3.3296,
      "grad_norm": 50.00906753540039,
      "learning_rate": 8.901333333333334e-05,
      "loss": -112.0353,
      "step": 41620
    },
    {
      "epoch": 3.3304,
      "grad_norm": 36.33747482299805,
      "learning_rate": 8.898666666666667e-05,
      "loss": -111.3884,
      "step": 41630
    },
    {
      "epoch": 3.3312,
      "grad_norm": 38.776390075683594,
      "learning_rate": 8.896e-05,
      "loss": -111.1948,
      "step": 41640
    },
    {
      "epoch": 3.332,
      "grad_norm": 104.01947784423828,
      "learning_rate": 8.893333333333333e-05,
      "loss": -111.6518,
      "step": 41650
    },
    {
      "epoch": 3.3327999999999998,
      "grad_norm": 106.13907623291016,
      "learning_rate": 8.890666666666667e-05,
      "loss": -111.5237,
      "step": 41660
    },
    {
      "epoch": 3.3336,
      "grad_norm": 75.78995513916016,
      "learning_rate": 8.888e-05,
      "loss": -111.2067,
      "step": 41670
    },
    {
      "epoch": 3.3344,
      "grad_norm": 88.58673858642578,
      "learning_rate": 8.885333333333333e-05,
      "loss": -111.9736,
      "step": 41680
    },
    {
      "epoch": 3.3352,
      "grad_norm": 62.70630645751953,
      "learning_rate": 8.882666666666667e-05,
      "loss": -111.7607,
      "step": 41690
    },
    {
      "epoch": 3.336,
      "grad_norm": 51.704978942871094,
      "learning_rate": 8.88e-05,
      "loss": -111.0877,
      "step": 41700
    },
    {
      "epoch": 3.3368,
      "grad_norm": 77.60910034179688,
      "learning_rate": 8.877333333333335e-05,
      "loss": -110.6765,
      "step": 41710
    },
    {
      "epoch": 3.3376,
      "grad_norm": 70.92022705078125,
      "learning_rate": 8.874666666666666e-05,
      "loss": -111.613,
      "step": 41720
    },
    {
      "epoch": 3.3384,
      "grad_norm": 41.03017044067383,
      "learning_rate": 8.872e-05,
      "loss": -111.4967,
      "step": 41730
    },
    {
      "epoch": 3.3392,
      "grad_norm": 148.79725646972656,
      "learning_rate": 8.869333333333333e-05,
      "loss": -111.1331,
      "step": 41740
    },
    {
      "epoch": 3.34,
      "grad_norm": 34.647621154785156,
      "learning_rate": 8.866666666666668e-05,
      "loss": -110.1075,
      "step": 41750
    },
    {
      "epoch": 3.3407999999999998,
      "grad_norm": 51.080970764160156,
      "learning_rate": 8.864e-05,
      "loss": -111.4171,
      "step": 41760
    },
    {
      "epoch": 3.3416,
      "grad_norm": 140.53089904785156,
      "learning_rate": 8.861333333333333e-05,
      "loss": -111.3435,
      "step": 41770
    },
    {
      "epoch": 3.3424,
      "grad_norm": 101.06233215332031,
      "learning_rate": 8.858666666666668e-05,
      "loss": -111.3831,
      "step": 41780
    },
    {
      "epoch": 3.3432,
      "grad_norm": 91.87543487548828,
      "learning_rate": 8.856e-05,
      "loss": -110.7365,
      "step": 41790
    },
    {
      "epoch": 3.344,
      "grad_norm": 53.373409271240234,
      "learning_rate": 8.853333333333333e-05,
      "loss": -111.1909,
      "step": 41800
    },
    {
      "epoch": 3.3448,
      "grad_norm": 38.589263916015625,
      "learning_rate": 8.850666666666668e-05,
      "loss": -111.7792,
      "step": 41810
    },
    {
      "epoch": 3.3456,
      "grad_norm": 162.46080017089844,
      "learning_rate": 8.848e-05,
      "loss": -112.5055,
      "step": 41820
    },
    {
      "epoch": 3.3464,
      "grad_norm": 168.60189819335938,
      "learning_rate": 8.845333333333334e-05,
      "loss": -111.4135,
      "step": 41830
    },
    {
      "epoch": 3.3472,
      "grad_norm": 42.23714828491211,
      "learning_rate": 8.842666666666666e-05,
      "loss": -111.1377,
      "step": 41840
    },
    {
      "epoch": 3.348,
      "grad_norm": 35.689754486083984,
      "learning_rate": 8.840000000000001e-05,
      "loss": -110.9998,
      "step": 41850
    },
    {
      "epoch": 3.3487999999999998,
      "grad_norm": 42.25265121459961,
      "learning_rate": 8.837333333333334e-05,
      "loss": -111.6564,
      "step": 41860
    },
    {
      "epoch": 3.3496,
      "grad_norm": 38.113853454589844,
      "learning_rate": 8.834666666666667e-05,
      "loss": -111.141,
      "step": 41870
    },
    {
      "epoch": 3.3504,
      "grad_norm": 23.598243713378906,
      "learning_rate": 8.832000000000001e-05,
      "loss": -110.9416,
      "step": 41880
    },
    {
      "epoch": 3.3512,
      "grad_norm": 135.22244262695312,
      "learning_rate": 8.829333333333334e-05,
      "loss": -111.9607,
      "step": 41890
    },
    {
      "epoch": 3.352,
      "grad_norm": 33.844642639160156,
      "learning_rate": 8.826666666666668e-05,
      "loss": -111.4329,
      "step": 41900
    },
    {
      "epoch": 3.3528000000000002,
      "grad_norm": 138.6682891845703,
      "learning_rate": 8.824e-05,
      "loss": -110.7433,
      "step": 41910
    },
    {
      "epoch": 3.3536,
      "grad_norm": 24.94054412841797,
      "learning_rate": 8.821333333333334e-05,
      "loss": -110.6389,
      "step": 41920
    },
    {
      "epoch": 3.3544,
      "grad_norm": 39.83132553100586,
      "learning_rate": 8.818666666666667e-05,
      "loss": -111.318,
      "step": 41930
    },
    {
      "epoch": 3.3552,
      "grad_norm": 70.15512084960938,
      "learning_rate": 8.816000000000001e-05,
      "loss": -109.8613,
      "step": 41940
    },
    {
      "epoch": 3.356,
      "grad_norm": 86.82110595703125,
      "learning_rate": 8.813333333333334e-05,
      "loss": -111.4062,
      "step": 41950
    },
    {
      "epoch": 3.3568,
      "grad_norm": 113.0111083984375,
      "learning_rate": 8.810666666666667e-05,
      "loss": -111.4373,
      "step": 41960
    },
    {
      "epoch": 3.3576,
      "grad_norm": 96.30180358886719,
      "learning_rate": 8.808000000000001e-05,
      "loss": -112.9424,
      "step": 41970
    },
    {
      "epoch": 3.3584,
      "grad_norm": 78.3019027709961,
      "learning_rate": 8.805333333333333e-05,
      "loss": -111.7815,
      "step": 41980
    },
    {
      "epoch": 3.3592,
      "grad_norm": 32.305965423583984,
      "learning_rate": 8.802666666666667e-05,
      "loss": -110.8668,
      "step": 41990
    },
    {
      "epoch": 3.36,
      "grad_norm": 48.626075744628906,
      "learning_rate": 8.800000000000001e-05,
      "loss": -111.6151,
      "step": 42000
    },
    {
      "epoch": 3.3608000000000002,
      "grad_norm": 204.1563262939453,
      "learning_rate": 8.797333333333334e-05,
      "loss": -110.5708,
      "step": 42010
    },
    {
      "epoch": 3.3616,
      "grad_norm": 107.0089340209961,
      "learning_rate": 8.794666666666667e-05,
      "loss": -111.7195,
      "step": 42020
    },
    {
      "epoch": 3.3624,
      "grad_norm": 98.40194702148438,
      "learning_rate": 8.792e-05,
      "loss": -111.5747,
      "step": 42030
    },
    {
      "epoch": 3.3632,
      "grad_norm": 27.349018096923828,
      "learning_rate": 8.789333333333334e-05,
      "loss": -111.4097,
      "step": 42040
    },
    {
      "epoch": 3.364,
      "grad_norm": 164.3209991455078,
      "learning_rate": 8.786666666666667e-05,
      "loss": -111.9044,
      "step": 42050
    },
    {
      "epoch": 3.3648,
      "grad_norm": 49.32423782348633,
      "learning_rate": 8.784e-05,
      "loss": -111.1713,
      "step": 42060
    },
    {
      "epoch": 3.3656,
      "grad_norm": 136.69021606445312,
      "learning_rate": 8.781333333333334e-05,
      "loss": -110.9781,
      "step": 42070
    },
    {
      "epoch": 3.3664,
      "grad_norm": 33.68918228149414,
      "learning_rate": 8.778666666666667e-05,
      "loss": -111.1595,
      "step": 42080
    },
    {
      "epoch": 3.3672,
      "grad_norm": 38.75394821166992,
      "learning_rate": 8.776000000000001e-05,
      "loss": -111.5092,
      "step": 42090
    },
    {
      "epoch": 3.368,
      "grad_norm": 65.58617401123047,
      "learning_rate": 8.773333333333333e-05,
      "loss": -112.5889,
      "step": 42100
    },
    {
      "epoch": 3.3688000000000002,
      "grad_norm": 74.09425354003906,
      "learning_rate": 8.770666666666667e-05,
      "loss": -110.5297,
      "step": 42110
    },
    {
      "epoch": 3.3696,
      "grad_norm": 110.17666625976562,
      "learning_rate": 8.768e-05,
      "loss": -111.2321,
      "step": 42120
    },
    {
      "epoch": 3.3704,
      "grad_norm": 39.78898239135742,
      "learning_rate": 8.765333333333334e-05,
      "loss": -111.2775,
      "step": 42130
    },
    {
      "epoch": 3.3712,
      "grad_norm": 40.757354736328125,
      "learning_rate": 8.762666666666667e-05,
      "loss": -111.2508,
      "step": 42140
    },
    {
      "epoch": 3.372,
      "grad_norm": 43.20082092285156,
      "learning_rate": 8.76e-05,
      "loss": -111.2456,
      "step": 42150
    },
    {
      "epoch": 3.3728,
      "grad_norm": 92.9530258178711,
      "learning_rate": 8.757333333333334e-05,
      "loss": -110.5757,
      "step": 42160
    },
    {
      "epoch": 3.3736,
      "grad_norm": 52.223411560058594,
      "learning_rate": 8.754666666666666e-05,
      "loss": -110.823,
      "step": 42170
    },
    {
      "epoch": 3.3744,
      "grad_norm": 40.43083190917969,
      "learning_rate": 8.752e-05,
      "loss": -110.2465,
      "step": 42180
    },
    {
      "epoch": 3.3752,
      "grad_norm": 37.725162506103516,
      "learning_rate": 8.749333333333334e-05,
      "loss": -110.5156,
      "step": 42190
    },
    {
      "epoch": 3.376,
      "grad_norm": 115.48281860351562,
      "learning_rate": 8.746666666666667e-05,
      "loss": -112.1943,
      "step": 42200
    },
    {
      "epoch": 3.3768000000000002,
      "grad_norm": 64.61431121826172,
      "learning_rate": 8.744e-05,
      "loss": -112.1239,
      "step": 42210
    },
    {
      "epoch": 3.3776,
      "grad_norm": 46.137916564941406,
      "learning_rate": 8.741333333333333e-05,
      "loss": -111.9145,
      "step": 42220
    },
    {
      "epoch": 3.3784,
      "grad_norm": 50.34053039550781,
      "learning_rate": 8.738666666666667e-05,
      "loss": -111.4756,
      "step": 42230
    },
    {
      "epoch": 3.3792,
      "grad_norm": 92.41655731201172,
      "learning_rate": 8.736e-05,
      "loss": -111.7007,
      "step": 42240
    },
    {
      "epoch": 3.38,
      "grad_norm": 90.0600814819336,
      "learning_rate": 8.733333333333333e-05,
      "loss": -110.9452,
      "step": 42250
    },
    {
      "epoch": 3.3808,
      "grad_norm": 40.88680648803711,
      "learning_rate": 8.730666666666668e-05,
      "loss": -111.9906,
      "step": 42260
    },
    {
      "epoch": 3.3816,
      "grad_norm": 62.057861328125,
      "learning_rate": 8.728e-05,
      "loss": -110.2541,
      "step": 42270
    },
    {
      "epoch": 3.3824,
      "grad_norm": 26.694297790527344,
      "learning_rate": 8.725333333333335e-05,
      "loss": -111.1222,
      "step": 42280
    },
    {
      "epoch": 3.3832,
      "grad_norm": 31.728256225585938,
      "learning_rate": 8.722666666666666e-05,
      "loss": -111.5931,
      "step": 42290
    },
    {
      "epoch": 3.384,
      "grad_norm": 53.7407112121582,
      "learning_rate": 8.72e-05,
      "loss": -111.7143,
      "step": 42300
    },
    {
      "epoch": 3.3848,
      "grad_norm": 38.11042022705078,
      "learning_rate": 8.717333333333333e-05,
      "loss": -111.1856,
      "step": 42310
    },
    {
      "epoch": 3.3856,
      "grad_norm": 109.53229522705078,
      "learning_rate": 8.714666666666666e-05,
      "loss": -111.2921,
      "step": 42320
    },
    {
      "epoch": 3.3864,
      "grad_norm": 47.93033981323242,
      "learning_rate": 8.712e-05,
      "loss": -111.7204,
      "step": 42330
    },
    {
      "epoch": 3.3872,
      "grad_norm": 33.21242904663086,
      "learning_rate": 8.709333333333334e-05,
      "loss": -111.4045,
      "step": 42340
    },
    {
      "epoch": 3.388,
      "grad_norm": 65.86109161376953,
      "learning_rate": 8.706666666666668e-05,
      "loss": -110.958,
      "step": 42350
    },
    {
      "epoch": 3.3888,
      "grad_norm": 60.081634521484375,
      "learning_rate": 8.704e-05,
      "loss": -110.5039,
      "step": 42360
    },
    {
      "epoch": 3.3896,
      "grad_norm": 55.48592758178711,
      "learning_rate": 8.701333333333334e-05,
      "loss": -110.9719,
      "step": 42370
    },
    {
      "epoch": 3.3904,
      "grad_norm": 109.21145629882812,
      "learning_rate": 8.698666666666668e-05,
      "loss": -111.0488,
      "step": 42380
    },
    {
      "epoch": 3.3912,
      "grad_norm": 74.65672302246094,
      "learning_rate": 8.696000000000001e-05,
      "loss": -110.7483,
      "step": 42390
    },
    {
      "epoch": 3.392,
      "grad_norm": 36.22970199584961,
      "learning_rate": 8.693333333333334e-05,
      "loss": -111.2689,
      "step": 42400
    },
    {
      "epoch": 3.3928,
      "grad_norm": 42.045570373535156,
      "learning_rate": 8.690666666666667e-05,
      "loss": -111.6882,
      "step": 42410
    },
    {
      "epoch": 3.3936,
      "grad_norm": 31.952180862426758,
      "learning_rate": 8.688000000000001e-05,
      "loss": -110.8221,
      "step": 42420
    },
    {
      "epoch": 3.3944,
      "grad_norm": 86.04235076904297,
      "learning_rate": 8.685333333333334e-05,
      "loss": -112.2205,
      "step": 42430
    },
    {
      "epoch": 3.3952,
      "grad_norm": 60.49272918701172,
      "learning_rate": 8.682666666666667e-05,
      "loss": -111.3657,
      "step": 42440
    },
    {
      "epoch": 3.396,
      "grad_norm": 57.442543029785156,
      "learning_rate": 8.680000000000001e-05,
      "loss": -111.6988,
      "step": 42450
    },
    {
      "epoch": 3.3968,
      "grad_norm": 31.759489059448242,
      "learning_rate": 8.677333333333334e-05,
      "loss": -111.275,
      "step": 42460
    },
    {
      "epoch": 3.3976,
      "grad_norm": 27.837430953979492,
      "learning_rate": 8.674666666666668e-05,
      "loss": -111.7682,
      "step": 42470
    },
    {
      "epoch": 3.3984,
      "grad_norm": 116.09705352783203,
      "learning_rate": 8.672e-05,
      "loss": -111.0156,
      "step": 42480
    },
    {
      "epoch": 3.3992,
      "grad_norm": 46.138877868652344,
      "learning_rate": 8.669333333333334e-05,
      "loss": -110.857,
      "step": 42490
    },
    {
      "epoch": 3.4,
      "grad_norm": 51.40980529785156,
      "learning_rate": 8.666666666666667e-05,
      "loss": -111.0485,
      "step": 42500
    },
    {
      "epoch": 3.4008,
      "grad_norm": 53.0946159362793,
      "learning_rate": 8.664e-05,
      "loss": -112.0571,
      "step": 42510
    },
    {
      "epoch": 3.4016,
      "grad_norm": 51.69338607788086,
      "learning_rate": 8.661333333333334e-05,
      "loss": -111.1505,
      "step": 42520
    },
    {
      "epoch": 3.4024,
      "grad_norm": 132.22007751464844,
      "learning_rate": 8.658666666666667e-05,
      "loss": -112.0423,
      "step": 42530
    },
    {
      "epoch": 3.4032,
      "grad_norm": 33.771644592285156,
      "learning_rate": 8.656000000000001e-05,
      "loss": -111.3863,
      "step": 42540
    },
    {
      "epoch": 3.404,
      "grad_norm": 55.0907096862793,
      "learning_rate": 8.653333333333333e-05,
      "loss": -111.3093,
      "step": 42550
    },
    {
      "epoch": 3.4048,
      "grad_norm": 458.9565734863281,
      "learning_rate": 8.650666666666667e-05,
      "loss": -111.816,
      "step": 42560
    },
    {
      "epoch": 3.4056,
      "grad_norm": 93.15802001953125,
      "learning_rate": 8.648e-05,
      "loss": -111.0665,
      "step": 42570
    },
    {
      "epoch": 3.4064,
      "grad_norm": 40.838958740234375,
      "learning_rate": 8.645333333333334e-05,
      "loss": -110.776,
      "step": 42580
    },
    {
      "epoch": 3.4072,
      "grad_norm": 60.470706939697266,
      "learning_rate": 8.642666666666667e-05,
      "loss": -112.006,
      "step": 42590
    },
    {
      "epoch": 3.408,
      "grad_norm": 29.610858917236328,
      "learning_rate": 8.64e-05,
      "loss": -111.4014,
      "step": 42600
    },
    {
      "epoch": 3.4088,
      "grad_norm": 22.640663146972656,
      "learning_rate": 8.637333333333334e-05,
      "loss": -111.1192,
      "step": 42610
    },
    {
      "epoch": 3.4096,
      "grad_norm": 22.203453063964844,
      "learning_rate": 8.634666666666667e-05,
      "loss": -112.1153,
      "step": 42620
    },
    {
      "epoch": 3.4104,
      "grad_norm": 50.08600616455078,
      "learning_rate": 8.632e-05,
      "loss": -111.4418,
      "step": 42630
    },
    {
      "epoch": 3.4112,
      "grad_norm": 38.736507415771484,
      "learning_rate": 8.629333333333334e-05,
      "loss": -110.0329,
      "step": 42640
    },
    {
      "epoch": 3.412,
      "grad_norm": 46.0704345703125,
      "learning_rate": 8.626666666666667e-05,
      "loss": -110.0203,
      "step": 42650
    },
    {
      "epoch": 3.4128,
      "grad_norm": 171.91741943359375,
      "learning_rate": 8.624000000000001e-05,
      "loss": -109.8755,
      "step": 42660
    },
    {
      "epoch": 3.4136,
      "grad_norm": 56.68942642211914,
      "learning_rate": 8.621333333333333e-05,
      "loss": -111.3456,
      "step": 42670
    },
    {
      "epoch": 3.4144,
      "grad_norm": 101.36979675292969,
      "learning_rate": 8.618666666666667e-05,
      "loss": -112.0429,
      "step": 42680
    },
    {
      "epoch": 3.4152,
      "grad_norm": 77.55879211425781,
      "learning_rate": 8.616e-05,
      "loss": -111.1022,
      "step": 42690
    },
    {
      "epoch": 3.416,
      "grad_norm": 24.35276985168457,
      "learning_rate": 8.613333333333333e-05,
      "loss": -111.3074,
      "step": 42700
    },
    {
      "epoch": 3.4168,
      "grad_norm": 38.02825927734375,
      "learning_rate": 8.610666666666667e-05,
      "loss": -111.3579,
      "step": 42710
    },
    {
      "epoch": 3.4176,
      "grad_norm": 27.524885177612305,
      "learning_rate": 8.608e-05,
      "loss": -111.526,
      "step": 42720
    },
    {
      "epoch": 3.4184,
      "grad_norm": 33.45344543457031,
      "learning_rate": 8.605333333333335e-05,
      "loss": -110.3429,
      "step": 42730
    },
    {
      "epoch": 3.4192,
      "grad_norm": 80.3014907836914,
      "learning_rate": 8.602666666666666e-05,
      "loss": -111.8028,
      "step": 42740
    },
    {
      "epoch": 3.42,
      "grad_norm": 46.68745422363281,
      "learning_rate": 8.6e-05,
      "loss": -111.0285,
      "step": 42750
    },
    {
      "epoch": 3.4208,
      "grad_norm": 53.70354080200195,
      "learning_rate": 8.597333333333333e-05,
      "loss": -111.4974,
      "step": 42760
    },
    {
      "epoch": 3.4215999999999998,
      "grad_norm": 47.35810852050781,
      "learning_rate": 8.594666666666668e-05,
      "loss": -111.296,
      "step": 42770
    },
    {
      "epoch": 3.4224,
      "grad_norm": 69.33526611328125,
      "learning_rate": 8.592e-05,
      "loss": -111.5269,
      "step": 42780
    },
    {
      "epoch": 3.4232,
      "grad_norm": 178.31527709960938,
      "learning_rate": 8.589333333333333e-05,
      "loss": -111.0573,
      "step": 42790
    },
    {
      "epoch": 3.424,
      "grad_norm": 197.11827087402344,
      "learning_rate": 8.586666666666668e-05,
      "loss": -111.6842,
      "step": 42800
    },
    {
      "epoch": 3.4248,
      "grad_norm": 375.11224365234375,
      "learning_rate": 8.584e-05,
      "loss": -111.3048,
      "step": 42810
    },
    {
      "epoch": 3.4256,
      "grad_norm": 22.543167114257812,
      "learning_rate": 8.581333333333333e-05,
      "loss": -110.2244,
      "step": 42820
    },
    {
      "epoch": 3.4264,
      "grad_norm": 67.08445739746094,
      "learning_rate": 8.578666666666668e-05,
      "loss": -111.3799,
      "step": 42830
    },
    {
      "epoch": 3.4272,
      "grad_norm": 28.680644989013672,
      "learning_rate": 8.576e-05,
      "loss": -111.5806,
      "step": 42840
    },
    {
      "epoch": 3.428,
      "grad_norm": 129.17982482910156,
      "learning_rate": 8.573333333333333e-05,
      "loss": -110.9251,
      "step": 42850
    },
    {
      "epoch": 3.4288,
      "grad_norm": 50.636634826660156,
      "learning_rate": 8.570666666666666e-05,
      "loss": -112.1293,
      "step": 42860
    },
    {
      "epoch": 3.4295999999999998,
      "grad_norm": 118.88456726074219,
      "learning_rate": 8.568e-05,
      "loss": -111.1107,
      "step": 42870
    },
    {
      "epoch": 3.4304,
      "grad_norm": 110.20787048339844,
      "learning_rate": 8.565333333333334e-05,
      "loss": -111.9513,
      "step": 42880
    },
    {
      "epoch": 3.4312,
      "grad_norm": 31.689462661743164,
      "learning_rate": 8.562666666666666e-05,
      "loss": -110.92,
      "step": 42890
    },
    {
      "epoch": 3.432,
      "grad_norm": 90.00273895263672,
      "learning_rate": 8.560000000000001e-05,
      "loss": -111.4101,
      "step": 42900
    },
    {
      "epoch": 3.4328,
      "grad_norm": 157.7938995361328,
      "learning_rate": 8.557333333333334e-05,
      "loss": -110.5764,
      "step": 42910
    },
    {
      "epoch": 3.4336,
      "grad_norm": 57.08159255981445,
      "learning_rate": 8.554666666666668e-05,
      "loss": -110.7677,
      "step": 42920
    },
    {
      "epoch": 3.4344,
      "grad_norm": 58.5230827331543,
      "learning_rate": 8.552e-05,
      "loss": -111.7725,
      "step": 42930
    },
    {
      "epoch": 3.4352,
      "grad_norm": 98.90808868408203,
      "learning_rate": 8.549333333333334e-05,
      "loss": -111.7464,
      "step": 42940
    },
    {
      "epoch": 3.436,
      "grad_norm": 34.78802490234375,
      "learning_rate": 8.546666666666667e-05,
      "loss": -111.2122,
      "step": 42950
    },
    {
      "epoch": 3.4368,
      "grad_norm": 41.40773391723633,
      "learning_rate": 8.544000000000001e-05,
      "loss": -111.9156,
      "step": 42960
    },
    {
      "epoch": 3.4375999999999998,
      "grad_norm": 50.930747985839844,
      "learning_rate": 8.541333333333334e-05,
      "loss": -111.2817,
      "step": 42970
    },
    {
      "epoch": 3.4384,
      "grad_norm": 31.578054428100586,
      "learning_rate": 8.538666666666667e-05,
      "loss": -111.2883,
      "step": 42980
    },
    {
      "epoch": 3.4392,
      "grad_norm": 35.64975357055664,
      "learning_rate": 8.536000000000001e-05,
      "loss": -111.0342,
      "step": 42990
    },
    {
      "epoch": 3.44,
      "grad_norm": 38.740238189697266,
      "learning_rate": 8.533333333333334e-05,
      "loss": -111.8927,
      "step": 43000
    },
    {
      "epoch": 3.4408,
      "grad_norm": 39.00850296020508,
      "learning_rate": 8.530666666666667e-05,
      "loss": -111.8456,
      "step": 43010
    },
    {
      "epoch": 3.4416,
      "grad_norm": 41.76163101196289,
      "learning_rate": 8.528000000000001e-05,
      "loss": -111.0772,
      "step": 43020
    },
    {
      "epoch": 3.4424,
      "grad_norm": 29.458786010742188,
      "learning_rate": 8.525333333333334e-05,
      "loss": -111.1088,
      "step": 43030
    },
    {
      "epoch": 3.4432,
      "grad_norm": 101.57965850830078,
      "learning_rate": 8.522666666666667e-05,
      "loss": -112.5512,
      "step": 43040
    },
    {
      "epoch": 3.444,
      "grad_norm": 45.41838836669922,
      "learning_rate": 8.52e-05,
      "loss": -111.6753,
      "step": 43050
    },
    {
      "epoch": 3.4448,
      "grad_norm": 34.9487190246582,
      "learning_rate": 8.517333333333334e-05,
      "loss": -110.9497,
      "step": 43060
    },
    {
      "epoch": 3.4455999999999998,
      "grad_norm": 42.77731704711914,
      "learning_rate": 8.514666666666667e-05,
      "loss": -111.3186,
      "step": 43070
    },
    {
      "epoch": 3.4464,
      "grad_norm": 51.376888275146484,
      "learning_rate": 8.512e-05,
      "loss": -112.4947,
      "step": 43080
    },
    {
      "epoch": 3.4472,
      "grad_norm": 99.82099151611328,
      "learning_rate": 8.509333333333334e-05,
      "loss": -111.0321,
      "step": 43090
    },
    {
      "epoch": 3.448,
      "grad_norm": 243.98167419433594,
      "learning_rate": 8.506666666666667e-05,
      "loss": -112.028,
      "step": 43100
    },
    {
      "epoch": 3.4488,
      "grad_norm": 52.130699157714844,
      "learning_rate": 8.504000000000001e-05,
      "loss": -111.2143,
      "step": 43110
    },
    {
      "epoch": 3.4496,
      "grad_norm": 107.6741943359375,
      "learning_rate": 8.501333333333333e-05,
      "loss": -111.1414,
      "step": 43120
    },
    {
      "epoch": 3.4504,
      "grad_norm": 42.5060920715332,
      "learning_rate": 8.498666666666667e-05,
      "loss": -110.9365,
      "step": 43130
    },
    {
      "epoch": 3.4512,
      "grad_norm": 51.88109588623047,
      "learning_rate": 8.496e-05,
      "loss": -111.8551,
      "step": 43140
    },
    {
      "epoch": 3.452,
      "grad_norm": 26.46042251586914,
      "learning_rate": 8.493333333333334e-05,
      "loss": -111.738,
      "step": 43150
    },
    {
      "epoch": 3.4528,
      "grad_norm": 68.57352447509766,
      "learning_rate": 8.490666666666667e-05,
      "loss": -110.6961,
      "step": 43160
    },
    {
      "epoch": 3.4536,
      "grad_norm": 47.53509521484375,
      "learning_rate": 8.488e-05,
      "loss": -111.0508,
      "step": 43170
    },
    {
      "epoch": 3.4544,
      "grad_norm": 34.25326919555664,
      "learning_rate": 8.485333333333334e-05,
      "loss": -111.5963,
      "step": 43180
    },
    {
      "epoch": 3.4552,
      "grad_norm": 30.63528060913086,
      "learning_rate": 8.482666666666666e-05,
      "loss": -110.4461,
      "step": 43190
    },
    {
      "epoch": 3.456,
      "grad_norm": 83.54176330566406,
      "learning_rate": 8.48e-05,
      "loss": -110.796,
      "step": 43200
    },
    {
      "epoch": 3.4568,
      "grad_norm": 50.31098937988281,
      "learning_rate": 8.477333333333334e-05,
      "loss": -110.7495,
      "step": 43210
    },
    {
      "epoch": 3.4576000000000002,
      "grad_norm": 67.53020477294922,
      "learning_rate": 8.474666666666667e-05,
      "loss": -111.9578,
      "step": 43220
    },
    {
      "epoch": 3.4584,
      "grad_norm": 40.07227325439453,
      "learning_rate": 8.472e-05,
      "loss": -110.226,
      "step": 43230
    },
    {
      "epoch": 3.4592,
      "grad_norm": 34.007938385009766,
      "learning_rate": 8.469333333333333e-05,
      "loss": -111.0935,
      "step": 43240
    },
    {
      "epoch": 3.46,
      "grad_norm": 44.86571502685547,
      "learning_rate": 8.466666666666667e-05,
      "loss": -111.3565,
      "step": 43250
    },
    {
      "epoch": 3.4608,
      "grad_norm": 58.1526985168457,
      "learning_rate": 8.464e-05,
      "loss": -110.6663,
      "step": 43260
    },
    {
      "epoch": 3.4616,
      "grad_norm": 23.80333137512207,
      "learning_rate": 8.461333333333333e-05,
      "loss": -111.4731,
      "step": 43270
    },
    {
      "epoch": 3.4624,
      "grad_norm": 26.86029052734375,
      "learning_rate": 8.458666666666667e-05,
      "loss": -110.7602,
      "step": 43280
    },
    {
      "epoch": 3.4632,
      "grad_norm": 36.605098724365234,
      "learning_rate": 8.456e-05,
      "loss": -110.4099,
      "step": 43290
    },
    {
      "epoch": 3.464,
      "grad_norm": 93.50223541259766,
      "learning_rate": 8.453333333333335e-05,
      "loss": -111.5147,
      "step": 43300
    },
    {
      "epoch": 3.4648,
      "grad_norm": 59.38697814941406,
      "learning_rate": 8.450666666666666e-05,
      "loss": -111.5262,
      "step": 43310
    },
    {
      "epoch": 3.4656000000000002,
      "grad_norm": 30.90618133544922,
      "learning_rate": 8.448e-05,
      "loss": -110.7703,
      "step": 43320
    },
    {
      "epoch": 3.4664,
      "grad_norm": 68.78778839111328,
      "learning_rate": 8.445333333333333e-05,
      "loss": -111.502,
      "step": 43330
    },
    {
      "epoch": 3.4672,
      "grad_norm": 60.49345779418945,
      "learning_rate": 8.442666666666668e-05,
      "loss": -111.2121,
      "step": 43340
    },
    {
      "epoch": 3.468,
      "grad_norm": 29.84166145324707,
      "learning_rate": 8.44e-05,
      "loss": -111.7673,
      "step": 43350
    },
    {
      "epoch": 3.4688,
      "grad_norm": 37.63874435424805,
      "learning_rate": 8.437333333333333e-05,
      "loss": -111.2565,
      "step": 43360
    },
    {
      "epoch": 3.4696,
      "grad_norm": 55.172183990478516,
      "learning_rate": 8.434666666666668e-05,
      "loss": -111.6815,
      "step": 43370
    },
    {
      "epoch": 3.4704,
      "grad_norm": 57.044559478759766,
      "learning_rate": 8.431999999999999e-05,
      "loss": -111.6,
      "step": 43380
    },
    {
      "epoch": 3.4712,
      "grad_norm": 52.91757583618164,
      "learning_rate": 8.429333333333333e-05,
      "loss": -111.1991,
      "step": 43390
    },
    {
      "epoch": 3.472,
      "grad_norm": 75.5350570678711,
      "learning_rate": 8.426666666666668e-05,
      "loss": -111.0303,
      "step": 43400
    },
    {
      "epoch": 3.4728,
      "grad_norm": 30.21624755859375,
      "learning_rate": 8.424e-05,
      "loss": -111.9956,
      "step": 43410
    },
    {
      "epoch": 3.4736000000000002,
      "grad_norm": 180.46067810058594,
      "learning_rate": 8.421333333333334e-05,
      "loss": -112.1604,
      "step": 43420
    },
    {
      "epoch": 3.4744,
      "grad_norm": 51.05671691894531,
      "learning_rate": 8.418666666666666e-05,
      "loss": -110.9525,
      "step": 43430
    },
    {
      "epoch": 3.4752,
      "grad_norm": 38.96549987792969,
      "learning_rate": 8.416000000000001e-05,
      "loss": -110.9348,
      "step": 43440
    },
    {
      "epoch": 3.476,
      "grad_norm": 52.48508071899414,
      "learning_rate": 8.413333333333334e-05,
      "loss": -111.2543,
      "step": 43450
    },
    {
      "epoch": 3.4768,
      "grad_norm": 51.47825241088867,
      "learning_rate": 8.410666666666667e-05,
      "loss": -111.7253,
      "step": 43460
    },
    {
      "epoch": 3.4776,
      "grad_norm": 125.64949035644531,
      "learning_rate": 8.408000000000001e-05,
      "loss": -110.5096,
      "step": 43470
    },
    {
      "epoch": 3.4784,
      "grad_norm": 131.77853393554688,
      "learning_rate": 8.405333333333334e-05,
      "loss": -111.779,
      "step": 43480
    },
    {
      "epoch": 3.4792,
      "grad_norm": 26.022619247436523,
      "learning_rate": 8.402666666666668e-05,
      "loss": -111.1076,
      "step": 43490
    },
    {
      "epoch": 3.48,
      "grad_norm": 25.51732635498047,
      "learning_rate": 8.4e-05,
      "loss": -110.2037,
      "step": 43500
    },
    {
      "epoch": 3.4808,
      "grad_norm": 53.04277801513672,
      "learning_rate": 8.397333333333334e-05,
      "loss": -111.4388,
      "step": 43510
    },
    {
      "epoch": 3.4816,
      "grad_norm": 41.705230712890625,
      "learning_rate": 8.394666666666667e-05,
      "loss": -110.9828,
      "step": 43520
    },
    {
      "epoch": 3.4824,
      "grad_norm": 75.93094635009766,
      "learning_rate": 8.392e-05,
      "loss": -110.8297,
      "step": 43530
    },
    {
      "epoch": 3.4832,
      "grad_norm": 52.31671905517578,
      "learning_rate": 8.389333333333334e-05,
      "loss": -111.3795,
      "step": 43540
    },
    {
      "epoch": 3.484,
      "grad_norm": 31.932828903198242,
      "learning_rate": 8.386666666666667e-05,
      "loss": -109.7569,
      "step": 43550
    },
    {
      "epoch": 3.4848,
      "grad_norm": 146.22048950195312,
      "learning_rate": 8.384000000000001e-05,
      "loss": -111.9691,
      "step": 43560
    },
    {
      "epoch": 3.4856,
      "grad_norm": 27.846860885620117,
      "learning_rate": 8.381333333333333e-05,
      "loss": -111.7732,
      "step": 43570
    },
    {
      "epoch": 3.4864,
      "grad_norm": 31.83275604248047,
      "learning_rate": 8.378666666666667e-05,
      "loss": -111.8811,
      "step": 43580
    },
    {
      "epoch": 3.4872,
      "grad_norm": 96.60900115966797,
      "learning_rate": 8.376000000000001e-05,
      "loss": -111.1528,
      "step": 43590
    },
    {
      "epoch": 3.488,
      "grad_norm": 30.11680030822754,
      "learning_rate": 8.373333333333334e-05,
      "loss": -111.58,
      "step": 43600
    },
    {
      "epoch": 3.4888,
      "grad_norm": 66.90162658691406,
      "learning_rate": 8.370666666666667e-05,
      "loss": -111.4839,
      "step": 43610
    },
    {
      "epoch": 3.4896,
      "grad_norm": 117.93115997314453,
      "learning_rate": 8.368e-05,
      "loss": -111.6029,
      "step": 43620
    },
    {
      "epoch": 3.4904,
      "grad_norm": 70.0543441772461,
      "learning_rate": 8.365333333333334e-05,
      "loss": -111.2361,
      "step": 43630
    },
    {
      "epoch": 3.4912,
      "grad_norm": 66.20816040039062,
      "learning_rate": 8.362666666666667e-05,
      "loss": -111.3698,
      "step": 43640
    },
    {
      "epoch": 3.492,
      "grad_norm": 193.98953247070312,
      "learning_rate": 8.36e-05,
      "loss": -111.2126,
      "step": 43650
    },
    {
      "epoch": 3.4928,
      "grad_norm": 46.578697204589844,
      "learning_rate": 8.357333333333334e-05,
      "loss": -110.7351,
      "step": 43660
    },
    {
      "epoch": 3.4936,
      "grad_norm": 33.857975006103516,
      "learning_rate": 8.354666666666667e-05,
      "loss": -112.1921,
      "step": 43670
    },
    {
      "epoch": 3.4944,
      "grad_norm": 37.805477142333984,
      "learning_rate": 8.352000000000001e-05,
      "loss": -111.8453,
      "step": 43680
    },
    {
      "epoch": 3.4952,
      "grad_norm": 44.28279495239258,
      "learning_rate": 8.349333333333333e-05,
      "loss": -112.0,
      "step": 43690
    },
    {
      "epoch": 3.496,
      "grad_norm": 37.15311813354492,
      "learning_rate": 8.346666666666667e-05,
      "loss": -112.3022,
      "step": 43700
    },
    {
      "epoch": 3.4968,
      "grad_norm": 105.20638275146484,
      "learning_rate": 8.344e-05,
      "loss": -111.9064,
      "step": 43710
    },
    {
      "epoch": 3.4976,
      "grad_norm": 74.55638885498047,
      "learning_rate": 8.341333333333333e-05,
      "loss": -111.6274,
      "step": 43720
    },
    {
      "epoch": 3.4984,
      "grad_norm": 117.95489501953125,
      "learning_rate": 8.338666666666667e-05,
      "loss": -111.2734,
      "step": 43730
    },
    {
      "epoch": 3.4992,
      "grad_norm": 32.07254409790039,
      "learning_rate": 8.336e-05,
      "loss": -111.3623,
      "step": 43740
    },
    {
      "epoch": 3.5,
      "grad_norm": 40.555233001708984,
      "learning_rate": 8.333333333333334e-05,
      "loss": -111.0499,
      "step": 43750
    },
    {
      "epoch": 3.5008,
      "grad_norm": 183.59970092773438,
      "learning_rate": 8.330666666666666e-05,
      "loss": -111.3335,
      "step": 43760
    },
    {
      "epoch": 3.5016,
      "grad_norm": 38.771663665771484,
      "learning_rate": 8.328e-05,
      "loss": -112.1836,
      "step": 43770
    },
    {
      "epoch": 3.5023999999999997,
      "grad_norm": 62.442955017089844,
      "learning_rate": 8.325333333333334e-05,
      "loss": -111.341,
      "step": 43780
    },
    {
      "epoch": 3.5032,
      "grad_norm": 80.81497192382812,
      "learning_rate": 8.322666666666667e-05,
      "loss": -111.5236,
      "step": 43790
    },
    {
      "epoch": 3.504,
      "grad_norm": 43.41510009765625,
      "learning_rate": 8.32e-05,
      "loss": -111.4665,
      "step": 43800
    },
    {
      "epoch": 3.5048,
      "grad_norm": 32.46952438354492,
      "learning_rate": 8.317333333333333e-05,
      "loss": -110.9667,
      "step": 43810
    },
    {
      "epoch": 3.5056000000000003,
      "grad_norm": 80.26915740966797,
      "learning_rate": 8.314666666666667e-05,
      "loss": -111.8746,
      "step": 43820
    },
    {
      "epoch": 3.5064,
      "grad_norm": 33.19334411621094,
      "learning_rate": 8.312e-05,
      "loss": -111.9543,
      "step": 43830
    },
    {
      "epoch": 3.5072,
      "grad_norm": 48.95038986206055,
      "learning_rate": 8.309333333333333e-05,
      "loss": -111.0638,
      "step": 43840
    },
    {
      "epoch": 3.508,
      "grad_norm": 66.7903823852539,
      "learning_rate": 8.306666666666668e-05,
      "loss": -111.359,
      "step": 43850
    },
    {
      "epoch": 3.5088,
      "grad_norm": 40.07395553588867,
      "learning_rate": 8.304e-05,
      "loss": -111.3364,
      "step": 43860
    },
    {
      "epoch": 3.5096,
      "grad_norm": 38.824092864990234,
      "learning_rate": 8.301333333333333e-05,
      "loss": -111.8397,
      "step": 43870
    },
    {
      "epoch": 3.5103999999999997,
      "grad_norm": 34.60139465332031,
      "learning_rate": 8.298666666666666e-05,
      "loss": -111.5584,
      "step": 43880
    },
    {
      "epoch": 3.5112,
      "grad_norm": 38.43040466308594,
      "learning_rate": 8.296e-05,
      "loss": -111.0063,
      "step": 43890
    },
    {
      "epoch": 3.512,
      "grad_norm": 70.70691680908203,
      "learning_rate": 8.293333333333333e-05,
      "loss": -112.1133,
      "step": 43900
    },
    {
      "epoch": 3.5128,
      "grad_norm": 129.9503631591797,
      "learning_rate": 8.290666666666666e-05,
      "loss": -111.3434,
      "step": 43910
    },
    {
      "epoch": 3.5136,
      "grad_norm": 111.79862213134766,
      "learning_rate": 8.288e-05,
      "loss": -111.728,
      "step": 43920
    },
    {
      "epoch": 3.5144,
      "grad_norm": 26.42044448852539,
      "learning_rate": 8.285333333333334e-05,
      "loss": -111.4932,
      "step": 43930
    },
    {
      "epoch": 3.5152,
      "grad_norm": 91.89397430419922,
      "learning_rate": 8.282666666666668e-05,
      "loss": -111.7359,
      "step": 43940
    },
    {
      "epoch": 3.516,
      "grad_norm": 48.53313064575195,
      "learning_rate": 8.28e-05,
      "loss": -111.4079,
      "step": 43950
    },
    {
      "epoch": 3.5168,
      "grad_norm": 55.39026641845703,
      "learning_rate": 8.277333333333334e-05,
      "loss": -111.0544,
      "step": 43960
    },
    {
      "epoch": 3.5176,
      "grad_norm": 294.5847473144531,
      "learning_rate": 8.274666666666668e-05,
      "loss": -111.5831,
      "step": 43970
    },
    {
      "epoch": 3.5183999999999997,
      "grad_norm": 50.776710510253906,
      "learning_rate": 8.272000000000001e-05,
      "loss": -110.9819,
      "step": 43980
    },
    {
      "epoch": 3.5192,
      "grad_norm": 73.72669219970703,
      "learning_rate": 8.269333333333334e-05,
      "loss": -111.7485,
      "step": 43990
    },
    {
      "epoch": 3.52,
      "grad_norm": 81.2850112915039,
      "learning_rate": 8.266666666666667e-05,
      "loss": -111.9815,
      "step": 44000
    },
    {
      "epoch": 3.5208,
      "grad_norm": 45.645992279052734,
      "learning_rate": 8.264000000000001e-05,
      "loss": -109.9903,
      "step": 44010
    },
    {
      "epoch": 3.5216,
      "grad_norm": 50.48612594604492,
      "learning_rate": 8.261333333333334e-05,
      "loss": -111.8796,
      "step": 44020
    },
    {
      "epoch": 3.5224,
      "grad_norm": 42.00862503051758,
      "learning_rate": 8.258666666666667e-05,
      "loss": -111.4659,
      "step": 44030
    },
    {
      "epoch": 3.5232,
      "grad_norm": 46.64592361450195,
      "learning_rate": 8.256000000000001e-05,
      "loss": -111.5879,
      "step": 44040
    },
    {
      "epoch": 3.524,
      "grad_norm": 128.33749389648438,
      "learning_rate": 8.253333333333334e-05,
      "loss": -111.0608,
      "step": 44050
    },
    {
      "epoch": 3.5248,
      "grad_norm": 51.43836212158203,
      "learning_rate": 8.250666666666667e-05,
      "loss": -111.595,
      "step": 44060
    },
    {
      "epoch": 3.5256,
      "grad_norm": 31.31437873840332,
      "learning_rate": 8.248e-05,
      "loss": -111.8605,
      "step": 44070
    },
    {
      "epoch": 3.5263999999999998,
      "grad_norm": 37.08047103881836,
      "learning_rate": 8.245333333333334e-05,
      "loss": -111.2999,
      "step": 44080
    },
    {
      "epoch": 3.5272,
      "grad_norm": 26.005590438842773,
      "learning_rate": 8.242666666666667e-05,
      "loss": -112.0861,
      "step": 44090
    },
    {
      "epoch": 3.528,
      "grad_norm": 39.35239028930664,
      "learning_rate": 8.24e-05,
      "loss": -111.3632,
      "step": 44100
    },
    {
      "epoch": 3.5288,
      "grad_norm": 45.76005935668945,
      "learning_rate": 8.237333333333334e-05,
      "loss": -111.249,
      "step": 44110
    },
    {
      "epoch": 3.5296,
      "grad_norm": 117.885986328125,
      "learning_rate": 8.234666666666667e-05,
      "loss": -112.0724,
      "step": 44120
    },
    {
      "epoch": 3.5304,
      "grad_norm": 71.27299499511719,
      "learning_rate": 8.232000000000001e-05,
      "loss": -111.3892,
      "step": 44130
    },
    {
      "epoch": 3.5312,
      "grad_norm": 28.68519401550293,
      "learning_rate": 8.229333333333333e-05,
      "loss": -111.2838,
      "step": 44140
    },
    {
      "epoch": 3.532,
      "grad_norm": 81.65473175048828,
      "learning_rate": 8.226666666666667e-05,
      "loss": -111.7217,
      "step": 44150
    },
    {
      "epoch": 3.5328,
      "grad_norm": 247.95428466796875,
      "learning_rate": 8.224000000000001e-05,
      "loss": -111.9397,
      "step": 44160
    },
    {
      "epoch": 3.5336,
      "grad_norm": 50.24918746948242,
      "learning_rate": 8.221333333333334e-05,
      "loss": -110.9066,
      "step": 44170
    },
    {
      "epoch": 3.5343999999999998,
      "grad_norm": 64.55989074707031,
      "learning_rate": 8.218666666666667e-05,
      "loss": -111.2318,
      "step": 44180
    },
    {
      "epoch": 3.5352,
      "grad_norm": 33.28671646118164,
      "learning_rate": 8.216e-05,
      "loss": -111.5784,
      "step": 44190
    },
    {
      "epoch": 3.536,
      "grad_norm": 28.67322540283203,
      "learning_rate": 8.213333333333334e-05,
      "loss": -111.2188,
      "step": 44200
    },
    {
      "epoch": 3.5368,
      "grad_norm": 35.66885757446289,
      "learning_rate": 8.210666666666667e-05,
      "loss": -110.7961,
      "step": 44210
    },
    {
      "epoch": 3.5376,
      "grad_norm": 92.40503692626953,
      "learning_rate": 8.208e-05,
      "loss": -111.8545,
      "step": 44220
    },
    {
      "epoch": 3.5384,
      "grad_norm": 27.067968368530273,
      "learning_rate": 8.205333333333334e-05,
      "loss": -112.7656,
      "step": 44230
    },
    {
      "epoch": 3.5392,
      "grad_norm": 43.252464294433594,
      "learning_rate": 8.202666666666667e-05,
      "loss": -110.9882,
      "step": 44240
    },
    {
      "epoch": 3.54,
      "grad_norm": 72.86941528320312,
      "learning_rate": 8.2e-05,
      "loss": -111.4701,
      "step": 44250
    },
    {
      "epoch": 3.5408,
      "grad_norm": 34.9102668762207,
      "learning_rate": 8.197333333333333e-05,
      "loss": -111.2417,
      "step": 44260
    },
    {
      "epoch": 3.5416,
      "grad_norm": 84.65422058105469,
      "learning_rate": 8.194666666666667e-05,
      "loss": -111.1655,
      "step": 44270
    },
    {
      "epoch": 3.5423999999999998,
      "grad_norm": 47.390525817871094,
      "learning_rate": 8.192e-05,
      "loss": -111.9728,
      "step": 44280
    },
    {
      "epoch": 3.5432,
      "grad_norm": 117.83122253417969,
      "learning_rate": 8.189333333333333e-05,
      "loss": -111.4695,
      "step": 44290
    },
    {
      "epoch": 3.544,
      "grad_norm": 44.41258239746094,
      "learning_rate": 8.186666666666667e-05,
      "loss": -111.581,
      "step": 44300
    },
    {
      "epoch": 3.5448,
      "grad_norm": 105.6176986694336,
      "learning_rate": 8.184e-05,
      "loss": -111.5079,
      "step": 44310
    },
    {
      "epoch": 3.5456,
      "grad_norm": 34.04124069213867,
      "learning_rate": 8.181333333333335e-05,
      "loss": -111.4207,
      "step": 44320
    },
    {
      "epoch": 3.5464,
      "grad_norm": 32.76441192626953,
      "learning_rate": 8.178666666666666e-05,
      "loss": -111.4089,
      "step": 44330
    },
    {
      "epoch": 3.5472,
      "grad_norm": 365.4984130859375,
      "learning_rate": 8.176e-05,
      "loss": -111.5668,
      "step": 44340
    },
    {
      "epoch": 3.548,
      "grad_norm": 59.410465240478516,
      "learning_rate": 8.173333333333335e-05,
      "loss": -111.4667,
      "step": 44350
    },
    {
      "epoch": 3.5488,
      "grad_norm": 25.945087432861328,
      "learning_rate": 8.170666666666667e-05,
      "loss": -111.3047,
      "step": 44360
    },
    {
      "epoch": 3.5496,
      "grad_norm": 132.8997802734375,
      "learning_rate": 8.168e-05,
      "loss": -112.4726,
      "step": 44370
    },
    {
      "epoch": 3.5504,
      "grad_norm": 48.648223876953125,
      "learning_rate": 8.165333333333333e-05,
      "loss": -111.5738,
      "step": 44380
    },
    {
      "epoch": 3.5512,
      "grad_norm": 51.62752914428711,
      "learning_rate": 8.162666666666668e-05,
      "loss": -112.1373,
      "step": 44390
    },
    {
      "epoch": 3.552,
      "grad_norm": 131.52210998535156,
      "learning_rate": 8.16e-05,
      "loss": -111.6867,
      "step": 44400
    },
    {
      "epoch": 3.5528,
      "grad_norm": 30.26441764831543,
      "learning_rate": 8.157333333333333e-05,
      "loss": -112.1698,
      "step": 44410
    },
    {
      "epoch": 3.5536,
      "grad_norm": 104.25504302978516,
      "learning_rate": 8.154666666666668e-05,
      "loss": -111.4792,
      "step": 44420
    },
    {
      "epoch": 3.5544000000000002,
      "grad_norm": 26.265636444091797,
      "learning_rate": 8.152e-05,
      "loss": -111.9964,
      "step": 44430
    },
    {
      "epoch": 3.5552,
      "grad_norm": 79.06270599365234,
      "learning_rate": 8.149333333333333e-05,
      "loss": -111.1782,
      "step": 44440
    },
    {
      "epoch": 3.556,
      "grad_norm": 77.08130645751953,
      "learning_rate": 8.146666666666666e-05,
      "loss": -111.4259,
      "step": 44450
    },
    {
      "epoch": 3.5568,
      "grad_norm": 66.07963562011719,
      "learning_rate": 8.144e-05,
      "loss": -112.5864,
      "step": 44460
    },
    {
      "epoch": 3.5576,
      "grad_norm": 55.262149810791016,
      "learning_rate": 8.141333333333334e-05,
      "loss": -111.1505,
      "step": 44470
    },
    {
      "epoch": 3.5584,
      "grad_norm": 142.38278198242188,
      "learning_rate": 8.138666666666666e-05,
      "loss": -111.7279,
      "step": 44480
    },
    {
      "epoch": 3.5592,
      "grad_norm": 78.56888580322266,
      "learning_rate": 8.136000000000001e-05,
      "loss": -111.407,
      "step": 44490
    },
    {
      "epoch": 3.56,
      "grad_norm": 40.67266082763672,
      "learning_rate": 8.133333333333334e-05,
      "loss": -110.8359,
      "step": 44500
    },
    {
      "epoch": 3.5608,
      "grad_norm": 43.889251708984375,
      "learning_rate": 8.130666666666668e-05,
      "loss": -111.3102,
      "step": 44510
    },
    {
      "epoch": 3.5616,
      "grad_norm": 55.64910125732422,
      "learning_rate": 8.128e-05,
      "loss": -110.9283,
      "step": 44520
    },
    {
      "epoch": 3.5624000000000002,
      "grad_norm": 44.15632247924805,
      "learning_rate": 8.125333333333334e-05,
      "loss": -112.1848,
      "step": 44530
    },
    {
      "epoch": 3.5632,
      "grad_norm": 58.04378128051758,
      "learning_rate": 8.122666666666668e-05,
      "loss": -110.5423,
      "step": 44540
    },
    {
      "epoch": 3.564,
      "grad_norm": 48.37211608886719,
      "learning_rate": 8.120000000000001e-05,
      "loss": -111.4428,
      "step": 44550
    },
    {
      "epoch": 3.5648,
      "grad_norm": 23.574857711791992,
      "learning_rate": 8.117333333333334e-05,
      "loss": -111.1136,
      "step": 44560
    },
    {
      "epoch": 3.5656,
      "grad_norm": 118.53699493408203,
      "learning_rate": 8.114666666666667e-05,
      "loss": -111.3814,
      "step": 44570
    },
    {
      "epoch": 3.5664,
      "grad_norm": 82.37521362304688,
      "learning_rate": 8.112000000000001e-05,
      "loss": -111.202,
      "step": 44580
    },
    {
      "epoch": 3.5672,
      "grad_norm": 41.48123550415039,
      "learning_rate": 8.109333333333334e-05,
      "loss": -111.8596,
      "step": 44590
    },
    {
      "epoch": 3.568,
      "grad_norm": 22.295236587524414,
      "learning_rate": 8.106666666666667e-05,
      "loss": -110.719,
      "step": 44600
    },
    {
      "epoch": 3.5688,
      "grad_norm": 47.714481353759766,
      "learning_rate": 8.104000000000001e-05,
      "loss": -110.8908,
      "step": 44610
    },
    {
      "epoch": 3.5696,
      "grad_norm": 103.82709503173828,
      "learning_rate": 8.101333333333334e-05,
      "loss": -112.0226,
      "step": 44620
    },
    {
      "epoch": 3.5704000000000002,
      "grad_norm": 123.86881256103516,
      "learning_rate": 8.098666666666667e-05,
      "loss": -111.2162,
      "step": 44630
    },
    {
      "epoch": 3.5712,
      "grad_norm": 59.79119110107422,
      "learning_rate": 8.096e-05,
      "loss": -112.1402,
      "step": 44640
    },
    {
      "epoch": 3.572,
      "grad_norm": 107.73833465576172,
      "learning_rate": 8.093333333333334e-05,
      "loss": -111.7658,
      "step": 44650
    },
    {
      "epoch": 3.5728,
      "grad_norm": 37.265377044677734,
      "learning_rate": 8.090666666666667e-05,
      "loss": -111.7503,
      "step": 44660
    },
    {
      "epoch": 3.5736,
      "grad_norm": 63.71156311035156,
      "learning_rate": 8.088e-05,
      "loss": -111.4895,
      "step": 44670
    },
    {
      "epoch": 3.5744,
      "grad_norm": 96.12643432617188,
      "learning_rate": 8.085333333333334e-05,
      "loss": -111.0806,
      "step": 44680
    },
    {
      "epoch": 3.5752,
      "grad_norm": 95.1500244140625,
      "learning_rate": 8.082666666666667e-05,
      "loss": -110.5863,
      "step": 44690
    },
    {
      "epoch": 3.576,
      "grad_norm": 25.46017074584961,
      "learning_rate": 8.080000000000001e-05,
      "loss": -112.333,
      "step": 44700
    },
    {
      "epoch": 3.5768,
      "grad_norm": 54.48619079589844,
      "learning_rate": 8.077333333333333e-05,
      "loss": -111.6038,
      "step": 44710
    },
    {
      "epoch": 3.5776,
      "grad_norm": 76.74911499023438,
      "learning_rate": 8.074666666666667e-05,
      "loss": -111.293,
      "step": 44720
    },
    {
      "epoch": 3.5784000000000002,
      "grad_norm": 38.004886627197266,
      "learning_rate": 8.072000000000001e-05,
      "loss": -112.3417,
      "step": 44730
    },
    {
      "epoch": 3.5792,
      "grad_norm": 33.85654067993164,
      "learning_rate": 8.069333333333333e-05,
      "loss": -110.7937,
      "step": 44740
    },
    {
      "epoch": 3.58,
      "grad_norm": 34.12985610961914,
      "learning_rate": 8.066666666666667e-05,
      "loss": -111.9983,
      "step": 44750
    },
    {
      "epoch": 3.5808,
      "grad_norm": 71.82158660888672,
      "learning_rate": 8.064e-05,
      "loss": -111.5641,
      "step": 44760
    },
    {
      "epoch": 3.5816,
      "grad_norm": 57.89344024658203,
      "learning_rate": 8.061333333333334e-05,
      "loss": -110.662,
      "step": 44770
    },
    {
      "epoch": 3.5824,
      "grad_norm": 40.48249816894531,
      "learning_rate": 8.058666666666667e-05,
      "loss": -111.1046,
      "step": 44780
    },
    {
      "epoch": 3.5832,
      "grad_norm": 103.27620697021484,
      "learning_rate": 8.056e-05,
      "loss": -111.9285,
      "step": 44790
    },
    {
      "epoch": 3.584,
      "grad_norm": 20.196151733398438,
      "learning_rate": 8.053333333333334e-05,
      "loss": -111.6919,
      "step": 44800
    },
    {
      "epoch": 3.5848,
      "grad_norm": 57.59223556518555,
      "learning_rate": 8.050666666666667e-05,
      "loss": -111.294,
      "step": 44810
    },
    {
      "epoch": 3.5856,
      "grad_norm": 65.26403045654297,
      "learning_rate": 8.048e-05,
      "loss": -111.4492,
      "step": 44820
    },
    {
      "epoch": 3.5864000000000003,
      "grad_norm": 33.41719055175781,
      "learning_rate": 8.045333333333333e-05,
      "loss": -111.289,
      "step": 44830
    },
    {
      "epoch": 3.5872,
      "grad_norm": 304.0921325683594,
      "learning_rate": 8.042666666666667e-05,
      "loss": -111.3322,
      "step": 44840
    },
    {
      "epoch": 3.588,
      "grad_norm": 40.22209548950195,
      "learning_rate": 8.04e-05,
      "loss": -110.8923,
      "step": 44850
    },
    {
      "epoch": 3.5888,
      "grad_norm": 99.89476776123047,
      "learning_rate": 8.037333333333333e-05,
      "loss": -110.6005,
      "step": 44860
    },
    {
      "epoch": 3.5896,
      "grad_norm": 48.22999954223633,
      "learning_rate": 8.034666666666667e-05,
      "loss": -111.394,
      "step": 44870
    },
    {
      "epoch": 3.5904,
      "grad_norm": 102.37599182128906,
      "learning_rate": 8.032e-05,
      "loss": -111.7165,
      "step": 44880
    },
    {
      "epoch": 3.5911999999999997,
      "grad_norm": 30.952180862426758,
      "learning_rate": 8.029333333333335e-05,
      "loss": -112.1713,
      "step": 44890
    },
    {
      "epoch": 3.592,
      "grad_norm": 32.01952362060547,
      "learning_rate": 8.026666666666666e-05,
      "loss": -110.6462,
      "step": 44900
    },
    {
      "epoch": 3.5928,
      "grad_norm": 54.5868034362793,
      "learning_rate": 8.024e-05,
      "loss": -111.9021,
      "step": 44910
    },
    {
      "epoch": 3.5936,
      "grad_norm": 183.12965393066406,
      "learning_rate": 8.021333333333335e-05,
      "loss": -111.9922,
      "step": 44920
    },
    {
      "epoch": 3.5944000000000003,
      "grad_norm": 39.18387222290039,
      "learning_rate": 8.018666666666666e-05,
      "loss": -110.8792,
      "step": 44930
    },
    {
      "epoch": 3.5952,
      "grad_norm": 44.725154876708984,
      "learning_rate": 8.016e-05,
      "loss": -112.4051,
      "step": 44940
    },
    {
      "epoch": 3.596,
      "grad_norm": 32.125701904296875,
      "learning_rate": 8.013333333333333e-05,
      "loss": -111.3201,
      "step": 44950
    },
    {
      "epoch": 3.5968,
      "grad_norm": 88.67329406738281,
      "learning_rate": 8.010666666666668e-05,
      "loss": -111.3874,
      "step": 44960
    },
    {
      "epoch": 3.5976,
      "grad_norm": 34.088172912597656,
      "learning_rate": 8.008e-05,
      "loss": -111.2795,
      "step": 44970
    },
    {
      "epoch": 3.5984,
      "grad_norm": 40.19898223876953,
      "learning_rate": 8.005333333333333e-05,
      "loss": -111.4023,
      "step": 44980
    },
    {
      "epoch": 3.5991999999999997,
      "grad_norm": 62.09534454345703,
      "learning_rate": 8.002666666666668e-05,
      "loss": -111.7361,
      "step": 44990
    },
    {
      "epoch": 3.6,
      "grad_norm": 67.17578887939453,
      "learning_rate": 8e-05,
      "loss": -111.7378,
      "step": 45000
    },
    {
      "epoch": 3.6008,
      "grad_norm": 30.0123233795166,
      "learning_rate": 7.997333333333334e-05,
      "loss": -111.4605,
      "step": 45010
    },
    {
      "epoch": 3.6016,
      "grad_norm": 20.901596069335938,
      "learning_rate": 7.994666666666666e-05,
      "loss": -111.2773,
      "step": 45020
    },
    {
      "epoch": 3.6024000000000003,
      "grad_norm": 40.62565994262695,
      "learning_rate": 7.992000000000001e-05,
      "loss": -111.5355,
      "step": 45030
    },
    {
      "epoch": 3.6032,
      "grad_norm": 84.15107727050781,
      "learning_rate": 7.989333333333334e-05,
      "loss": -110.5943,
      "step": 45040
    },
    {
      "epoch": 3.604,
      "grad_norm": 30.859771728515625,
      "learning_rate": 7.986666666666667e-05,
      "loss": -111.9043,
      "step": 45050
    },
    {
      "epoch": 3.6048,
      "grad_norm": 33.25714111328125,
      "learning_rate": 7.984000000000001e-05,
      "loss": -111.9438,
      "step": 45060
    },
    {
      "epoch": 3.6056,
      "grad_norm": 100.42222595214844,
      "learning_rate": 7.981333333333334e-05,
      "loss": -112.4103,
      "step": 45070
    },
    {
      "epoch": 3.6064,
      "grad_norm": 105.9513168334961,
      "learning_rate": 7.978666666666667e-05,
      "loss": -111.8703,
      "step": 45080
    },
    {
      "epoch": 3.6071999999999997,
      "grad_norm": 34.70768737792969,
      "learning_rate": 7.976e-05,
      "loss": -111.0241,
      "step": 45090
    },
    {
      "epoch": 3.608,
      "grad_norm": 24.903112411499023,
      "learning_rate": 7.973333333333334e-05,
      "loss": -111.4211,
      "step": 45100
    },
    {
      "epoch": 3.6088,
      "grad_norm": 188.25547790527344,
      "learning_rate": 7.970666666666668e-05,
      "loss": -112.2633,
      "step": 45110
    },
    {
      "epoch": 3.6096,
      "grad_norm": 31.09589195251465,
      "learning_rate": 7.968e-05,
      "loss": -110.6423,
      "step": 45120
    },
    {
      "epoch": 3.6104000000000003,
      "grad_norm": 44.200347900390625,
      "learning_rate": 7.965333333333334e-05,
      "loss": -111.348,
      "step": 45130
    },
    {
      "epoch": 3.6112,
      "grad_norm": 130.240966796875,
      "learning_rate": 7.962666666666667e-05,
      "loss": -110.8793,
      "step": 45140
    },
    {
      "epoch": 3.612,
      "grad_norm": 116.2200698852539,
      "learning_rate": 7.960000000000001e-05,
      "loss": -112.5516,
      "step": 45150
    },
    {
      "epoch": 3.6128,
      "grad_norm": 34.99578857421875,
      "learning_rate": 7.957333333333334e-05,
      "loss": -112.2748,
      "step": 45160
    },
    {
      "epoch": 3.6136,
      "grad_norm": 43.17888259887695,
      "learning_rate": 7.954666666666667e-05,
      "loss": -111.6545,
      "step": 45170
    },
    {
      "epoch": 3.6144,
      "grad_norm": 26.88173484802246,
      "learning_rate": 7.952000000000001e-05,
      "loss": -110.9277,
      "step": 45180
    },
    {
      "epoch": 3.6151999999999997,
      "grad_norm": 141.26223754882812,
      "learning_rate": 7.949333333333334e-05,
      "loss": -111.9463,
      "step": 45190
    },
    {
      "epoch": 3.616,
      "grad_norm": 46.56279373168945,
      "learning_rate": 7.946666666666667e-05,
      "loss": -111.3796,
      "step": 45200
    },
    {
      "epoch": 3.6168,
      "grad_norm": 31.12086296081543,
      "learning_rate": 7.944e-05,
      "loss": -110.8521,
      "step": 45210
    },
    {
      "epoch": 3.6176,
      "grad_norm": 44.36042785644531,
      "learning_rate": 7.941333333333334e-05,
      "loss": -111.327,
      "step": 45220
    },
    {
      "epoch": 3.6184,
      "grad_norm": 53.383907318115234,
      "learning_rate": 7.938666666666667e-05,
      "loss": -111.6584,
      "step": 45230
    },
    {
      "epoch": 3.6192,
      "grad_norm": 25.840309143066406,
      "learning_rate": 7.936e-05,
      "loss": -111.6598,
      "step": 45240
    },
    {
      "epoch": 3.62,
      "grad_norm": 21.70255470275879,
      "learning_rate": 7.933333333333334e-05,
      "loss": -112.283,
      "step": 45250
    },
    {
      "epoch": 3.6208,
      "grad_norm": 29.823680877685547,
      "learning_rate": 7.930666666666667e-05,
      "loss": -111.3085,
      "step": 45260
    },
    {
      "epoch": 3.6216,
      "grad_norm": 32.027584075927734,
      "learning_rate": 7.928e-05,
      "loss": -110.2923,
      "step": 45270
    },
    {
      "epoch": 3.6224,
      "grad_norm": 39.68007278442383,
      "learning_rate": 7.925333333333333e-05,
      "loss": -112.1566,
      "step": 45280
    },
    {
      "epoch": 3.6231999999999998,
      "grad_norm": 78.3901596069336,
      "learning_rate": 7.922666666666667e-05,
      "loss": -112.0036,
      "step": 45290
    },
    {
      "epoch": 3.624,
      "grad_norm": 29.119388580322266,
      "learning_rate": 7.920000000000001e-05,
      "loss": -111.2911,
      "step": 45300
    },
    {
      "epoch": 3.6248,
      "grad_norm": 58.21194076538086,
      "learning_rate": 7.917333333333333e-05,
      "loss": -111.741,
      "step": 45310
    },
    {
      "epoch": 3.6256,
      "grad_norm": 27.919889450073242,
      "learning_rate": 7.914666666666667e-05,
      "loss": -111.8035,
      "step": 45320
    },
    {
      "epoch": 3.6264,
      "grad_norm": 44.70851135253906,
      "learning_rate": 7.912e-05,
      "loss": -111.9004,
      "step": 45330
    },
    {
      "epoch": 3.6272,
      "grad_norm": 55.8381462097168,
      "learning_rate": 7.909333333333334e-05,
      "loss": -111.4517,
      "step": 45340
    },
    {
      "epoch": 3.628,
      "grad_norm": 39.74173355102539,
      "learning_rate": 7.906666666666667e-05,
      "loss": -111.6631,
      "step": 45350
    },
    {
      "epoch": 3.6288,
      "grad_norm": 58.69203186035156,
      "learning_rate": 7.904e-05,
      "loss": -111.1592,
      "step": 45360
    },
    {
      "epoch": 3.6296,
      "grad_norm": 29.78574562072754,
      "learning_rate": 7.901333333333334e-05,
      "loss": -110.6979,
      "step": 45370
    },
    {
      "epoch": 3.6304,
      "grad_norm": 81.81885528564453,
      "learning_rate": 7.898666666666667e-05,
      "loss": -110.5231,
      "step": 45380
    },
    {
      "epoch": 3.6311999999999998,
      "grad_norm": 93.2042465209961,
      "learning_rate": 7.896e-05,
      "loss": -111.7415,
      "step": 45390
    },
    {
      "epoch": 3.632,
      "grad_norm": 32.16972732543945,
      "learning_rate": 7.893333333333333e-05,
      "loss": -110.7984,
      "step": 45400
    },
    {
      "epoch": 3.6328,
      "grad_norm": 27.044265747070312,
      "learning_rate": 7.890666666666667e-05,
      "loss": -111.7014,
      "step": 45410
    },
    {
      "epoch": 3.6336,
      "grad_norm": 54.420997619628906,
      "learning_rate": 7.888e-05,
      "loss": -111.3585,
      "step": 45420
    },
    {
      "epoch": 3.6344,
      "grad_norm": 48.8603630065918,
      "learning_rate": 7.885333333333333e-05,
      "loss": -111.6223,
      "step": 45430
    },
    {
      "epoch": 3.6352,
      "grad_norm": 33.55851745605469,
      "learning_rate": 7.882666666666668e-05,
      "loss": -111.6067,
      "step": 45440
    },
    {
      "epoch": 3.636,
      "grad_norm": 62.61241912841797,
      "learning_rate": 7.88e-05,
      "loss": -111.4499,
      "step": 45450
    },
    {
      "epoch": 3.6368,
      "grad_norm": 59.933143615722656,
      "learning_rate": 7.877333333333333e-05,
      "loss": -111.1787,
      "step": 45460
    },
    {
      "epoch": 3.6376,
      "grad_norm": 27.00020408630371,
      "learning_rate": 7.874666666666666e-05,
      "loss": -110.2831,
      "step": 45470
    },
    {
      "epoch": 3.6384,
      "grad_norm": 73.9065933227539,
      "learning_rate": 7.872e-05,
      "loss": -111.6986,
      "step": 45480
    },
    {
      "epoch": 3.6391999999999998,
      "grad_norm": 503.64044189453125,
      "learning_rate": 7.869333333333335e-05,
      "loss": -110.7777,
      "step": 45490
    },
    {
      "epoch": 3.64,
      "grad_norm": 31.226604461669922,
      "learning_rate": 7.866666666666666e-05,
      "loss": -111.0592,
      "step": 45500
    },
    {
      "epoch": 3.6408,
      "grad_norm": 68.88925170898438,
      "learning_rate": 7.864e-05,
      "loss": -111.7533,
      "step": 45510
    },
    {
      "epoch": 3.6416,
      "grad_norm": 73.06487274169922,
      "learning_rate": 7.861333333333334e-05,
      "loss": -111.1617,
      "step": 45520
    },
    {
      "epoch": 3.6424,
      "grad_norm": 118.27896118164062,
      "learning_rate": 7.858666666666668e-05,
      "loss": -111.3661,
      "step": 45530
    },
    {
      "epoch": 3.6432,
      "grad_norm": 98.93141174316406,
      "learning_rate": 7.856000000000001e-05,
      "loss": -112.3235,
      "step": 45540
    },
    {
      "epoch": 3.644,
      "grad_norm": 90.81263732910156,
      "learning_rate": 7.853333333333334e-05,
      "loss": -111.4806,
      "step": 45550
    },
    {
      "epoch": 3.6448,
      "grad_norm": 24.790752410888672,
      "learning_rate": 7.850666666666668e-05,
      "loss": -111.9973,
      "step": 45560
    },
    {
      "epoch": 3.6456,
      "grad_norm": 2744.904052734375,
      "learning_rate": 7.848000000000001e-05,
      "loss": -111.3467,
      "step": 45570
    },
    {
      "epoch": 3.6464,
      "grad_norm": 136.13731384277344,
      "learning_rate": 7.845333333333334e-05,
      "loss": -111.2857,
      "step": 45580
    },
    {
      "epoch": 3.6471999999999998,
      "grad_norm": 62.02170944213867,
      "learning_rate": 7.842666666666667e-05,
      "loss": -112.0872,
      "step": 45590
    },
    {
      "epoch": 3.648,
      "grad_norm": 30.68977928161621,
      "learning_rate": 7.840000000000001e-05,
      "loss": -111.0003,
      "step": 45600
    },
    {
      "epoch": 3.6488,
      "grad_norm": 78.34796142578125,
      "learning_rate": 7.837333333333334e-05,
      "loss": -111.8975,
      "step": 45610
    },
    {
      "epoch": 3.6496,
      "grad_norm": 59.0953369140625,
      "learning_rate": 7.834666666666667e-05,
      "loss": -112.0174,
      "step": 45620
    },
    {
      "epoch": 3.6504,
      "grad_norm": 102.43183135986328,
      "learning_rate": 7.832000000000001e-05,
      "loss": -111.2918,
      "step": 45630
    },
    {
      "epoch": 3.6512000000000002,
      "grad_norm": 34.52424240112305,
      "learning_rate": 7.829333333333334e-05,
      "loss": -111.1395,
      "step": 45640
    },
    {
      "epoch": 3.652,
      "grad_norm": 75.85316467285156,
      "learning_rate": 7.826666666666667e-05,
      "loss": -110.9276,
      "step": 45650
    },
    {
      "epoch": 3.6528,
      "grad_norm": 31.261747360229492,
      "learning_rate": 7.824e-05,
      "loss": -111.703,
      "step": 45660
    },
    {
      "epoch": 3.6536,
      "grad_norm": 66.06733703613281,
      "learning_rate": 7.821333333333334e-05,
      "loss": -111.6508,
      "step": 45670
    },
    {
      "epoch": 3.6544,
      "grad_norm": 57.641204833984375,
      "learning_rate": 7.818666666666667e-05,
      "loss": -111.5405,
      "step": 45680
    },
    {
      "epoch": 3.6552,
      "grad_norm": 34.2083854675293,
      "learning_rate": 7.816e-05,
      "loss": -112.0237,
      "step": 45690
    },
    {
      "epoch": 3.656,
      "grad_norm": 60.04465866088867,
      "learning_rate": 7.813333333333334e-05,
      "loss": -111.544,
      "step": 45700
    },
    {
      "epoch": 3.6568,
      "grad_norm": 41.96025848388672,
      "learning_rate": 7.810666666666667e-05,
      "loss": -111.133,
      "step": 45710
    },
    {
      "epoch": 3.6576,
      "grad_norm": 33.4051399230957,
      "learning_rate": 7.808000000000001e-05,
      "loss": -111.344,
      "step": 45720
    },
    {
      "epoch": 3.6584,
      "grad_norm": 48.106048583984375,
      "learning_rate": 7.805333333333334e-05,
      "loss": -111.2657,
      "step": 45730
    },
    {
      "epoch": 3.6592000000000002,
      "grad_norm": 49.07170104980469,
      "learning_rate": 7.802666666666667e-05,
      "loss": -111.6391,
      "step": 45740
    },
    {
      "epoch": 3.66,
      "grad_norm": 34.42952346801758,
      "learning_rate": 7.800000000000001e-05,
      "loss": -111.6983,
      "step": 45750
    },
    {
      "epoch": 3.6608,
      "grad_norm": 85.28894805908203,
      "learning_rate": 7.797333333333333e-05,
      "loss": -111.1869,
      "step": 45760
    },
    {
      "epoch": 3.6616,
      "grad_norm": 109.93775939941406,
      "learning_rate": 7.794666666666667e-05,
      "loss": -111.3251,
      "step": 45770
    },
    {
      "epoch": 3.6624,
      "grad_norm": 94.46925354003906,
      "learning_rate": 7.792e-05,
      "loss": -111.5985,
      "step": 45780
    },
    {
      "epoch": 3.6632,
      "grad_norm": 63.15584945678711,
      "learning_rate": 7.789333333333334e-05,
      "loss": -110.6501,
      "step": 45790
    },
    {
      "epoch": 3.664,
      "grad_norm": 61.11097717285156,
      "learning_rate": 7.786666666666667e-05,
      "loss": -110.8189,
      "step": 45800
    },
    {
      "epoch": 3.6648,
      "grad_norm": 97.89227294921875,
      "learning_rate": 7.784e-05,
      "loss": -111.6016,
      "step": 45810
    },
    {
      "epoch": 3.6656,
      "grad_norm": 72.1829833984375,
      "learning_rate": 7.781333333333334e-05,
      "loss": -110.9807,
      "step": 45820
    },
    {
      "epoch": 3.6664,
      "grad_norm": 113.05744171142578,
      "learning_rate": 7.778666666666667e-05,
      "loss": -111.1337,
      "step": 45830
    },
    {
      "epoch": 3.6672000000000002,
      "grad_norm": 55.54572677612305,
      "learning_rate": 7.776e-05,
      "loss": -111.7727,
      "step": 45840
    },
    {
      "epoch": 3.668,
      "grad_norm": 60.71306610107422,
      "learning_rate": 7.773333333333333e-05,
      "loss": -111.9661,
      "step": 45850
    },
    {
      "epoch": 3.6688,
      "grad_norm": 52.452980041503906,
      "learning_rate": 7.770666666666667e-05,
      "loss": -111.5614,
      "step": 45860
    },
    {
      "epoch": 3.6696,
      "grad_norm": 67.2098617553711,
      "learning_rate": 7.768e-05,
      "loss": -111.3536,
      "step": 45870
    },
    {
      "epoch": 3.6704,
      "grad_norm": 33.70081329345703,
      "learning_rate": 7.765333333333333e-05,
      "loss": -111.2073,
      "step": 45880
    },
    {
      "epoch": 3.6712,
      "grad_norm": 28.26893424987793,
      "learning_rate": 7.762666666666667e-05,
      "loss": -111.1794,
      "step": 45890
    },
    {
      "epoch": 3.672,
      "grad_norm": 73.66577911376953,
      "learning_rate": 7.76e-05,
      "loss": -111.4502,
      "step": 45900
    },
    {
      "epoch": 3.6728,
      "grad_norm": 74.21328735351562,
      "learning_rate": 7.757333333333335e-05,
      "loss": -110.7978,
      "step": 45910
    },
    {
      "epoch": 3.6736,
      "grad_norm": 84.99468994140625,
      "learning_rate": 7.754666666666667e-05,
      "loss": -112.1351,
      "step": 45920
    },
    {
      "epoch": 3.6744,
      "grad_norm": 74.9343490600586,
      "learning_rate": 7.752e-05,
      "loss": -111.227,
      "step": 45930
    },
    {
      "epoch": 3.6752000000000002,
      "grad_norm": 29.99639892578125,
      "learning_rate": 7.749333333333335e-05,
      "loss": -110.8006,
      "step": 45940
    },
    {
      "epoch": 3.676,
      "grad_norm": 63.603675842285156,
      "learning_rate": 7.746666666666666e-05,
      "loss": -112.2774,
      "step": 45950
    },
    {
      "epoch": 3.6768,
      "grad_norm": 46.754188537597656,
      "learning_rate": 7.744e-05,
      "loss": -110.0736,
      "step": 45960
    },
    {
      "epoch": 3.6776,
      "grad_norm": 53.33839797973633,
      "learning_rate": 7.741333333333333e-05,
      "loss": -111.537,
      "step": 45970
    },
    {
      "epoch": 3.6784,
      "grad_norm": 92.52735900878906,
      "learning_rate": 7.738666666666668e-05,
      "loss": -111.1301,
      "step": 45980
    },
    {
      "epoch": 3.6792,
      "grad_norm": 149.0948486328125,
      "learning_rate": 7.736e-05,
      "loss": -110.8891,
      "step": 45990
    },
    {
      "epoch": 3.68,
      "grad_norm": 2764.47705078125,
      "learning_rate": 7.733333333333333e-05,
      "loss": -112.1521,
      "step": 46000
    },
    {
      "epoch": 3.6808,
      "grad_norm": 40.05188751220703,
      "learning_rate": 7.730666666666668e-05,
      "loss": -111.4511,
      "step": 46010
    },
    {
      "epoch": 3.6816,
      "grad_norm": 89.11796569824219,
      "learning_rate": 7.728e-05,
      "loss": -111.1116,
      "step": 46020
    },
    {
      "epoch": 3.6824,
      "grad_norm": 50.47806930541992,
      "learning_rate": 7.725333333333333e-05,
      "loss": -112.1776,
      "step": 46030
    },
    {
      "epoch": 3.6832000000000003,
      "grad_norm": 33.701107025146484,
      "learning_rate": 7.722666666666666e-05,
      "loss": -110.1207,
      "step": 46040
    },
    {
      "epoch": 3.684,
      "grad_norm": 50.60415267944336,
      "learning_rate": 7.72e-05,
      "loss": -111.184,
      "step": 46050
    },
    {
      "epoch": 3.6848,
      "grad_norm": 30.69745445251465,
      "learning_rate": 7.717333333333334e-05,
      "loss": -112.1173,
      "step": 46060
    },
    {
      "epoch": 3.6856,
      "grad_norm": 51.51918029785156,
      "learning_rate": 7.714666666666666e-05,
      "loss": -110.6269,
      "step": 46070
    },
    {
      "epoch": 3.6864,
      "grad_norm": 29.44828987121582,
      "learning_rate": 7.712000000000001e-05,
      "loss": -111.5615,
      "step": 46080
    },
    {
      "epoch": 3.6872,
      "grad_norm": 35.20843505859375,
      "learning_rate": 7.709333333333334e-05,
      "loss": -110.5966,
      "step": 46090
    },
    {
      "epoch": 3.6879999999999997,
      "grad_norm": 32.06095886230469,
      "learning_rate": 7.706666666666668e-05,
      "loss": -111.1066,
      "step": 46100
    },
    {
      "epoch": 3.6888,
      "grad_norm": 32.92232894897461,
      "learning_rate": 7.704000000000001e-05,
      "loss": -111.9646,
      "step": 46110
    },
    {
      "epoch": 3.6896,
      "grad_norm": 31.785404205322266,
      "learning_rate": 7.701333333333334e-05,
      "loss": -111.6097,
      "step": 46120
    },
    {
      "epoch": 3.6904,
      "grad_norm": 32.22369384765625,
      "learning_rate": 7.698666666666668e-05,
      "loss": -110.8897,
      "step": 46130
    },
    {
      "epoch": 3.6912000000000003,
      "grad_norm": 39.743019104003906,
      "learning_rate": 7.696e-05,
      "loss": -109.8893,
      "step": 46140
    },
    {
      "epoch": 3.692,
      "grad_norm": 151.1382598876953,
      "learning_rate": 7.693333333333334e-05,
      "loss": -112.158,
      "step": 46150
    },
    {
      "epoch": 3.6928,
      "grad_norm": 51.69971466064453,
      "learning_rate": 7.690666666666667e-05,
      "loss": -110.752,
      "step": 46160
    },
    {
      "epoch": 3.6936,
      "grad_norm": 63.059635162353516,
      "learning_rate": 7.688000000000001e-05,
      "loss": -111.5164,
      "step": 46170
    },
    {
      "epoch": 3.6944,
      "grad_norm": 135.85488891601562,
      "learning_rate": 7.685333333333334e-05,
      "loss": -111.1971,
      "step": 46180
    },
    {
      "epoch": 3.6952,
      "grad_norm": 72.35347747802734,
      "learning_rate": 7.682666666666667e-05,
      "loss": -112.0899,
      "step": 46190
    },
    {
      "epoch": 3.6959999999999997,
      "grad_norm": 62.78002166748047,
      "learning_rate": 7.680000000000001e-05,
      "loss": -110.5149,
      "step": 46200
    },
    {
      "epoch": 3.6968,
      "grad_norm": 28.277408599853516,
      "learning_rate": 7.677333333333334e-05,
      "loss": -111.8441,
      "step": 46210
    },
    {
      "epoch": 3.6976,
      "grad_norm": 39.523536682128906,
      "learning_rate": 7.674666666666667e-05,
      "loss": -111.1469,
      "step": 46220
    },
    {
      "epoch": 3.6984,
      "grad_norm": 29.238208770751953,
      "learning_rate": 7.672e-05,
      "loss": -112.0004,
      "step": 46230
    },
    {
      "epoch": 3.6992000000000003,
      "grad_norm": 97.91881561279297,
      "learning_rate": 7.669333333333334e-05,
      "loss": -111.3137,
      "step": 46240
    },
    {
      "epoch": 3.7,
      "grad_norm": 82.87481689453125,
      "learning_rate": 7.666666666666667e-05,
      "loss": -111.7798,
      "step": 46250
    },
    {
      "epoch": 3.7008,
      "grad_norm": 51.98680877685547,
      "learning_rate": 7.664e-05,
      "loss": -111.0242,
      "step": 46260
    },
    {
      "epoch": 3.7016,
      "grad_norm": 35.19526672363281,
      "learning_rate": 7.661333333333334e-05,
      "loss": -111.2848,
      "step": 46270
    },
    {
      "epoch": 3.7024,
      "grad_norm": 111.91925811767578,
      "learning_rate": 7.658666666666667e-05,
      "loss": -111.8801,
      "step": 46280
    },
    {
      "epoch": 3.7032,
      "grad_norm": 44.288421630859375,
      "learning_rate": 7.656e-05,
      "loss": -110.9537,
      "step": 46290
    },
    {
      "epoch": 3.7039999999999997,
      "grad_norm": 61.28770446777344,
      "learning_rate": 7.653333333333333e-05,
      "loss": -112.1714,
      "step": 46300
    },
    {
      "epoch": 3.7048,
      "grad_norm": 56.29267883300781,
      "learning_rate": 7.650666666666667e-05,
      "loss": -111.9096,
      "step": 46310
    },
    {
      "epoch": 3.7056,
      "grad_norm": 41.23085021972656,
      "learning_rate": 7.648000000000001e-05,
      "loss": -110.9222,
      "step": 46320
    },
    {
      "epoch": 3.7064,
      "grad_norm": 55.8948860168457,
      "learning_rate": 7.645333333333333e-05,
      "loss": -111.2136,
      "step": 46330
    },
    {
      "epoch": 3.7072000000000003,
      "grad_norm": 35.947975158691406,
      "learning_rate": 7.642666666666667e-05,
      "loss": -110.9375,
      "step": 46340
    },
    {
      "epoch": 3.708,
      "grad_norm": 32.075618743896484,
      "learning_rate": 7.64e-05,
      "loss": -111.9403,
      "step": 46350
    },
    {
      "epoch": 3.7088,
      "grad_norm": 77.9349594116211,
      "learning_rate": 7.637333333333334e-05,
      "loss": -111.7451,
      "step": 46360
    },
    {
      "epoch": 3.7096,
      "grad_norm": 61.70235824584961,
      "learning_rate": 7.634666666666667e-05,
      "loss": -111.0612,
      "step": 46370
    },
    {
      "epoch": 3.7104,
      "grad_norm": 52.71894454956055,
      "learning_rate": 7.632e-05,
      "loss": -110.697,
      "step": 46380
    },
    {
      "epoch": 3.7112,
      "grad_norm": 34.53092956542969,
      "learning_rate": 7.629333333333334e-05,
      "loss": -113.0025,
      "step": 46390
    },
    {
      "epoch": 3.7119999999999997,
      "grad_norm": 38.866172790527344,
      "learning_rate": 7.626666666666667e-05,
      "loss": -110.8725,
      "step": 46400
    },
    {
      "epoch": 3.7128,
      "grad_norm": 35.5819206237793,
      "learning_rate": 7.624e-05,
      "loss": -112.1257,
      "step": 46410
    },
    {
      "epoch": 3.7136,
      "grad_norm": 65.9898452758789,
      "learning_rate": 7.621333333333333e-05,
      "loss": -110.9245,
      "step": 46420
    },
    {
      "epoch": 3.7144,
      "grad_norm": 33.90511703491211,
      "learning_rate": 7.618666666666667e-05,
      "loss": -110.9402,
      "step": 46430
    },
    {
      "epoch": 3.7152,
      "grad_norm": 135.6141815185547,
      "learning_rate": 7.616e-05,
      "loss": -111.7167,
      "step": 46440
    },
    {
      "epoch": 3.716,
      "grad_norm": 231.74642944335938,
      "learning_rate": 7.613333333333333e-05,
      "loss": -110.6012,
      "step": 46450
    },
    {
      "epoch": 3.7168,
      "grad_norm": 86.23246765136719,
      "learning_rate": 7.610666666666667e-05,
      "loss": -110.9607,
      "step": 46460
    },
    {
      "epoch": 3.7176,
      "grad_norm": 24.44741439819336,
      "learning_rate": 7.608e-05,
      "loss": -111.3028,
      "step": 46470
    },
    {
      "epoch": 3.7184,
      "grad_norm": 134.26193237304688,
      "learning_rate": 7.605333333333333e-05,
      "loss": -110.1721,
      "step": 46480
    },
    {
      "epoch": 3.7192,
      "grad_norm": 134.42027282714844,
      "learning_rate": 7.602666666666666e-05,
      "loss": -110.2761,
      "step": 46490
    },
    {
      "epoch": 3.7199999999999998,
      "grad_norm": 126.30406951904297,
      "learning_rate": 7.6e-05,
      "loss": -111.4957,
      "step": 46500
    },
    {
      "epoch": 3.7208,
      "grad_norm": 46.054222106933594,
      "learning_rate": 7.597333333333335e-05,
      "loss": -111.5739,
      "step": 46510
    },
    {
      "epoch": 3.7216,
      "grad_norm": 6430.7421875,
      "learning_rate": 7.594666666666666e-05,
      "loss": -110.8429,
      "step": 46520
    },
    {
      "epoch": 3.7224,
      "grad_norm": 41.67256164550781,
      "learning_rate": 7.592e-05,
      "loss": -111.8885,
      "step": 46530
    },
    {
      "epoch": 3.7232,
      "grad_norm": 197.87437438964844,
      "learning_rate": 7.589333333333333e-05,
      "loss": -110.772,
      "step": 46540
    },
    {
      "epoch": 3.724,
      "grad_norm": 187.44187927246094,
      "learning_rate": 7.586666666666668e-05,
      "loss": -110.7662,
      "step": 46550
    },
    {
      "epoch": 3.7248,
      "grad_norm": 93.68598175048828,
      "learning_rate": 7.584e-05,
      "loss": -111.4294,
      "step": 46560
    },
    {
      "epoch": 3.7256,
      "grad_norm": 48.55282974243164,
      "learning_rate": 7.581333333333333e-05,
      "loss": -111.1957,
      "step": 46570
    },
    {
      "epoch": 3.7264,
      "grad_norm": 82.00113677978516,
      "learning_rate": 7.578666666666668e-05,
      "loss": -111.5064,
      "step": 46580
    },
    {
      "epoch": 3.7272,
      "grad_norm": 52.14048767089844,
      "learning_rate": 7.576e-05,
      "loss": -110.8159,
      "step": 46590
    },
    {
      "epoch": 3.7279999999999998,
      "grad_norm": 84.33307647705078,
      "learning_rate": 7.573333333333334e-05,
      "loss": -111.9339,
      "step": 46600
    },
    {
      "epoch": 3.7288,
      "grad_norm": 39.19702911376953,
      "learning_rate": 7.570666666666666e-05,
      "loss": -111.3678,
      "step": 46610
    },
    {
      "epoch": 3.7296,
      "grad_norm": 109.35610961914062,
      "learning_rate": 7.568000000000001e-05,
      "loss": -111.7702,
      "step": 46620
    },
    {
      "epoch": 3.7304,
      "grad_norm": 45.98822784423828,
      "learning_rate": 7.565333333333334e-05,
      "loss": -111.4778,
      "step": 46630
    },
    {
      "epoch": 3.7312,
      "grad_norm": 130.815673828125,
      "learning_rate": 7.562666666666667e-05,
      "loss": -111.0613,
      "step": 46640
    },
    {
      "epoch": 3.732,
      "grad_norm": 67.05474090576172,
      "learning_rate": 7.560000000000001e-05,
      "loss": -112.0077,
      "step": 46650
    },
    {
      "epoch": 3.7328,
      "grad_norm": 73.31608581542969,
      "learning_rate": 7.557333333333334e-05,
      "loss": -111.2972,
      "step": 46660
    },
    {
      "epoch": 3.7336,
      "grad_norm": 62.76324462890625,
      "learning_rate": 7.554666666666667e-05,
      "loss": -111.5467,
      "step": 46670
    },
    {
      "epoch": 3.7344,
      "grad_norm": 65.3499526977539,
      "learning_rate": 7.552e-05,
      "loss": -111.2974,
      "step": 46680
    },
    {
      "epoch": 3.7352,
      "grad_norm": 38.8110466003418,
      "learning_rate": 7.549333333333334e-05,
      "loss": -112.0817,
      "step": 46690
    },
    {
      "epoch": 3.7359999999999998,
      "grad_norm": 143.44818115234375,
      "learning_rate": 7.546666666666668e-05,
      "loss": -111.4386,
      "step": 46700
    },
    {
      "epoch": 3.7368,
      "grad_norm": 109.30781555175781,
      "learning_rate": 7.544e-05,
      "loss": -111.0638,
      "step": 46710
    },
    {
      "epoch": 3.7376,
      "grad_norm": 34.21164321899414,
      "learning_rate": 7.541333333333334e-05,
      "loss": -110.7661,
      "step": 46720
    },
    {
      "epoch": 3.7384,
      "grad_norm": 54.2440071105957,
      "learning_rate": 7.538666666666667e-05,
      "loss": -111.3599,
      "step": 46730
    },
    {
      "epoch": 3.7392,
      "grad_norm": 32.84284973144531,
      "learning_rate": 7.536000000000001e-05,
      "loss": -111.5628,
      "step": 46740
    },
    {
      "epoch": 3.74,
      "grad_norm": 188.7039031982422,
      "learning_rate": 7.533333333333334e-05,
      "loss": -111.5527,
      "step": 46750
    },
    {
      "epoch": 3.7408,
      "grad_norm": 29.036476135253906,
      "learning_rate": 7.530666666666667e-05,
      "loss": -111.2131,
      "step": 46760
    },
    {
      "epoch": 3.7416,
      "grad_norm": 86.80862426757812,
      "learning_rate": 7.528000000000001e-05,
      "loss": -111.2112,
      "step": 46770
    },
    {
      "epoch": 3.7424,
      "grad_norm": 110.94564819335938,
      "learning_rate": 7.525333333333334e-05,
      "loss": -112.1983,
      "step": 46780
    },
    {
      "epoch": 3.7432,
      "grad_norm": 49.20888137817383,
      "learning_rate": 7.522666666666667e-05,
      "loss": -111.3177,
      "step": 46790
    },
    {
      "epoch": 3.7439999999999998,
      "grad_norm": 39.9013786315918,
      "learning_rate": 7.52e-05,
      "loss": -111.6967,
      "step": 46800
    },
    {
      "epoch": 3.7448,
      "grad_norm": 75.06953430175781,
      "learning_rate": 7.517333333333334e-05,
      "loss": -111.7028,
      "step": 46810
    },
    {
      "epoch": 3.7456,
      "grad_norm": 27.506824493408203,
      "learning_rate": 7.514666666666667e-05,
      "loss": -111.8389,
      "step": 46820
    },
    {
      "epoch": 3.7464,
      "grad_norm": 47.63574981689453,
      "learning_rate": 7.512e-05,
      "loss": -112.1221,
      "step": 46830
    },
    {
      "epoch": 3.7472,
      "grad_norm": 30.111082077026367,
      "learning_rate": 7.509333333333334e-05,
      "loss": -111.6371,
      "step": 46840
    },
    {
      "epoch": 3.748,
      "grad_norm": 28.456987380981445,
      "learning_rate": 7.506666666666667e-05,
      "loss": -111.3666,
      "step": 46850
    },
    {
      "epoch": 3.7488,
      "grad_norm": 66.06217193603516,
      "learning_rate": 7.504e-05,
      "loss": -111.1365,
      "step": 46860
    },
    {
      "epoch": 3.7496,
      "grad_norm": 30.630712509155273,
      "learning_rate": 7.501333333333333e-05,
      "loss": -110.5547,
      "step": 46870
    },
    {
      "epoch": 3.7504,
      "grad_norm": 25.01334571838379,
      "learning_rate": 7.498666666666667e-05,
      "loss": -111.5939,
      "step": 46880
    },
    {
      "epoch": 3.7512,
      "grad_norm": 55.31086349487305,
      "learning_rate": 7.496000000000001e-05,
      "loss": -110.6484,
      "step": 46890
    },
    {
      "epoch": 3.752,
      "grad_norm": 54.12968063354492,
      "learning_rate": 7.493333333333333e-05,
      "loss": -111.254,
      "step": 46900
    },
    {
      "epoch": 3.7528,
      "grad_norm": 78.72183227539062,
      "learning_rate": 7.490666666666667e-05,
      "loss": -110.6875,
      "step": 46910
    },
    {
      "epoch": 3.7536,
      "grad_norm": 52.532752990722656,
      "learning_rate": 7.488e-05,
      "loss": -111.5664,
      "step": 46920
    },
    {
      "epoch": 3.7544,
      "grad_norm": 63.251651763916016,
      "learning_rate": 7.485333333333334e-05,
      "loss": -111.3382,
      "step": 46930
    },
    {
      "epoch": 3.7552,
      "grad_norm": 45.57883834838867,
      "learning_rate": 7.482666666666667e-05,
      "loss": -110.9952,
      "step": 46940
    },
    {
      "epoch": 3.7560000000000002,
      "grad_norm": 30.419113159179688,
      "learning_rate": 7.48e-05,
      "loss": -110.8056,
      "step": 46950
    },
    {
      "epoch": 3.7568,
      "grad_norm": 43.82199478149414,
      "learning_rate": 7.477333333333334e-05,
      "loss": -111.7304,
      "step": 46960
    },
    {
      "epoch": 3.7576,
      "grad_norm": 89.48934936523438,
      "learning_rate": 7.474666666666666e-05,
      "loss": -111.6637,
      "step": 46970
    },
    {
      "epoch": 3.7584,
      "grad_norm": 51.00333786010742,
      "learning_rate": 7.472e-05,
      "loss": -111.2541,
      "step": 46980
    },
    {
      "epoch": 3.7592,
      "grad_norm": 65.27178955078125,
      "learning_rate": 7.469333333333333e-05,
      "loss": -111.6156,
      "step": 46990
    },
    {
      "epoch": 3.76,
      "grad_norm": 31.924232482910156,
      "learning_rate": 7.466666666666667e-05,
      "loss": -112.0939,
      "step": 47000
    },
    {
      "epoch": 3.7608,
      "grad_norm": 33.04695129394531,
      "learning_rate": 7.464e-05,
      "loss": -111.6189,
      "step": 47010
    },
    {
      "epoch": 3.7616,
      "grad_norm": 93.13652038574219,
      "learning_rate": 7.461333333333333e-05,
      "loss": -112.0372,
      "step": 47020
    },
    {
      "epoch": 3.7624,
      "grad_norm": 55.28826141357422,
      "learning_rate": 7.458666666666668e-05,
      "loss": -110.9552,
      "step": 47030
    },
    {
      "epoch": 3.7632,
      "grad_norm": 58.36078643798828,
      "learning_rate": 7.456e-05,
      "loss": -112.3523,
      "step": 47040
    },
    {
      "epoch": 3.7640000000000002,
      "grad_norm": 61.75510025024414,
      "learning_rate": 7.453333333333333e-05,
      "loss": -111.4967,
      "step": 47050
    },
    {
      "epoch": 3.7648,
      "grad_norm": 58.660099029541016,
      "learning_rate": 7.450666666666666e-05,
      "loss": -111.8114,
      "step": 47060
    },
    {
      "epoch": 3.7656,
      "grad_norm": 38.73515319824219,
      "learning_rate": 7.448e-05,
      "loss": -111.1511,
      "step": 47070
    },
    {
      "epoch": 3.7664,
      "grad_norm": 31.984498977661133,
      "learning_rate": 7.445333333333335e-05,
      "loss": -110.9162,
      "step": 47080
    },
    {
      "epoch": 3.7672,
      "grad_norm": 28.85884666442871,
      "learning_rate": 7.442666666666666e-05,
      "loss": -111.2527,
      "step": 47090
    },
    {
      "epoch": 3.768,
      "grad_norm": 92.06060028076172,
      "learning_rate": 7.44e-05,
      "loss": -110.8869,
      "step": 47100
    },
    {
      "epoch": 3.7688,
      "grad_norm": 85.32704162597656,
      "learning_rate": 7.437333333333334e-05,
      "loss": -111.8552,
      "step": 47110
    },
    {
      "epoch": 3.7696,
      "grad_norm": 61.05063247680664,
      "learning_rate": 7.434666666666668e-05,
      "loss": -111.1599,
      "step": 47120
    },
    {
      "epoch": 3.7704,
      "grad_norm": 62.91483688354492,
      "learning_rate": 7.432e-05,
      "loss": -111.3776,
      "step": 47130
    },
    {
      "epoch": 3.7712,
      "grad_norm": 156.8070526123047,
      "learning_rate": 7.429333333333334e-05,
      "loss": -110.747,
      "step": 47140
    },
    {
      "epoch": 3.7720000000000002,
      "grad_norm": 23.431194305419922,
      "learning_rate": 7.426666666666668e-05,
      "loss": -110.7135,
      "step": 47150
    },
    {
      "epoch": 3.7728,
      "grad_norm": 42.48374557495117,
      "learning_rate": 7.424e-05,
      "loss": -112.0011,
      "step": 47160
    },
    {
      "epoch": 3.7736,
      "grad_norm": 72.21988677978516,
      "learning_rate": 7.421333333333334e-05,
      "loss": -110.9071,
      "step": 47170
    },
    {
      "epoch": 3.7744,
      "grad_norm": 68.3193588256836,
      "learning_rate": 7.418666666666667e-05,
      "loss": -110.9494,
      "step": 47180
    },
    {
      "epoch": 3.7752,
      "grad_norm": 37.89217758178711,
      "learning_rate": 7.416000000000001e-05,
      "loss": -111.2065,
      "step": 47190
    },
    {
      "epoch": 3.776,
      "grad_norm": 105.14753723144531,
      "learning_rate": 7.413333333333334e-05,
      "loss": -112.1147,
      "step": 47200
    },
    {
      "epoch": 3.7768,
      "grad_norm": 65.56660461425781,
      "learning_rate": 7.410666666666667e-05,
      "loss": -111.431,
      "step": 47210
    },
    {
      "epoch": 3.7776,
      "grad_norm": 33.08757781982422,
      "learning_rate": 7.408000000000001e-05,
      "loss": -110.5319,
      "step": 47220
    },
    {
      "epoch": 3.7784,
      "grad_norm": 91.76051330566406,
      "learning_rate": 7.405333333333334e-05,
      "loss": -111.1278,
      "step": 47230
    },
    {
      "epoch": 3.7792,
      "grad_norm": 171.49874877929688,
      "learning_rate": 7.402666666666667e-05,
      "loss": -111.9142,
      "step": 47240
    },
    {
      "epoch": 3.7800000000000002,
      "grad_norm": 50.231712341308594,
      "learning_rate": 7.4e-05,
      "loss": -111.9703,
      "step": 47250
    },
    {
      "epoch": 3.7808,
      "grad_norm": 85.48139953613281,
      "learning_rate": 7.397333333333334e-05,
      "loss": -111.5691,
      "step": 47260
    },
    {
      "epoch": 3.7816,
      "grad_norm": 38.78990173339844,
      "learning_rate": 7.394666666666668e-05,
      "loss": -111.6163,
      "step": 47270
    },
    {
      "epoch": 3.7824,
      "grad_norm": 56.6331787109375,
      "learning_rate": 7.392e-05,
      "loss": -111.5132,
      "step": 47280
    },
    {
      "epoch": 3.7832,
      "grad_norm": 43.50080871582031,
      "learning_rate": 7.389333333333334e-05,
      "loss": -110.2893,
      "step": 47290
    },
    {
      "epoch": 3.784,
      "grad_norm": 67.6247329711914,
      "learning_rate": 7.386666666666667e-05,
      "loss": -111.5576,
      "step": 47300
    },
    {
      "epoch": 3.7848,
      "grad_norm": 88.4331283569336,
      "learning_rate": 7.384e-05,
      "loss": -112.337,
      "step": 47310
    },
    {
      "epoch": 3.7856,
      "grad_norm": 29.50271987915039,
      "learning_rate": 7.381333333333334e-05,
      "loss": -111.492,
      "step": 47320
    },
    {
      "epoch": 3.7864,
      "grad_norm": 36.957740783691406,
      "learning_rate": 7.378666666666667e-05,
      "loss": -111.6668,
      "step": 47330
    },
    {
      "epoch": 3.7872,
      "grad_norm": 92.64964294433594,
      "learning_rate": 7.376000000000001e-05,
      "loss": -111.6805,
      "step": 47340
    },
    {
      "epoch": 3.7880000000000003,
      "grad_norm": 61.27194595336914,
      "learning_rate": 7.373333333333333e-05,
      "loss": -111.6679,
      "step": 47350
    },
    {
      "epoch": 3.7888,
      "grad_norm": 54.39234161376953,
      "learning_rate": 7.370666666666667e-05,
      "loss": -112.0333,
      "step": 47360
    },
    {
      "epoch": 3.7896,
      "grad_norm": 56.65934753417969,
      "learning_rate": 7.368e-05,
      "loss": -111.3274,
      "step": 47370
    },
    {
      "epoch": 3.7904,
      "grad_norm": 50.513450622558594,
      "learning_rate": 7.365333333333334e-05,
      "loss": -110.9995,
      "step": 47380
    },
    {
      "epoch": 3.7912,
      "grad_norm": 77.13655853271484,
      "learning_rate": 7.362666666666667e-05,
      "loss": -111.5515,
      "step": 47390
    },
    {
      "epoch": 3.792,
      "grad_norm": 55.05490493774414,
      "learning_rate": 7.36e-05,
      "loss": -112.5985,
      "step": 47400
    },
    {
      "epoch": 3.7927999999999997,
      "grad_norm": 174.98443603515625,
      "learning_rate": 7.357333333333334e-05,
      "loss": -112.6253,
      "step": 47410
    },
    {
      "epoch": 3.7936,
      "grad_norm": 124.63691711425781,
      "learning_rate": 7.354666666666667e-05,
      "loss": -111.4872,
      "step": 47420
    },
    {
      "epoch": 3.7944,
      "grad_norm": 271.7619934082031,
      "learning_rate": 7.352e-05,
      "loss": -110.931,
      "step": 47430
    },
    {
      "epoch": 3.7952,
      "grad_norm": 42.95058822631836,
      "learning_rate": 7.349333333333333e-05,
      "loss": -111.3004,
      "step": 47440
    },
    {
      "epoch": 3.7960000000000003,
      "grad_norm": 51.159488677978516,
      "learning_rate": 7.346666666666667e-05,
      "loss": -112.1518,
      "step": 47450
    },
    {
      "epoch": 3.7968,
      "grad_norm": 80.57125091552734,
      "learning_rate": 7.344000000000002e-05,
      "loss": -112.1247,
      "step": 47460
    },
    {
      "epoch": 3.7976,
      "grad_norm": 29.105762481689453,
      "learning_rate": 7.341333333333333e-05,
      "loss": -111.1638,
      "step": 47470
    },
    {
      "epoch": 3.7984,
      "grad_norm": 77.38386535644531,
      "learning_rate": 7.338666666666667e-05,
      "loss": -111.6591,
      "step": 47480
    },
    {
      "epoch": 3.7992,
      "grad_norm": 34.954002380371094,
      "learning_rate": 7.336e-05,
      "loss": -111.5151,
      "step": 47490
    },
    {
      "epoch": 3.8,
      "grad_norm": 34.97407150268555,
      "learning_rate": 7.333333333333333e-05,
      "loss": -110.1979,
      "step": 47500
    },
    {
      "epoch": 3.8007999999999997,
      "grad_norm": 58.93400192260742,
      "learning_rate": 7.330666666666667e-05,
      "loss": -110.2758,
      "step": 47510
    },
    {
      "epoch": 3.8016,
      "grad_norm": 42.6440544128418,
      "learning_rate": 7.328e-05,
      "loss": -111.0591,
      "step": 47520
    },
    {
      "epoch": 3.8024,
      "grad_norm": 123.34754180908203,
      "learning_rate": 7.325333333333335e-05,
      "loss": -110.4808,
      "step": 47530
    },
    {
      "epoch": 3.8032,
      "grad_norm": 294.7017517089844,
      "learning_rate": 7.322666666666666e-05,
      "loss": -111.8695,
      "step": 47540
    },
    {
      "epoch": 3.8040000000000003,
      "grad_norm": 28.239349365234375,
      "learning_rate": 7.32e-05,
      "loss": -111.5215,
      "step": 47550
    },
    {
      "epoch": 3.8048,
      "grad_norm": 109.22773742675781,
      "learning_rate": 7.317333333333333e-05,
      "loss": -111.3824,
      "step": 47560
    },
    {
      "epoch": 3.8056,
      "grad_norm": 94.56873321533203,
      "learning_rate": 7.314666666666668e-05,
      "loss": -111.3304,
      "step": 47570
    },
    {
      "epoch": 3.8064,
      "grad_norm": 40.87496566772461,
      "learning_rate": 7.312e-05,
      "loss": -110.3902,
      "step": 47580
    },
    {
      "epoch": 3.8072,
      "grad_norm": 82.80965423583984,
      "learning_rate": 7.309333333333333e-05,
      "loss": -111.2516,
      "step": 47590
    },
    {
      "epoch": 3.808,
      "grad_norm": 60.62433624267578,
      "learning_rate": 7.306666666666668e-05,
      "loss": -112.1062,
      "step": 47600
    },
    {
      "epoch": 3.8087999999999997,
      "grad_norm": 23.28911590576172,
      "learning_rate": 7.304e-05,
      "loss": -111.7983,
      "step": 47610
    },
    {
      "epoch": 3.8096,
      "grad_norm": 76.064208984375,
      "learning_rate": 7.301333333333333e-05,
      "loss": -110.8954,
      "step": 47620
    },
    {
      "epoch": 3.8104,
      "grad_norm": 54.30056381225586,
      "learning_rate": 7.298666666666666e-05,
      "loss": -111.0306,
      "step": 47630
    },
    {
      "epoch": 3.8112,
      "grad_norm": 70.03897857666016,
      "learning_rate": 7.296e-05,
      "loss": -111.7867,
      "step": 47640
    },
    {
      "epoch": 3.8120000000000003,
      "grad_norm": 37.18017578125,
      "learning_rate": 7.293333333333334e-05,
      "loss": -111.0893,
      "step": 47650
    },
    {
      "epoch": 3.8128,
      "grad_norm": 62.2481803894043,
      "learning_rate": 7.290666666666666e-05,
      "loss": -111.9561,
      "step": 47660
    },
    {
      "epoch": 3.8136,
      "grad_norm": 69.42655944824219,
      "learning_rate": 7.288000000000001e-05,
      "loss": -111.5359,
      "step": 47670
    },
    {
      "epoch": 3.8144,
      "grad_norm": 64.42718505859375,
      "learning_rate": 7.285333333333334e-05,
      "loss": -112.202,
      "step": 47680
    },
    {
      "epoch": 3.8152,
      "grad_norm": 30.10881233215332,
      "learning_rate": 7.282666666666667e-05,
      "loss": -111.9177,
      "step": 47690
    },
    {
      "epoch": 3.816,
      "grad_norm": 83.42376708984375,
      "learning_rate": 7.280000000000001e-05,
      "loss": -111.9322,
      "step": 47700
    },
    {
      "epoch": 3.8167999999999997,
      "grad_norm": 41.45222854614258,
      "learning_rate": 7.277333333333334e-05,
      "loss": -111.4344,
      "step": 47710
    },
    {
      "epoch": 3.8176,
      "grad_norm": 38.81413269042969,
      "learning_rate": 7.274666666666668e-05,
      "loss": -111.5031,
      "step": 47720
    },
    {
      "epoch": 3.8184,
      "grad_norm": 51.45942306518555,
      "learning_rate": 7.272e-05,
      "loss": -111.4188,
      "step": 47730
    },
    {
      "epoch": 3.8192,
      "grad_norm": 36.85099411010742,
      "learning_rate": 7.269333333333334e-05,
      "loss": -111.7907,
      "step": 47740
    },
    {
      "epoch": 3.82,
      "grad_norm": 53.28932571411133,
      "learning_rate": 7.266666666666667e-05,
      "loss": -111.1412,
      "step": 47750
    },
    {
      "epoch": 3.8208,
      "grad_norm": 202.3607940673828,
      "learning_rate": 7.264000000000001e-05,
      "loss": -111.0276,
      "step": 47760
    },
    {
      "epoch": 3.8216,
      "grad_norm": 46.298500061035156,
      "learning_rate": 7.261333333333334e-05,
      "loss": -110.9448,
      "step": 47770
    },
    {
      "epoch": 3.8224,
      "grad_norm": 48.349727630615234,
      "learning_rate": 7.258666666666667e-05,
      "loss": -111.6823,
      "step": 47780
    },
    {
      "epoch": 3.8232,
      "grad_norm": 194.8773651123047,
      "learning_rate": 7.256000000000001e-05,
      "loss": -111.2899,
      "step": 47790
    },
    {
      "epoch": 3.824,
      "grad_norm": 35.23929977416992,
      "learning_rate": 7.253333333333334e-05,
      "loss": -112.2184,
      "step": 47800
    },
    {
      "epoch": 3.8247999999999998,
      "grad_norm": 46.042118072509766,
      "learning_rate": 7.250666666666667e-05,
      "loss": -111.9453,
      "step": 47810
    },
    {
      "epoch": 3.8256,
      "grad_norm": 87.65963745117188,
      "learning_rate": 7.248e-05,
      "loss": -111.8427,
      "step": 47820
    },
    {
      "epoch": 3.8264,
      "grad_norm": 74.1427993774414,
      "learning_rate": 7.245333333333334e-05,
      "loss": -110.4247,
      "step": 47830
    },
    {
      "epoch": 3.8272,
      "grad_norm": 35.505977630615234,
      "learning_rate": 7.242666666666667e-05,
      "loss": -111.8438,
      "step": 47840
    },
    {
      "epoch": 3.828,
      "grad_norm": 159.40090942382812,
      "learning_rate": 7.24e-05,
      "loss": -112.0683,
      "step": 47850
    },
    {
      "epoch": 3.8288,
      "grad_norm": 132.60018920898438,
      "learning_rate": 7.237333333333334e-05,
      "loss": -111.3794,
      "step": 47860
    },
    {
      "epoch": 3.8296,
      "grad_norm": 38.42751693725586,
      "learning_rate": 7.234666666666667e-05,
      "loss": -111.1497,
      "step": 47870
    },
    {
      "epoch": 3.8304,
      "grad_norm": 60.42300796508789,
      "learning_rate": 7.232e-05,
      "loss": -111.0426,
      "step": 47880
    },
    {
      "epoch": 3.8312,
      "grad_norm": 35.54636001586914,
      "learning_rate": 7.229333333333334e-05,
      "loss": -111.8606,
      "step": 47890
    },
    {
      "epoch": 3.832,
      "grad_norm": 49.650630950927734,
      "learning_rate": 7.226666666666667e-05,
      "loss": -110.699,
      "step": 47900
    },
    {
      "epoch": 3.8327999999999998,
      "grad_norm": 39.358734130859375,
      "learning_rate": 7.224000000000001e-05,
      "loss": -111.381,
      "step": 47910
    },
    {
      "epoch": 3.8336,
      "grad_norm": 105.78761291503906,
      "learning_rate": 7.221333333333333e-05,
      "loss": -112.195,
      "step": 47920
    },
    {
      "epoch": 3.8344,
      "grad_norm": 102.90615844726562,
      "learning_rate": 7.218666666666667e-05,
      "loss": -111.6818,
      "step": 47930
    },
    {
      "epoch": 3.8352,
      "grad_norm": 152.39187622070312,
      "learning_rate": 7.216e-05,
      "loss": -110.9291,
      "step": 47940
    },
    {
      "epoch": 3.836,
      "grad_norm": 48.42155838012695,
      "learning_rate": 7.213333333333334e-05,
      "loss": -111.0146,
      "step": 47950
    },
    {
      "epoch": 3.8368,
      "grad_norm": 62.21927261352539,
      "learning_rate": 7.210666666666667e-05,
      "loss": -111.3808,
      "step": 47960
    },
    {
      "epoch": 3.8376,
      "grad_norm": 113.9566879272461,
      "learning_rate": 7.208e-05,
      "loss": -112.4576,
      "step": 47970
    },
    {
      "epoch": 3.8384,
      "grad_norm": 105.83097076416016,
      "learning_rate": 7.205333333333334e-05,
      "loss": -111.9826,
      "step": 47980
    },
    {
      "epoch": 3.8392,
      "grad_norm": 38.6864013671875,
      "learning_rate": 7.202666666666667e-05,
      "loss": -112.4841,
      "step": 47990
    },
    {
      "epoch": 3.84,
      "grad_norm": 58.982566833496094,
      "learning_rate": 7.2e-05,
      "loss": -110.5558,
      "step": 48000
    },
    {
      "epoch": 3.8407999999999998,
      "grad_norm": 27.776901245117188,
      "learning_rate": 7.197333333333333e-05,
      "loss": -111.5105,
      "step": 48010
    },
    {
      "epoch": 3.8416,
      "grad_norm": 34.003387451171875,
      "learning_rate": 7.194666666666667e-05,
      "loss": -111.1905,
      "step": 48020
    },
    {
      "epoch": 3.8424,
      "grad_norm": 35.09404754638672,
      "learning_rate": 7.192e-05,
      "loss": -111.1965,
      "step": 48030
    },
    {
      "epoch": 3.8432,
      "grad_norm": 101.2715835571289,
      "learning_rate": 7.189333333333333e-05,
      "loss": -112.0096,
      "step": 48040
    },
    {
      "epoch": 3.844,
      "grad_norm": 57.50212860107422,
      "learning_rate": 7.186666666666667e-05,
      "loss": -111.4411,
      "step": 48050
    },
    {
      "epoch": 3.8448,
      "grad_norm": 92.0971450805664,
      "learning_rate": 7.184e-05,
      "loss": -111.6035,
      "step": 48060
    },
    {
      "epoch": 3.8456,
      "grad_norm": 1078.313720703125,
      "learning_rate": 7.181333333333333e-05,
      "loss": -111.4637,
      "step": 48070
    },
    {
      "epoch": 3.8464,
      "grad_norm": 86.27821350097656,
      "learning_rate": 7.178666666666668e-05,
      "loss": -111.0307,
      "step": 48080
    },
    {
      "epoch": 3.8472,
      "grad_norm": 131.7122039794922,
      "learning_rate": 7.176e-05,
      "loss": -111.5162,
      "step": 48090
    },
    {
      "epoch": 3.848,
      "grad_norm": 131.37730407714844,
      "learning_rate": 7.173333333333335e-05,
      "loss": -112.0569,
      "step": 48100
    },
    {
      "epoch": 3.8487999999999998,
      "grad_norm": 90.26654815673828,
      "learning_rate": 7.170666666666666e-05,
      "loss": -111.5946,
      "step": 48110
    },
    {
      "epoch": 3.8496,
      "grad_norm": 44.41782760620117,
      "learning_rate": 7.168e-05,
      "loss": -111.6533,
      "step": 48120
    },
    {
      "epoch": 3.8504,
      "grad_norm": 104.77361297607422,
      "learning_rate": 7.165333333333333e-05,
      "loss": -111.7463,
      "step": 48130
    },
    {
      "epoch": 3.8512,
      "grad_norm": 80.45480346679688,
      "learning_rate": 7.162666666666668e-05,
      "loss": -111.4054,
      "step": 48140
    },
    {
      "epoch": 3.852,
      "grad_norm": 51.45985412597656,
      "learning_rate": 7.16e-05,
      "loss": -110.9829,
      "step": 48150
    },
    {
      "epoch": 3.8528000000000002,
      "grad_norm": 75.93385314941406,
      "learning_rate": 7.157333333333333e-05,
      "loss": -110.7541,
      "step": 48160
    },
    {
      "epoch": 3.8536,
      "grad_norm": 31.654733657836914,
      "learning_rate": 7.154666666666668e-05,
      "loss": -111.8855,
      "step": 48170
    },
    {
      "epoch": 3.8544,
      "grad_norm": 71.71206665039062,
      "learning_rate": 7.151999999999999e-05,
      "loss": -111.3616,
      "step": 48180
    },
    {
      "epoch": 3.8552,
      "grad_norm": 33.70541763305664,
      "learning_rate": 7.149333333333334e-05,
      "loss": -111.668,
      "step": 48190
    },
    {
      "epoch": 3.856,
      "grad_norm": 62.44491195678711,
      "learning_rate": 7.146666666666666e-05,
      "loss": -110.9048,
      "step": 48200
    },
    {
      "epoch": 3.8568,
      "grad_norm": 36.457759857177734,
      "learning_rate": 7.144000000000001e-05,
      "loss": -111.5961,
      "step": 48210
    },
    {
      "epoch": 3.8576,
      "grad_norm": 42.48291015625,
      "learning_rate": 7.141333333333334e-05,
      "loss": -112.3935,
      "step": 48220
    },
    {
      "epoch": 3.8584,
      "grad_norm": 35.95171356201172,
      "learning_rate": 7.138666666666667e-05,
      "loss": -112.288,
      "step": 48230
    },
    {
      "epoch": 3.8592,
      "grad_norm": 40.441532135009766,
      "learning_rate": 7.136000000000001e-05,
      "loss": -110.9971,
      "step": 48240
    },
    {
      "epoch": 3.86,
      "grad_norm": 36.972557067871094,
      "learning_rate": 7.133333333333334e-05,
      "loss": -111.5633,
      "step": 48250
    },
    {
      "epoch": 3.8608000000000002,
      "grad_norm": 46.797908782958984,
      "learning_rate": 7.130666666666667e-05,
      "loss": -111.7443,
      "step": 48260
    },
    {
      "epoch": 3.8616,
      "grad_norm": 36.02486801147461,
      "learning_rate": 7.128000000000001e-05,
      "loss": -111.7765,
      "step": 48270
    },
    {
      "epoch": 3.8624,
      "grad_norm": 37.41154861450195,
      "learning_rate": 7.125333333333334e-05,
      "loss": -111.9085,
      "step": 48280
    },
    {
      "epoch": 3.8632,
      "grad_norm": 115.09819030761719,
      "learning_rate": 7.122666666666668e-05,
      "loss": -112.0344,
      "step": 48290
    },
    {
      "epoch": 3.864,
      "grad_norm": 246.1289825439453,
      "learning_rate": 7.12e-05,
      "loss": -111.0004,
      "step": 48300
    },
    {
      "epoch": 3.8648,
      "grad_norm": 64.8357925415039,
      "learning_rate": 7.117333333333334e-05,
      "loss": -111.9612,
      "step": 48310
    },
    {
      "epoch": 3.8656,
      "grad_norm": 40.14347457885742,
      "learning_rate": 7.114666666666667e-05,
      "loss": -110.8602,
      "step": 48320
    },
    {
      "epoch": 3.8664,
      "grad_norm": 29.404857635498047,
      "learning_rate": 7.112000000000001e-05,
      "loss": -110.7944,
      "step": 48330
    },
    {
      "epoch": 3.8672,
      "grad_norm": 52.29311752319336,
      "learning_rate": 7.109333333333334e-05,
      "loss": -111.5286,
      "step": 48340
    },
    {
      "epoch": 3.868,
      "grad_norm": 51.14293670654297,
      "learning_rate": 7.106666666666667e-05,
      "loss": -111.1351,
      "step": 48350
    },
    {
      "epoch": 3.8688000000000002,
      "grad_norm": 113.41300964355469,
      "learning_rate": 7.104000000000001e-05,
      "loss": -112.3406,
      "step": 48360
    },
    {
      "epoch": 3.8696,
      "grad_norm": 111.54503631591797,
      "learning_rate": 7.101333333333333e-05,
      "loss": -111.5714,
      "step": 48370
    },
    {
      "epoch": 3.8704,
      "grad_norm": 46.811038970947266,
      "learning_rate": 7.098666666666667e-05,
      "loss": -111.0958,
      "step": 48380
    },
    {
      "epoch": 3.8712,
      "grad_norm": 248.99928283691406,
      "learning_rate": 7.096e-05,
      "loss": -111.0868,
      "step": 48390
    },
    {
      "epoch": 3.872,
      "grad_norm": 49.69511032104492,
      "learning_rate": 7.093333333333334e-05,
      "loss": -111.562,
      "step": 48400
    },
    {
      "epoch": 3.8728,
      "grad_norm": 91.30403137207031,
      "learning_rate": 7.090666666666667e-05,
      "loss": -111.4028,
      "step": 48410
    },
    {
      "epoch": 3.8736,
      "grad_norm": 28.94269371032715,
      "learning_rate": 7.088e-05,
      "loss": -111.2323,
      "step": 48420
    },
    {
      "epoch": 3.8744,
      "grad_norm": 111.64502716064453,
      "learning_rate": 7.085333333333334e-05,
      "loss": -110.5203,
      "step": 48430
    },
    {
      "epoch": 3.8752,
      "grad_norm": 43.37114334106445,
      "learning_rate": 7.082666666666667e-05,
      "loss": -111.3354,
      "step": 48440
    },
    {
      "epoch": 3.876,
      "grad_norm": 65.77621459960938,
      "learning_rate": 7.08e-05,
      "loss": -111.802,
      "step": 48450
    },
    {
      "epoch": 3.8768000000000002,
      "grad_norm": 61.775047302246094,
      "learning_rate": 7.077333333333334e-05,
      "loss": -111.685,
      "step": 48460
    },
    {
      "epoch": 3.8776,
      "grad_norm": 117.9990005493164,
      "learning_rate": 7.074666666666667e-05,
      "loss": -111.6894,
      "step": 48470
    },
    {
      "epoch": 3.8784,
      "grad_norm": 35.83586120605469,
      "learning_rate": 7.072000000000001e-05,
      "loss": -111.626,
      "step": 48480
    },
    {
      "epoch": 3.8792,
      "grad_norm": 202.7429962158203,
      "learning_rate": 7.069333333333333e-05,
      "loss": -111.092,
      "step": 48490
    },
    {
      "epoch": 3.88,
      "grad_norm": 36.93485641479492,
      "learning_rate": 7.066666666666667e-05,
      "loss": -111.2053,
      "step": 48500
    },
    {
      "epoch": 3.8808,
      "grad_norm": 44.9000358581543,
      "learning_rate": 7.064e-05,
      "loss": -110.9078,
      "step": 48510
    },
    {
      "epoch": 3.8816,
      "grad_norm": 38.99913787841797,
      "learning_rate": 7.061333333333333e-05,
      "loss": -110.8234,
      "step": 48520
    },
    {
      "epoch": 3.8824,
      "grad_norm": 52.3504524230957,
      "learning_rate": 7.058666666666667e-05,
      "loss": -111.7407,
      "step": 48530
    },
    {
      "epoch": 3.8832,
      "grad_norm": 492.10687255859375,
      "learning_rate": 7.056e-05,
      "loss": -111.7674,
      "step": 48540
    },
    {
      "epoch": 3.884,
      "grad_norm": 153.65565490722656,
      "learning_rate": 7.053333333333334e-05,
      "loss": -111.5266,
      "step": 48550
    },
    {
      "epoch": 3.8848000000000003,
      "grad_norm": 80.89298248291016,
      "learning_rate": 7.050666666666666e-05,
      "loss": -112.273,
      "step": 48560
    },
    {
      "epoch": 3.8856,
      "grad_norm": 93.15540313720703,
      "learning_rate": 7.048e-05,
      "loss": -111.6435,
      "step": 48570
    },
    {
      "epoch": 3.8864,
      "grad_norm": 63.51585006713867,
      "learning_rate": 7.045333333333333e-05,
      "loss": -110.862,
      "step": 48580
    },
    {
      "epoch": 3.8872,
      "grad_norm": 38.35615158081055,
      "learning_rate": 7.042666666666667e-05,
      "loss": -111.5819,
      "step": 48590
    },
    {
      "epoch": 3.888,
      "grad_norm": 51.228538513183594,
      "learning_rate": 7.04e-05,
      "loss": -110.782,
      "step": 48600
    },
    {
      "epoch": 3.8888,
      "grad_norm": 67.89659881591797,
      "learning_rate": 7.037333333333333e-05,
      "loss": -112.6133,
      "step": 48610
    },
    {
      "epoch": 3.8895999999999997,
      "grad_norm": 70.505615234375,
      "learning_rate": 7.034666666666668e-05,
      "loss": -111.4406,
      "step": 48620
    },
    {
      "epoch": 3.8904,
      "grad_norm": 97.2496566772461,
      "learning_rate": 7.032e-05,
      "loss": -111.5482,
      "step": 48630
    },
    {
      "epoch": 3.8912,
      "grad_norm": 65.31523895263672,
      "learning_rate": 7.029333333333333e-05,
      "loss": -112.7565,
      "step": 48640
    },
    {
      "epoch": 3.892,
      "grad_norm": 51.8526725769043,
      "learning_rate": 7.026666666666668e-05,
      "loss": -111.0032,
      "step": 48650
    },
    {
      "epoch": 3.8928000000000003,
      "grad_norm": 40.210838317871094,
      "learning_rate": 7.024e-05,
      "loss": -112.23,
      "step": 48660
    },
    {
      "epoch": 3.8936,
      "grad_norm": 24.534975051879883,
      "learning_rate": 7.021333333333335e-05,
      "loss": -110.6613,
      "step": 48670
    },
    {
      "epoch": 3.8944,
      "grad_norm": 80.71321105957031,
      "learning_rate": 7.018666666666666e-05,
      "loss": -111.9401,
      "step": 48680
    },
    {
      "epoch": 3.8952,
      "grad_norm": 145.86148071289062,
      "learning_rate": 7.016e-05,
      "loss": -112.2049,
      "step": 48690
    },
    {
      "epoch": 3.896,
      "grad_norm": 57.51431655883789,
      "learning_rate": 7.013333333333333e-05,
      "loss": -111.2788,
      "step": 48700
    },
    {
      "epoch": 3.8968,
      "grad_norm": 43.186927795410156,
      "learning_rate": 7.010666666666666e-05,
      "loss": -111.0016,
      "step": 48710
    },
    {
      "epoch": 3.8975999999999997,
      "grad_norm": 38.79369354248047,
      "learning_rate": 7.008e-05,
      "loss": -111.6627,
      "step": 48720
    },
    {
      "epoch": 3.8984,
      "grad_norm": 44.39738845825195,
      "learning_rate": 7.005333333333334e-05,
      "loss": -111.4856,
      "step": 48730
    },
    {
      "epoch": 3.8992,
      "grad_norm": 61.69624710083008,
      "learning_rate": 7.002666666666668e-05,
      "loss": -111.2693,
      "step": 48740
    },
    {
      "epoch": 3.9,
      "grad_norm": 41.467315673828125,
      "learning_rate": 7e-05,
      "loss": -111.7006,
      "step": 48750
    },
    {
      "epoch": 3.9008000000000003,
      "grad_norm": 226.2288055419922,
      "learning_rate": 6.997333333333334e-05,
      "loss": -110.6781,
      "step": 48760
    },
    {
      "epoch": 3.9016,
      "grad_norm": 90.28189086914062,
      "learning_rate": 6.994666666666667e-05,
      "loss": -112.0078,
      "step": 48770
    },
    {
      "epoch": 3.9024,
      "grad_norm": 40.28076934814453,
      "learning_rate": 6.992000000000001e-05,
      "loss": -111.4009,
      "step": 48780
    },
    {
      "epoch": 3.9032,
      "grad_norm": 50.272335052490234,
      "learning_rate": 6.989333333333334e-05,
      "loss": -112.1514,
      "step": 48790
    },
    {
      "epoch": 3.904,
      "grad_norm": 23.413227081298828,
      "learning_rate": 6.986666666666667e-05,
      "loss": -112.024,
      "step": 48800
    },
    {
      "epoch": 3.9048,
      "grad_norm": 49.648780822753906,
      "learning_rate": 6.984000000000001e-05,
      "loss": -111.1423,
      "step": 48810
    },
    {
      "epoch": 3.9055999999999997,
      "grad_norm": 102.11454010009766,
      "learning_rate": 6.981333333333334e-05,
      "loss": -110.4366,
      "step": 48820
    },
    {
      "epoch": 3.9064,
      "grad_norm": 187.4674835205078,
      "learning_rate": 6.978666666666667e-05,
      "loss": -111.3255,
      "step": 48830
    },
    {
      "epoch": 3.9072,
      "grad_norm": 35.373138427734375,
      "learning_rate": 6.976000000000001e-05,
      "loss": -111.2054,
      "step": 48840
    },
    {
      "epoch": 3.908,
      "grad_norm": 37.520931243896484,
      "learning_rate": 6.973333333333334e-05,
      "loss": -111.5455,
      "step": 48850
    },
    {
      "epoch": 3.9088000000000003,
      "grad_norm": 35.912803649902344,
      "learning_rate": 6.970666666666667e-05,
      "loss": -111.4849,
      "step": 48860
    },
    {
      "epoch": 3.9096,
      "grad_norm": 38.01201248168945,
      "learning_rate": 6.968e-05,
      "loss": -111.667,
      "step": 48870
    },
    {
      "epoch": 3.9104,
      "grad_norm": 66.34459686279297,
      "learning_rate": 6.965333333333334e-05,
      "loss": -112.7218,
      "step": 48880
    },
    {
      "epoch": 3.9112,
      "grad_norm": 90.98200225830078,
      "learning_rate": 6.962666666666667e-05,
      "loss": -111.3799,
      "step": 48890
    },
    {
      "epoch": 3.912,
      "grad_norm": 34.282920837402344,
      "learning_rate": 6.96e-05,
      "loss": -111.4753,
      "step": 48900
    },
    {
      "epoch": 3.9128,
      "grad_norm": 25.978500366210938,
      "learning_rate": 6.957333333333334e-05,
      "loss": -111.8373,
      "step": 48910
    },
    {
      "epoch": 3.9135999999999997,
      "grad_norm": 83.96192932128906,
      "learning_rate": 6.954666666666667e-05,
      "loss": -111.717,
      "step": 48920
    },
    {
      "epoch": 3.9144,
      "grad_norm": 77.69495391845703,
      "learning_rate": 6.952000000000001e-05,
      "loss": -111.0275,
      "step": 48930
    },
    {
      "epoch": 3.9152,
      "grad_norm": 170.96768188476562,
      "learning_rate": 6.949333333333333e-05,
      "loss": -111.8374,
      "step": 48940
    },
    {
      "epoch": 3.916,
      "grad_norm": 141.81753540039062,
      "learning_rate": 6.946666666666667e-05,
      "loss": -111.0554,
      "step": 48950
    },
    {
      "epoch": 3.9168,
      "grad_norm": 58.98914337158203,
      "learning_rate": 6.944e-05,
      "loss": -111.3464,
      "step": 48960
    },
    {
      "epoch": 3.9176,
      "grad_norm": 92.94364929199219,
      "learning_rate": 6.941333333333334e-05,
      "loss": -111.3752,
      "step": 48970
    },
    {
      "epoch": 3.9184,
      "grad_norm": 38.90571212768555,
      "learning_rate": 6.938666666666667e-05,
      "loss": -112.0939,
      "step": 48980
    },
    {
      "epoch": 3.9192,
      "grad_norm": 22.523590087890625,
      "learning_rate": 6.936e-05,
      "loss": -111.8352,
      "step": 48990
    },
    {
      "epoch": 3.92,
      "grad_norm": 26.958568572998047,
      "learning_rate": 6.933333333333334e-05,
      "loss": -111.2581,
      "step": 49000
    },
    {
      "epoch": 3.9208,
      "grad_norm": 197.2332763671875,
      "learning_rate": 6.930666666666667e-05,
      "loss": -110.194,
      "step": 49010
    },
    {
      "epoch": 3.9215999999999998,
      "grad_norm": 76.14623260498047,
      "learning_rate": 6.928e-05,
      "loss": -111.5979,
      "step": 49020
    },
    {
      "epoch": 3.9224,
      "grad_norm": 22.44136619567871,
      "learning_rate": 6.925333333333334e-05,
      "loss": -111.241,
      "step": 49030
    },
    {
      "epoch": 3.9232,
      "grad_norm": 84.38286590576172,
      "learning_rate": 6.922666666666667e-05,
      "loss": -111.021,
      "step": 49040
    },
    {
      "epoch": 3.924,
      "grad_norm": 117.3259506225586,
      "learning_rate": 6.92e-05,
      "loss": -111.9729,
      "step": 49050
    },
    {
      "epoch": 3.9248,
      "grad_norm": 58.95656967163086,
      "learning_rate": 6.917333333333333e-05,
      "loss": -112.5709,
      "step": 49060
    },
    {
      "epoch": 3.9256,
      "grad_norm": 41.71845626831055,
      "learning_rate": 6.914666666666667e-05,
      "loss": -111.2816,
      "step": 49070
    },
    {
      "epoch": 3.9264,
      "grad_norm": 68.30743408203125,
      "learning_rate": 6.912e-05,
      "loss": -111.864,
      "step": 49080
    },
    {
      "epoch": 3.9272,
      "grad_norm": 41.0619010925293,
      "learning_rate": 6.909333333333333e-05,
      "loss": -110.7849,
      "step": 49090
    },
    {
      "epoch": 3.928,
      "grad_norm": 44.81471252441406,
      "learning_rate": 6.906666666666667e-05,
      "loss": -112.1988,
      "step": 49100
    },
    {
      "epoch": 3.9288,
      "grad_norm": 115.69998931884766,
      "learning_rate": 6.904e-05,
      "loss": -111.3298,
      "step": 49110
    },
    {
      "epoch": 3.9295999999999998,
      "grad_norm": 36.97019958496094,
      "learning_rate": 6.901333333333335e-05,
      "loss": -110.2743,
      "step": 49120
    },
    {
      "epoch": 3.9304,
      "grad_norm": 52.631160736083984,
      "learning_rate": 6.898666666666666e-05,
      "loss": -111.1076,
      "step": 49130
    },
    {
      "epoch": 3.9312,
      "grad_norm": 164.00344848632812,
      "learning_rate": 6.896e-05,
      "loss": -112.1724,
      "step": 49140
    },
    {
      "epoch": 3.932,
      "grad_norm": 97.90814971923828,
      "learning_rate": 6.893333333333333e-05,
      "loss": -110.6367,
      "step": 49150
    },
    {
      "epoch": 3.9328,
      "grad_norm": 196.96728515625,
      "learning_rate": 6.890666666666668e-05,
      "loss": -112.7727,
      "step": 49160
    },
    {
      "epoch": 3.9336,
      "grad_norm": 50.96276092529297,
      "learning_rate": 6.888e-05,
      "loss": -112.0416,
      "step": 49170
    },
    {
      "epoch": 3.9344,
      "grad_norm": 36.014591217041016,
      "learning_rate": 6.885333333333333e-05,
      "loss": -112.0847,
      "step": 49180
    },
    {
      "epoch": 3.9352,
      "grad_norm": 58.87934875488281,
      "learning_rate": 6.882666666666668e-05,
      "loss": -111.5094,
      "step": 49190
    },
    {
      "epoch": 3.936,
      "grad_norm": 48.12807846069336,
      "learning_rate": 6.879999999999999e-05,
      "loss": -110.8919,
      "step": 49200
    },
    {
      "epoch": 3.9368,
      "grad_norm": 50.86573028564453,
      "learning_rate": 6.877333333333333e-05,
      "loss": -110.9517,
      "step": 49210
    },
    {
      "epoch": 3.9375999999999998,
      "grad_norm": 108.1315689086914,
      "learning_rate": 6.874666666666668e-05,
      "loss": -110.7379,
      "step": 49220
    },
    {
      "epoch": 3.9384,
      "grad_norm": 51.26044464111328,
      "learning_rate": 6.872e-05,
      "loss": -111.298,
      "step": 49230
    },
    {
      "epoch": 3.9392,
      "grad_norm": 27.744173049926758,
      "learning_rate": 6.869333333333334e-05,
      "loss": -111.4015,
      "step": 49240
    },
    {
      "epoch": 3.94,
      "grad_norm": 39.22489547729492,
      "learning_rate": 6.866666666666666e-05,
      "loss": -110.3702,
      "step": 49250
    },
    {
      "epoch": 3.9408,
      "grad_norm": 96.51537322998047,
      "learning_rate": 6.864000000000001e-05,
      "loss": -111.2187,
      "step": 49260
    },
    {
      "epoch": 3.9416,
      "grad_norm": 41.38676834106445,
      "learning_rate": 6.861333333333334e-05,
      "loss": -110.963,
      "step": 49270
    },
    {
      "epoch": 3.9424,
      "grad_norm": 37.136295318603516,
      "learning_rate": 6.858666666666667e-05,
      "loss": -111.6658,
      "step": 49280
    },
    {
      "epoch": 3.9432,
      "grad_norm": 29.359174728393555,
      "learning_rate": 6.856000000000001e-05,
      "loss": -111.9659,
      "step": 49290
    },
    {
      "epoch": 3.944,
      "grad_norm": 115.23063659667969,
      "learning_rate": 6.853333333333334e-05,
      "loss": -111.4471,
      "step": 49300
    },
    {
      "epoch": 3.9448,
      "grad_norm": 30.620508193969727,
      "learning_rate": 6.850666666666668e-05,
      "loss": -110.9376,
      "step": 49310
    },
    {
      "epoch": 3.9455999999999998,
      "grad_norm": 60.655094146728516,
      "learning_rate": 6.848e-05,
      "loss": -111.8098,
      "step": 49320
    },
    {
      "epoch": 3.9464,
      "grad_norm": 87.33812713623047,
      "learning_rate": 6.845333333333334e-05,
      "loss": -112.5405,
      "step": 49330
    },
    {
      "epoch": 3.9472,
      "grad_norm": 94.438232421875,
      "learning_rate": 6.842666666666667e-05,
      "loss": -111.5364,
      "step": 49340
    },
    {
      "epoch": 3.948,
      "grad_norm": 168.71701049804688,
      "learning_rate": 6.840000000000001e-05,
      "loss": -111.5467,
      "step": 49350
    },
    {
      "epoch": 3.9488,
      "grad_norm": 164.5147705078125,
      "learning_rate": 6.837333333333334e-05,
      "loss": -111.5965,
      "step": 49360
    },
    {
      "epoch": 3.9496,
      "grad_norm": 49.611175537109375,
      "learning_rate": 6.834666666666667e-05,
      "loss": -111.6076,
      "step": 49370
    },
    {
      "epoch": 3.9504,
      "grad_norm": 48.31479263305664,
      "learning_rate": 6.832000000000001e-05,
      "loss": -109.8126,
      "step": 49380
    },
    {
      "epoch": 3.9512,
      "grad_norm": 51.455623626708984,
      "learning_rate": 6.829333333333333e-05,
      "loss": -110.5892,
      "step": 49390
    },
    {
      "epoch": 3.952,
      "grad_norm": 76.58399200439453,
      "learning_rate": 6.826666666666667e-05,
      "loss": -111.2762,
      "step": 49400
    },
    {
      "epoch": 3.9528,
      "grad_norm": 36.883338928222656,
      "learning_rate": 6.824e-05,
      "loss": -112.4096,
      "step": 49410
    },
    {
      "epoch": 3.9536,
      "grad_norm": 64.68871307373047,
      "learning_rate": 6.821333333333334e-05,
      "loss": -112.3326,
      "step": 49420
    },
    {
      "epoch": 3.9544,
      "grad_norm": 278.4802551269531,
      "learning_rate": 6.818666666666667e-05,
      "loss": -112.9908,
      "step": 49430
    },
    {
      "epoch": 3.9552,
      "grad_norm": 92.97784423828125,
      "learning_rate": 6.816e-05,
      "loss": -111.4478,
      "step": 49440
    },
    {
      "epoch": 3.956,
      "grad_norm": 68.75553131103516,
      "learning_rate": 6.813333333333334e-05,
      "loss": -111.0981,
      "step": 49450
    },
    {
      "epoch": 3.9568,
      "grad_norm": 125.24828338623047,
      "learning_rate": 6.810666666666667e-05,
      "loss": -111.4002,
      "step": 49460
    },
    {
      "epoch": 3.9576000000000002,
      "grad_norm": 42.96258544921875,
      "learning_rate": 6.808e-05,
      "loss": -111.7978,
      "step": 49470
    },
    {
      "epoch": 3.9584,
      "grad_norm": 65.77040100097656,
      "learning_rate": 6.805333333333334e-05,
      "loss": -111.5861,
      "step": 49480
    },
    {
      "epoch": 3.9592,
      "grad_norm": 47.515594482421875,
      "learning_rate": 6.802666666666667e-05,
      "loss": -111.735,
      "step": 49490
    },
    {
      "epoch": 3.96,
      "grad_norm": 26.411521911621094,
      "learning_rate": 6.800000000000001e-05,
      "loss": -111.7762,
      "step": 49500
    },
    {
      "epoch": 3.9608,
      "grad_norm": 40.36684036254883,
      "learning_rate": 6.797333333333333e-05,
      "loss": -110.4314,
      "step": 49510
    },
    {
      "epoch": 3.9616,
      "grad_norm": 123.79592895507812,
      "learning_rate": 6.794666666666667e-05,
      "loss": -111.099,
      "step": 49520
    },
    {
      "epoch": 3.9624,
      "grad_norm": 54.229637145996094,
      "learning_rate": 6.792e-05,
      "loss": -111.2744,
      "step": 49530
    },
    {
      "epoch": 3.9632,
      "grad_norm": 58.23291778564453,
      "learning_rate": 6.789333333333334e-05,
      "loss": -112.3874,
      "step": 49540
    },
    {
      "epoch": 3.964,
      "grad_norm": 30.5834903717041,
      "learning_rate": 6.786666666666667e-05,
      "loss": -110.5593,
      "step": 49550
    },
    {
      "epoch": 3.9648,
      "grad_norm": 41.428810119628906,
      "learning_rate": 6.784e-05,
      "loss": -111.9382,
      "step": 49560
    },
    {
      "epoch": 3.9656000000000002,
      "grad_norm": 33.727745056152344,
      "learning_rate": 6.781333333333334e-05,
      "loss": -111.7971,
      "step": 49570
    },
    {
      "epoch": 3.9664,
      "grad_norm": 36.339141845703125,
      "learning_rate": 6.778666666666666e-05,
      "loss": -111.4967,
      "step": 49580
    },
    {
      "epoch": 3.9672,
      "grad_norm": 112.74591827392578,
      "learning_rate": 6.776e-05,
      "loss": -111.2162,
      "step": 49590
    },
    {
      "epoch": 3.968,
      "grad_norm": 57.57939147949219,
      "learning_rate": 6.773333333333333e-05,
      "loss": -111.3714,
      "step": 49600
    },
    {
      "epoch": 3.9688,
      "grad_norm": 172.1604461669922,
      "learning_rate": 6.770666666666667e-05,
      "loss": -111.514,
      "step": 49610
    },
    {
      "epoch": 3.9696,
      "grad_norm": 30.950883865356445,
      "learning_rate": 6.768e-05,
      "loss": -111.0833,
      "step": 49620
    },
    {
      "epoch": 3.9704,
      "grad_norm": 78.34297180175781,
      "learning_rate": 6.765333333333333e-05,
      "loss": -111.3089,
      "step": 49630
    },
    {
      "epoch": 3.9712,
      "grad_norm": 75.57111358642578,
      "learning_rate": 6.762666666666667e-05,
      "loss": -110.939,
      "step": 49640
    },
    {
      "epoch": 3.972,
      "grad_norm": 143.72506713867188,
      "learning_rate": 6.76e-05,
      "loss": -112.0117,
      "step": 49650
    },
    {
      "epoch": 3.9728,
      "grad_norm": 82.87214660644531,
      "learning_rate": 6.757333333333333e-05,
      "loss": -111.1702,
      "step": 49660
    },
    {
      "epoch": 3.9736000000000002,
      "grad_norm": 201.1174774169922,
      "learning_rate": 6.754666666666667e-05,
      "loss": -111.9366,
      "step": 49670
    },
    {
      "epoch": 3.9744,
      "grad_norm": 103.80879211425781,
      "learning_rate": 6.752e-05,
      "loss": -111.9599,
      "step": 49680
    },
    {
      "epoch": 3.9752,
      "grad_norm": 56.76603698730469,
      "learning_rate": 6.749333333333335e-05,
      "loss": -111.3858,
      "step": 49690
    },
    {
      "epoch": 3.976,
      "grad_norm": 50.49058151245117,
      "learning_rate": 6.746666666666666e-05,
      "loss": -111.913,
      "step": 49700
    },
    {
      "epoch": 3.9768,
      "grad_norm": 96.92344665527344,
      "learning_rate": 6.744e-05,
      "loss": -111.6865,
      "step": 49710
    },
    {
      "epoch": 3.9776,
      "grad_norm": 60.41184616088867,
      "learning_rate": 6.741333333333333e-05,
      "loss": -111.5614,
      "step": 49720
    },
    {
      "epoch": 3.9784,
      "grad_norm": 55.03672409057617,
      "learning_rate": 6.738666666666666e-05,
      "loss": -112.0648,
      "step": 49730
    },
    {
      "epoch": 3.9792,
      "grad_norm": 27.263681411743164,
      "learning_rate": 6.736e-05,
      "loss": -112.3721,
      "step": 49740
    },
    {
      "epoch": 3.98,
      "grad_norm": 43.20363998413086,
      "learning_rate": 6.733333333333333e-05,
      "loss": -111.3054,
      "step": 49750
    },
    {
      "epoch": 3.9808,
      "grad_norm": 32.715232849121094,
      "learning_rate": 6.730666666666668e-05,
      "loss": -111.6231,
      "step": 49760
    },
    {
      "epoch": 3.9816000000000003,
      "grad_norm": 66.40422821044922,
      "learning_rate": 6.727999999999999e-05,
      "loss": -111.9435,
      "step": 49770
    },
    {
      "epoch": 3.9824,
      "grad_norm": 43.09952163696289,
      "learning_rate": 6.725333333333334e-05,
      "loss": -110.9294,
      "step": 49780
    },
    {
      "epoch": 3.9832,
      "grad_norm": 43.850341796875,
      "learning_rate": 6.722666666666666e-05,
      "loss": -111.6554,
      "step": 49790
    },
    {
      "epoch": 3.984,
      "grad_norm": 60.88130569458008,
      "learning_rate": 6.720000000000001e-05,
      "loss": -111.9635,
      "step": 49800
    },
    {
      "epoch": 3.9848,
      "grad_norm": 50.74357604980469,
      "learning_rate": 6.717333333333334e-05,
      "loss": -110.6366,
      "step": 49810
    },
    {
      "epoch": 3.9856,
      "grad_norm": 46.60508346557617,
      "learning_rate": 6.714666666666667e-05,
      "loss": -112.1881,
      "step": 49820
    },
    {
      "epoch": 3.9864,
      "grad_norm": 68.77590942382812,
      "learning_rate": 6.712000000000001e-05,
      "loss": -109.7072,
      "step": 49830
    },
    {
      "epoch": 3.9872,
      "grad_norm": 89.75272369384766,
      "learning_rate": 6.709333333333334e-05,
      "loss": -111.2181,
      "step": 49840
    },
    {
      "epoch": 3.988,
      "grad_norm": 57.00614929199219,
      "learning_rate": 6.706666666666667e-05,
      "loss": -111.3559,
      "step": 49850
    },
    {
      "epoch": 3.9888,
      "grad_norm": 79.07868194580078,
      "learning_rate": 6.704000000000001e-05,
      "loss": -110.2826,
      "step": 49860
    },
    {
      "epoch": 3.9896000000000003,
      "grad_norm": 83.07029724121094,
      "learning_rate": 6.701333333333334e-05,
      "loss": -111.8191,
      "step": 49870
    },
    {
      "epoch": 3.9904,
      "grad_norm": 98.82715606689453,
      "learning_rate": 6.698666666666668e-05,
      "loss": -112.1783,
      "step": 49880
    },
    {
      "epoch": 3.9912,
      "grad_norm": 28.139869689941406,
      "learning_rate": 6.696e-05,
      "loss": -110.9194,
      "step": 49890
    },
    {
      "epoch": 3.992,
      "grad_norm": 53.169883728027344,
      "learning_rate": 6.693333333333334e-05,
      "loss": -112.2491,
      "step": 49900
    },
    {
      "epoch": 3.9928,
      "grad_norm": 33.12636184692383,
      "learning_rate": 6.690666666666667e-05,
      "loss": -111.6795,
      "step": 49910
    },
    {
      "epoch": 3.9936,
      "grad_norm": 61.616703033447266,
      "learning_rate": 6.688e-05,
      "loss": -112.9131,
      "step": 49920
    },
    {
      "epoch": 3.9943999999999997,
      "grad_norm": 48.2901611328125,
      "learning_rate": 6.685333333333334e-05,
      "loss": -112.2667,
      "step": 49930
    },
    {
      "epoch": 3.9952,
      "grad_norm": 45.56051254272461,
      "learning_rate": 6.682666666666667e-05,
      "loss": -111.1036,
      "step": 49940
    },
    {
      "epoch": 3.996,
      "grad_norm": 28.420677185058594,
      "learning_rate": 6.680000000000001e-05,
      "loss": -111.5172,
      "step": 49950
    },
    {
      "epoch": 3.9968,
      "grad_norm": 45.28389358520508,
      "learning_rate": 6.677333333333333e-05,
      "loss": -112.0713,
      "step": 49960
    },
    {
      "epoch": 3.9976000000000003,
      "grad_norm": 37.681922912597656,
      "learning_rate": 6.674666666666667e-05,
      "loss": -110.889,
      "step": 49970
    },
    {
      "epoch": 3.9984,
      "grad_norm": 43.30651092529297,
      "learning_rate": 6.672e-05,
      "loss": -111.4122,
      "step": 49980
    },
    {
      "epoch": 3.9992,
      "grad_norm": 46.751808166503906,
      "learning_rate": 6.669333333333334e-05,
      "loss": -111.5118,
      "step": 49990
    },
    {
      "epoch": 4.0,
      "grad_norm": 44.418739318847656,
      "learning_rate": 6.666666666666667e-05,
      "loss": -111.5991,
      "step": 50000
    },
    {
      "epoch": 4.0008,
      "grad_norm": 100.99360656738281,
      "learning_rate": 6.664e-05,
      "loss": -111.8528,
      "step": 50010
    },
    {
      "epoch": 4.0016,
      "grad_norm": 86.56498718261719,
      "learning_rate": 6.661333333333334e-05,
      "loss": -111.8035,
      "step": 50020
    },
    {
      "epoch": 4.0024,
      "grad_norm": 73.11016082763672,
      "learning_rate": 6.658666666666667e-05,
      "loss": -110.8669,
      "step": 50030
    },
    {
      "epoch": 4.0032,
      "grad_norm": 67.94886779785156,
      "learning_rate": 6.656e-05,
      "loss": -111.4519,
      "step": 50040
    },
    {
      "epoch": 4.004,
      "grad_norm": 59.74888610839844,
      "learning_rate": 6.653333333333334e-05,
      "loss": -111.6062,
      "step": 50050
    },
    {
      "epoch": 4.0048,
      "grad_norm": 43.91120910644531,
      "learning_rate": 6.650666666666667e-05,
      "loss": -111.5696,
      "step": 50060
    },
    {
      "epoch": 4.0056,
      "grad_norm": 27.35360336303711,
      "learning_rate": 6.648e-05,
      "loss": -111.8489,
      "step": 50070
    },
    {
      "epoch": 4.0064,
      "grad_norm": 98.7093505859375,
      "learning_rate": 6.645333333333333e-05,
      "loss": -111.6111,
      "step": 50080
    },
    {
      "epoch": 4.0072,
      "grad_norm": 23.62535858154297,
      "learning_rate": 6.642666666666667e-05,
      "loss": -112.2594,
      "step": 50090
    },
    {
      "epoch": 4.008,
      "grad_norm": 39.456363677978516,
      "learning_rate": 6.64e-05,
      "loss": -111.0658,
      "step": 50100
    },
    {
      "epoch": 4.0088,
      "grad_norm": 124.09284210205078,
      "learning_rate": 6.637333333333333e-05,
      "loss": -110.893,
      "step": 50110
    },
    {
      "epoch": 4.0096,
      "grad_norm": 155.89376831054688,
      "learning_rate": 6.634666666666667e-05,
      "loss": -112.8109,
      "step": 50120
    },
    {
      "epoch": 4.0104,
      "grad_norm": 248.17208862304688,
      "learning_rate": 6.632e-05,
      "loss": -112.2958,
      "step": 50130
    },
    {
      "epoch": 4.0112,
      "grad_norm": 47.251251220703125,
      "learning_rate": 6.629333333333334e-05,
      "loss": -112.1333,
      "step": 50140
    },
    {
      "epoch": 4.012,
      "grad_norm": 59.96686553955078,
      "learning_rate": 6.626666666666666e-05,
      "loss": -111.3562,
      "step": 50150
    },
    {
      "epoch": 4.0128,
      "grad_norm": 42.520179748535156,
      "learning_rate": 6.624e-05,
      "loss": -111.1051,
      "step": 50160
    },
    {
      "epoch": 4.0136,
      "grad_norm": 39.918827056884766,
      "learning_rate": 6.621333333333333e-05,
      "loss": -111.8618,
      "step": 50170
    },
    {
      "epoch": 4.0144,
      "grad_norm": 25.76405906677246,
      "learning_rate": 6.618666666666667e-05,
      "loss": -111.742,
      "step": 50180
    },
    {
      "epoch": 4.0152,
      "grad_norm": 351.8457946777344,
      "learning_rate": 6.616e-05,
      "loss": -111.7777,
      "step": 50190
    },
    {
      "epoch": 4.016,
      "grad_norm": 34.699302673339844,
      "learning_rate": 6.613333333333333e-05,
      "loss": -111.0732,
      "step": 50200
    },
    {
      "epoch": 4.0168,
      "grad_norm": 52.37743377685547,
      "learning_rate": 6.610666666666668e-05,
      "loss": -111.3129,
      "step": 50210
    },
    {
      "epoch": 4.0176,
      "grad_norm": 68.22552490234375,
      "learning_rate": 6.608e-05,
      "loss": -111.7046,
      "step": 50220
    },
    {
      "epoch": 4.0184,
      "grad_norm": 28.598220825195312,
      "learning_rate": 6.605333333333333e-05,
      "loss": -110.471,
      "step": 50230
    },
    {
      "epoch": 4.0192,
      "grad_norm": 60.73981857299805,
      "learning_rate": 6.602666666666668e-05,
      "loss": -112.015,
      "step": 50240
    },
    {
      "epoch": 4.02,
      "grad_norm": 38.98711395263672,
      "learning_rate": 6.6e-05,
      "loss": -110.5966,
      "step": 50250
    },
    {
      "epoch": 4.0208,
      "grad_norm": 89.12147521972656,
      "learning_rate": 6.597333333333333e-05,
      "loss": -111.0057,
      "step": 50260
    },
    {
      "epoch": 4.0216,
      "grad_norm": 29.672740936279297,
      "learning_rate": 6.594666666666666e-05,
      "loss": -111.8128,
      "step": 50270
    },
    {
      "epoch": 4.0224,
      "grad_norm": 180.55789184570312,
      "learning_rate": 6.592e-05,
      "loss": -112.5427,
      "step": 50280
    },
    {
      "epoch": 4.0232,
      "grad_norm": 114.1630859375,
      "learning_rate": 6.589333333333333e-05,
      "loss": -110.55,
      "step": 50290
    },
    {
      "epoch": 4.024,
      "grad_norm": 96.65242767333984,
      "learning_rate": 6.586666666666666e-05,
      "loss": -112.122,
      "step": 50300
    },
    {
      "epoch": 4.0248,
      "grad_norm": 70.32855224609375,
      "learning_rate": 6.584e-05,
      "loss": -112.1083,
      "step": 50310
    },
    {
      "epoch": 4.0256,
      "grad_norm": 40.92213821411133,
      "learning_rate": 6.581333333333334e-05,
      "loss": -111.3162,
      "step": 50320
    },
    {
      "epoch": 4.0264,
      "grad_norm": 39.68980026245117,
      "learning_rate": 6.578666666666668e-05,
      "loss": -111.8366,
      "step": 50330
    },
    {
      "epoch": 4.0272,
      "grad_norm": 1972.8896484375,
      "learning_rate": 6.576e-05,
      "loss": -112.0207,
      "step": 50340
    },
    {
      "epoch": 4.028,
      "grad_norm": 40.61220932006836,
      "learning_rate": 6.573333333333334e-05,
      "loss": -111.3652,
      "step": 50350
    },
    {
      "epoch": 4.0288,
      "grad_norm": 51.177825927734375,
      "learning_rate": 6.570666666666667e-05,
      "loss": -112.0669,
      "step": 50360
    },
    {
      "epoch": 4.0296,
      "grad_norm": 63.09661865234375,
      "learning_rate": 6.568000000000001e-05,
      "loss": -112.0166,
      "step": 50370
    },
    {
      "epoch": 4.0304,
      "grad_norm": 26.00559425354004,
      "learning_rate": 6.565333333333334e-05,
      "loss": -112.0046,
      "step": 50380
    },
    {
      "epoch": 4.0312,
      "grad_norm": 49.72579574584961,
      "learning_rate": 6.562666666666667e-05,
      "loss": -110.9275,
      "step": 50390
    },
    {
      "epoch": 4.032,
      "grad_norm": 99.54563903808594,
      "learning_rate": 6.560000000000001e-05,
      "loss": -110.8729,
      "step": 50400
    },
    {
      "epoch": 4.0328,
      "grad_norm": 59.887088775634766,
      "learning_rate": 6.557333333333332e-05,
      "loss": -111.4916,
      "step": 50410
    },
    {
      "epoch": 4.0336,
      "grad_norm": 34.21868133544922,
      "learning_rate": 6.554666666666667e-05,
      "loss": -111.7063,
      "step": 50420
    },
    {
      "epoch": 4.0344,
      "grad_norm": 53.051719665527344,
      "learning_rate": 6.552000000000001e-05,
      "loss": -111.6656,
      "step": 50430
    },
    {
      "epoch": 4.0352,
      "grad_norm": 27.11687469482422,
      "learning_rate": 6.549333333333334e-05,
      "loss": -111.9431,
      "step": 50440
    },
    {
      "epoch": 4.036,
      "grad_norm": 64.3764419555664,
      "learning_rate": 6.546666666666667e-05,
      "loss": -112.2367,
      "step": 50450
    },
    {
      "epoch": 4.0368,
      "grad_norm": 69.1365966796875,
      "learning_rate": 6.544e-05,
      "loss": -111.8317,
      "step": 50460
    },
    {
      "epoch": 4.0376,
      "grad_norm": 80.36410522460938,
      "learning_rate": 6.541333333333334e-05,
      "loss": -111.5369,
      "step": 50470
    },
    {
      "epoch": 4.0384,
      "grad_norm": 33.93996047973633,
      "learning_rate": 6.538666666666667e-05,
      "loss": -110.9414,
      "step": 50480
    },
    {
      "epoch": 4.0392,
      "grad_norm": 109.4877700805664,
      "learning_rate": 6.536e-05,
      "loss": -110.9024,
      "step": 50490
    },
    {
      "epoch": 4.04,
      "grad_norm": 42.037471771240234,
      "learning_rate": 6.533333333333334e-05,
      "loss": -111.6679,
      "step": 50500
    },
    {
      "epoch": 4.0408,
      "grad_norm": 55.09101867675781,
      "learning_rate": 6.530666666666667e-05,
      "loss": -112.0411,
      "step": 50510
    },
    {
      "epoch": 4.0416,
      "grad_norm": 85.42781829833984,
      "learning_rate": 6.528000000000001e-05,
      "loss": -112.3301,
      "step": 50520
    },
    {
      "epoch": 4.0424,
      "grad_norm": 135.6666717529297,
      "learning_rate": 6.525333333333333e-05,
      "loss": -111.295,
      "step": 50530
    },
    {
      "epoch": 4.0432,
      "grad_norm": 57.19678497314453,
      "learning_rate": 6.522666666666667e-05,
      "loss": -111.6686,
      "step": 50540
    },
    {
      "epoch": 4.044,
      "grad_norm": 133.8834228515625,
      "learning_rate": 6.52e-05,
      "loss": -111.7178,
      "step": 50550
    },
    {
      "epoch": 4.0448,
      "grad_norm": 24.720178604125977,
      "learning_rate": 6.517333333333334e-05,
      "loss": -111.6575,
      "step": 50560
    },
    {
      "epoch": 4.0456,
      "grad_norm": 51.1668586730957,
      "learning_rate": 6.514666666666667e-05,
      "loss": -111.0466,
      "step": 50570
    },
    {
      "epoch": 4.0464,
      "grad_norm": 54.224212646484375,
      "learning_rate": 6.512e-05,
      "loss": -110.9257,
      "step": 50580
    },
    {
      "epoch": 4.0472,
      "grad_norm": 43.05891799926758,
      "learning_rate": 6.509333333333334e-05,
      "loss": -111.2278,
      "step": 50590
    },
    {
      "epoch": 4.048,
      "grad_norm": 52.14932632446289,
      "learning_rate": 6.506666666666666e-05,
      "loss": -111.4513,
      "step": 50600
    },
    {
      "epoch": 4.0488,
      "grad_norm": 95.0075912475586,
      "learning_rate": 6.504e-05,
      "loss": -111.8507,
      "step": 50610
    },
    {
      "epoch": 4.0496,
      "grad_norm": 61.20779037475586,
      "learning_rate": 6.501333333333334e-05,
      "loss": -111.9103,
      "step": 50620
    },
    {
      "epoch": 4.0504,
      "grad_norm": 29.307649612426758,
      "learning_rate": 6.498666666666667e-05,
      "loss": -110.0519,
      "step": 50630
    },
    {
      "epoch": 4.0512,
      "grad_norm": 68.43403625488281,
      "learning_rate": 6.496e-05,
      "loss": -110.7514,
      "step": 50640
    },
    {
      "epoch": 4.052,
      "grad_norm": 50.63087844848633,
      "learning_rate": 6.493333333333333e-05,
      "loss": -112.1041,
      "step": 50650
    },
    {
      "epoch": 4.0528,
      "grad_norm": 40.772743225097656,
      "learning_rate": 6.490666666666667e-05,
      "loss": -111.1065,
      "step": 50660
    },
    {
      "epoch": 4.0536,
      "grad_norm": 66.71546936035156,
      "learning_rate": 6.488e-05,
      "loss": -112.3396,
      "step": 50670
    },
    {
      "epoch": 4.0544,
      "grad_norm": 54.51691818237305,
      "learning_rate": 6.485333333333333e-05,
      "loss": -111.6287,
      "step": 50680
    },
    {
      "epoch": 4.0552,
      "grad_norm": 24.10575294494629,
      "learning_rate": 6.482666666666667e-05,
      "loss": -111.3315,
      "step": 50690
    },
    {
      "epoch": 4.056,
      "grad_norm": 89.35057830810547,
      "learning_rate": 6.48e-05,
      "loss": -110.8994,
      "step": 50700
    },
    {
      "epoch": 4.0568,
      "grad_norm": 75.0699234008789,
      "learning_rate": 6.477333333333335e-05,
      "loss": -110.4781,
      "step": 50710
    },
    {
      "epoch": 4.0576,
      "grad_norm": 33.6561279296875,
      "learning_rate": 6.474666666666666e-05,
      "loss": -111.6119,
      "step": 50720
    },
    {
      "epoch": 4.0584,
      "grad_norm": 56.352821350097656,
      "learning_rate": 6.472e-05,
      "loss": -111.7055,
      "step": 50730
    },
    {
      "epoch": 4.0592,
      "grad_norm": 50.96638488769531,
      "learning_rate": 6.469333333333333e-05,
      "loss": -112.0634,
      "step": 50740
    },
    {
      "epoch": 4.06,
      "grad_norm": 98.42803955078125,
      "learning_rate": 6.466666666666666e-05,
      "loss": -112.0421,
      "step": 50750
    },
    {
      "epoch": 4.0608,
      "grad_norm": 34.36460876464844,
      "learning_rate": 6.464e-05,
      "loss": -111.2729,
      "step": 50760
    },
    {
      "epoch": 4.0616,
      "grad_norm": 157.3453826904297,
      "learning_rate": 6.461333333333333e-05,
      "loss": -111.9398,
      "step": 50770
    },
    {
      "epoch": 4.0624,
      "grad_norm": 46.25212097167969,
      "learning_rate": 6.458666666666668e-05,
      "loss": -112.0624,
      "step": 50780
    },
    {
      "epoch": 4.0632,
      "grad_norm": 46.61738204956055,
      "learning_rate": 6.455999999999999e-05,
      "loss": -111.1987,
      "step": 50790
    },
    {
      "epoch": 4.064,
      "grad_norm": 61.5568962097168,
      "learning_rate": 6.453333333333333e-05,
      "loss": -111.2953,
      "step": 50800
    },
    {
      "epoch": 4.0648,
      "grad_norm": 70.18061828613281,
      "learning_rate": 6.450666666666668e-05,
      "loss": -111.2316,
      "step": 50810
    },
    {
      "epoch": 4.0656,
      "grad_norm": 51.72121047973633,
      "learning_rate": 6.448e-05,
      "loss": -111.4108,
      "step": 50820
    },
    {
      "epoch": 4.0664,
      "grad_norm": 40.5112190246582,
      "learning_rate": 6.445333333333334e-05,
      "loss": -112.1663,
      "step": 50830
    },
    {
      "epoch": 4.0672,
      "grad_norm": 82.09524536132812,
      "learning_rate": 6.442666666666666e-05,
      "loss": -110.5228,
      "step": 50840
    },
    {
      "epoch": 4.068,
      "grad_norm": 72.76375579833984,
      "learning_rate": 6.440000000000001e-05,
      "loss": -111.5572,
      "step": 50850
    },
    {
      "epoch": 4.0688,
      "grad_norm": 74.19159698486328,
      "learning_rate": 6.437333333333334e-05,
      "loss": -111.0541,
      "step": 50860
    },
    {
      "epoch": 4.0696,
      "grad_norm": 46.06669235229492,
      "learning_rate": 6.434666666666666e-05,
      "loss": -111.2366,
      "step": 50870
    },
    {
      "epoch": 4.0704,
      "grad_norm": 37.5854377746582,
      "learning_rate": 6.432000000000001e-05,
      "loss": -111.6081,
      "step": 50880
    },
    {
      "epoch": 4.0712,
      "grad_norm": 49.89433670043945,
      "learning_rate": 6.429333333333334e-05,
      "loss": -111.0952,
      "step": 50890
    },
    {
      "epoch": 4.072,
      "grad_norm": 137.48428344726562,
      "learning_rate": 6.426666666666668e-05,
      "loss": -110.9664,
      "step": 50900
    },
    {
      "epoch": 4.0728,
      "grad_norm": 63.36703109741211,
      "learning_rate": 6.424e-05,
      "loss": -111.716,
      "step": 50910
    },
    {
      "epoch": 4.0736,
      "grad_norm": 53.74271774291992,
      "learning_rate": 6.421333333333334e-05,
      "loss": -111.7847,
      "step": 50920
    },
    {
      "epoch": 4.0744,
      "grad_norm": 46.727783203125,
      "learning_rate": 6.418666666666667e-05,
      "loss": -112.1954,
      "step": 50930
    },
    {
      "epoch": 4.0752,
      "grad_norm": 39.99163055419922,
      "learning_rate": 6.416e-05,
      "loss": -111.77,
      "step": 50940
    },
    {
      "epoch": 4.076,
      "grad_norm": 37.99750518798828,
      "learning_rate": 6.413333333333334e-05,
      "loss": -110.9,
      "step": 50950
    },
    {
      "epoch": 4.0768,
      "grad_norm": 73.34241485595703,
      "learning_rate": 6.410666666666667e-05,
      "loss": -111.1403,
      "step": 50960
    },
    {
      "epoch": 4.0776,
      "grad_norm": 37.68927001953125,
      "learning_rate": 6.408000000000001e-05,
      "loss": -111.9777,
      "step": 50970
    },
    {
      "epoch": 4.0784,
      "grad_norm": 52.13044738769531,
      "learning_rate": 6.405333333333333e-05,
      "loss": -112.0031,
      "step": 50980
    },
    {
      "epoch": 4.0792,
      "grad_norm": 76.54530334472656,
      "learning_rate": 6.402666666666667e-05,
      "loss": -113.0273,
      "step": 50990
    },
    {
      "epoch": 4.08,
      "grad_norm": 48.24917221069336,
      "learning_rate": 6.400000000000001e-05,
      "loss": -110.6987,
      "step": 51000
    },
    {
      "epoch": 4.0808,
      "grad_norm": 55.84427261352539,
      "learning_rate": 6.397333333333334e-05,
      "loss": -111.9858,
      "step": 51010
    },
    {
      "epoch": 4.0816,
      "grad_norm": 103.25335693359375,
      "learning_rate": 6.394666666666667e-05,
      "loss": -112.6205,
      "step": 51020
    },
    {
      "epoch": 4.0824,
      "grad_norm": 61.958457946777344,
      "learning_rate": 6.392e-05,
      "loss": -111.6462,
      "step": 51030
    },
    {
      "epoch": 4.0832,
      "grad_norm": 55.638092041015625,
      "learning_rate": 6.389333333333334e-05,
      "loss": -111.2545,
      "step": 51040
    },
    {
      "epoch": 4.084,
      "grad_norm": 45.03780746459961,
      "learning_rate": 6.386666666666667e-05,
      "loss": -111.6592,
      "step": 51050
    },
    {
      "epoch": 4.0848,
      "grad_norm": 88.45835876464844,
      "learning_rate": 6.384e-05,
      "loss": -111.7059,
      "step": 51060
    },
    {
      "epoch": 4.0856,
      "grad_norm": 89.62967681884766,
      "learning_rate": 6.381333333333334e-05,
      "loss": -111.8275,
      "step": 51070
    },
    {
      "epoch": 4.0864,
      "grad_norm": 100.88707733154297,
      "learning_rate": 6.378666666666667e-05,
      "loss": -111.7529,
      "step": 51080
    },
    {
      "epoch": 4.0872,
      "grad_norm": 33.66193389892578,
      "learning_rate": 6.376e-05,
      "loss": -111.1719,
      "step": 51090
    },
    {
      "epoch": 4.088,
      "grad_norm": 60.60382080078125,
      "learning_rate": 6.373333333333333e-05,
      "loss": -111.8643,
      "step": 51100
    },
    {
      "epoch": 4.0888,
      "grad_norm": 24.065120697021484,
      "learning_rate": 6.370666666666667e-05,
      "loss": -111.0373,
      "step": 51110
    },
    {
      "epoch": 4.0896,
      "grad_norm": 49.56155014038086,
      "learning_rate": 6.368e-05,
      "loss": -111.3384,
      "step": 51120
    },
    {
      "epoch": 4.0904,
      "grad_norm": 84.95259857177734,
      "learning_rate": 6.365333333333333e-05,
      "loss": -110.5953,
      "step": 51130
    },
    {
      "epoch": 4.0912,
      "grad_norm": 78.47686767578125,
      "learning_rate": 6.362666666666667e-05,
      "loss": -112.0392,
      "step": 51140
    },
    {
      "epoch": 4.092,
      "grad_norm": 75.19001770019531,
      "learning_rate": 6.36e-05,
      "loss": -111.6713,
      "step": 51150
    },
    {
      "epoch": 4.0928,
      "grad_norm": 89.16775512695312,
      "learning_rate": 6.357333333333334e-05,
      "loss": -112.0267,
      "step": 51160
    },
    {
      "epoch": 4.0936,
      "grad_norm": 87.29662322998047,
      "learning_rate": 6.354666666666666e-05,
      "loss": -111.2354,
      "step": 51170
    },
    {
      "epoch": 4.0944,
      "grad_norm": 149.01449584960938,
      "learning_rate": 6.352e-05,
      "loss": -112.9354,
      "step": 51180
    },
    {
      "epoch": 4.0952,
      "grad_norm": 88.60614776611328,
      "learning_rate": 6.349333333333334e-05,
      "loss": -111.3639,
      "step": 51190
    },
    {
      "epoch": 4.096,
      "grad_norm": 68.35648345947266,
      "learning_rate": 6.346666666666667e-05,
      "loss": -111.7475,
      "step": 51200
    },
    {
      "epoch": 4.0968,
      "grad_norm": 34.5777473449707,
      "learning_rate": 6.344e-05,
      "loss": -111.2313,
      "step": 51210
    },
    {
      "epoch": 4.0976,
      "grad_norm": 100.96736145019531,
      "learning_rate": 6.341333333333333e-05,
      "loss": -112.3855,
      "step": 51220
    },
    {
      "epoch": 4.0984,
      "grad_norm": 45.8154296875,
      "learning_rate": 6.338666666666667e-05,
      "loss": -110.987,
      "step": 51230
    },
    {
      "epoch": 4.0992,
      "grad_norm": 78.16387176513672,
      "learning_rate": 6.336e-05,
      "loss": -110.9628,
      "step": 51240
    },
    {
      "epoch": 4.1,
      "grad_norm": 23.797121047973633,
      "learning_rate": 6.333333333333333e-05,
      "loss": -112.1174,
      "step": 51250
    },
    {
      "epoch": 4.1008,
      "grad_norm": 120.9790267944336,
      "learning_rate": 6.330666666666667e-05,
      "loss": -112.6573,
      "step": 51260
    },
    {
      "epoch": 4.1016,
      "grad_norm": 98.77420806884766,
      "learning_rate": 6.328e-05,
      "loss": -111.6063,
      "step": 51270
    },
    {
      "epoch": 4.1024,
      "grad_norm": 39.49021911621094,
      "learning_rate": 6.325333333333333e-05,
      "loss": -111.447,
      "step": 51280
    },
    {
      "epoch": 4.1032,
      "grad_norm": 53.681461334228516,
      "learning_rate": 6.322666666666666e-05,
      "loss": -110.8994,
      "step": 51290
    },
    {
      "epoch": 4.104,
      "grad_norm": 53.13235092163086,
      "learning_rate": 6.32e-05,
      "loss": -111.2474,
      "step": 51300
    },
    {
      "epoch": 4.1048,
      "grad_norm": 59.20662307739258,
      "learning_rate": 6.317333333333333e-05,
      "loss": -111.1243,
      "step": 51310
    },
    {
      "epoch": 4.1056,
      "grad_norm": 78.15950012207031,
      "learning_rate": 6.314666666666666e-05,
      "loss": -112.4989,
      "step": 51320
    },
    {
      "epoch": 4.1064,
      "grad_norm": 33.64419174194336,
      "learning_rate": 6.312e-05,
      "loss": -111.6081,
      "step": 51330
    },
    {
      "epoch": 4.1072,
      "grad_norm": 37.55224609375,
      "learning_rate": 6.309333333333333e-05,
      "loss": -112.2307,
      "step": 51340
    },
    {
      "epoch": 4.108,
      "grad_norm": 149.15774536132812,
      "learning_rate": 6.306666666666668e-05,
      "loss": -111.8749,
      "step": 51350
    },
    {
      "epoch": 4.1088,
      "grad_norm": 22.314722061157227,
      "learning_rate": 6.303999999999999e-05,
      "loss": -110.705,
      "step": 51360
    },
    {
      "epoch": 4.1096,
      "grad_norm": 25.29064178466797,
      "learning_rate": 6.301333333333334e-05,
      "loss": -111.7278,
      "step": 51370
    },
    {
      "epoch": 4.1104,
      "grad_norm": 132.81124877929688,
      "learning_rate": 6.298666666666668e-05,
      "loss": -111.4155,
      "step": 51380
    },
    {
      "epoch": 4.1112,
      "grad_norm": 36.50937271118164,
      "learning_rate": 6.296000000000001e-05,
      "loss": -110.6335,
      "step": 51390
    },
    {
      "epoch": 4.112,
      "grad_norm": 78.41765594482422,
      "learning_rate": 6.293333333333334e-05,
      "loss": -111.61,
      "step": 51400
    },
    {
      "epoch": 4.1128,
      "grad_norm": 42.256103515625,
      "learning_rate": 6.290666666666667e-05,
      "loss": -111.3199,
      "step": 51410
    },
    {
      "epoch": 4.1136,
      "grad_norm": 40.55744934082031,
      "learning_rate": 6.288000000000001e-05,
      "loss": -111.4798,
      "step": 51420
    },
    {
      "epoch": 4.1144,
      "grad_norm": 30.560773849487305,
      "learning_rate": 6.285333333333334e-05,
      "loss": -111.7329,
      "step": 51430
    },
    {
      "epoch": 4.1152,
      "grad_norm": 177.9715118408203,
      "learning_rate": 6.282666666666667e-05,
      "loss": -112.6229,
      "step": 51440
    },
    {
      "epoch": 4.116,
      "grad_norm": 109.4979019165039,
      "learning_rate": 6.280000000000001e-05,
      "loss": -111.9657,
      "step": 51450
    },
    {
      "epoch": 4.1168,
      "grad_norm": 127.49341583251953,
      "learning_rate": 6.277333333333334e-05,
      "loss": -111.4449,
      "step": 51460
    },
    {
      "epoch": 4.1176,
      "grad_norm": 32.381717681884766,
      "learning_rate": 6.274666666666667e-05,
      "loss": -112.1,
      "step": 51470
    },
    {
      "epoch": 4.1184,
      "grad_norm": 31.270469665527344,
      "learning_rate": 6.272e-05,
      "loss": -110.4931,
      "step": 51480
    },
    {
      "epoch": 4.1192,
      "grad_norm": 51.68553161621094,
      "learning_rate": 6.269333333333334e-05,
      "loss": -111.123,
      "step": 51490
    },
    {
      "epoch": 4.12,
      "grad_norm": 55.89820861816406,
      "learning_rate": 6.266666666666667e-05,
      "loss": -111.2463,
      "step": 51500
    },
    {
      "epoch": 4.1208,
      "grad_norm": 84.83834075927734,
      "learning_rate": 6.264e-05,
      "loss": -112.1876,
      "step": 51510
    },
    {
      "epoch": 4.1216,
      "grad_norm": 132.08863830566406,
      "learning_rate": 6.261333333333334e-05,
      "loss": -111.8924,
      "step": 51520
    },
    {
      "epoch": 4.1224,
      "grad_norm": 58.365623474121094,
      "learning_rate": 6.258666666666667e-05,
      "loss": -111.5802,
      "step": 51530
    },
    {
      "epoch": 4.1232,
      "grad_norm": 31.354433059692383,
      "learning_rate": 6.256000000000001e-05,
      "loss": -111.447,
      "step": 51540
    },
    {
      "epoch": 4.124,
      "grad_norm": 57.26865768432617,
      "learning_rate": 6.253333333333333e-05,
      "loss": -111.1619,
      "step": 51550
    },
    {
      "epoch": 4.1248,
      "grad_norm": 32.98839569091797,
      "learning_rate": 6.250666666666667e-05,
      "loss": -112.086,
      "step": 51560
    },
    {
      "epoch": 4.1256,
      "grad_norm": 115.34679412841797,
      "learning_rate": 6.248000000000001e-05,
      "loss": -111.3597,
      "step": 51570
    },
    {
      "epoch": 4.1264,
      "grad_norm": 41.938926696777344,
      "learning_rate": 6.245333333333334e-05,
      "loss": -111.2748,
      "step": 51580
    },
    {
      "epoch": 4.1272,
      "grad_norm": 34.85150909423828,
      "learning_rate": 6.242666666666667e-05,
      "loss": -110.777,
      "step": 51590
    },
    {
      "epoch": 4.128,
      "grad_norm": 109.73533630371094,
      "learning_rate": 6.24e-05,
      "loss": -111.5265,
      "step": 51600
    },
    {
      "epoch": 4.1288,
      "grad_norm": 55.371246337890625,
      "learning_rate": 6.237333333333334e-05,
      "loss": -110.708,
      "step": 51610
    },
    {
      "epoch": 4.1296,
      "grad_norm": 104.88191986083984,
      "learning_rate": 6.234666666666667e-05,
      "loss": -111.626,
      "step": 51620
    },
    {
      "epoch": 4.1304,
      "grad_norm": 38.596351623535156,
      "learning_rate": 6.232e-05,
      "loss": -110.9642,
      "step": 51630
    },
    {
      "epoch": 4.1312,
      "grad_norm": 39.31340026855469,
      "learning_rate": 6.229333333333334e-05,
      "loss": -112.2152,
      "step": 51640
    },
    {
      "epoch": 4.132,
      "grad_norm": 33.92249298095703,
      "learning_rate": 6.226666666666667e-05,
      "loss": -111.586,
      "step": 51650
    },
    {
      "epoch": 4.1328,
      "grad_norm": 40.836639404296875,
      "learning_rate": 6.224e-05,
      "loss": -112.2114,
      "step": 51660
    },
    {
      "epoch": 4.1336,
      "grad_norm": 24.299745559692383,
      "learning_rate": 6.221333333333333e-05,
      "loss": -111.8365,
      "step": 51670
    },
    {
      "epoch": 4.1344,
      "grad_norm": 77.92029571533203,
      "learning_rate": 6.218666666666667e-05,
      "loss": -111.5401,
      "step": 51680
    },
    {
      "epoch": 4.1352,
      "grad_norm": 38.320438385009766,
      "learning_rate": 6.216e-05,
      "loss": -111.9774,
      "step": 51690
    },
    {
      "epoch": 4.136,
      "grad_norm": 133.0874481201172,
      "learning_rate": 6.213333333333333e-05,
      "loss": -111.3195,
      "step": 51700
    },
    {
      "epoch": 4.1368,
      "grad_norm": 141.24746704101562,
      "learning_rate": 6.210666666666667e-05,
      "loss": -111.1024,
      "step": 51710
    },
    {
      "epoch": 4.1376,
      "grad_norm": 136.3776092529297,
      "learning_rate": 6.208e-05,
      "loss": -111.8964,
      "step": 51720
    },
    {
      "epoch": 4.1384,
      "grad_norm": 25.049484252929688,
      "learning_rate": 6.205333333333334e-05,
      "loss": -110.8285,
      "step": 51730
    },
    {
      "epoch": 4.1392,
      "grad_norm": 113.61339569091797,
      "learning_rate": 6.202666666666666e-05,
      "loss": -111.9148,
      "step": 51740
    },
    {
      "epoch": 4.14,
      "grad_norm": 36.82512283325195,
      "learning_rate": 6.2e-05,
      "loss": -110.8006,
      "step": 51750
    },
    {
      "epoch": 4.1408,
      "grad_norm": 54.07020950317383,
      "learning_rate": 6.197333333333335e-05,
      "loss": -111.6141,
      "step": 51760
    },
    {
      "epoch": 4.1416,
      "grad_norm": 45.921016693115234,
      "learning_rate": 6.194666666666667e-05,
      "loss": -110.7892,
      "step": 51770
    },
    {
      "epoch": 4.1424,
      "grad_norm": 97.98858642578125,
      "learning_rate": 6.192e-05,
      "loss": -112.0938,
      "step": 51780
    },
    {
      "epoch": 4.1432,
      "grad_norm": 38.71593475341797,
      "learning_rate": 6.189333333333333e-05,
      "loss": -111.8975,
      "step": 51790
    },
    {
      "epoch": 4.144,
      "grad_norm": 60.27729415893555,
      "learning_rate": 6.186666666666668e-05,
      "loss": -111.692,
      "step": 51800
    },
    {
      "epoch": 4.1448,
      "grad_norm": 79.973876953125,
      "learning_rate": 6.184e-05,
      "loss": -111.2137,
      "step": 51810
    },
    {
      "epoch": 4.1456,
      "grad_norm": 41.68427658081055,
      "learning_rate": 6.181333333333333e-05,
      "loss": -111.6589,
      "step": 51820
    },
    {
      "epoch": 4.1464,
      "grad_norm": 83.25638580322266,
      "learning_rate": 6.178666666666668e-05,
      "loss": -111.2754,
      "step": 51830
    },
    {
      "epoch": 4.1472,
      "grad_norm": 165.21270751953125,
      "learning_rate": 6.176e-05,
      "loss": -111.3944,
      "step": 51840
    },
    {
      "epoch": 4.148,
      "grad_norm": 30.637622833251953,
      "learning_rate": 6.173333333333333e-05,
      "loss": -111.4567,
      "step": 51850
    },
    {
      "epoch": 4.1488,
      "grad_norm": 90.19660949707031,
      "learning_rate": 6.170666666666666e-05,
      "loss": -111.2302,
      "step": 51860
    },
    {
      "epoch": 4.1496,
      "grad_norm": 33.80951690673828,
      "learning_rate": 6.168e-05,
      "loss": -111.9461,
      "step": 51870
    },
    {
      "epoch": 4.1504,
      "grad_norm": 118.52366638183594,
      "learning_rate": 6.165333333333333e-05,
      "loss": -111.4687,
      "step": 51880
    },
    {
      "epoch": 4.1512,
      "grad_norm": 53.08109664916992,
      "learning_rate": 6.162666666666666e-05,
      "loss": -111.6722,
      "step": 51890
    },
    {
      "epoch": 4.152,
      "grad_norm": 92.82231903076172,
      "learning_rate": 6.16e-05,
      "loss": -111.2885,
      "step": 51900
    },
    {
      "epoch": 4.1528,
      "grad_norm": 93.17903900146484,
      "learning_rate": 6.157333333333334e-05,
      "loss": -111.203,
      "step": 51910
    },
    {
      "epoch": 4.1536,
      "grad_norm": 50.99751663208008,
      "learning_rate": 6.154666666666668e-05,
      "loss": -112.0088,
      "step": 51920
    },
    {
      "epoch": 4.1544,
      "grad_norm": 39.919151306152344,
      "learning_rate": 6.152e-05,
      "loss": -111.9568,
      "step": 51930
    },
    {
      "epoch": 4.1552,
      "grad_norm": 156.43614196777344,
      "learning_rate": 6.149333333333334e-05,
      "loss": -110.7365,
      "step": 51940
    },
    {
      "epoch": 4.156,
      "grad_norm": 46.79084396362305,
      "learning_rate": 6.146666666666668e-05,
      "loss": -110.4289,
      "step": 51950
    },
    {
      "epoch": 4.1568,
      "grad_norm": 31.396352767944336,
      "learning_rate": 6.144e-05,
      "loss": -110.9301,
      "step": 51960
    },
    {
      "epoch": 4.1576,
      "grad_norm": 86.97259521484375,
      "learning_rate": 6.141333333333334e-05,
      "loss": -111.4429,
      "step": 51970
    },
    {
      "epoch": 4.1584,
      "grad_norm": 89.38528442382812,
      "learning_rate": 6.138666666666667e-05,
      "loss": -111.2826,
      "step": 51980
    },
    {
      "epoch": 4.1592,
      "grad_norm": 83.39295196533203,
      "learning_rate": 6.136000000000001e-05,
      "loss": -111.7229,
      "step": 51990
    },
    {
      "epoch": 4.16,
      "grad_norm": 42.0246696472168,
      "learning_rate": 6.133333333333334e-05,
      "loss": -111.2296,
      "step": 52000
    },
    {
      "epoch": 4.1608,
      "grad_norm": 97.54235076904297,
      "learning_rate": 6.130666666666667e-05,
      "loss": -111.6603,
      "step": 52010
    },
    {
      "epoch": 4.1616,
      "grad_norm": 74.61470031738281,
      "learning_rate": 6.128000000000001e-05,
      "loss": -111.0605,
      "step": 52020
    },
    {
      "epoch": 4.1624,
      "grad_norm": 131.9547576904297,
      "learning_rate": 6.125333333333334e-05,
      "loss": -112.0332,
      "step": 52030
    },
    {
      "epoch": 4.1632,
      "grad_norm": 34.256526947021484,
      "learning_rate": 6.122666666666667e-05,
      "loss": -111.5046,
      "step": 52040
    },
    {
      "epoch": 4.164,
      "grad_norm": 48.647029876708984,
      "learning_rate": 6.12e-05,
      "loss": -110.5855,
      "step": 52050
    },
    {
      "epoch": 4.1648,
      "grad_norm": 56.48542022705078,
      "learning_rate": 6.117333333333334e-05,
      "loss": -112.0149,
      "step": 52060
    },
    {
      "epoch": 4.1656,
      "grad_norm": 70.24964904785156,
      "learning_rate": 6.114666666666667e-05,
      "loss": -111.4386,
      "step": 52070
    },
    {
      "epoch": 4.1664,
      "grad_norm": 88.94827270507812,
      "learning_rate": 6.112e-05,
      "loss": -111.1105,
      "step": 52080
    },
    {
      "epoch": 4.1672,
      "grad_norm": 164.84718322753906,
      "learning_rate": 6.109333333333334e-05,
      "loss": -111.483,
      "step": 52090
    },
    {
      "epoch": 4.168,
      "grad_norm": 101.41165924072266,
      "learning_rate": 6.106666666666667e-05,
      "loss": -111.3388,
      "step": 52100
    },
    {
      "epoch": 4.1688,
      "grad_norm": 91.59110260009766,
      "learning_rate": 6.104000000000001e-05,
      "loss": -111.4083,
      "step": 52110
    },
    {
      "epoch": 4.1696,
      "grad_norm": 28.97580909729004,
      "learning_rate": 6.1013333333333334e-05,
      "loss": -111.0725,
      "step": 52120
    },
    {
      "epoch": 4.1704,
      "grad_norm": 29.623058319091797,
      "learning_rate": 6.098666666666667e-05,
      "loss": -110.9983,
      "step": 52130
    },
    {
      "epoch": 4.1712,
      "grad_norm": 73.726806640625,
      "learning_rate": 6.0960000000000006e-05,
      "loss": -111.4857,
      "step": 52140
    },
    {
      "epoch": 4.172,
      "grad_norm": 67.43180084228516,
      "learning_rate": 6.093333333333333e-05,
      "loss": -111.9541,
      "step": 52150
    },
    {
      "epoch": 4.1728,
      "grad_norm": 185.2897491455078,
      "learning_rate": 6.090666666666667e-05,
      "loss": -112.3475,
      "step": 52160
    },
    {
      "epoch": 4.1736,
      "grad_norm": 33.076568603515625,
      "learning_rate": 6.088000000000001e-05,
      "loss": -110.6513,
      "step": 52170
    },
    {
      "epoch": 4.1744,
      "grad_norm": 129.8370819091797,
      "learning_rate": 6.085333333333334e-05,
      "loss": -111.759,
      "step": 52180
    },
    {
      "epoch": 4.1752,
      "grad_norm": 75.39401245117188,
      "learning_rate": 6.0826666666666665e-05,
      "loss": -111.4996,
      "step": 52190
    },
    {
      "epoch": 4.176,
      "grad_norm": 30.143991470336914,
      "learning_rate": 6.08e-05,
      "loss": -111.8574,
      "step": 52200
    },
    {
      "epoch": 4.1768,
      "grad_norm": 30.54765510559082,
      "learning_rate": 6.0773333333333336e-05,
      "loss": -111.1463,
      "step": 52210
    },
    {
      "epoch": 4.1776,
      "grad_norm": 47.34572219848633,
      "learning_rate": 6.074666666666667e-05,
      "loss": -111.7166,
      "step": 52220
    },
    {
      "epoch": 4.1784,
      "grad_norm": 32.24619674682617,
      "learning_rate": 6.072e-05,
      "loss": -112.6989,
      "step": 52230
    },
    {
      "epoch": 4.1792,
      "grad_norm": 22.79889678955078,
      "learning_rate": 6.069333333333334e-05,
      "loss": -111.4459,
      "step": 52240
    },
    {
      "epoch": 4.18,
      "grad_norm": 94.20228576660156,
      "learning_rate": 6.066666666666667e-05,
      "loss": -112.6753,
      "step": 52250
    },
    {
      "epoch": 4.1808,
      "grad_norm": 97.08442687988281,
      "learning_rate": 6.064000000000001e-05,
      "loss": -111.3084,
      "step": 52260
    },
    {
      "epoch": 4.1816,
      "grad_norm": 46.978904724121094,
      "learning_rate": 6.061333333333333e-05,
      "loss": -110.5578,
      "step": 52270
    },
    {
      "epoch": 4.1824,
      "grad_norm": 48.29569625854492,
      "learning_rate": 6.058666666666667e-05,
      "loss": -111.5203,
      "step": 52280
    },
    {
      "epoch": 4.1832,
      "grad_norm": 38.60308074951172,
      "learning_rate": 6.056e-05,
      "loss": -110.9788,
      "step": 52290
    },
    {
      "epoch": 4.184,
      "grad_norm": 113.90882873535156,
      "learning_rate": 6.053333333333333e-05,
      "loss": -111.6598,
      "step": 52300
    },
    {
      "epoch": 4.1848,
      "grad_norm": 35.05881118774414,
      "learning_rate": 6.050666666666667e-05,
      "loss": -111.4729,
      "step": 52310
    },
    {
      "epoch": 4.1856,
      "grad_norm": 50.41560745239258,
      "learning_rate": 6.0480000000000004e-05,
      "loss": -111.7451,
      "step": 52320
    },
    {
      "epoch": 4.1864,
      "grad_norm": 51.184654235839844,
      "learning_rate": 6.045333333333334e-05,
      "loss": -111.05,
      "step": 52330
    },
    {
      "epoch": 4.1872,
      "grad_norm": 67.03953552246094,
      "learning_rate": 6.042666666666666e-05,
      "loss": -112.093,
      "step": 52340
    },
    {
      "epoch": 4.188,
      "grad_norm": 83.24772644042969,
      "learning_rate": 6.04e-05,
      "loss": -111.4594,
      "step": 52350
    },
    {
      "epoch": 4.1888,
      "grad_norm": 48.97765350341797,
      "learning_rate": 6.037333333333334e-05,
      "loss": -111.1798,
      "step": 52360
    },
    {
      "epoch": 4.1896,
      "grad_norm": 91.6019515991211,
      "learning_rate": 6.0346666666666676e-05,
      "loss": -111.9465,
      "step": 52370
    },
    {
      "epoch": 4.1904,
      "grad_norm": 77.37218475341797,
      "learning_rate": 6.032e-05,
      "loss": -112.4596,
      "step": 52380
    },
    {
      "epoch": 4.1912,
      "grad_norm": 52.909149169921875,
      "learning_rate": 6.0293333333333334e-05,
      "loss": -111.1151,
      "step": 52390
    },
    {
      "epoch": 4.192,
      "grad_norm": 26.3577880859375,
      "learning_rate": 6.026666666666667e-05,
      "loss": -111.6373,
      "step": 52400
    },
    {
      "epoch": 4.1928,
      "grad_norm": 64.906494140625,
      "learning_rate": 6.0240000000000006e-05,
      "loss": -111.8754,
      "step": 52410
    },
    {
      "epoch": 4.1936,
      "grad_norm": 25.825027465820312,
      "learning_rate": 6.0213333333333335e-05,
      "loss": -110.6943,
      "step": 52420
    },
    {
      "epoch": 4.1944,
      "grad_norm": 61.50238800048828,
      "learning_rate": 6.018666666666667e-05,
      "loss": -111.6312,
      "step": 52430
    },
    {
      "epoch": 4.1952,
      "grad_norm": 70.97819519042969,
      "learning_rate": 6.016000000000001e-05,
      "loss": -111.8507,
      "step": 52440
    },
    {
      "epoch": 4.196,
      "grad_norm": 27.98453712463379,
      "learning_rate": 6.013333333333334e-05,
      "loss": -111.987,
      "step": 52450
    },
    {
      "epoch": 4.1968,
      "grad_norm": 84.70185089111328,
      "learning_rate": 6.0106666666666665e-05,
      "loss": -111.7395,
      "step": 52460
    },
    {
      "epoch": 4.1975999999999996,
      "grad_norm": 50.07976150512695,
      "learning_rate": 6.008e-05,
      "loss": -111.7125,
      "step": 52470
    },
    {
      "epoch": 4.1984,
      "grad_norm": 57.98855972290039,
      "learning_rate": 6.0053333333333337e-05,
      "loss": -111.6529,
      "step": 52480
    },
    {
      "epoch": 4.1992,
      "grad_norm": 48.874969482421875,
      "learning_rate": 6.0026666666666666e-05,
      "loss": -111.5027,
      "step": 52490
    },
    {
      "epoch": 4.2,
      "grad_norm": 173.4029541015625,
      "learning_rate": 6e-05,
      "loss": -111.9328,
      "step": 52500
    },
    {
      "epoch": 4.2008,
      "grad_norm": 38.735618591308594,
      "learning_rate": 5.997333333333334e-05,
      "loss": -111.7857,
      "step": 52510
    },
    {
      "epoch": 4.2016,
      "grad_norm": 31.063064575195312,
      "learning_rate": 5.994666666666667e-05,
      "loss": -112.4752,
      "step": 52520
    },
    {
      "epoch": 4.2024,
      "grad_norm": 41.91187286376953,
      "learning_rate": 5.9919999999999996e-05,
      "loss": -111.2989,
      "step": 52530
    },
    {
      "epoch": 4.2032,
      "grad_norm": 24.8863525390625,
      "learning_rate": 5.989333333333333e-05,
      "loss": -110.8038,
      "step": 52540
    },
    {
      "epoch": 4.204,
      "grad_norm": 116.68956756591797,
      "learning_rate": 5.9866666666666674e-05,
      "loss": -111.6413,
      "step": 52550
    },
    {
      "epoch": 4.2048,
      "grad_norm": 1693.764404296875,
      "learning_rate": 5.984000000000001e-05,
      "loss": -111.8733,
      "step": 52560
    },
    {
      "epoch": 4.2056000000000004,
      "grad_norm": 72.19754028320312,
      "learning_rate": 5.981333333333333e-05,
      "loss": -111.4712,
      "step": 52570
    },
    {
      "epoch": 4.2064,
      "grad_norm": 51.33727264404297,
      "learning_rate": 5.978666666666667e-05,
      "loss": -111.2811,
      "step": 52580
    },
    {
      "epoch": 4.2072,
      "grad_norm": 110.32994842529297,
      "learning_rate": 5.9760000000000004e-05,
      "loss": -111.3992,
      "step": 52590
    },
    {
      "epoch": 4.208,
      "grad_norm": 120.50259399414062,
      "learning_rate": 5.973333333333334e-05,
      "loss": -111.1054,
      "step": 52600
    },
    {
      "epoch": 4.2088,
      "grad_norm": 76.64575958251953,
      "learning_rate": 5.970666666666667e-05,
      "loss": -111.9801,
      "step": 52610
    },
    {
      "epoch": 4.2096,
      "grad_norm": 26.989683151245117,
      "learning_rate": 5.9680000000000005e-05,
      "loss": -111.1874,
      "step": 52620
    },
    {
      "epoch": 4.2104,
      "grad_norm": 90.74747467041016,
      "learning_rate": 5.965333333333334e-05,
      "loss": -111.4322,
      "step": 52630
    },
    {
      "epoch": 4.2112,
      "grad_norm": 44.61918258666992,
      "learning_rate": 5.962666666666666e-05,
      "loss": -111.2352,
      "step": 52640
    },
    {
      "epoch": 4.212,
      "grad_norm": 64.63825988769531,
      "learning_rate": 5.96e-05,
      "loss": -111.342,
      "step": 52650
    },
    {
      "epoch": 4.2128,
      "grad_norm": 150.27137756347656,
      "learning_rate": 5.9573333333333334e-05,
      "loss": -111.1863,
      "step": 52660
    },
    {
      "epoch": 4.2136,
      "grad_norm": 80.02747344970703,
      "learning_rate": 5.954666666666667e-05,
      "loss": -111.6673,
      "step": 52670
    },
    {
      "epoch": 4.2144,
      "grad_norm": 163.69146728515625,
      "learning_rate": 5.952e-05,
      "loss": -111.8023,
      "step": 52680
    },
    {
      "epoch": 4.2152,
      "grad_norm": 60.58356475830078,
      "learning_rate": 5.9493333333333335e-05,
      "loss": -112.6088,
      "step": 52690
    },
    {
      "epoch": 4.216,
      "grad_norm": 62.130775451660156,
      "learning_rate": 5.946666666666667e-05,
      "loss": -111.2192,
      "step": 52700
    },
    {
      "epoch": 4.2168,
      "grad_norm": 159.60816955566406,
      "learning_rate": 5.944000000000001e-05,
      "loss": -111.4057,
      "step": 52710
    },
    {
      "epoch": 4.2176,
      "grad_norm": 131.14749145507812,
      "learning_rate": 5.941333333333333e-05,
      "loss": -111.6093,
      "step": 52720
    },
    {
      "epoch": 4.2184,
      "grad_norm": 37.33169174194336,
      "learning_rate": 5.9386666666666665e-05,
      "loss": -111.34,
      "step": 52730
    },
    {
      "epoch": 4.2192,
      "grad_norm": 48.777530670166016,
      "learning_rate": 5.936000000000001e-05,
      "loss": -111.1815,
      "step": 52740
    },
    {
      "epoch": 4.22,
      "grad_norm": 167.95062255859375,
      "learning_rate": 5.9333333333333343e-05,
      "loss": -110.5831,
      "step": 52750
    },
    {
      "epoch": 4.2208,
      "grad_norm": 51.01275634765625,
      "learning_rate": 5.9306666666666666e-05,
      "loss": -111.4367,
      "step": 52760
    },
    {
      "epoch": 4.2216,
      "grad_norm": 30.69167709350586,
      "learning_rate": 5.928e-05,
      "loss": -111.5055,
      "step": 52770
    },
    {
      "epoch": 4.2224,
      "grad_norm": 136.97146606445312,
      "learning_rate": 5.925333333333334e-05,
      "loss": -112.1879,
      "step": 52780
    },
    {
      "epoch": 4.2232,
      "grad_norm": 55.359718322753906,
      "learning_rate": 5.922666666666667e-05,
      "loss": -111.0305,
      "step": 52790
    },
    {
      "epoch": 4.224,
      "grad_norm": 163.0884246826172,
      "learning_rate": 5.92e-05,
      "loss": -111.7915,
      "step": 52800
    },
    {
      "epoch": 4.2248,
      "grad_norm": 83.354736328125,
      "learning_rate": 5.917333333333334e-05,
      "loss": -112.3945,
      "step": 52810
    },
    {
      "epoch": 4.2256,
      "grad_norm": 61.59580612182617,
      "learning_rate": 5.9146666666666674e-05,
      "loss": -110.9625,
      "step": 52820
    },
    {
      "epoch": 4.2264,
      "grad_norm": 60.627464294433594,
      "learning_rate": 5.9119999999999996e-05,
      "loss": -112.2333,
      "step": 52830
    },
    {
      "epoch": 4.2272,
      "grad_norm": 37.705509185791016,
      "learning_rate": 5.909333333333333e-05,
      "loss": -111.409,
      "step": 52840
    },
    {
      "epoch": 4.228,
      "grad_norm": 94.72267150878906,
      "learning_rate": 5.906666666666667e-05,
      "loss": -111.2047,
      "step": 52850
    },
    {
      "epoch": 4.2288,
      "grad_norm": 48.083518981933594,
      "learning_rate": 5.9040000000000004e-05,
      "loss": -111.4396,
      "step": 52860
    },
    {
      "epoch": 4.2296,
      "grad_norm": 70.4721908569336,
      "learning_rate": 5.901333333333333e-05,
      "loss": -110.8127,
      "step": 52870
    },
    {
      "epoch": 4.2304,
      "grad_norm": 118.68054962158203,
      "learning_rate": 5.898666666666667e-05,
      "loss": -111.996,
      "step": 52880
    },
    {
      "epoch": 4.2312,
      "grad_norm": 67.85283660888672,
      "learning_rate": 5.8960000000000005e-05,
      "loss": -111.0536,
      "step": 52890
    },
    {
      "epoch": 4.232,
      "grad_norm": 28.697900772094727,
      "learning_rate": 5.893333333333334e-05,
      "loss": -111.5296,
      "step": 52900
    },
    {
      "epoch": 4.2328,
      "grad_norm": 81.23290252685547,
      "learning_rate": 5.890666666666666e-05,
      "loss": -112.5818,
      "step": 52910
    },
    {
      "epoch": 4.2336,
      "grad_norm": 68.15521240234375,
      "learning_rate": 5.888e-05,
      "loss": -112.1347,
      "step": 52920
    },
    {
      "epoch": 4.2344,
      "grad_norm": 46.481689453125,
      "learning_rate": 5.885333333333334e-05,
      "loss": -110.4637,
      "step": 52930
    },
    {
      "epoch": 4.2352,
      "grad_norm": 107.63695526123047,
      "learning_rate": 5.882666666666668e-05,
      "loss": -111.3058,
      "step": 52940
    },
    {
      "epoch": 4.236,
      "grad_norm": 46.716064453125,
      "learning_rate": 5.88e-05,
      "loss": -112.0088,
      "step": 52950
    },
    {
      "epoch": 4.2368,
      "grad_norm": 34.25149917602539,
      "learning_rate": 5.8773333333333335e-05,
      "loss": -110.8824,
      "step": 52960
    },
    {
      "epoch": 4.2376,
      "grad_norm": 35.24934768676758,
      "learning_rate": 5.874666666666667e-05,
      "loss": -111.5134,
      "step": 52970
    },
    {
      "epoch": 4.2384,
      "grad_norm": 33.72768020629883,
      "learning_rate": 5.872000000000001e-05,
      "loss": -110.9557,
      "step": 52980
    },
    {
      "epoch": 4.2392,
      "grad_norm": 107.28953552246094,
      "learning_rate": 5.8693333333333336e-05,
      "loss": -111.9726,
      "step": 52990
    },
    {
      "epoch": 4.24,
      "grad_norm": 39.120723724365234,
      "learning_rate": 5.866666666666667e-05,
      "loss": -111.5327,
      "step": 53000
    },
    {
      "epoch": 4.2408,
      "grad_norm": 26.73858070373535,
      "learning_rate": 5.864000000000001e-05,
      "loss": -111.728,
      "step": 53010
    },
    {
      "epoch": 4.2416,
      "grad_norm": 83.5119857788086,
      "learning_rate": 5.861333333333333e-05,
      "loss": -111.665,
      "step": 53020
    },
    {
      "epoch": 4.2424,
      "grad_norm": 62.15705490112305,
      "learning_rate": 5.8586666666666666e-05,
      "loss": -110.747,
      "step": 53030
    },
    {
      "epoch": 4.2432,
      "grad_norm": 334.8353271484375,
      "learning_rate": 5.856e-05,
      "loss": -111.9038,
      "step": 53040
    },
    {
      "epoch": 4.244,
      "grad_norm": 41.39583206176758,
      "learning_rate": 5.853333333333334e-05,
      "loss": -111.6338,
      "step": 53050
    },
    {
      "epoch": 4.2448,
      "grad_norm": 81.01189422607422,
      "learning_rate": 5.850666666666667e-05,
      "loss": -111.6512,
      "step": 53060
    },
    {
      "epoch": 4.2456,
      "grad_norm": 37.91325759887695,
      "learning_rate": 5.848e-05,
      "loss": -111.586,
      "step": 53070
    },
    {
      "epoch": 4.2464,
      "grad_norm": 86.30168151855469,
      "learning_rate": 5.845333333333334e-05,
      "loss": -111.9848,
      "step": 53080
    },
    {
      "epoch": 4.2472,
      "grad_norm": 81.79521179199219,
      "learning_rate": 5.8426666666666674e-05,
      "loss": -111.9672,
      "step": 53090
    },
    {
      "epoch": 4.248,
      "grad_norm": 36.88568878173828,
      "learning_rate": 5.8399999999999997e-05,
      "loss": -109.9073,
      "step": 53100
    },
    {
      "epoch": 4.2488,
      "grad_norm": 25.078880310058594,
      "learning_rate": 5.837333333333333e-05,
      "loss": -110.9595,
      "step": 53110
    },
    {
      "epoch": 4.2496,
      "grad_norm": 99.15153503417969,
      "learning_rate": 5.8346666666666675e-05,
      "loss": -111.2049,
      "step": 53120
    },
    {
      "epoch": 4.2504,
      "grad_norm": 48.58622741699219,
      "learning_rate": 5.832000000000001e-05,
      "loss": -112.9364,
      "step": 53130
    },
    {
      "epoch": 4.2512,
      "grad_norm": 61.50630187988281,
      "learning_rate": 5.829333333333333e-05,
      "loss": -110.9404,
      "step": 53140
    },
    {
      "epoch": 4.252,
      "grad_norm": 68.74000549316406,
      "learning_rate": 5.826666666666667e-05,
      "loss": -111.4798,
      "step": 53150
    },
    {
      "epoch": 4.2528,
      "grad_norm": 104.70601654052734,
      "learning_rate": 5.8240000000000005e-05,
      "loss": -110.5754,
      "step": 53160
    },
    {
      "epoch": 4.2536,
      "grad_norm": 42.88190841674805,
      "learning_rate": 5.8213333333333334e-05,
      "loss": -112.7562,
      "step": 53170
    },
    {
      "epoch": 4.2544,
      "grad_norm": 139.20289611816406,
      "learning_rate": 5.818666666666667e-05,
      "loss": -111.6634,
      "step": 53180
    },
    {
      "epoch": 4.2552,
      "grad_norm": 25.43898582458496,
      "learning_rate": 5.8160000000000006e-05,
      "loss": -112.0436,
      "step": 53190
    },
    {
      "epoch": 4.256,
      "grad_norm": 70.29228210449219,
      "learning_rate": 5.813333333333334e-05,
      "loss": -111.2751,
      "step": 53200
    },
    {
      "epoch": 4.2568,
      "grad_norm": 55.0043830871582,
      "learning_rate": 5.8106666666666664e-05,
      "loss": -111.2778,
      "step": 53210
    },
    {
      "epoch": 4.2576,
      "grad_norm": 52.18421173095703,
      "learning_rate": 5.808e-05,
      "loss": -112.1278,
      "step": 53220
    },
    {
      "epoch": 4.2584,
      "grad_norm": 28.324161529541016,
      "learning_rate": 5.8053333333333335e-05,
      "loss": -111.2734,
      "step": 53230
    },
    {
      "epoch": 4.2592,
      "grad_norm": 42.06000900268555,
      "learning_rate": 5.802666666666667e-05,
      "loss": -110.832,
      "step": 53240
    },
    {
      "epoch": 4.26,
      "grad_norm": 29.14497184753418,
      "learning_rate": 5.8e-05,
      "loss": -111.7165,
      "step": 53250
    },
    {
      "epoch": 4.2608,
      "grad_norm": 56.696632385253906,
      "learning_rate": 5.7973333333333336e-05,
      "loss": -110.7106,
      "step": 53260
    },
    {
      "epoch": 4.2616,
      "grad_norm": 83.04673767089844,
      "learning_rate": 5.794666666666667e-05,
      "loss": -111.1948,
      "step": 53270
    },
    {
      "epoch": 4.2624,
      "grad_norm": 60.39472198486328,
      "learning_rate": 5.792000000000001e-05,
      "loss": -111.8956,
      "step": 53280
    },
    {
      "epoch": 4.2632,
      "grad_norm": 127.46058654785156,
      "learning_rate": 5.789333333333333e-05,
      "loss": -111.819,
      "step": 53290
    },
    {
      "epoch": 4.264,
      "grad_norm": 38.09273147583008,
      "learning_rate": 5.7866666666666666e-05,
      "loss": -110.8196,
      "step": 53300
    },
    {
      "epoch": 4.2648,
      "grad_norm": 47.88875198364258,
      "learning_rate": 5.784000000000001e-05,
      "loss": -111.4426,
      "step": 53310
    },
    {
      "epoch": 4.2656,
      "grad_norm": 73.0191879272461,
      "learning_rate": 5.7813333333333344e-05,
      "loss": -111.0797,
      "step": 53320
    },
    {
      "epoch": 4.2664,
      "grad_norm": 156.81382751464844,
      "learning_rate": 5.778666666666667e-05,
      "loss": -112.4549,
      "step": 53330
    },
    {
      "epoch": 4.2672,
      "grad_norm": 45.407527923583984,
      "learning_rate": 5.776e-05,
      "loss": -111.7412,
      "step": 53340
    },
    {
      "epoch": 4.268,
      "grad_norm": 132.74398803710938,
      "learning_rate": 5.773333333333334e-05,
      "loss": -111.1826,
      "step": 53350
    },
    {
      "epoch": 4.2688,
      "grad_norm": 25.669878005981445,
      "learning_rate": 5.770666666666667e-05,
      "loss": -111.4122,
      "step": 53360
    },
    {
      "epoch": 4.2696,
      "grad_norm": 35.48835372924805,
      "learning_rate": 5.7680000000000003e-05,
      "loss": -111.5499,
      "step": 53370
    },
    {
      "epoch": 4.2704,
      "grad_norm": 20.77974510192871,
      "learning_rate": 5.765333333333334e-05,
      "loss": -111.3907,
      "step": 53380
    },
    {
      "epoch": 4.2712,
      "grad_norm": 79.5453109741211,
      "learning_rate": 5.7626666666666675e-05,
      "loss": -111.1894,
      "step": 53390
    },
    {
      "epoch": 4.272,
      "grad_norm": 64.71866607666016,
      "learning_rate": 5.76e-05,
      "loss": -111.68,
      "step": 53400
    },
    {
      "epoch": 4.2728,
      "grad_norm": 41.60161590576172,
      "learning_rate": 5.757333333333333e-05,
      "loss": -112.2772,
      "step": 53410
    },
    {
      "epoch": 4.2736,
      "grad_norm": 110.86314392089844,
      "learning_rate": 5.754666666666667e-05,
      "loss": -111.3394,
      "step": 53420
    },
    {
      "epoch": 4.2744,
      "grad_norm": 63.68994903564453,
      "learning_rate": 5.7520000000000005e-05,
      "loss": -112.2954,
      "step": 53430
    },
    {
      "epoch": 4.2752,
      "grad_norm": 32.67447280883789,
      "learning_rate": 5.7493333333333334e-05,
      "loss": -112.054,
      "step": 53440
    },
    {
      "epoch": 4.276,
      "grad_norm": 112.14112854003906,
      "learning_rate": 5.746666666666667e-05,
      "loss": -111.9354,
      "step": 53450
    },
    {
      "epoch": 4.2768,
      "grad_norm": 112.05072021484375,
      "learning_rate": 5.7440000000000006e-05,
      "loss": -111.6875,
      "step": 53460
    },
    {
      "epoch": 4.2776,
      "grad_norm": 61.61956024169922,
      "learning_rate": 5.741333333333334e-05,
      "loss": -111.6579,
      "step": 53470
    },
    {
      "epoch": 4.2783999999999995,
      "grad_norm": 30.27458381652832,
      "learning_rate": 5.7386666666666664e-05,
      "loss": -111.1576,
      "step": 53480
    },
    {
      "epoch": 4.2792,
      "grad_norm": 41.08439636230469,
      "learning_rate": 5.736e-05,
      "loss": -111.7572,
      "step": 53490
    },
    {
      "epoch": 4.28,
      "grad_norm": 84.65658569335938,
      "learning_rate": 5.7333333333333336e-05,
      "loss": -111.4508,
      "step": 53500
    },
    {
      "epoch": 4.2808,
      "grad_norm": 84.80217742919922,
      "learning_rate": 5.7306666666666665e-05,
      "loss": -111.6503,
      "step": 53510
    },
    {
      "epoch": 4.2816,
      "grad_norm": 43.23537826538086,
      "learning_rate": 5.728e-05,
      "loss": -111.3612,
      "step": 53520
    },
    {
      "epoch": 4.2824,
      "grad_norm": 126.01376342773438,
      "learning_rate": 5.7253333333333336e-05,
      "loss": -111.0659,
      "step": 53530
    },
    {
      "epoch": 4.2832,
      "grad_norm": 39.28890609741211,
      "learning_rate": 5.722666666666667e-05,
      "loss": -112.7128,
      "step": 53540
    },
    {
      "epoch": 4.284,
      "grad_norm": 31.146791458129883,
      "learning_rate": 5.72e-05,
      "loss": -112.3686,
      "step": 53550
    },
    {
      "epoch": 4.2848,
      "grad_norm": 108.22935485839844,
      "learning_rate": 5.717333333333334e-05,
      "loss": -111.8199,
      "step": 53560
    },
    {
      "epoch": 4.2856,
      "grad_norm": 56.39373016357422,
      "learning_rate": 5.714666666666667e-05,
      "loss": -110.9196,
      "step": 53570
    },
    {
      "epoch": 4.2864,
      "grad_norm": 31.965539932250977,
      "learning_rate": 5.712000000000001e-05,
      "loss": -111.6206,
      "step": 53580
    },
    {
      "epoch": 4.2872,
      "grad_norm": 46.764888763427734,
      "learning_rate": 5.709333333333333e-05,
      "loss": -111.3046,
      "step": 53590
    },
    {
      "epoch": 4.288,
      "grad_norm": 1634.9637451171875,
      "learning_rate": 5.706666666666667e-05,
      "loss": -110.9511,
      "step": 53600
    },
    {
      "epoch": 4.2888,
      "grad_norm": 67.09548950195312,
      "learning_rate": 5.704e-05,
      "loss": -111.005,
      "step": 53610
    },
    {
      "epoch": 4.2896,
      "grad_norm": 48.58846664428711,
      "learning_rate": 5.701333333333334e-05,
      "loss": -110.9741,
      "step": 53620
    },
    {
      "epoch": 4.2904,
      "grad_norm": 54.448665618896484,
      "learning_rate": 5.698666666666667e-05,
      "loss": -111.7816,
      "step": 53630
    },
    {
      "epoch": 4.2912,
      "grad_norm": 109.09918212890625,
      "learning_rate": 5.6960000000000004e-05,
      "loss": -112.1151,
      "step": 53640
    },
    {
      "epoch": 4.292,
      "grad_norm": 25.861167907714844,
      "learning_rate": 5.693333333333334e-05,
      "loss": -111.5037,
      "step": 53650
    },
    {
      "epoch": 4.2928,
      "grad_norm": 34.682960510253906,
      "learning_rate": 5.6906666666666675e-05,
      "loss": -110.8529,
      "step": 53660
    },
    {
      "epoch": 4.2936,
      "grad_norm": 24.00655746459961,
      "learning_rate": 5.688e-05,
      "loss": -111.3945,
      "step": 53670
    },
    {
      "epoch": 4.2943999999999996,
      "grad_norm": 159.0953826904297,
      "learning_rate": 5.685333333333333e-05,
      "loss": -111.0431,
      "step": 53680
    },
    {
      "epoch": 4.2952,
      "grad_norm": 97.6911849975586,
      "learning_rate": 5.682666666666667e-05,
      "loss": -111.2826,
      "step": 53690
    },
    {
      "epoch": 4.296,
      "grad_norm": 65.94957733154297,
      "learning_rate": 5.68e-05,
      "loss": -112.0816,
      "step": 53700
    },
    {
      "epoch": 4.2968,
      "grad_norm": 38.37629318237305,
      "learning_rate": 5.6773333333333334e-05,
      "loss": -111.625,
      "step": 53710
    },
    {
      "epoch": 4.2976,
      "grad_norm": 56.96522903442383,
      "learning_rate": 5.674666666666667e-05,
      "loss": -111.9979,
      "step": 53720
    },
    {
      "epoch": 4.2984,
      "grad_norm": 26.146596908569336,
      "learning_rate": 5.6720000000000006e-05,
      "loss": -110.7819,
      "step": 53730
    },
    {
      "epoch": 4.2992,
      "grad_norm": 26.017127990722656,
      "learning_rate": 5.6693333333333335e-05,
      "loss": -112.2106,
      "step": 53740
    },
    {
      "epoch": 4.3,
      "grad_norm": 45.465484619140625,
      "learning_rate": 5.666666666666667e-05,
      "loss": -112.2063,
      "step": 53750
    },
    {
      "epoch": 4.3008,
      "grad_norm": 121.75248718261719,
      "learning_rate": 5.6640000000000007e-05,
      "loss": -111.2348,
      "step": 53760
    },
    {
      "epoch": 4.3016,
      "grad_norm": 100.08541107177734,
      "learning_rate": 5.661333333333334e-05,
      "loss": -111.5952,
      "step": 53770
    },
    {
      "epoch": 4.3024000000000004,
      "grad_norm": 109.04251098632812,
      "learning_rate": 5.6586666666666665e-05,
      "loss": -112.4134,
      "step": 53780
    },
    {
      "epoch": 4.3032,
      "grad_norm": 34.47344207763672,
      "learning_rate": 5.656e-05,
      "loss": -112.2694,
      "step": 53790
    },
    {
      "epoch": 4.304,
      "grad_norm": 45.15715789794922,
      "learning_rate": 5.6533333333333336e-05,
      "loss": -111.7458,
      "step": 53800
    },
    {
      "epoch": 4.3048,
      "grad_norm": 44.749900817871094,
      "learning_rate": 5.650666666666667e-05,
      "loss": -112.0666,
      "step": 53810
    },
    {
      "epoch": 4.3056,
      "grad_norm": 27.275800704956055,
      "learning_rate": 5.648e-05,
      "loss": -111.862,
      "step": 53820
    },
    {
      "epoch": 4.3064,
      "grad_norm": 38.07128143310547,
      "learning_rate": 5.645333333333334e-05,
      "loss": -111.7007,
      "step": 53830
    },
    {
      "epoch": 4.3072,
      "grad_norm": 31.360637664794922,
      "learning_rate": 5.642666666666667e-05,
      "loss": -111.7588,
      "step": 53840
    },
    {
      "epoch": 4.308,
      "grad_norm": 38.18034744262695,
      "learning_rate": 5.6399999999999995e-05,
      "loss": -112.4504,
      "step": 53850
    },
    {
      "epoch": 4.3088,
      "grad_norm": 34.684410095214844,
      "learning_rate": 5.637333333333333e-05,
      "loss": -111.6324,
      "step": 53860
    },
    {
      "epoch": 4.3096,
      "grad_norm": 108.77526092529297,
      "learning_rate": 5.634666666666667e-05,
      "loss": -112.3436,
      "step": 53870
    },
    {
      "epoch": 4.3104,
      "grad_norm": 63.35719299316406,
      "learning_rate": 5.632e-05,
      "loss": -111.7575,
      "step": 53880
    },
    {
      "epoch": 4.3112,
      "grad_norm": 43.577980041503906,
      "learning_rate": 5.629333333333333e-05,
      "loss": -111.7633,
      "step": 53890
    },
    {
      "epoch": 4.312,
      "grad_norm": 40.75785446166992,
      "learning_rate": 5.626666666666667e-05,
      "loss": -112.4293,
      "step": 53900
    },
    {
      "epoch": 4.3128,
      "grad_norm": 56.37801742553711,
      "learning_rate": 5.6240000000000004e-05,
      "loss": -111.754,
      "step": 53910
    },
    {
      "epoch": 4.3136,
      "grad_norm": 26.79793357849121,
      "learning_rate": 5.621333333333334e-05,
      "loss": -111.5571,
      "step": 53920
    },
    {
      "epoch": 4.3144,
      "grad_norm": 46.09008026123047,
      "learning_rate": 5.618666666666667e-05,
      "loss": -111.8905,
      "step": 53930
    },
    {
      "epoch": 4.3152,
      "grad_norm": 52.69961929321289,
      "learning_rate": 5.6160000000000004e-05,
      "loss": -111.8029,
      "step": 53940
    },
    {
      "epoch": 4.316,
      "grad_norm": 46.948402404785156,
      "learning_rate": 5.613333333333334e-05,
      "loss": -111.3057,
      "step": 53950
    },
    {
      "epoch": 4.3168,
      "grad_norm": 85.7094497680664,
      "learning_rate": 5.6106666666666676e-05,
      "loss": -112.3174,
      "step": 53960
    },
    {
      "epoch": 4.3176,
      "grad_norm": 36.03498840332031,
      "learning_rate": 5.608e-05,
      "loss": -112.047,
      "step": 53970
    },
    {
      "epoch": 4.3184000000000005,
      "grad_norm": 112.9466323852539,
      "learning_rate": 5.6053333333333334e-05,
      "loss": -111.5014,
      "step": 53980
    },
    {
      "epoch": 4.3192,
      "grad_norm": 39.32609176635742,
      "learning_rate": 5.602666666666667e-05,
      "loss": -111.0569,
      "step": 53990
    },
    {
      "epoch": 4.32,
      "grad_norm": 26.358564376831055,
      "learning_rate": 5.6000000000000006e-05,
      "loss": -111.6119,
      "step": 54000
    },
    {
      "epoch": 4.3208,
      "grad_norm": 30.80340576171875,
      "learning_rate": 5.5973333333333335e-05,
      "loss": -110.4811,
      "step": 54010
    },
    {
      "epoch": 4.3216,
      "grad_norm": 21.98554801940918,
      "learning_rate": 5.594666666666667e-05,
      "loss": -111.1648,
      "step": 54020
    },
    {
      "epoch": 4.3224,
      "grad_norm": 27.971471786499023,
      "learning_rate": 5.592000000000001e-05,
      "loss": -112.006,
      "step": 54030
    },
    {
      "epoch": 4.3232,
      "grad_norm": 59.72504806518555,
      "learning_rate": 5.589333333333333e-05,
      "loss": -112.0733,
      "step": 54040
    },
    {
      "epoch": 4.324,
      "grad_norm": 25.90509605407715,
      "learning_rate": 5.5866666666666665e-05,
      "loss": -111.6623,
      "step": 54050
    },
    {
      "epoch": 4.3248,
      "grad_norm": 75.92313385009766,
      "learning_rate": 5.584e-05,
      "loss": -111.8669,
      "step": 54060
    },
    {
      "epoch": 4.3256,
      "grad_norm": 30.55406951904297,
      "learning_rate": 5.5813333333333337e-05,
      "loss": -110.639,
      "step": 54070
    },
    {
      "epoch": 4.3264,
      "grad_norm": 72.46586608886719,
      "learning_rate": 5.5786666666666666e-05,
      "loss": -111.6835,
      "step": 54080
    },
    {
      "epoch": 4.3272,
      "grad_norm": 49.110206604003906,
      "learning_rate": 5.576e-05,
      "loss": -111.9575,
      "step": 54090
    },
    {
      "epoch": 4.328,
      "grad_norm": 47.505271911621094,
      "learning_rate": 5.573333333333334e-05,
      "loss": -111.5094,
      "step": 54100
    },
    {
      "epoch": 4.3288,
      "grad_norm": 132.8782196044922,
      "learning_rate": 5.570666666666667e-05,
      "loss": -112.1732,
      "step": 54110
    },
    {
      "epoch": 4.3296,
      "grad_norm": 65.15319061279297,
      "learning_rate": 5.5679999999999995e-05,
      "loss": -111.395,
      "step": 54120
    },
    {
      "epoch": 4.3304,
      "grad_norm": 65.70121002197266,
      "learning_rate": 5.565333333333334e-05,
      "loss": -112.6519,
      "step": 54130
    },
    {
      "epoch": 4.3312,
      "grad_norm": 45.89225387573242,
      "learning_rate": 5.5626666666666674e-05,
      "loss": -110.9033,
      "step": 54140
    },
    {
      "epoch": 4.332,
      "grad_norm": 50.06911087036133,
      "learning_rate": 5.560000000000001e-05,
      "loss": -111.5655,
      "step": 54150
    },
    {
      "epoch": 4.3328,
      "grad_norm": 30.978328704833984,
      "learning_rate": 5.557333333333333e-05,
      "loss": -112.0873,
      "step": 54160
    },
    {
      "epoch": 4.3336,
      "grad_norm": 44.972686767578125,
      "learning_rate": 5.554666666666667e-05,
      "loss": -112.186,
      "step": 54170
    },
    {
      "epoch": 4.3344,
      "grad_norm": 52.82506561279297,
      "learning_rate": 5.5520000000000004e-05,
      "loss": -111.4464,
      "step": 54180
    },
    {
      "epoch": 4.3352,
      "grad_norm": 49.948402404785156,
      "learning_rate": 5.549333333333333e-05,
      "loss": -111.5092,
      "step": 54190
    },
    {
      "epoch": 4.336,
      "grad_norm": 79.03507995605469,
      "learning_rate": 5.546666666666667e-05,
      "loss": -111.1807,
      "step": 54200
    },
    {
      "epoch": 4.3368,
      "grad_norm": 71.20079803466797,
      "learning_rate": 5.5440000000000005e-05,
      "loss": -112.0728,
      "step": 54210
    },
    {
      "epoch": 4.3376,
      "grad_norm": 31.878890991210938,
      "learning_rate": 5.541333333333334e-05,
      "loss": -111.7246,
      "step": 54220
    },
    {
      "epoch": 4.3384,
      "grad_norm": 28.65709114074707,
      "learning_rate": 5.538666666666666e-05,
      "loss": -111.7358,
      "step": 54230
    },
    {
      "epoch": 4.3392,
      "grad_norm": 87.61909484863281,
      "learning_rate": 5.536e-05,
      "loss": -111.6615,
      "step": 54240
    },
    {
      "epoch": 4.34,
      "grad_norm": 30.672935485839844,
      "learning_rate": 5.5333333333333334e-05,
      "loss": -111.9719,
      "step": 54250
    },
    {
      "epoch": 4.3408,
      "grad_norm": 44.62559509277344,
      "learning_rate": 5.530666666666667e-05,
      "loss": -111.0018,
      "step": 54260
    },
    {
      "epoch": 4.3416,
      "grad_norm": 190.66159057617188,
      "learning_rate": 5.528e-05,
      "loss": -111.6632,
      "step": 54270
    },
    {
      "epoch": 4.3424,
      "grad_norm": 91.79559326171875,
      "learning_rate": 5.5253333333333335e-05,
      "loss": -111.5936,
      "step": 54280
    },
    {
      "epoch": 4.3432,
      "grad_norm": 37.2137451171875,
      "learning_rate": 5.522666666666667e-05,
      "loss": -112.2564,
      "step": 54290
    },
    {
      "epoch": 4.344,
      "grad_norm": 32.61193084716797,
      "learning_rate": 5.520000000000001e-05,
      "loss": -111.1282,
      "step": 54300
    },
    {
      "epoch": 4.3448,
      "grad_norm": 78.65656280517578,
      "learning_rate": 5.517333333333333e-05,
      "loss": -112.0905,
      "step": 54310
    },
    {
      "epoch": 4.3456,
      "grad_norm": 30.955596923828125,
      "learning_rate": 5.514666666666667e-05,
      "loss": -111.5117,
      "step": 54320
    },
    {
      "epoch": 4.3464,
      "grad_norm": 68.44261169433594,
      "learning_rate": 5.512000000000001e-05,
      "loss": -111.5511,
      "step": 54330
    },
    {
      "epoch": 4.3472,
      "grad_norm": 42.35783004760742,
      "learning_rate": 5.5093333333333343e-05,
      "loss": -111.2646,
      "step": 54340
    },
    {
      "epoch": 4.348,
      "grad_norm": 59.97630310058594,
      "learning_rate": 5.5066666666666666e-05,
      "loss": -110.9315,
      "step": 54350
    },
    {
      "epoch": 4.3488,
      "grad_norm": 71.78160095214844,
      "learning_rate": 5.504e-05,
      "loss": -111.7989,
      "step": 54360
    },
    {
      "epoch": 4.3496,
      "grad_norm": 35.05402374267578,
      "learning_rate": 5.501333333333334e-05,
      "loss": -111.6226,
      "step": 54370
    },
    {
      "epoch": 4.3504,
      "grad_norm": 32.09380340576172,
      "learning_rate": 5.4986666666666666e-05,
      "loss": -112.1994,
      "step": 54380
    },
    {
      "epoch": 4.3512,
      "grad_norm": 103.04430389404297,
      "learning_rate": 5.496e-05,
      "loss": -111.8315,
      "step": 54390
    },
    {
      "epoch": 4.352,
      "grad_norm": 76.60875701904297,
      "learning_rate": 5.493333333333334e-05,
      "loss": -111.6662,
      "step": 54400
    },
    {
      "epoch": 4.3528,
      "grad_norm": 44.27521514892578,
      "learning_rate": 5.4906666666666674e-05,
      "loss": -111.4765,
      "step": 54410
    },
    {
      "epoch": 4.3536,
      "grad_norm": 25.213468551635742,
      "learning_rate": 5.4879999999999996e-05,
      "loss": -110.3021,
      "step": 54420
    },
    {
      "epoch": 4.3544,
      "grad_norm": 72.41841125488281,
      "learning_rate": 5.485333333333333e-05,
      "loss": -110.8946,
      "step": 54430
    },
    {
      "epoch": 4.3552,
      "grad_norm": 71.31026458740234,
      "learning_rate": 5.482666666666667e-05,
      "loss": -111.6605,
      "step": 54440
    },
    {
      "epoch": 4.356,
      "grad_norm": 25.7510929107666,
      "learning_rate": 5.4800000000000004e-05,
      "loss": -110.9987,
      "step": 54450
    },
    {
      "epoch": 4.3568,
      "grad_norm": 167.06968688964844,
      "learning_rate": 5.477333333333333e-05,
      "loss": -111.8562,
      "step": 54460
    },
    {
      "epoch": 4.3576,
      "grad_norm": 28.123239517211914,
      "learning_rate": 5.474666666666667e-05,
      "loss": -110.7075,
      "step": 54470
    },
    {
      "epoch": 4.3584,
      "grad_norm": 1548.08984375,
      "learning_rate": 5.4720000000000005e-05,
      "loss": -111.4595,
      "step": 54480
    },
    {
      "epoch": 4.3592,
      "grad_norm": 56.33100128173828,
      "learning_rate": 5.469333333333334e-05,
      "loss": -111.986,
      "step": 54490
    },
    {
      "epoch": 4.36,
      "grad_norm": 91.56403350830078,
      "learning_rate": 5.466666666666666e-05,
      "loss": -111.7119,
      "step": 54500
    },
    {
      "epoch": 4.3608,
      "grad_norm": 80.15851593017578,
      "learning_rate": 5.4640000000000005e-05,
      "loss": -111.8828,
      "step": 54510
    },
    {
      "epoch": 4.3616,
      "grad_norm": 89.39823913574219,
      "learning_rate": 5.461333333333334e-05,
      "loss": -112.2655,
      "step": 54520
    },
    {
      "epoch": 4.3624,
      "grad_norm": 52.30835723876953,
      "learning_rate": 5.4586666666666664e-05,
      "loss": -112.0325,
      "step": 54530
    },
    {
      "epoch": 4.3632,
      "grad_norm": 48.05194854736328,
      "learning_rate": 5.456e-05,
      "loss": -111.5756,
      "step": 54540
    },
    {
      "epoch": 4.364,
      "grad_norm": 99.24063873291016,
      "learning_rate": 5.4533333333333335e-05,
      "loss": -111.1945,
      "step": 54550
    },
    {
      "epoch": 4.3648,
      "grad_norm": 88.14627075195312,
      "learning_rate": 5.450666666666667e-05,
      "loss": -112.3701,
      "step": 54560
    },
    {
      "epoch": 4.3656,
      "grad_norm": 86.85211944580078,
      "learning_rate": 5.448e-05,
      "loss": -111.2263,
      "step": 54570
    },
    {
      "epoch": 4.3664,
      "grad_norm": 55.833492279052734,
      "learning_rate": 5.4453333333333336e-05,
      "loss": -111.8072,
      "step": 54580
    },
    {
      "epoch": 4.3672,
      "grad_norm": 103.3121566772461,
      "learning_rate": 5.442666666666667e-05,
      "loss": -112.8343,
      "step": 54590
    },
    {
      "epoch": 4.368,
      "grad_norm": 89.55643463134766,
      "learning_rate": 5.440000000000001e-05,
      "loss": -112.0414,
      "step": 54600
    },
    {
      "epoch": 4.3688,
      "grad_norm": 229.2196807861328,
      "learning_rate": 5.437333333333333e-05,
      "loss": -111.2927,
      "step": 54610
    },
    {
      "epoch": 4.3696,
      "grad_norm": 42.08049392700195,
      "learning_rate": 5.4346666666666666e-05,
      "loss": -111.8049,
      "step": 54620
    },
    {
      "epoch": 4.3704,
      "grad_norm": 42.18451690673828,
      "learning_rate": 5.432e-05,
      "loss": -111.0659,
      "step": 54630
    },
    {
      "epoch": 4.3712,
      "grad_norm": 40.100318908691406,
      "learning_rate": 5.429333333333334e-05,
      "loss": -111.5338,
      "step": 54640
    },
    {
      "epoch": 4.372,
      "grad_norm": 27.05624008178711,
      "learning_rate": 5.4266666666666667e-05,
      "loss": -112.0202,
      "step": 54650
    },
    {
      "epoch": 4.3728,
      "grad_norm": 49.677371978759766,
      "learning_rate": 5.424e-05,
      "loss": -112.0474,
      "step": 54660
    },
    {
      "epoch": 4.3736,
      "grad_norm": 110.9306411743164,
      "learning_rate": 5.421333333333334e-05,
      "loss": -112.0777,
      "step": 54670
    },
    {
      "epoch": 4.3744,
      "grad_norm": 30.47909927368164,
      "learning_rate": 5.4186666666666674e-05,
      "loss": -111.3614,
      "step": 54680
    },
    {
      "epoch": 4.3751999999999995,
      "grad_norm": 29.2803955078125,
      "learning_rate": 5.4159999999999996e-05,
      "loss": -110.6453,
      "step": 54690
    },
    {
      "epoch": 4.376,
      "grad_norm": 26.732345581054688,
      "learning_rate": 5.413333333333334e-05,
      "loss": -112.2198,
      "step": 54700
    },
    {
      "epoch": 4.3768,
      "grad_norm": 54.246585845947266,
      "learning_rate": 5.4106666666666675e-05,
      "loss": -111.5787,
      "step": 54710
    },
    {
      "epoch": 4.3776,
      "grad_norm": 55.688472747802734,
      "learning_rate": 5.408e-05,
      "loss": -110.9589,
      "step": 54720
    },
    {
      "epoch": 4.3784,
      "grad_norm": 42.024024963378906,
      "learning_rate": 5.405333333333333e-05,
      "loss": -111.4014,
      "step": 54730
    },
    {
      "epoch": 4.3792,
      "grad_norm": 102.81475830078125,
      "learning_rate": 5.402666666666667e-05,
      "loss": -111.9706,
      "step": 54740
    },
    {
      "epoch": 4.38,
      "grad_norm": 38.146121978759766,
      "learning_rate": 5.4000000000000005e-05,
      "loss": -111.5802,
      "step": 54750
    },
    {
      "epoch": 4.3808,
      "grad_norm": 68.3277587890625,
      "learning_rate": 5.3973333333333334e-05,
      "loss": -111.2086,
      "step": 54760
    },
    {
      "epoch": 4.3816,
      "grad_norm": 35.72938537597656,
      "learning_rate": 5.394666666666667e-05,
      "loss": -110.9309,
      "step": 54770
    },
    {
      "epoch": 4.3824,
      "grad_norm": 23.781400680541992,
      "learning_rate": 5.3920000000000006e-05,
      "loss": -112.1408,
      "step": 54780
    },
    {
      "epoch": 4.3832,
      "grad_norm": 48.683956146240234,
      "learning_rate": 5.389333333333334e-05,
      "loss": -111.4805,
      "step": 54790
    },
    {
      "epoch": 4.384,
      "grad_norm": 78.5244140625,
      "learning_rate": 5.3866666666666664e-05,
      "loss": -111.3847,
      "step": 54800
    },
    {
      "epoch": 4.3848,
      "grad_norm": 87.49848175048828,
      "learning_rate": 5.384e-05,
      "loss": -112.0916,
      "step": 54810
    },
    {
      "epoch": 4.3856,
      "grad_norm": 40.38691711425781,
      "learning_rate": 5.3813333333333335e-05,
      "loss": -111.8628,
      "step": 54820
    },
    {
      "epoch": 4.3864,
      "grad_norm": 29.384580612182617,
      "learning_rate": 5.378666666666667e-05,
      "loss": -111.1845,
      "step": 54830
    },
    {
      "epoch": 4.3872,
      "grad_norm": 77.12378692626953,
      "learning_rate": 5.376e-05,
      "loss": -111.2848,
      "step": 54840
    },
    {
      "epoch": 4.388,
      "grad_norm": 81.38379669189453,
      "learning_rate": 5.3733333333333336e-05,
      "loss": -110.5528,
      "step": 54850
    },
    {
      "epoch": 4.3888,
      "grad_norm": 65.62496185302734,
      "learning_rate": 5.370666666666667e-05,
      "loss": -112.1457,
      "step": 54860
    },
    {
      "epoch": 4.3896,
      "grad_norm": 67.82464599609375,
      "learning_rate": 5.368000000000001e-05,
      "loss": -111.1955,
      "step": 54870
    },
    {
      "epoch": 4.3904,
      "grad_norm": 27.117340087890625,
      "learning_rate": 5.365333333333333e-05,
      "loss": -110.9745,
      "step": 54880
    },
    {
      "epoch": 4.3911999999999995,
      "grad_norm": 104.84664154052734,
      "learning_rate": 5.362666666666667e-05,
      "loss": -111.5792,
      "step": 54890
    },
    {
      "epoch": 4.392,
      "grad_norm": 61.185211181640625,
      "learning_rate": 5.360000000000001e-05,
      "loss": -111.9087,
      "step": 54900
    },
    {
      "epoch": 4.3928,
      "grad_norm": 51.3779411315918,
      "learning_rate": 5.357333333333333e-05,
      "loss": -111.333,
      "step": 54910
    },
    {
      "epoch": 4.3936,
      "grad_norm": 123.89190673828125,
      "learning_rate": 5.354666666666667e-05,
      "loss": -112.0735,
      "step": 54920
    },
    {
      "epoch": 4.3944,
      "grad_norm": 92.40492248535156,
      "learning_rate": 5.352e-05,
      "loss": -111.291,
      "step": 54930
    },
    {
      "epoch": 4.3952,
      "grad_norm": 66.43597412109375,
      "learning_rate": 5.349333333333334e-05,
      "loss": -111.9033,
      "step": 54940
    },
    {
      "epoch": 4.396,
      "grad_norm": 38.19295120239258,
      "learning_rate": 5.346666666666667e-05,
      "loss": -110.9737,
      "step": 54950
    },
    {
      "epoch": 4.3968,
      "grad_norm": 96.14839172363281,
      "learning_rate": 5.344e-05,
      "loss": -111.9035,
      "step": 54960
    },
    {
      "epoch": 4.3976,
      "grad_norm": 89.50191497802734,
      "learning_rate": 5.341333333333334e-05,
      "loss": -111.8258,
      "step": 54970
    },
    {
      "epoch": 4.3984,
      "grad_norm": 257.92108154296875,
      "learning_rate": 5.3386666666666675e-05,
      "loss": -111.2944,
      "step": 54980
    },
    {
      "epoch": 4.3992,
      "grad_norm": 39.533817291259766,
      "learning_rate": 5.336e-05,
      "loss": -111.8498,
      "step": 54990
    },
    {
      "epoch": 4.4,
      "grad_norm": 79.2096939086914,
      "learning_rate": 5.333333333333333e-05,
      "loss": -110.8085,
      "step": 55000
    },
    {
      "epoch": 4.4008,
      "grad_norm": 26.637128829956055,
      "learning_rate": 5.330666666666667e-05,
      "loss": -112.0756,
      "step": 55010
    },
    {
      "epoch": 4.4016,
      "grad_norm": 68.65856170654297,
      "learning_rate": 5.3280000000000005e-05,
      "loss": -111.5693,
      "step": 55020
    },
    {
      "epoch": 4.4024,
      "grad_norm": 68.99449157714844,
      "learning_rate": 5.3253333333333334e-05,
      "loss": -111.3651,
      "step": 55030
    },
    {
      "epoch": 4.4032,
      "grad_norm": 92.65470886230469,
      "learning_rate": 5.322666666666667e-05,
      "loss": -112.3773,
      "step": 55040
    },
    {
      "epoch": 4.404,
      "grad_norm": 69.73283386230469,
      "learning_rate": 5.3200000000000006e-05,
      "loss": -111.5843,
      "step": 55050
    },
    {
      "epoch": 4.4048,
      "grad_norm": 100.41290283203125,
      "learning_rate": 5.317333333333333e-05,
      "loss": -111.5318,
      "step": 55060
    },
    {
      "epoch": 4.4056,
      "grad_norm": 71.87661743164062,
      "learning_rate": 5.3146666666666664e-05,
      "loss": -110.8815,
      "step": 55070
    },
    {
      "epoch": 4.4064,
      "grad_norm": 35.30270004272461,
      "learning_rate": 5.3120000000000006e-05,
      "loss": -111.5311,
      "step": 55080
    },
    {
      "epoch": 4.4072,
      "grad_norm": 99.53279876708984,
      "learning_rate": 5.309333333333334e-05,
      "loss": -110.8326,
      "step": 55090
    },
    {
      "epoch": 4.408,
      "grad_norm": 87.66219329833984,
      "learning_rate": 5.3066666666666665e-05,
      "loss": -111.5765,
      "step": 55100
    },
    {
      "epoch": 4.4088,
      "grad_norm": 169.08251953125,
      "learning_rate": 5.304e-05,
      "loss": -111.5557,
      "step": 55110
    },
    {
      "epoch": 4.4096,
      "grad_norm": 30.234619140625,
      "learning_rate": 5.3013333333333336e-05,
      "loss": -112.0264,
      "step": 55120
    },
    {
      "epoch": 4.4104,
      "grad_norm": 229.03948974609375,
      "learning_rate": 5.298666666666667e-05,
      "loss": -111.6259,
      "step": 55130
    },
    {
      "epoch": 4.4112,
      "grad_norm": 78.78617858886719,
      "learning_rate": 5.296e-05,
      "loss": -111.8986,
      "step": 55140
    },
    {
      "epoch": 4.412,
      "grad_norm": 23.8828125,
      "learning_rate": 5.293333333333334e-05,
      "loss": -111.3441,
      "step": 55150
    },
    {
      "epoch": 4.4128,
      "grad_norm": 53.806339263916016,
      "learning_rate": 5.290666666666667e-05,
      "loss": -111.4331,
      "step": 55160
    },
    {
      "epoch": 4.4136,
      "grad_norm": 66.82747650146484,
      "learning_rate": 5.288000000000001e-05,
      "loss": -111.8443,
      "step": 55170
    },
    {
      "epoch": 4.4144,
      "grad_norm": 55.160888671875,
      "learning_rate": 5.285333333333333e-05,
      "loss": -111.5658,
      "step": 55180
    },
    {
      "epoch": 4.4152000000000005,
      "grad_norm": 29.515230178833008,
      "learning_rate": 5.282666666666667e-05,
      "loss": -111.4531,
      "step": 55190
    },
    {
      "epoch": 4.416,
      "grad_norm": 88.0387954711914,
      "learning_rate": 5.28e-05,
      "loss": -111.1729,
      "step": 55200
    },
    {
      "epoch": 4.4168,
      "grad_norm": 84.39041900634766,
      "learning_rate": 5.277333333333334e-05,
      "loss": -110.9418,
      "step": 55210
    },
    {
      "epoch": 4.4176,
      "grad_norm": 57.88434600830078,
      "learning_rate": 5.274666666666667e-05,
      "loss": -111.7803,
      "step": 55220
    },
    {
      "epoch": 4.4184,
      "grad_norm": 42.09661102294922,
      "learning_rate": 5.2720000000000003e-05,
      "loss": -111.4832,
      "step": 55230
    },
    {
      "epoch": 4.4192,
      "grad_norm": 19.870269775390625,
      "learning_rate": 5.269333333333334e-05,
      "loss": -110.6521,
      "step": 55240
    },
    {
      "epoch": 4.42,
      "grad_norm": 185.13327026367188,
      "learning_rate": 5.266666666666666e-05,
      "loss": -112.6069,
      "step": 55250
    },
    {
      "epoch": 4.4208,
      "grad_norm": 46.9462776184082,
      "learning_rate": 5.264e-05,
      "loss": -111.2694,
      "step": 55260
    },
    {
      "epoch": 4.4216,
      "grad_norm": 37.13219451904297,
      "learning_rate": 5.261333333333334e-05,
      "loss": -111.5933,
      "step": 55270
    },
    {
      "epoch": 4.4224,
      "grad_norm": 126.78253173828125,
      "learning_rate": 5.2586666666666676e-05,
      "loss": -111.3093,
      "step": 55280
    },
    {
      "epoch": 4.4232,
      "grad_norm": 30.74775505065918,
      "learning_rate": 5.256e-05,
      "loss": -110.6851,
      "step": 55290
    },
    {
      "epoch": 4.424,
      "grad_norm": 24.04669761657715,
      "learning_rate": 5.2533333333333334e-05,
      "loss": -111.7169,
      "step": 55300
    },
    {
      "epoch": 4.4248,
      "grad_norm": 39.24254608154297,
      "learning_rate": 5.250666666666667e-05,
      "loss": -111.406,
      "step": 55310
    },
    {
      "epoch": 4.4256,
      "grad_norm": 29.39780616760254,
      "learning_rate": 5.2480000000000006e-05,
      "loss": -111.3895,
      "step": 55320
    },
    {
      "epoch": 4.4264,
      "grad_norm": 36.77601623535156,
      "learning_rate": 5.2453333333333335e-05,
      "loss": -112.2167,
      "step": 55330
    },
    {
      "epoch": 4.4272,
      "grad_norm": 111.23523712158203,
      "learning_rate": 5.242666666666667e-05,
      "loss": -112.1553,
      "step": 55340
    },
    {
      "epoch": 4.428,
      "grad_norm": 55.06722640991211,
      "learning_rate": 5.2400000000000007e-05,
      "loss": -112.2844,
      "step": 55350
    },
    {
      "epoch": 4.4288,
      "grad_norm": 73.76333618164062,
      "learning_rate": 5.237333333333334e-05,
      "loss": -111.2997,
      "step": 55360
    },
    {
      "epoch": 4.4296,
      "grad_norm": 56.26449203491211,
      "learning_rate": 5.2346666666666665e-05,
      "loss": -111.9131,
      "step": 55370
    },
    {
      "epoch": 4.4304,
      "grad_norm": 70.09873962402344,
      "learning_rate": 5.232e-05,
      "loss": -111.9298,
      "step": 55380
    },
    {
      "epoch": 4.4312000000000005,
      "grad_norm": 49.237213134765625,
      "learning_rate": 5.2293333333333336e-05,
      "loss": -111.9583,
      "step": 55390
    },
    {
      "epoch": 4.432,
      "grad_norm": 89.2159194946289,
      "learning_rate": 5.2266666666666665e-05,
      "loss": -111.5309,
      "step": 55400
    },
    {
      "epoch": 4.4328,
      "grad_norm": 115.78898620605469,
      "learning_rate": 5.224e-05,
      "loss": -111.0988,
      "step": 55410
    },
    {
      "epoch": 4.4336,
      "grad_norm": 48.93160629272461,
      "learning_rate": 5.221333333333334e-05,
      "loss": -111.9104,
      "step": 55420
    },
    {
      "epoch": 4.4344,
      "grad_norm": 36.681392669677734,
      "learning_rate": 5.218666666666667e-05,
      "loss": -111.0491,
      "step": 55430
    },
    {
      "epoch": 4.4352,
      "grad_norm": 38.01803207397461,
      "learning_rate": 5.2159999999999995e-05,
      "loss": -110.4328,
      "step": 55440
    },
    {
      "epoch": 4.436,
      "grad_norm": 130.18638610839844,
      "learning_rate": 5.213333333333333e-05,
      "loss": -111.0449,
      "step": 55450
    },
    {
      "epoch": 4.4368,
      "grad_norm": 69.0191650390625,
      "learning_rate": 5.210666666666667e-05,
      "loss": -111.9644,
      "step": 55460
    },
    {
      "epoch": 4.4376,
      "grad_norm": 65.90654754638672,
      "learning_rate": 5.208000000000001e-05,
      "loss": -112.029,
      "step": 55470
    },
    {
      "epoch": 4.4384,
      "grad_norm": 82.10212707519531,
      "learning_rate": 5.205333333333333e-05,
      "loss": -112.2294,
      "step": 55480
    },
    {
      "epoch": 4.4392,
      "grad_norm": 79.1573486328125,
      "learning_rate": 5.202666666666667e-05,
      "loss": -111.2611,
      "step": 55490
    },
    {
      "epoch": 4.44,
      "grad_norm": 29.548601150512695,
      "learning_rate": 5.2000000000000004e-05,
      "loss": -111.7002,
      "step": 55500
    },
    {
      "epoch": 4.4408,
      "grad_norm": 53.08961486816406,
      "learning_rate": 5.197333333333334e-05,
      "loss": -110.6564,
      "step": 55510
    },
    {
      "epoch": 4.4416,
      "grad_norm": 62.74950408935547,
      "learning_rate": 5.194666666666667e-05,
      "loss": -111.4034,
      "step": 55520
    },
    {
      "epoch": 4.4424,
      "grad_norm": 71.89614868164062,
      "learning_rate": 5.1920000000000004e-05,
      "loss": -111.9442,
      "step": 55530
    },
    {
      "epoch": 4.4432,
      "grad_norm": 41.86873245239258,
      "learning_rate": 5.189333333333334e-05,
      "loss": -111.817,
      "step": 55540
    },
    {
      "epoch": 4.444,
      "grad_norm": 86.44774627685547,
      "learning_rate": 5.1866666666666676e-05,
      "loss": -112.9554,
      "step": 55550
    },
    {
      "epoch": 4.4448,
      "grad_norm": 69.98843383789062,
      "learning_rate": 5.184e-05,
      "loss": -110.8039,
      "step": 55560
    },
    {
      "epoch": 4.4456,
      "grad_norm": 183.5448455810547,
      "learning_rate": 5.1813333333333334e-05,
      "loss": -111.1777,
      "step": 55570
    },
    {
      "epoch": 4.4464,
      "grad_norm": 58.475582122802734,
      "learning_rate": 5.178666666666667e-05,
      "loss": -112.1404,
      "step": 55580
    },
    {
      "epoch": 4.4472,
      "grad_norm": 37.65229034423828,
      "learning_rate": 5.176e-05,
      "loss": -111.8102,
      "step": 55590
    },
    {
      "epoch": 4.448,
      "grad_norm": 55.68824005126953,
      "learning_rate": 5.1733333333333335e-05,
      "loss": -112.3094,
      "step": 55600
    },
    {
      "epoch": 4.4488,
      "grad_norm": 60.18467330932617,
      "learning_rate": 5.170666666666667e-05,
      "loss": -112.227,
      "step": 55610
    },
    {
      "epoch": 4.4496,
      "grad_norm": 125.04730987548828,
      "learning_rate": 5.168000000000001e-05,
      "loss": -111.8203,
      "step": 55620
    },
    {
      "epoch": 4.4504,
      "grad_norm": 63.8868408203125,
      "learning_rate": 5.165333333333333e-05,
      "loss": -111.5667,
      "step": 55630
    },
    {
      "epoch": 4.4512,
      "grad_norm": 34.61994171142578,
      "learning_rate": 5.1626666666666665e-05,
      "loss": -111.2942,
      "step": 55640
    },
    {
      "epoch": 4.452,
      "grad_norm": 33.812191009521484,
      "learning_rate": 5.16e-05,
      "loss": -111.3914,
      "step": 55650
    },
    {
      "epoch": 4.4528,
      "grad_norm": 29.490324020385742,
      "learning_rate": 5.157333333333334e-05,
      "loss": -111.4467,
      "step": 55660
    },
    {
      "epoch": 4.4536,
      "grad_norm": 171.36639404296875,
      "learning_rate": 5.1546666666666666e-05,
      "loss": -111.8863,
      "step": 55670
    },
    {
      "epoch": 4.4544,
      "grad_norm": 90.0063705444336,
      "learning_rate": 5.152e-05,
      "loss": -110.5984,
      "step": 55680
    },
    {
      "epoch": 4.4552,
      "grad_norm": 72.06368255615234,
      "learning_rate": 5.149333333333334e-05,
      "loss": -111.2623,
      "step": 55690
    },
    {
      "epoch": 4.456,
      "grad_norm": 123.09469604492188,
      "learning_rate": 5.146666666666667e-05,
      "loss": -110.1555,
      "step": 55700
    },
    {
      "epoch": 4.4568,
      "grad_norm": 46.30339431762695,
      "learning_rate": 5.144e-05,
      "loss": -111.5856,
      "step": 55710
    },
    {
      "epoch": 4.4576,
      "grad_norm": 179.6822052001953,
      "learning_rate": 5.141333333333334e-05,
      "loss": -111.1266,
      "step": 55720
    },
    {
      "epoch": 4.4584,
      "grad_norm": 66.81661224365234,
      "learning_rate": 5.1386666666666674e-05,
      "loss": -112.0434,
      "step": 55730
    },
    {
      "epoch": 4.4592,
      "grad_norm": 44.566349029541016,
      "learning_rate": 5.1359999999999996e-05,
      "loss": -112.2136,
      "step": 55740
    },
    {
      "epoch": 4.46,
      "grad_norm": 26.970319747924805,
      "learning_rate": 5.133333333333333e-05,
      "loss": -111.4852,
      "step": 55750
    },
    {
      "epoch": 4.4608,
      "grad_norm": 35.48398971557617,
      "learning_rate": 5.130666666666667e-05,
      "loss": -111.3234,
      "step": 55760
    },
    {
      "epoch": 4.4616,
      "grad_norm": 51.113468170166016,
      "learning_rate": 5.1280000000000004e-05,
      "loss": -111.273,
      "step": 55770
    },
    {
      "epoch": 4.4624,
      "grad_norm": 86.74375915527344,
      "learning_rate": 5.125333333333333e-05,
      "loss": -111.3103,
      "step": 55780
    },
    {
      "epoch": 4.4632,
      "grad_norm": 63.849342346191406,
      "learning_rate": 5.122666666666667e-05,
      "loss": -110.6928,
      "step": 55790
    },
    {
      "epoch": 4.464,
      "grad_norm": 85.6280517578125,
      "learning_rate": 5.1200000000000004e-05,
      "loss": -111.2208,
      "step": 55800
    },
    {
      "epoch": 4.4648,
      "grad_norm": 61.05568313598633,
      "learning_rate": 5.117333333333334e-05,
      "loss": -110.9711,
      "step": 55810
    },
    {
      "epoch": 4.4656,
      "grad_norm": 104.98695373535156,
      "learning_rate": 5.114666666666666e-05,
      "loss": -110.9847,
      "step": 55820
    },
    {
      "epoch": 4.4664,
      "grad_norm": 32.86549758911133,
      "learning_rate": 5.112e-05,
      "loss": -111.7706,
      "step": 55830
    },
    {
      "epoch": 4.4672,
      "grad_norm": 54.409828186035156,
      "learning_rate": 5.1093333333333334e-05,
      "loss": -110.777,
      "step": 55840
    },
    {
      "epoch": 4.468,
      "grad_norm": 49.62593460083008,
      "learning_rate": 5.106666666666668e-05,
      "loss": -111.3941,
      "step": 55850
    },
    {
      "epoch": 4.4688,
      "grad_norm": 27.00442123413086,
      "learning_rate": 5.104e-05,
      "loss": -112.0852,
      "step": 55860
    },
    {
      "epoch": 4.4696,
      "grad_norm": 99.54199981689453,
      "learning_rate": 5.1013333333333335e-05,
      "loss": -111.1382,
      "step": 55870
    },
    {
      "epoch": 4.4704,
      "grad_norm": 45.389137268066406,
      "learning_rate": 5.098666666666667e-05,
      "loss": -110.8615,
      "step": 55880
    },
    {
      "epoch": 4.4712,
      "grad_norm": 84.52928924560547,
      "learning_rate": 5.096000000000001e-05,
      "loss": -111.8941,
      "step": 55890
    },
    {
      "epoch": 4.4719999999999995,
      "grad_norm": 66.70550537109375,
      "learning_rate": 5.0933333333333336e-05,
      "loss": -112.0139,
      "step": 55900
    },
    {
      "epoch": 4.4728,
      "grad_norm": 105.4392318725586,
      "learning_rate": 5.090666666666667e-05,
      "loss": -111.6239,
      "step": 55910
    },
    {
      "epoch": 4.4736,
      "grad_norm": 93.50379943847656,
      "learning_rate": 5.088000000000001e-05,
      "loss": -111.5175,
      "step": 55920
    },
    {
      "epoch": 4.4744,
      "grad_norm": 79.59329223632812,
      "learning_rate": 5.085333333333333e-05,
      "loss": -110.7471,
      "step": 55930
    },
    {
      "epoch": 4.4752,
      "grad_norm": 63.997196197509766,
      "learning_rate": 5.0826666666666666e-05,
      "loss": -111.4277,
      "step": 55940
    },
    {
      "epoch": 4.476,
      "grad_norm": 146.9267578125,
      "learning_rate": 5.08e-05,
      "loss": -111.0529,
      "step": 55950
    },
    {
      "epoch": 4.4768,
      "grad_norm": 55.47616195678711,
      "learning_rate": 5.077333333333334e-05,
      "loss": -112.2212,
      "step": 55960
    },
    {
      "epoch": 4.4776,
      "grad_norm": 48.83848571777344,
      "learning_rate": 5.0746666666666666e-05,
      "loss": -111.7662,
      "step": 55970
    },
    {
      "epoch": 4.4784,
      "grad_norm": 71.35053253173828,
      "learning_rate": 5.072e-05,
      "loss": -112.2406,
      "step": 55980
    },
    {
      "epoch": 4.4792,
      "grad_norm": 62.6704216003418,
      "learning_rate": 5.069333333333334e-05,
      "loss": -112.1609,
      "step": 55990
    },
    {
      "epoch": 4.48,
      "grad_norm": 118.71159362792969,
      "learning_rate": 5.0666666666666674e-05,
      "loss": -111.9257,
      "step": 56000
    },
    {
      "epoch": 4.4808,
      "grad_norm": 64.13391876220703,
      "learning_rate": 5.0639999999999996e-05,
      "loss": -111.6351,
      "step": 56010
    },
    {
      "epoch": 4.4816,
      "grad_norm": 69.44499969482422,
      "learning_rate": 5.061333333333333e-05,
      "loss": -111.5612,
      "step": 56020
    },
    {
      "epoch": 4.4824,
      "grad_norm": 102.05401611328125,
      "learning_rate": 5.058666666666667e-05,
      "loss": -112.101,
      "step": 56030
    },
    {
      "epoch": 4.4832,
      "grad_norm": 133.68032836914062,
      "learning_rate": 5.056000000000001e-05,
      "loss": -111.1713,
      "step": 56040
    },
    {
      "epoch": 4.484,
      "grad_norm": 180.8199005126953,
      "learning_rate": 5.053333333333333e-05,
      "loss": -110.5854,
      "step": 56050
    },
    {
      "epoch": 4.4848,
      "grad_norm": 162.34303283691406,
      "learning_rate": 5.050666666666667e-05,
      "loss": -112.0558,
      "step": 56060
    },
    {
      "epoch": 4.4856,
      "grad_norm": 34.403194427490234,
      "learning_rate": 5.0480000000000005e-05,
      "loss": -111.7499,
      "step": 56070
    },
    {
      "epoch": 4.4864,
      "grad_norm": 34.40349197387695,
      "learning_rate": 5.045333333333333e-05,
      "loss": -111.9798,
      "step": 56080
    },
    {
      "epoch": 4.4872,
      "grad_norm": 32.75031280517578,
      "learning_rate": 5.042666666666667e-05,
      "loss": -111.921,
      "step": 56090
    },
    {
      "epoch": 4.4879999999999995,
      "grad_norm": 72.85072326660156,
      "learning_rate": 5.0400000000000005e-05,
      "loss": -111.4093,
      "step": 56100
    },
    {
      "epoch": 4.4888,
      "grad_norm": 110.42170715332031,
      "learning_rate": 5.037333333333334e-05,
      "loss": -110.4812,
      "step": 56110
    },
    {
      "epoch": 4.4896,
      "grad_norm": 88.9094467163086,
      "learning_rate": 5.0346666666666663e-05,
      "loss": -111.3323,
      "step": 56120
    },
    {
      "epoch": 4.4904,
      "grad_norm": 77.1158676147461,
      "learning_rate": 5.032e-05,
      "loss": -111.0913,
      "step": 56130
    },
    {
      "epoch": 4.4912,
      "grad_norm": 58.63935852050781,
      "learning_rate": 5.0293333333333335e-05,
      "loss": -111.9115,
      "step": 56140
    },
    {
      "epoch": 4.492,
      "grad_norm": 42.83632278442383,
      "learning_rate": 5.026666666666667e-05,
      "loss": -112.2258,
      "step": 56150
    },
    {
      "epoch": 4.4928,
      "grad_norm": 64.58573913574219,
      "learning_rate": 5.024e-05,
      "loss": -112.1076,
      "step": 56160
    },
    {
      "epoch": 4.4936,
      "grad_norm": 36.34041213989258,
      "learning_rate": 5.0213333333333336e-05,
      "loss": -110.5844,
      "step": 56170
    },
    {
      "epoch": 4.4944,
      "grad_norm": 36.774803161621094,
      "learning_rate": 5.018666666666667e-05,
      "loss": -111.523,
      "step": 56180
    },
    {
      "epoch": 4.4952,
      "grad_norm": 36.2642936706543,
      "learning_rate": 5.016000000000001e-05,
      "loss": -111.9737,
      "step": 56190
    },
    {
      "epoch": 4.496,
      "grad_norm": 60.30849838256836,
      "learning_rate": 5.013333333333333e-05,
      "loss": -112.1074,
      "step": 56200
    },
    {
      "epoch": 4.4968,
      "grad_norm": 127.53291320800781,
      "learning_rate": 5.0106666666666666e-05,
      "loss": -111.4414,
      "step": 56210
    },
    {
      "epoch": 4.4976,
      "grad_norm": 31.912548065185547,
      "learning_rate": 5.008e-05,
      "loss": -112.0974,
      "step": 56220
    },
    {
      "epoch": 4.4984,
      "grad_norm": 23.0671443939209,
      "learning_rate": 5.0053333333333344e-05,
      "loss": -111.0505,
      "step": 56230
    },
    {
      "epoch": 4.4992,
      "grad_norm": 76.87419891357422,
      "learning_rate": 5.0026666666666667e-05,
      "loss": -112.5135,
      "step": 56240
    },
    {
      "epoch": 4.5,
      "grad_norm": 26.81985092163086,
      "learning_rate": 5e-05,
      "loss": -111.422,
      "step": 56250
    },
    {
      "epoch": 4.5008,
      "grad_norm": 38.4951057434082,
      "learning_rate": 4.997333333333333e-05,
      "loss": -110.7515,
      "step": 56260
    },
    {
      "epoch": 4.5016,
      "grad_norm": 120.48182678222656,
      "learning_rate": 4.994666666666667e-05,
      "loss": -112.159,
      "step": 56270
    },
    {
      "epoch": 4.5024,
      "grad_norm": 58.800357818603516,
      "learning_rate": 4.992e-05,
      "loss": -111.749,
      "step": 56280
    },
    {
      "epoch": 4.5032,
      "grad_norm": 243.2324676513672,
      "learning_rate": 4.989333333333334e-05,
      "loss": -112.542,
      "step": 56290
    },
    {
      "epoch": 4.504,
      "grad_norm": 108.47532653808594,
      "learning_rate": 4.986666666666667e-05,
      "loss": -110.4338,
      "step": 56300
    },
    {
      "epoch": 4.5048,
      "grad_norm": 25.013729095458984,
      "learning_rate": 4.9840000000000004e-05,
      "loss": -111.6918,
      "step": 56310
    },
    {
      "epoch": 4.5056,
      "grad_norm": 1408.991455078125,
      "learning_rate": 4.981333333333333e-05,
      "loss": -112.4091,
      "step": 56320
    },
    {
      "epoch": 4.5064,
      "grad_norm": 109.99263763427734,
      "learning_rate": 4.978666666666667e-05,
      "loss": -110.2659,
      "step": 56330
    },
    {
      "epoch": 4.5072,
      "grad_norm": 35.57117462158203,
      "learning_rate": 4.976e-05,
      "loss": -111.8225,
      "step": 56340
    },
    {
      "epoch": 4.508,
      "grad_norm": 19.179811477661133,
      "learning_rate": 4.973333333333334e-05,
      "loss": -111.423,
      "step": 56350
    },
    {
      "epoch": 4.5088,
      "grad_norm": 39.440513610839844,
      "learning_rate": 4.970666666666667e-05,
      "loss": -110.7354,
      "step": 56360
    },
    {
      "epoch": 4.5096,
      "grad_norm": 83.87031555175781,
      "learning_rate": 4.9680000000000005e-05,
      "loss": -111.1366,
      "step": 56370
    },
    {
      "epoch": 4.5104,
      "grad_norm": 25.793601989746094,
      "learning_rate": 4.9653333333333335e-05,
      "loss": -111.6554,
      "step": 56380
    },
    {
      "epoch": 4.5112,
      "grad_norm": 53.83377456665039,
      "learning_rate": 4.962666666666667e-05,
      "loss": -112.104,
      "step": 56390
    },
    {
      "epoch": 4.5120000000000005,
      "grad_norm": 24.157384872436523,
      "learning_rate": 4.96e-05,
      "loss": -111.5335,
      "step": 56400
    },
    {
      "epoch": 4.5128,
      "grad_norm": 29.04001235961914,
      "learning_rate": 4.9573333333333335e-05,
      "loss": -110.8432,
      "step": 56410
    },
    {
      "epoch": 4.5136,
      "grad_norm": 65.46063995361328,
      "learning_rate": 4.954666666666667e-05,
      "loss": -111.9302,
      "step": 56420
    },
    {
      "epoch": 4.5144,
      "grad_norm": 58.88886260986328,
      "learning_rate": 4.952e-05,
      "loss": -111.4157,
      "step": 56430
    },
    {
      "epoch": 4.5152,
      "grad_norm": 108.6037826538086,
      "learning_rate": 4.9493333333333336e-05,
      "loss": -111.2441,
      "step": 56440
    },
    {
      "epoch": 4.516,
      "grad_norm": 56.80613708496094,
      "learning_rate": 4.9466666666666665e-05,
      "loss": -111.9904,
      "step": 56450
    },
    {
      "epoch": 4.5168,
      "grad_norm": 44.755126953125,
      "learning_rate": 4.944e-05,
      "loss": -112.0104,
      "step": 56460
    },
    {
      "epoch": 4.5176,
      "grad_norm": 62.25139236450195,
      "learning_rate": 4.941333333333334e-05,
      "loss": -112.2063,
      "step": 56470
    },
    {
      "epoch": 4.5184,
      "grad_norm": 20.712032318115234,
      "learning_rate": 4.938666666666667e-05,
      "loss": -112.2229,
      "step": 56480
    },
    {
      "epoch": 4.5192,
      "grad_norm": 51.75646209716797,
      "learning_rate": 4.936e-05,
      "loss": -111.4932,
      "step": 56490
    },
    {
      "epoch": 4.52,
      "grad_norm": 58.326324462890625,
      "learning_rate": 4.933333333333334e-05,
      "loss": -111.5449,
      "step": 56500
    },
    {
      "epoch": 4.5208,
      "grad_norm": 52.83803939819336,
      "learning_rate": 4.930666666666667e-05,
      "loss": -112.1043,
      "step": 56510
    },
    {
      "epoch": 4.5216,
      "grad_norm": 46.526885986328125,
      "learning_rate": 4.928e-05,
      "loss": -111.5043,
      "step": 56520
    },
    {
      "epoch": 4.5224,
      "grad_norm": 28.550186157226562,
      "learning_rate": 4.925333333333333e-05,
      "loss": -111.4248,
      "step": 56530
    },
    {
      "epoch": 4.5232,
      "grad_norm": 101.48214721679688,
      "learning_rate": 4.9226666666666674e-05,
      "loss": -111.4667,
      "step": 56540
    },
    {
      "epoch": 4.524,
      "grad_norm": 56.94919967651367,
      "learning_rate": 4.92e-05,
      "loss": -112.0157,
      "step": 56550
    },
    {
      "epoch": 4.5248,
      "grad_norm": 115.01197052001953,
      "learning_rate": 4.917333333333334e-05,
      "loss": -111.8495,
      "step": 56560
    },
    {
      "epoch": 4.5256,
      "grad_norm": 154.39793395996094,
      "learning_rate": 4.914666666666667e-05,
      "loss": -111.9148,
      "step": 56570
    },
    {
      "epoch": 4.5264,
      "grad_norm": 98.55904388427734,
      "learning_rate": 4.9120000000000004e-05,
      "loss": -111.5023,
      "step": 56580
    },
    {
      "epoch": 4.5272,
      "grad_norm": 137.40298461914062,
      "learning_rate": 4.909333333333333e-05,
      "loss": -111.5313,
      "step": 56590
    },
    {
      "epoch": 4.5280000000000005,
      "grad_norm": 75.04485321044922,
      "learning_rate": 4.906666666666667e-05,
      "loss": -111.4213,
      "step": 56600
    },
    {
      "epoch": 4.5288,
      "grad_norm": 84.11268615722656,
      "learning_rate": 4.9040000000000005e-05,
      "loss": -111.9594,
      "step": 56610
    },
    {
      "epoch": 4.5296,
      "grad_norm": 50.23629379272461,
      "learning_rate": 4.9013333333333334e-05,
      "loss": -111.2758,
      "step": 56620
    },
    {
      "epoch": 4.5304,
      "grad_norm": 45.15285873413086,
      "learning_rate": 4.898666666666667e-05,
      "loss": -110.7514,
      "step": 56630
    },
    {
      "epoch": 4.5312,
      "grad_norm": 26.940370559692383,
      "learning_rate": 4.896e-05,
      "loss": -111.0922,
      "step": 56640
    },
    {
      "epoch": 4.532,
      "grad_norm": 46.2790641784668,
      "learning_rate": 4.8933333333333335e-05,
      "loss": -111.5205,
      "step": 56650
    },
    {
      "epoch": 4.5328,
      "grad_norm": 45.80564498901367,
      "learning_rate": 4.890666666666667e-05,
      "loss": -110.7282,
      "step": 56660
    },
    {
      "epoch": 4.5336,
      "grad_norm": 62.337379455566406,
      "learning_rate": 4.8880000000000006e-05,
      "loss": -111.9511,
      "step": 56670
    },
    {
      "epoch": 4.5344,
      "grad_norm": 52.86794662475586,
      "learning_rate": 4.8853333333333335e-05,
      "loss": -112.4579,
      "step": 56680
    },
    {
      "epoch": 4.5352,
      "grad_norm": 58.80002975463867,
      "learning_rate": 4.882666666666667e-05,
      "loss": -111.9871,
      "step": 56690
    },
    {
      "epoch": 4.536,
      "grad_norm": 97.31194305419922,
      "learning_rate": 4.88e-05,
      "loss": -112.6131,
      "step": 56700
    },
    {
      "epoch": 4.5368,
      "grad_norm": 58.95825958251953,
      "learning_rate": 4.8773333333333336e-05,
      "loss": -111.404,
      "step": 56710
    },
    {
      "epoch": 4.5376,
      "grad_norm": 132.49667358398438,
      "learning_rate": 4.8746666666666665e-05,
      "loss": -111.7829,
      "step": 56720
    },
    {
      "epoch": 4.5384,
      "grad_norm": 29.315526962280273,
      "learning_rate": 4.872000000000001e-05,
      "loss": -111.7735,
      "step": 56730
    },
    {
      "epoch": 4.5392,
      "grad_norm": 147.56678771972656,
      "learning_rate": 4.869333333333334e-05,
      "loss": -111.6027,
      "step": 56740
    },
    {
      "epoch": 4.54,
      "grad_norm": 32.880855560302734,
      "learning_rate": 4.866666666666667e-05,
      "loss": -112.2475,
      "step": 56750
    },
    {
      "epoch": 4.5408,
      "grad_norm": 26.045961380004883,
      "learning_rate": 4.864e-05,
      "loss": -111.5344,
      "step": 56760
    },
    {
      "epoch": 4.5416,
      "grad_norm": 26.14874839782715,
      "learning_rate": 4.861333333333333e-05,
      "loss": -111.2557,
      "step": 56770
    },
    {
      "epoch": 4.5424,
      "grad_norm": 36.10386276245117,
      "learning_rate": 4.858666666666667e-05,
      "loss": -111.7524,
      "step": 56780
    },
    {
      "epoch": 4.5432,
      "grad_norm": 38.601104736328125,
      "learning_rate": 4.856e-05,
      "loss": -111.986,
      "step": 56790
    },
    {
      "epoch": 4.5440000000000005,
      "grad_norm": 23.0734806060791,
      "learning_rate": 4.853333333333334e-05,
      "loss": -111.7318,
      "step": 56800
    },
    {
      "epoch": 4.5448,
      "grad_norm": 44.203216552734375,
      "learning_rate": 4.850666666666667e-05,
      "loss": -111.0544,
      "step": 56810
    },
    {
      "epoch": 4.5456,
      "grad_norm": 23.691078186035156,
      "learning_rate": 4.8480000000000003e-05,
      "loss": -112.1541,
      "step": 56820
    },
    {
      "epoch": 4.5464,
      "grad_norm": 58.98219299316406,
      "learning_rate": 4.845333333333333e-05,
      "loss": -111.4837,
      "step": 56830
    },
    {
      "epoch": 4.5472,
      "grad_norm": 30.099905014038086,
      "learning_rate": 4.842666666666667e-05,
      "loss": -111.8527,
      "step": 56840
    },
    {
      "epoch": 4.548,
      "grad_norm": 55.77909469604492,
      "learning_rate": 4.8400000000000004e-05,
      "loss": -111.7142,
      "step": 56850
    },
    {
      "epoch": 4.5488,
      "grad_norm": 117.71204376220703,
      "learning_rate": 4.837333333333334e-05,
      "loss": -111.7953,
      "step": 56860
    },
    {
      "epoch": 4.5496,
      "grad_norm": 52.57579040527344,
      "learning_rate": 4.834666666666667e-05,
      "loss": -111.3358,
      "step": 56870
    },
    {
      "epoch": 4.5504,
      "grad_norm": 65.98600006103516,
      "learning_rate": 4.8320000000000005e-05,
      "loss": -111.889,
      "step": 56880
    },
    {
      "epoch": 4.5512,
      "grad_norm": 129.35853576660156,
      "learning_rate": 4.8293333333333334e-05,
      "loss": -111.0425,
      "step": 56890
    },
    {
      "epoch": 4.552,
      "grad_norm": 88.1974105834961,
      "learning_rate": 4.826666666666667e-05,
      "loss": -111.7202,
      "step": 56900
    },
    {
      "epoch": 4.5527999999999995,
      "grad_norm": 58.09403610229492,
      "learning_rate": 4.824e-05,
      "loss": -112.0919,
      "step": 56910
    },
    {
      "epoch": 4.5536,
      "grad_norm": 138.55726623535156,
      "learning_rate": 4.8213333333333335e-05,
      "loss": -111.2869,
      "step": 56920
    },
    {
      "epoch": 4.5544,
      "grad_norm": 27.787906646728516,
      "learning_rate": 4.818666666666667e-05,
      "loss": -112.016,
      "step": 56930
    },
    {
      "epoch": 4.5552,
      "grad_norm": 46.47414779663086,
      "learning_rate": 4.816e-05,
      "loss": -111.5436,
      "step": 56940
    },
    {
      "epoch": 4.556,
      "grad_norm": 1784.3939208984375,
      "learning_rate": 4.8133333333333336e-05,
      "loss": -111.4344,
      "step": 56950
    },
    {
      "epoch": 4.5568,
      "grad_norm": 45.881099700927734,
      "learning_rate": 4.8106666666666665e-05,
      "loss": -111.5053,
      "step": 56960
    },
    {
      "epoch": 4.5576,
      "grad_norm": 32.100181579589844,
      "learning_rate": 4.808e-05,
      "loss": -110.9287,
      "step": 56970
    },
    {
      "epoch": 4.5584,
      "grad_norm": 120.71189880371094,
      "learning_rate": 4.8053333333333336e-05,
      "loss": -111.3836,
      "step": 56980
    },
    {
      "epoch": 4.5592,
      "grad_norm": 90.62303161621094,
      "learning_rate": 4.802666666666667e-05,
      "loss": -111.1049,
      "step": 56990
    },
    {
      "epoch": 4.5600000000000005,
      "grad_norm": 48.048187255859375,
      "learning_rate": 4.8e-05,
      "loss": -110.8193,
      "step": 57000
    },
    {
      "epoch": 4.5608,
      "grad_norm": 92.26004791259766,
      "learning_rate": 4.797333333333334e-05,
      "loss": -111.769,
      "step": 57010
    },
    {
      "epoch": 4.5616,
      "grad_norm": 77.89818572998047,
      "learning_rate": 4.7946666666666666e-05,
      "loss": -111.5134,
      "step": 57020
    },
    {
      "epoch": 4.5624,
      "grad_norm": 112.40062713623047,
      "learning_rate": 4.792e-05,
      "loss": -111.4888,
      "step": 57030
    },
    {
      "epoch": 4.5632,
      "grad_norm": 47.928646087646484,
      "learning_rate": 4.789333333333334e-05,
      "loss": -111.2657,
      "step": 57040
    },
    {
      "epoch": 4.564,
      "grad_norm": 31.59039306640625,
      "learning_rate": 4.7866666666666674e-05,
      "loss": -111.4272,
      "step": 57050
    },
    {
      "epoch": 4.5648,
      "grad_norm": 115.6688461303711,
      "learning_rate": 4.784e-05,
      "loss": -111.5639,
      "step": 57060
    },
    {
      "epoch": 4.5656,
      "grad_norm": 58.722129821777344,
      "learning_rate": 4.781333333333334e-05,
      "loss": -112.685,
      "step": 57070
    },
    {
      "epoch": 4.5664,
      "grad_norm": 29.34205436706543,
      "learning_rate": 4.778666666666667e-05,
      "loss": -111.226,
      "step": 57080
    },
    {
      "epoch": 4.5672,
      "grad_norm": 46.21833419799805,
      "learning_rate": 4.7760000000000004e-05,
      "loss": -111.1409,
      "step": 57090
    },
    {
      "epoch": 4.568,
      "grad_norm": 127.59147644042969,
      "learning_rate": 4.773333333333333e-05,
      "loss": -111.0429,
      "step": 57100
    },
    {
      "epoch": 4.5687999999999995,
      "grad_norm": 49.28067398071289,
      "learning_rate": 4.770666666666667e-05,
      "loss": -111.6638,
      "step": 57110
    },
    {
      "epoch": 4.5696,
      "grad_norm": 77.83760833740234,
      "learning_rate": 4.7680000000000004e-05,
      "loss": -112.1815,
      "step": 57120
    },
    {
      "epoch": 4.5704,
      "grad_norm": 64.90556335449219,
      "learning_rate": 4.765333333333333e-05,
      "loss": -112.1053,
      "step": 57130
    },
    {
      "epoch": 4.5712,
      "grad_norm": 126.44795989990234,
      "learning_rate": 4.762666666666667e-05,
      "loss": -111.3644,
      "step": 57140
    },
    {
      "epoch": 4.572,
      "grad_norm": 96.3994140625,
      "learning_rate": 4.76e-05,
      "loss": -111.2226,
      "step": 57150
    },
    {
      "epoch": 4.5728,
      "grad_norm": 74.32747650146484,
      "learning_rate": 4.7573333333333334e-05,
      "loss": -110.3613,
      "step": 57160
    },
    {
      "epoch": 4.5736,
      "grad_norm": 115.02926635742188,
      "learning_rate": 4.754666666666667e-05,
      "loss": -110.4986,
      "step": 57170
    },
    {
      "epoch": 4.5744,
      "grad_norm": 57.66691589355469,
      "learning_rate": 4.7520000000000006e-05,
      "loss": -110.8674,
      "step": 57180
    },
    {
      "epoch": 4.5752,
      "grad_norm": 61.84457778930664,
      "learning_rate": 4.7493333333333335e-05,
      "loss": -111.7938,
      "step": 57190
    },
    {
      "epoch": 4.576,
      "grad_norm": 123.967529296875,
      "learning_rate": 4.746666666666667e-05,
      "loss": -112.2541,
      "step": 57200
    },
    {
      "epoch": 4.5768,
      "grad_norm": 104.37742614746094,
      "learning_rate": 4.744e-05,
      "loss": -111.9333,
      "step": 57210
    },
    {
      "epoch": 4.5776,
      "grad_norm": 49.025901794433594,
      "learning_rate": 4.7413333333333336e-05,
      "loss": -110.8395,
      "step": 57220
    },
    {
      "epoch": 4.5784,
      "grad_norm": 45.8955078125,
      "learning_rate": 4.7386666666666665e-05,
      "loss": -112.3622,
      "step": 57230
    },
    {
      "epoch": 4.5792,
      "grad_norm": 32.590728759765625,
      "learning_rate": 4.736000000000001e-05,
      "loss": -111.2792,
      "step": 57240
    },
    {
      "epoch": 4.58,
      "grad_norm": 84.76433563232422,
      "learning_rate": 4.7333333333333336e-05,
      "loss": -110.9692,
      "step": 57250
    },
    {
      "epoch": 4.5808,
      "grad_norm": 62.3539924621582,
      "learning_rate": 4.730666666666667e-05,
      "loss": -111.0483,
      "step": 57260
    },
    {
      "epoch": 4.5816,
      "grad_norm": 51.956024169921875,
      "learning_rate": 4.728e-05,
      "loss": -111.5714,
      "step": 57270
    },
    {
      "epoch": 4.5824,
      "grad_norm": 44.685447692871094,
      "learning_rate": 4.725333333333334e-05,
      "loss": -111.6369,
      "step": 57280
    },
    {
      "epoch": 4.5832,
      "grad_norm": 42.66621398925781,
      "learning_rate": 4.7226666666666666e-05,
      "loss": -112.0834,
      "step": 57290
    },
    {
      "epoch": 4.584,
      "grad_norm": 57.62040328979492,
      "learning_rate": 4.72e-05,
      "loss": -112.3628,
      "step": 57300
    },
    {
      "epoch": 4.5847999999999995,
      "grad_norm": 33.486812591552734,
      "learning_rate": 4.717333333333334e-05,
      "loss": -111.9661,
      "step": 57310
    },
    {
      "epoch": 4.5856,
      "grad_norm": 47.89258575439453,
      "learning_rate": 4.714666666666667e-05,
      "loss": -112.0759,
      "step": 57320
    },
    {
      "epoch": 4.5864,
      "grad_norm": 62.694847106933594,
      "learning_rate": 4.712e-05,
      "loss": -112.4601,
      "step": 57330
    },
    {
      "epoch": 4.5872,
      "grad_norm": 53.58644485473633,
      "learning_rate": 4.709333333333333e-05,
      "loss": -111.6487,
      "step": 57340
    },
    {
      "epoch": 4.588,
      "grad_norm": 60.80717849731445,
      "learning_rate": 4.706666666666667e-05,
      "loss": -110.9163,
      "step": 57350
    },
    {
      "epoch": 4.5888,
      "grad_norm": 92.73558044433594,
      "learning_rate": 4.7040000000000004e-05,
      "loss": -111.5728,
      "step": 57360
    },
    {
      "epoch": 4.5896,
      "grad_norm": 41.0981330871582,
      "learning_rate": 4.701333333333334e-05,
      "loss": -112.0425,
      "step": 57370
    },
    {
      "epoch": 4.5904,
      "grad_norm": 37.91106414794922,
      "learning_rate": 4.698666666666667e-05,
      "loss": -111.0987,
      "step": 57380
    },
    {
      "epoch": 4.5912,
      "grad_norm": 71.537109375,
      "learning_rate": 4.6960000000000004e-05,
      "loss": -111.9858,
      "step": 57390
    },
    {
      "epoch": 4.592,
      "grad_norm": 71.47535705566406,
      "learning_rate": 4.6933333333333333e-05,
      "loss": -111.2865,
      "step": 57400
    },
    {
      "epoch": 4.5928,
      "grad_norm": 52.33914566040039,
      "learning_rate": 4.690666666666667e-05,
      "loss": -110.9924,
      "step": 57410
    },
    {
      "epoch": 4.5936,
      "grad_norm": 273.8218994140625,
      "learning_rate": 4.688e-05,
      "loss": -111.9333,
      "step": 57420
    },
    {
      "epoch": 4.5944,
      "grad_norm": 96.53746795654297,
      "learning_rate": 4.685333333333334e-05,
      "loss": -111.5241,
      "step": 57430
    },
    {
      "epoch": 4.5952,
      "grad_norm": 363.4016418457031,
      "learning_rate": 4.682666666666667e-05,
      "loss": -112.5548,
      "step": 57440
    },
    {
      "epoch": 4.596,
      "grad_norm": 34.86893081665039,
      "learning_rate": 4.6800000000000006e-05,
      "loss": -110.7106,
      "step": 57450
    },
    {
      "epoch": 4.5968,
      "grad_norm": 38.40803527832031,
      "learning_rate": 4.6773333333333335e-05,
      "loss": -111.563,
      "step": 57460
    },
    {
      "epoch": 4.5976,
      "grad_norm": 22.224700927734375,
      "learning_rate": 4.6746666666666664e-05,
      "loss": -111.3777,
      "step": 57470
    },
    {
      "epoch": 4.5984,
      "grad_norm": 46.37971878051758,
      "learning_rate": 4.672e-05,
      "loss": -111.9307,
      "step": 57480
    },
    {
      "epoch": 4.5992,
      "grad_norm": 82.00555419921875,
      "learning_rate": 4.6693333333333336e-05,
      "loss": -111.9324,
      "step": 57490
    },
    {
      "epoch": 4.6,
      "grad_norm": 45.48565673828125,
      "learning_rate": 4.666666666666667e-05,
      "loss": -110.72,
      "step": 57500
    },
    {
      "epoch": 4.6008,
      "grad_norm": 34.019351959228516,
      "learning_rate": 4.664e-05,
      "loss": -110.9686,
      "step": 57510
    },
    {
      "epoch": 4.6016,
      "grad_norm": 41.9927978515625,
      "learning_rate": 4.6613333333333337e-05,
      "loss": -111.3179,
      "step": 57520
    },
    {
      "epoch": 4.6024,
      "grad_norm": 88.74006652832031,
      "learning_rate": 4.6586666666666666e-05,
      "loss": -111.4457,
      "step": 57530
    },
    {
      "epoch": 4.6032,
      "grad_norm": 27.241195678710938,
      "learning_rate": 4.656e-05,
      "loss": -111.1835,
      "step": 57540
    },
    {
      "epoch": 4.604,
      "grad_norm": 38.6760368347168,
      "learning_rate": 4.653333333333334e-05,
      "loss": -111.0777,
      "step": 57550
    },
    {
      "epoch": 4.6048,
      "grad_norm": 54.11689376831055,
      "learning_rate": 4.650666666666667e-05,
      "loss": -111.5121,
      "step": 57560
    },
    {
      "epoch": 4.6056,
      "grad_norm": 98.06565856933594,
      "learning_rate": 4.648e-05,
      "loss": -111.3203,
      "step": 57570
    },
    {
      "epoch": 4.6064,
      "grad_norm": 29.045787811279297,
      "learning_rate": 4.645333333333334e-05,
      "loss": -111.7861,
      "step": 57580
    },
    {
      "epoch": 4.6072,
      "grad_norm": 47.9465217590332,
      "learning_rate": 4.642666666666667e-05,
      "loss": -111.2887,
      "step": 57590
    },
    {
      "epoch": 4.608,
      "grad_norm": 126.91419219970703,
      "learning_rate": 4.64e-05,
      "loss": -112.024,
      "step": 57600
    },
    {
      "epoch": 4.6088000000000005,
      "grad_norm": 78.20262145996094,
      "learning_rate": 4.637333333333333e-05,
      "loss": -110.4492,
      "step": 57610
    },
    {
      "epoch": 4.6096,
      "grad_norm": 213.13906860351562,
      "learning_rate": 4.6346666666666675e-05,
      "loss": -112.8661,
      "step": 57620
    },
    {
      "epoch": 4.6104,
      "grad_norm": 52.46066665649414,
      "learning_rate": 4.6320000000000004e-05,
      "loss": -111.7545,
      "step": 57630
    },
    {
      "epoch": 4.6112,
      "grad_norm": 38.42790603637695,
      "learning_rate": 4.629333333333333e-05,
      "loss": -110.7437,
      "step": 57640
    },
    {
      "epoch": 4.612,
      "grad_norm": 108.80770111083984,
      "learning_rate": 4.626666666666667e-05,
      "loss": -112.2781,
      "step": 57650
    },
    {
      "epoch": 4.6128,
      "grad_norm": 128.8136444091797,
      "learning_rate": 4.624e-05,
      "loss": -110.8656,
      "step": 57660
    },
    {
      "epoch": 4.6136,
      "grad_norm": 59.93489456176758,
      "learning_rate": 4.6213333333333334e-05,
      "loss": -111.7068,
      "step": 57670
    },
    {
      "epoch": 4.6144,
      "grad_norm": 28.184368133544922,
      "learning_rate": 4.618666666666667e-05,
      "loss": -110.94,
      "step": 57680
    },
    {
      "epoch": 4.6152,
      "grad_norm": 36.479244232177734,
      "learning_rate": 4.6160000000000005e-05,
      "loss": -111.9145,
      "step": 57690
    },
    {
      "epoch": 4.616,
      "grad_norm": 40.5200309753418,
      "learning_rate": 4.6133333333333334e-05,
      "loss": -110.9712,
      "step": 57700
    },
    {
      "epoch": 4.6168,
      "grad_norm": 93.42713165283203,
      "learning_rate": 4.610666666666667e-05,
      "loss": -112.3514,
      "step": 57710
    },
    {
      "epoch": 4.6176,
      "grad_norm": 127.40800476074219,
      "learning_rate": 4.608e-05,
      "loss": -111.3552,
      "step": 57720
    },
    {
      "epoch": 4.6184,
      "grad_norm": 32.8828125,
      "learning_rate": 4.6053333333333335e-05,
      "loss": -111.2075,
      "step": 57730
    },
    {
      "epoch": 4.6192,
      "grad_norm": 26.83785057067871,
      "learning_rate": 4.602666666666667e-05,
      "loss": -111.1569,
      "step": 57740
    },
    {
      "epoch": 4.62,
      "grad_norm": 40.211753845214844,
      "learning_rate": 4.600000000000001e-05,
      "loss": -111.4173,
      "step": 57750
    },
    {
      "epoch": 4.6208,
      "grad_norm": 27.049903869628906,
      "learning_rate": 4.5973333333333336e-05,
      "loss": -111.3745,
      "step": 57760
    },
    {
      "epoch": 4.6216,
      "grad_norm": 74.74148559570312,
      "learning_rate": 4.594666666666667e-05,
      "loss": -111.6561,
      "step": 57770
    },
    {
      "epoch": 4.6224,
      "grad_norm": 204.433349609375,
      "learning_rate": 4.592e-05,
      "loss": -111.296,
      "step": 57780
    },
    {
      "epoch": 4.6232,
      "grad_norm": 63.202125549316406,
      "learning_rate": 4.589333333333334e-05,
      "loss": -112.2107,
      "step": 57790
    },
    {
      "epoch": 4.624,
      "grad_norm": 26.06596565246582,
      "learning_rate": 4.5866666666666666e-05,
      "loss": -110.8825,
      "step": 57800
    },
    {
      "epoch": 4.6248000000000005,
      "grad_norm": 50.966217041015625,
      "learning_rate": 4.584e-05,
      "loss": -111.3384,
      "step": 57810
    },
    {
      "epoch": 4.6256,
      "grad_norm": 82.66127014160156,
      "learning_rate": 4.581333333333334e-05,
      "loss": -112.3752,
      "step": 57820
    },
    {
      "epoch": 4.6264,
      "grad_norm": 27.36225700378418,
      "learning_rate": 4.5786666666666666e-05,
      "loss": -110.8246,
      "step": 57830
    },
    {
      "epoch": 4.6272,
      "grad_norm": 206.5705108642578,
      "learning_rate": 4.576e-05,
      "loss": -110.9762,
      "step": 57840
    },
    {
      "epoch": 4.628,
      "grad_norm": 36.97208023071289,
      "learning_rate": 4.573333333333333e-05,
      "loss": -111.2464,
      "step": 57850
    },
    {
      "epoch": 4.6288,
      "grad_norm": 38.517337799072266,
      "learning_rate": 4.570666666666667e-05,
      "loss": -111.6701,
      "step": 57860
    },
    {
      "epoch": 4.6296,
      "grad_norm": 23.82251739501953,
      "learning_rate": 4.568e-05,
      "loss": -111.7812,
      "step": 57870
    },
    {
      "epoch": 4.6304,
      "grad_norm": 57.22575759887695,
      "learning_rate": 4.565333333333334e-05,
      "loss": -111.4615,
      "step": 57880
    },
    {
      "epoch": 4.6312,
      "grad_norm": 53.797481536865234,
      "learning_rate": 4.562666666666667e-05,
      "loss": -111.4323,
      "step": 57890
    },
    {
      "epoch": 4.632,
      "grad_norm": 83.03630065917969,
      "learning_rate": 4.5600000000000004e-05,
      "loss": -110.8036,
      "step": 57900
    },
    {
      "epoch": 4.6328,
      "grad_norm": 88.26274108886719,
      "learning_rate": 4.557333333333333e-05,
      "loss": -111.7965,
      "step": 57910
    },
    {
      "epoch": 4.6336,
      "grad_norm": 60.73995590209961,
      "learning_rate": 4.554666666666667e-05,
      "loss": -111.4094,
      "step": 57920
    },
    {
      "epoch": 4.6344,
      "grad_norm": 170.63540649414062,
      "learning_rate": 4.5520000000000005e-05,
      "loss": -111.5625,
      "step": 57930
    },
    {
      "epoch": 4.6352,
      "grad_norm": 33.28693389892578,
      "learning_rate": 4.549333333333334e-05,
      "loss": -112.5777,
      "step": 57940
    },
    {
      "epoch": 4.636,
      "grad_norm": 27.316673278808594,
      "learning_rate": 4.546666666666667e-05,
      "loss": -110.7884,
      "step": 57950
    },
    {
      "epoch": 4.6368,
      "grad_norm": 51.5070915222168,
      "learning_rate": 4.5440000000000005e-05,
      "loss": -111.5602,
      "step": 57960
    },
    {
      "epoch": 4.6376,
      "grad_norm": 27.012104034423828,
      "learning_rate": 4.5413333333333334e-05,
      "loss": -110.5314,
      "step": 57970
    },
    {
      "epoch": 4.6384,
      "grad_norm": 58.272132873535156,
      "learning_rate": 4.5386666666666664e-05,
      "loss": -111.8074,
      "step": 57980
    },
    {
      "epoch": 4.6392,
      "grad_norm": 46.964168548583984,
      "learning_rate": 4.536e-05,
      "loss": -111.4074,
      "step": 57990
    },
    {
      "epoch": 4.64,
      "grad_norm": 67.61554718017578,
      "learning_rate": 4.5333333333333335e-05,
      "loss": -110.8441,
      "step": 58000
    },
    {
      "epoch": 4.6408000000000005,
      "grad_norm": 72.32229614257812,
      "learning_rate": 4.530666666666667e-05,
      "loss": -112.1578,
      "step": 58010
    },
    {
      "epoch": 4.6416,
      "grad_norm": 25.600147247314453,
      "learning_rate": 4.528e-05,
      "loss": -111.2692,
      "step": 58020
    },
    {
      "epoch": 4.6424,
      "grad_norm": 32.089111328125,
      "learning_rate": 4.5253333333333336e-05,
      "loss": -111.7039,
      "step": 58030
    },
    {
      "epoch": 4.6432,
      "grad_norm": 28.08948516845703,
      "learning_rate": 4.5226666666666665e-05,
      "loss": -111.7803,
      "step": 58040
    },
    {
      "epoch": 4.644,
      "grad_norm": 78.91447448730469,
      "learning_rate": 4.52e-05,
      "loss": -110.9575,
      "step": 58050
    },
    {
      "epoch": 4.6448,
      "grad_norm": 141.95050048828125,
      "learning_rate": 4.517333333333334e-05,
      "loss": -111.8136,
      "step": 58060
    },
    {
      "epoch": 4.6456,
      "grad_norm": 32.03418731689453,
      "learning_rate": 4.514666666666667e-05,
      "loss": -111.3144,
      "step": 58070
    },
    {
      "epoch": 4.6464,
      "grad_norm": 45.77785110473633,
      "learning_rate": 4.512e-05,
      "loss": -111.7188,
      "step": 58080
    },
    {
      "epoch": 4.6472,
      "grad_norm": 34.052879333496094,
      "learning_rate": 4.509333333333334e-05,
      "loss": -111.7879,
      "step": 58090
    },
    {
      "epoch": 4.648,
      "grad_norm": 61.65591812133789,
      "learning_rate": 4.5066666666666667e-05,
      "loss": -112.0794,
      "step": 58100
    },
    {
      "epoch": 4.6488,
      "grad_norm": 80.78517150878906,
      "learning_rate": 4.504e-05,
      "loss": -111.5798,
      "step": 58110
    },
    {
      "epoch": 4.6495999999999995,
      "grad_norm": 43.983306884765625,
      "learning_rate": 4.501333333333334e-05,
      "loss": -111.14,
      "step": 58120
    },
    {
      "epoch": 4.6504,
      "grad_norm": 32.35068893432617,
      "learning_rate": 4.4986666666666674e-05,
      "loss": -111.7502,
      "step": 58130
    },
    {
      "epoch": 4.6512,
      "grad_norm": 78.91543579101562,
      "learning_rate": 4.496e-05,
      "loss": -111.8267,
      "step": 58140
    },
    {
      "epoch": 4.652,
      "grad_norm": 72.0969009399414,
      "learning_rate": 4.493333333333333e-05,
      "loss": -111.525,
      "step": 58150
    },
    {
      "epoch": 4.6528,
      "grad_norm": 78.3176040649414,
      "learning_rate": 4.490666666666667e-05,
      "loss": -110.908,
      "step": 58160
    },
    {
      "epoch": 4.6536,
      "grad_norm": 49.64311599731445,
      "learning_rate": 4.488e-05,
      "loss": -111.8237,
      "step": 58170
    },
    {
      "epoch": 4.6544,
      "grad_norm": 53.83738708496094,
      "learning_rate": 4.485333333333333e-05,
      "loss": -111.4847,
      "step": 58180
    },
    {
      "epoch": 4.6552,
      "grad_norm": 100.16728973388672,
      "learning_rate": 4.482666666666667e-05,
      "loss": -111.6831,
      "step": 58190
    },
    {
      "epoch": 4.656,
      "grad_norm": 48.81538772583008,
      "learning_rate": 4.4800000000000005e-05,
      "loss": -111.9245,
      "step": 58200
    },
    {
      "epoch": 4.6568000000000005,
      "grad_norm": 65.81507110595703,
      "learning_rate": 4.4773333333333334e-05,
      "loss": -111.6159,
      "step": 58210
    },
    {
      "epoch": 4.6576,
      "grad_norm": 42.41114807128906,
      "learning_rate": 4.474666666666667e-05,
      "loss": -111.5203,
      "step": 58220
    },
    {
      "epoch": 4.6584,
      "grad_norm": 156.81443786621094,
      "learning_rate": 4.472e-05,
      "loss": -111.647,
      "step": 58230
    },
    {
      "epoch": 4.6592,
      "grad_norm": 143.15855407714844,
      "learning_rate": 4.4693333333333335e-05,
      "loss": -112.2271,
      "step": 58240
    },
    {
      "epoch": 4.66,
      "grad_norm": 61.041297912597656,
      "learning_rate": 4.466666666666667e-05,
      "loss": -111.4454,
      "step": 58250
    },
    {
      "epoch": 4.6608,
      "grad_norm": 99.61212921142578,
      "learning_rate": 4.4640000000000006e-05,
      "loss": -111.9245,
      "step": 58260
    },
    {
      "epoch": 4.6616,
      "grad_norm": 26.03915786743164,
      "learning_rate": 4.4613333333333335e-05,
      "loss": -110.7398,
      "step": 58270
    },
    {
      "epoch": 4.6624,
      "grad_norm": 145.78594970703125,
      "learning_rate": 4.458666666666667e-05,
      "loss": -111.1699,
      "step": 58280
    },
    {
      "epoch": 4.6632,
      "grad_norm": 101.08792114257812,
      "learning_rate": 4.456e-05,
      "loss": -111.5249,
      "step": 58290
    },
    {
      "epoch": 4.664,
      "grad_norm": 34.28396987915039,
      "learning_rate": 4.4533333333333336e-05,
      "loss": -111.9187,
      "step": 58300
    },
    {
      "epoch": 4.6648,
      "grad_norm": 53.2762451171875,
      "learning_rate": 4.450666666666667e-05,
      "loss": -111.7411,
      "step": 58310
    },
    {
      "epoch": 4.6655999999999995,
      "grad_norm": 123.81486511230469,
      "learning_rate": 4.448e-05,
      "loss": -111.7583,
      "step": 58320
    },
    {
      "epoch": 4.6664,
      "grad_norm": 37.30170440673828,
      "learning_rate": 4.445333333333334e-05,
      "loss": -111.068,
      "step": 58330
    },
    {
      "epoch": 4.6672,
      "grad_norm": 101.5783462524414,
      "learning_rate": 4.4426666666666666e-05,
      "loss": -111.9499,
      "step": 58340
    },
    {
      "epoch": 4.668,
      "grad_norm": 53.018653869628906,
      "learning_rate": 4.44e-05,
      "loss": -111.0852,
      "step": 58350
    },
    {
      "epoch": 4.6688,
      "grad_norm": 44.60858154296875,
      "learning_rate": 4.437333333333333e-05,
      "loss": -111.6031,
      "step": 58360
    },
    {
      "epoch": 4.6696,
      "grad_norm": 106.53529357910156,
      "learning_rate": 4.434666666666667e-05,
      "loss": -111.9831,
      "step": 58370
    },
    {
      "epoch": 4.6704,
      "grad_norm": 81.31011962890625,
      "learning_rate": 4.432e-05,
      "loss": -111.4548,
      "step": 58380
    },
    {
      "epoch": 4.6712,
      "grad_norm": 74.13394927978516,
      "learning_rate": 4.429333333333334e-05,
      "loss": -111.6324,
      "step": 58390
    },
    {
      "epoch": 4.672,
      "grad_norm": 78.07433319091797,
      "learning_rate": 4.426666666666667e-05,
      "loss": -112.1054,
      "step": 58400
    },
    {
      "epoch": 4.6728,
      "grad_norm": 34.845115661621094,
      "learning_rate": 4.424e-05,
      "loss": -111.2615,
      "step": 58410
    },
    {
      "epoch": 4.6736,
      "grad_norm": 75.89649200439453,
      "learning_rate": 4.421333333333333e-05,
      "loss": -111.4648,
      "step": 58420
    },
    {
      "epoch": 4.6744,
      "grad_norm": 54.19993591308594,
      "learning_rate": 4.418666666666667e-05,
      "loss": -110.6805,
      "step": 58430
    },
    {
      "epoch": 4.6752,
      "grad_norm": 309.79852294921875,
      "learning_rate": 4.4160000000000004e-05,
      "loss": -111.904,
      "step": 58440
    },
    {
      "epoch": 4.676,
      "grad_norm": 54.793556213378906,
      "learning_rate": 4.413333333333334e-05,
      "loss": -111.1037,
      "step": 58450
    },
    {
      "epoch": 4.6768,
      "grad_norm": 125.15263366699219,
      "learning_rate": 4.410666666666667e-05,
      "loss": -111.2415,
      "step": 58460
    },
    {
      "epoch": 4.6776,
      "grad_norm": 49.297176361083984,
      "learning_rate": 4.4080000000000005e-05,
      "loss": -111.7333,
      "step": 58470
    },
    {
      "epoch": 4.6784,
      "grad_norm": 140.5747528076172,
      "learning_rate": 4.4053333333333334e-05,
      "loss": -111.2233,
      "step": 58480
    },
    {
      "epoch": 4.6792,
      "grad_norm": 49.67950439453125,
      "learning_rate": 4.402666666666666e-05,
      "loss": -111.7644,
      "step": 58490
    },
    {
      "epoch": 4.68,
      "grad_norm": 44.50754165649414,
      "learning_rate": 4.4000000000000006e-05,
      "loss": -111.1594,
      "step": 58500
    },
    {
      "epoch": 4.6808,
      "grad_norm": 30.334362030029297,
      "learning_rate": 4.3973333333333335e-05,
      "loss": -112.0633,
      "step": 58510
    },
    {
      "epoch": 4.6815999999999995,
      "grad_norm": 58.2563362121582,
      "learning_rate": 4.394666666666667e-05,
      "loss": -111.6428,
      "step": 58520
    },
    {
      "epoch": 4.6824,
      "grad_norm": 50.081539154052734,
      "learning_rate": 4.392e-05,
      "loss": -111.6875,
      "step": 58530
    },
    {
      "epoch": 4.6832,
      "grad_norm": 31.310474395751953,
      "learning_rate": 4.3893333333333335e-05,
      "loss": -111.8435,
      "step": 58540
    },
    {
      "epoch": 4.684,
      "grad_norm": 116.18762969970703,
      "learning_rate": 4.3866666666666665e-05,
      "loss": -111.5193,
      "step": 58550
    },
    {
      "epoch": 4.6848,
      "grad_norm": 127.87535095214844,
      "learning_rate": 4.384e-05,
      "loss": -111.6032,
      "step": 58560
    },
    {
      "epoch": 4.6856,
      "grad_norm": 64.73190307617188,
      "learning_rate": 4.3813333333333336e-05,
      "loss": -111.9695,
      "step": 58570
    },
    {
      "epoch": 4.6864,
      "grad_norm": 125.61119842529297,
      "learning_rate": 4.378666666666667e-05,
      "loss": -112.0695,
      "step": 58580
    },
    {
      "epoch": 4.6872,
      "grad_norm": 34.52985382080078,
      "learning_rate": 4.376e-05,
      "loss": -111.0192,
      "step": 58590
    },
    {
      "epoch": 4.688,
      "grad_norm": 68.19103240966797,
      "learning_rate": 4.373333333333334e-05,
      "loss": -110.8767,
      "step": 58600
    },
    {
      "epoch": 4.6888,
      "grad_norm": 98.74809265136719,
      "learning_rate": 4.3706666666666666e-05,
      "loss": -111.0002,
      "step": 58610
    },
    {
      "epoch": 4.6896,
      "grad_norm": 24.14824104309082,
      "learning_rate": 4.368e-05,
      "loss": -110.8272,
      "step": 58620
    },
    {
      "epoch": 4.6904,
      "grad_norm": 53.19168472290039,
      "learning_rate": 4.365333333333334e-05,
      "loss": -110.8752,
      "step": 58630
    },
    {
      "epoch": 4.6912,
      "grad_norm": 147.7303466796875,
      "learning_rate": 4.3626666666666674e-05,
      "loss": -111.097,
      "step": 58640
    },
    {
      "epoch": 4.692,
      "grad_norm": 46.27812957763672,
      "learning_rate": 4.36e-05,
      "loss": -111.0328,
      "step": 58650
    },
    {
      "epoch": 4.6928,
      "grad_norm": 32.3189582824707,
      "learning_rate": 4.357333333333333e-05,
      "loss": -111.2442,
      "step": 58660
    },
    {
      "epoch": 4.6936,
      "grad_norm": 33.23917007446289,
      "learning_rate": 4.354666666666667e-05,
      "loss": -111.853,
      "step": 58670
    },
    {
      "epoch": 4.6944,
      "grad_norm": 89.22042083740234,
      "learning_rate": 4.352e-05,
      "loss": -112.5124,
      "step": 58680
    },
    {
      "epoch": 4.6952,
      "grad_norm": 93.00636291503906,
      "learning_rate": 4.349333333333334e-05,
      "loss": -112.0312,
      "step": 58690
    },
    {
      "epoch": 4.696,
      "grad_norm": 40.8831901550293,
      "learning_rate": 4.346666666666667e-05,
      "loss": -110.1508,
      "step": 58700
    },
    {
      "epoch": 4.6968,
      "grad_norm": 56.27556610107422,
      "learning_rate": 4.3440000000000004e-05,
      "loss": -111.9802,
      "step": 58710
    },
    {
      "epoch": 4.6975999999999996,
      "grad_norm": 76.26468658447266,
      "learning_rate": 4.341333333333333e-05,
      "loss": -111.9035,
      "step": 58720
    },
    {
      "epoch": 4.6984,
      "grad_norm": 98.68915557861328,
      "learning_rate": 4.338666666666667e-05,
      "loss": -112.2835,
      "step": 58730
    },
    {
      "epoch": 4.6992,
      "grad_norm": 43.84284591674805,
      "learning_rate": 4.336e-05,
      "loss": -111.2341,
      "step": 58740
    },
    {
      "epoch": 4.7,
      "grad_norm": 1104.0927734375,
      "learning_rate": 4.3333333333333334e-05,
      "loss": -112.028,
      "step": 58750
    },
    {
      "epoch": 4.7008,
      "grad_norm": 32.76757049560547,
      "learning_rate": 4.330666666666667e-05,
      "loss": -111.3197,
      "step": 58760
    },
    {
      "epoch": 4.7016,
      "grad_norm": 152.41958618164062,
      "learning_rate": 4.3280000000000006e-05,
      "loss": -111.8576,
      "step": 58770
    },
    {
      "epoch": 4.7024,
      "grad_norm": 43.05569839477539,
      "learning_rate": 4.3253333333333335e-05,
      "loss": -111.3577,
      "step": 58780
    },
    {
      "epoch": 4.7032,
      "grad_norm": 134.68357849121094,
      "learning_rate": 4.322666666666667e-05,
      "loss": -111.4958,
      "step": 58790
    },
    {
      "epoch": 4.704,
      "grad_norm": 19.467878341674805,
      "learning_rate": 4.32e-05,
      "loss": -110.8771,
      "step": 58800
    },
    {
      "epoch": 4.7048,
      "grad_norm": 36.06420135498047,
      "learning_rate": 4.3173333333333336e-05,
      "loss": -110.9829,
      "step": 58810
    },
    {
      "epoch": 4.7056000000000004,
      "grad_norm": 41.345664978027344,
      "learning_rate": 4.314666666666667e-05,
      "loss": -111.3603,
      "step": 58820
    },
    {
      "epoch": 4.7064,
      "grad_norm": 70.8858642578125,
      "learning_rate": 4.312000000000001e-05,
      "loss": -110.9729,
      "step": 58830
    },
    {
      "epoch": 4.7072,
      "grad_norm": 134.1551513671875,
      "learning_rate": 4.3093333333333336e-05,
      "loss": -111.5701,
      "step": 58840
    },
    {
      "epoch": 4.708,
      "grad_norm": 40.0967903137207,
      "learning_rate": 4.3066666666666665e-05,
      "loss": -111.6249,
      "step": 58850
    },
    {
      "epoch": 4.7088,
      "grad_norm": 88.81289672851562,
      "learning_rate": 4.304e-05,
      "loss": -112.6826,
      "step": 58860
    },
    {
      "epoch": 4.7096,
      "grad_norm": 33.82908248901367,
      "learning_rate": 4.301333333333333e-05,
      "loss": -111.6688,
      "step": 58870
    },
    {
      "epoch": 4.7104,
      "grad_norm": 51.27360534667969,
      "learning_rate": 4.2986666666666666e-05,
      "loss": -111.6475,
      "step": 58880
    },
    {
      "epoch": 4.7112,
      "grad_norm": 160.53114318847656,
      "learning_rate": 4.296e-05,
      "loss": -112.6101,
      "step": 58890
    },
    {
      "epoch": 4.712,
      "grad_norm": 57.60852813720703,
      "learning_rate": 4.293333333333334e-05,
      "loss": -112.3715,
      "step": 58900
    },
    {
      "epoch": 4.7128,
      "grad_norm": 31.021469116210938,
      "learning_rate": 4.290666666666667e-05,
      "loss": -110.6468,
      "step": 58910
    },
    {
      "epoch": 4.7136,
      "grad_norm": 31.390371322631836,
      "learning_rate": 4.288e-05,
      "loss": -111.9204,
      "step": 58920
    },
    {
      "epoch": 4.7144,
      "grad_norm": 44.712303161621094,
      "learning_rate": 4.285333333333333e-05,
      "loss": -110.9659,
      "step": 58930
    },
    {
      "epoch": 4.7152,
      "grad_norm": 45.237953186035156,
      "learning_rate": 4.282666666666667e-05,
      "loss": -110.983,
      "step": 58940
    },
    {
      "epoch": 4.716,
      "grad_norm": 29.815746307373047,
      "learning_rate": 4.2800000000000004e-05,
      "loss": -111.3992,
      "step": 58950
    },
    {
      "epoch": 4.7168,
      "grad_norm": 40.13337707519531,
      "learning_rate": 4.277333333333334e-05,
      "loss": -111.7185,
      "step": 58960
    },
    {
      "epoch": 4.7176,
      "grad_norm": 104.98530578613281,
      "learning_rate": 4.274666666666667e-05,
      "loss": -111.433,
      "step": 58970
    },
    {
      "epoch": 4.7184,
      "grad_norm": 92.01472473144531,
      "learning_rate": 4.2720000000000004e-05,
      "loss": -111.5898,
      "step": 58980
    },
    {
      "epoch": 4.7192,
      "grad_norm": 46.78351593017578,
      "learning_rate": 4.2693333333333333e-05,
      "loss": -111.478,
      "step": 58990
    },
    {
      "epoch": 4.72,
      "grad_norm": 53.542293548583984,
      "learning_rate": 4.266666666666667e-05,
      "loss": -111.3888,
      "step": 59000
    },
    {
      "epoch": 4.7208,
      "grad_norm": 29.125137329101562,
      "learning_rate": 4.2640000000000005e-05,
      "loss": -111.8052,
      "step": 59010
    },
    {
      "epoch": 4.7216000000000005,
      "grad_norm": 31.83858871459961,
      "learning_rate": 4.2613333333333334e-05,
      "loss": -111.3638,
      "step": 59020
    },
    {
      "epoch": 4.7224,
      "grad_norm": 59.678218841552734,
      "learning_rate": 4.258666666666667e-05,
      "loss": -112.0278,
      "step": 59030
    },
    {
      "epoch": 4.7232,
      "grad_norm": 103.56982421875,
      "learning_rate": 4.256e-05,
      "loss": -110.9103,
      "step": 59040
    },
    {
      "epoch": 4.724,
      "grad_norm": 69.78749084472656,
      "learning_rate": 4.2533333333333335e-05,
      "loss": -111.1294,
      "step": 59050
    },
    {
      "epoch": 4.7248,
      "grad_norm": 59.662906646728516,
      "learning_rate": 4.2506666666666664e-05,
      "loss": -111.8331,
      "step": 59060
    },
    {
      "epoch": 4.7256,
      "grad_norm": 178.90234375,
      "learning_rate": 4.248e-05,
      "loss": -111.1788,
      "step": 59070
    },
    {
      "epoch": 4.7264,
      "grad_norm": 185.46343994140625,
      "learning_rate": 4.2453333333333336e-05,
      "loss": -110.7409,
      "step": 59080
    },
    {
      "epoch": 4.7272,
      "grad_norm": 27.08385467529297,
      "learning_rate": 4.242666666666667e-05,
      "loss": -110.0643,
      "step": 59090
    },
    {
      "epoch": 4.728,
      "grad_norm": 45.66513442993164,
      "learning_rate": 4.24e-05,
      "loss": -112.1484,
      "step": 59100
    },
    {
      "epoch": 4.7288,
      "grad_norm": 66.7066879272461,
      "learning_rate": 4.2373333333333336e-05,
      "loss": -111.617,
      "step": 59110
    },
    {
      "epoch": 4.7296,
      "grad_norm": 23.791168212890625,
      "learning_rate": 4.2346666666666666e-05,
      "loss": -111.2846,
      "step": 59120
    },
    {
      "epoch": 4.7304,
      "grad_norm": 44.733001708984375,
      "learning_rate": 4.232e-05,
      "loss": -110.8076,
      "step": 59130
    },
    {
      "epoch": 4.7312,
      "grad_norm": 28.290544509887695,
      "learning_rate": 4.229333333333334e-05,
      "loss": -111.6373,
      "step": 59140
    },
    {
      "epoch": 4.732,
      "grad_norm": 125.55175018310547,
      "learning_rate": 4.226666666666667e-05,
      "loss": -112.3189,
      "step": 59150
    },
    {
      "epoch": 4.7328,
      "grad_norm": 29.6434383392334,
      "learning_rate": 4.224e-05,
      "loss": -111.1908,
      "step": 59160
    },
    {
      "epoch": 4.7336,
      "grad_norm": 39.199161529541016,
      "learning_rate": 4.221333333333334e-05,
      "loss": -111.2784,
      "step": 59170
    },
    {
      "epoch": 4.7344,
      "grad_norm": 121.3050765991211,
      "learning_rate": 4.218666666666667e-05,
      "loss": -111.1645,
      "step": 59180
    },
    {
      "epoch": 4.7352,
      "grad_norm": 42.385353088378906,
      "learning_rate": 4.2159999999999996e-05,
      "loss": -111.9097,
      "step": 59190
    },
    {
      "epoch": 4.736,
      "grad_norm": 121.76432800292969,
      "learning_rate": 4.213333333333334e-05,
      "loss": -111.5534,
      "step": 59200
    },
    {
      "epoch": 4.7368,
      "grad_norm": 39.33346176147461,
      "learning_rate": 4.210666666666667e-05,
      "loss": -111.2375,
      "step": 59210
    },
    {
      "epoch": 4.7376000000000005,
      "grad_norm": 43.903526306152344,
      "learning_rate": 4.2080000000000004e-05,
      "loss": -111.0735,
      "step": 59220
    },
    {
      "epoch": 4.7384,
      "grad_norm": 63.31752014160156,
      "learning_rate": 4.205333333333333e-05,
      "loss": -111.765,
      "step": 59230
    },
    {
      "epoch": 4.7392,
      "grad_norm": 24.929941177368164,
      "learning_rate": 4.202666666666667e-05,
      "loss": -111.4494,
      "step": 59240
    },
    {
      "epoch": 4.74,
      "grad_norm": 93.1061019897461,
      "learning_rate": 4.2e-05,
      "loss": -111.0076,
      "step": 59250
    },
    {
      "epoch": 4.7408,
      "grad_norm": 25.492645263671875,
      "learning_rate": 4.1973333333333334e-05,
      "loss": -112.5051,
      "step": 59260
    },
    {
      "epoch": 4.7416,
      "grad_norm": 45.79578399658203,
      "learning_rate": 4.194666666666667e-05,
      "loss": -111.8795,
      "step": 59270
    },
    {
      "epoch": 4.7424,
      "grad_norm": 57.495269775390625,
      "learning_rate": 4.1920000000000005e-05,
      "loss": -111.664,
      "step": 59280
    },
    {
      "epoch": 4.7432,
      "grad_norm": 115.48624420166016,
      "learning_rate": 4.1893333333333334e-05,
      "loss": -111.0484,
      "step": 59290
    },
    {
      "epoch": 4.744,
      "grad_norm": 63.25895309448242,
      "learning_rate": 4.186666666666667e-05,
      "loss": -111.777,
      "step": 59300
    },
    {
      "epoch": 4.7448,
      "grad_norm": 72.92671966552734,
      "learning_rate": 4.184e-05,
      "loss": -111.1683,
      "step": 59310
    },
    {
      "epoch": 4.7456,
      "grad_norm": 101.46180725097656,
      "learning_rate": 4.1813333333333335e-05,
      "loss": -112.519,
      "step": 59320
    },
    {
      "epoch": 4.7463999999999995,
      "grad_norm": 39.59913635253906,
      "learning_rate": 4.178666666666667e-05,
      "loss": -111.5575,
      "step": 59330
    },
    {
      "epoch": 4.7472,
      "grad_norm": 47.538291931152344,
      "learning_rate": 4.176000000000001e-05,
      "loss": -111.7576,
      "step": 59340
    },
    {
      "epoch": 4.748,
      "grad_norm": 80.77490234375,
      "learning_rate": 4.1733333333333336e-05,
      "loss": -112.46,
      "step": 59350
    },
    {
      "epoch": 4.7488,
      "grad_norm": 182.51846313476562,
      "learning_rate": 4.1706666666666665e-05,
      "loss": -111.3572,
      "step": 59360
    },
    {
      "epoch": 4.7496,
      "grad_norm": 31.598600387573242,
      "learning_rate": 4.168e-05,
      "loss": -111.0751,
      "step": 59370
    },
    {
      "epoch": 4.7504,
      "grad_norm": 30.90229606628418,
      "learning_rate": 4.165333333333333e-05,
      "loss": -111.0719,
      "step": 59380
    },
    {
      "epoch": 4.7512,
      "grad_norm": 44.19724655151367,
      "learning_rate": 4.162666666666667e-05,
      "loss": -111.0583,
      "step": 59390
    },
    {
      "epoch": 4.752,
      "grad_norm": 124.37139892578125,
      "learning_rate": 4.16e-05,
      "loss": -112.0553,
      "step": 59400
    },
    {
      "epoch": 4.7528,
      "grad_norm": 39.76753616333008,
      "learning_rate": 4.157333333333334e-05,
      "loss": -112.1159,
      "step": 59410
    },
    {
      "epoch": 4.7536000000000005,
      "grad_norm": 36.49984359741211,
      "learning_rate": 4.1546666666666666e-05,
      "loss": -111.0945,
      "step": 59420
    },
    {
      "epoch": 4.7544,
      "grad_norm": 49.7734260559082,
      "learning_rate": 4.152e-05,
      "loss": -111.5362,
      "step": 59430
    },
    {
      "epoch": 4.7552,
      "grad_norm": 57.63597869873047,
      "learning_rate": 4.149333333333333e-05,
      "loss": -111.5911,
      "step": 59440
    },
    {
      "epoch": 4.756,
      "grad_norm": 24.87342071533203,
      "learning_rate": 4.146666666666667e-05,
      "loss": -111.3948,
      "step": 59450
    },
    {
      "epoch": 4.7568,
      "grad_norm": 199.6341552734375,
      "learning_rate": 4.144e-05,
      "loss": -112.2102,
      "step": 59460
    },
    {
      "epoch": 4.7576,
      "grad_norm": 33.55390167236328,
      "learning_rate": 4.141333333333334e-05,
      "loss": -111.6712,
      "step": 59470
    },
    {
      "epoch": 4.7584,
      "grad_norm": 45.297340393066406,
      "learning_rate": 4.138666666666667e-05,
      "loss": -111.7836,
      "step": 59480
    },
    {
      "epoch": 4.7592,
      "grad_norm": 59.0925407409668,
      "learning_rate": 4.1360000000000004e-05,
      "loss": -111.7119,
      "step": 59490
    },
    {
      "epoch": 4.76,
      "grad_norm": 49.05904769897461,
      "learning_rate": 4.133333333333333e-05,
      "loss": -111.3789,
      "step": 59500
    },
    {
      "epoch": 4.7608,
      "grad_norm": 38.122352600097656,
      "learning_rate": 4.130666666666667e-05,
      "loss": -111.8152,
      "step": 59510
    },
    {
      "epoch": 4.7616,
      "grad_norm": 33.397193908691406,
      "learning_rate": 4.1280000000000005e-05,
      "loss": -112.2387,
      "step": 59520
    },
    {
      "epoch": 4.7623999999999995,
      "grad_norm": 60.477622985839844,
      "learning_rate": 4.1253333333333334e-05,
      "loss": -111.629,
      "step": 59530
    },
    {
      "epoch": 4.7632,
      "grad_norm": 21.06568145751953,
      "learning_rate": 4.122666666666667e-05,
      "loss": -111.0302,
      "step": 59540
    },
    {
      "epoch": 4.764,
      "grad_norm": 107.70621490478516,
      "learning_rate": 4.12e-05,
      "loss": -112.0618,
      "step": 59550
    },
    {
      "epoch": 4.7648,
      "grad_norm": 19.86823081970215,
      "learning_rate": 4.1173333333333334e-05,
      "loss": -111.7456,
      "step": 59560
    },
    {
      "epoch": 4.7656,
      "grad_norm": 24.630250930786133,
      "learning_rate": 4.1146666666666663e-05,
      "loss": -111.2769,
      "step": 59570
    },
    {
      "epoch": 4.7664,
      "grad_norm": 51.8323860168457,
      "learning_rate": 4.1120000000000006e-05,
      "loss": -111.8578,
      "step": 59580
    },
    {
      "epoch": 4.7672,
      "grad_norm": 27.01947021484375,
      "learning_rate": 4.1093333333333335e-05,
      "loss": -111.912,
      "step": 59590
    },
    {
      "epoch": 4.768,
      "grad_norm": 43.19143295288086,
      "learning_rate": 4.106666666666667e-05,
      "loss": -111.2662,
      "step": 59600
    },
    {
      "epoch": 4.7688,
      "grad_norm": 30.62635612487793,
      "learning_rate": 4.104e-05,
      "loss": -111.5404,
      "step": 59610
    },
    {
      "epoch": 4.7696,
      "grad_norm": 46.10814666748047,
      "learning_rate": 4.1013333333333336e-05,
      "loss": -111.5371,
      "step": 59620
    },
    {
      "epoch": 4.7704,
      "grad_norm": 28.928804397583008,
      "learning_rate": 4.0986666666666665e-05,
      "loss": -111.3861,
      "step": 59630
    },
    {
      "epoch": 4.7712,
      "grad_norm": 53.21097946166992,
      "learning_rate": 4.096e-05,
      "loss": -111.5643,
      "step": 59640
    },
    {
      "epoch": 4.772,
      "grad_norm": 82.41853332519531,
      "learning_rate": 4.093333333333334e-05,
      "loss": -111.8573,
      "step": 59650
    },
    {
      "epoch": 4.7728,
      "grad_norm": 1602.2431640625,
      "learning_rate": 4.090666666666667e-05,
      "loss": -111.3921,
      "step": 59660
    },
    {
      "epoch": 4.7736,
      "grad_norm": 31.504852294921875,
      "learning_rate": 4.088e-05,
      "loss": -111.8786,
      "step": 59670
    },
    {
      "epoch": 4.7744,
      "grad_norm": 33.4539909362793,
      "learning_rate": 4.085333333333334e-05,
      "loss": -111.3968,
      "step": 59680
    },
    {
      "epoch": 4.7752,
      "grad_norm": 35.70352554321289,
      "learning_rate": 4.0826666666666667e-05,
      "loss": -111.7198,
      "step": 59690
    },
    {
      "epoch": 4.776,
      "grad_norm": 54.97148513793945,
      "learning_rate": 4.08e-05,
      "loss": -111.5936,
      "step": 59700
    },
    {
      "epoch": 4.7768,
      "grad_norm": 50.84809875488281,
      "learning_rate": 4.077333333333334e-05,
      "loss": -111.4357,
      "step": 59710
    },
    {
      "epoch": 4.7776,
      "grad_norm": 71.77777099609375,
      "learning_rate": 4.074666666666667e-05,
      "loss": -110.5249,
      "step": 59720
    },
    {
      "epoch": 4.7783999999999995,
      "grad_norm": 26.507413864135742,
      "learning_rate": 4.072e-05,
      "loss": -112.028,
      "step": 59730
    },
    {
      "epoch": 4.7792,
      "grad_norm": 40.9472541809082,
      "learning_rate": 4.069333333333333e-05,
      "loss": -111.9016,
      "step": 59740
    },
    {
      "epoch": 4.78,
      "grad_norm": 39.00587463378906,
      "learning_rate": 4.066666666666667e-05,
      "loss": -110.9679,
      "step": 59750
    },
    {
      "epoch": 4.7808,
      "grad_norm": 104.14398193359375,
      "learning_rate": 4.064e-05,
      "loss": -110.8677,
      "step": 59760
    },
    {
      "epoch": 4.7816,
      "grad_norm": 119.71461486816406,
      "learning_rate": 4.061333333333334e-05,
      "loss": -110.2555,
      "step": 59770
    },
    {
      "epoch": 4.7824,
      "grad_norm": 55.37115478515625,
      "learning_rate": 4.058666666666667e-05,
      "loss": -112.2905,
      "step": 59780
    },
    {
      "epoch": 4.7832,
      "grad_norm": 44.15047073364258,
      "learning_rate": 4.0560000000000005e-05,
      "loss": -112.3751,
      "step": 59790
    },
    {
      "epoch": 4.784,
      "grad_norm": 74.07994079589844,
      "learning_rate": 4.0533333333333334e-05,
      "loss": -110.9104,
      "step": 59800
    },
    {
      "epoch": 4.7848,
      "grad_norm": 34.980613708496094,
      "learning_rate": 4.050666666666667e-05,
      "loss": -111.4954,
      "step": 59810
    },
    {
      "epoch": 4.7856,
      "grad_norm": 47.43910217285156,
      "learning_rate": 4.048e-05,
      "loss": -111.0584,
      "step": 59820
    },
    {
      "epoch": 4.7864,
      "grad_norm": 30.82828140258789,
      "learning_rate": 4.0453333333333335e-05,
      "loss": -111.765,
      "step": 59830
    },
    {
      "epoch": 4.7872,
      "grad_norm": 64.2824935913086,
      "learning_rate": 4.042666666666667e-05,
      "loss": -111.0345,
      "step": 59840
    },
    {
      "epoch": 4.788,
      "grad_norm": 47.47267532348633,
      "learning_rate": 4.0400000000000006e-05,
      "loss": -111.2693,
      "step": 59850
    },
    {
      "epoch": 4.7888,
      "grad_norm": 32.087684631347656,
      "learning_rate": 4.0373333333333335e-05,
      "loss": -111.1144,
      "step": 59860
    },
    {
      "epoch": 4.7896,
      "grad_norm": 89.04277801513672,
      "learning_rate": 4.0346666666666664e-05,
      "loss": -111.8773,
      "step": 59870
    },
    {
      "epoch": 4.7904,
      "grad_norm": 147.83566284179688,
      "learning_rate": 4.032e-05,
      "loss": -110.1879,
      "step": 59880
    },
    {
      "epoch": 4.7912,
      "grad_norm": 27.893407821655273,
      "learning_rate": 4.0293333333333336e-05,
      "loss": -111.8605,
      "step": 59890
    },
    {
      "epoch": 4.792,
      "grad_norm": 48.99143981933594,
      "learning_rate": 4.026666666666667e-05,
      "loss": -111.2443,
      "step": 59900
    },
    {
      "epoch": 4.7928,
      "grad_norm": 638.1713256835938,
      "learning_rate": 4.024e-05,
      "loss": -112.0115,
      "step": 59910
    },
    {
      "epoch": 4.7936,
      "grad_norm": 102.40482330322266,
      "learning_rate": 4.021333333333334e-05,
      "loss": -111.2068,
      "step": 59920
    },
    {
      "epoch": 4.7943999999999996,
      "grad_norm": 33.60820388793945,
      "learning_rate": 4.0186666666666666e-05,
      "loss": -111.1393,
      "step": 59930
    },
    {
      "epoch": 4.7952,
      "grad_norm": 60.92833709716797,
      "learning_rate": 4.016e-05,
      "loss": -112.104,
      "step": 59940
    },
    {
      "epoch": 4.796,
      "grad_norm": 86.4056396484375,
      "learning_rate": 4.013333333333333e-05,
      "loss": -112.1621,
      "step": 59950
    },
    {
      "epoch": 4.7968,
      "grad_norm": 192.72230529785156,
      "learning_rate": 4.0106666666666673e-05,
      "loss": -112.0185,
      "step": 59960
    },
    {
      "epoch": 4.7976,
      "grad_norm": 32.38374328613281,
      "learning_rate": 4.008e-05,
      "loss": -111.2638,
      "step": 59970
    },
    {
      "epoch": 4.7984,
      "grad_norm": 46.063968658447266,
      "learning_rate": 4.005333333333334e-05,
      "loss": -112.0998,
      "step": 59980
    },
    {
      "epoch": 4.7992,
      "grad_norm": 134.5112762451172,
      "learning_rate": 4.002666666666667e-05,
      "loss": -111.9163,
      "step": 59990
    },
    {
      "epoch": 4.8,
      "grad_norm": 156.1627655029297,
      "learning_rate": 4e-05,
      "loss": -111.5512,
      "step": 60000
    },
    {
      "epoch": 4.8008,
      "grad_norm": 38.14813232421875,
      "learning_rate": 3.997333333333333e-05,
      "loss": -111.2981,
      "step": 60010
    },
    {
      "epoch": 4.8016,
      "grad_norm": 62.30433654785156,
      "learning_rate": 3.994666666666667e-05,
      "loss": -112.0116,
      "step": 60020
    },
    {
      "epoch": 4.8024000000000004,
      "grad_norm": 118.92227172851562,
      "learning_rate": 3.9920000000000004e-05,
      "loss": -112.097,
      "step": 60030
    },
    {
      "epoch": 4.8032,
      "grad_norm": 132.17979431152344,
      "learning_rate": 3.989333333333333e-05,
      "loss": -111.3681,
      "step": 60040
    },
    {
      "epoch": 4.804,
      "grad_norm": 50.038082122802734,
      "learning_rate": 3.986666666666667e-05,
      "loss": -112.4396,
      "step": 60050
    },
    {
      "epoch": 4.8048,
      "grad_norm": 42.32836151123047,
      "learning_rate": 3.984e-05,
      "loss": -111.8766,
      "step": 60060
    },
    {
      "epoch": 4.8056,
      "grad_norm": 129.93792724609375,
      "learning_rate": 3.9813333333333334e-05,
      "loss": -111.9566,
      "step": 60070
    },
    {
      "epoch": 4.8064,
      "grad_norm": 74.11847686767578,
      "learning_rate": 3.978666666666667e-05,
      "loss": -111.6628,
      "step": 60080
    },
    {
      "epoch": 4.8072,
      "grad_norm": 20.091251373291016,
      "learning_rate": 3.9760000000000006e-05,
      "loss": -111.8851,
      "step": 60090
    },
    {
      "epoch": 4.808,
      "grad_norm": 188.75157165527344,
      "learning_rate": 3.9733333333333335e-05,
      "loss": -111.0583,
      "step": 60100
    },
    {
      "epoch": 4.8088,
      "grad_norm": 274.37725830078125,
      "learning_rate": 3.970666666666667e-05,
      "loss": -112.1709,
      "step": 60110
    },
    {
      "epoch": 4.8096,
      "grad_norm": 53.70857238769531,
      "learning_rate": 3.968e-05,
      "loss": -110.8564,
      "step": 60120
    },
    {
      "epoch": 4.8104,
      "grad_norm": 42.00919723510742,
      "learning_rate": 3.9653333333333335e-05,
      "loss": -111.5426,
      "step": 60130
    },
    {
      "epoch": 4.8112,
      "grad_norm": 218.22462463378906,
      "learning_rate": 3.9626666666666664e-05,
      "loss": -112.0606,
      "step": 60140
    },
    {
      "epoch": 4.812,
      "grad_norm": 146.18702697753906,
      "learning_rate": 3.960000000000001e-05,
      "loss": -112.1023,
      "step": 60150
    },
    {
      "epoch": 4.8128,
      "grad_norm": 21.29707145690918,
      "learning_rate": 3.9573333333333336e-05,
      "loss": -112.4018,
      "step": 60160
    },
    {
      "epoch": 4.8136,
      "grad_norm": 63.355682373046875,
      "learning_rate": 3.954666666666667e-05,
      "loss": -111.2628,
      "step": 60170
    },
    {
      "epoch": 4.8144,
      "grad_norm": 58.591548919677734,
      "learning_rate": 3.952e-05,
      "loss": -111.658,
      "step": 60180
    },
    {
      "epoch": 4.8152,
      "grad_norm": 32.137447357177734,
      "learning_rate": 3.949333333333334e-05,
      "loss": -111.7031,
      "step": 60190
    },
    {
      "epoch": 4.816,
      "grad_norm": 54.3509521484375,
      "learning_rate": 3.9466666666666666e-05,
      "loss": -111.8822,
      "step": 60200
    },
    {
      "epoch": 4.8168,
      "grad_norm": 68.24005889892578,
      "learning_rate": 3.944e-05,
      "loss": -112.3216,
      "step": 60210
    },
    {
      "epoch": 4.8176,
      "grad_norm": 71.68937683105469,
      "learning_rate": 3.941333333333334e-05,
      "loss": -112.3072,
      "step": 60220
    },
    {
      "epoch": 4.8184000000000005,
      "grad_norm": 35.99198913574219,
      "learning_rate": 3.938666666666667e-05,
      "loss": -111.5951,
      "step": 60230
    },
    {
      "epoch": 4.8192,
      "grad_norm": 58.556396484375,
      "learning_rate": 3.936e-05,
      "loss": -111.1554,
      "step": 60240
    },
    {
      "epoch": 4.82,
      "grad_norm": 56.75630569458008,
      "learning_rate": 3.933333333333333e-05,
      "loss": -110.8303,
      "step": 60250
    },
    {
      "epoch": 4.8208,
      "grad_norm": 57.29958724975586,
      "learning_rate": 3.930666666666667e-05,
      "loss": -111.6749,
      "step": 60260
    },
    {
      "epoch": 4.8216,
      "grad_norm": 67.5211181640625,
      "learning_rate": 3.9280000000000003e-05,
      "loss": -111.2069,
      "step": 60270
    },
    {
      "epoch": 4.8224,
      "grad_norm": 32.98799133300781,
      "learning_rate": 3.925333333333334e-05,
      "loss": -112.3998,
      "step": 60280
    },
    {
      "epoch": 4.8232,
      "grad_norm": 27.03333854675293,
      "learning_rate": 3.922666666666667e-05,
      "loss": -112.6905,
      "step": 60290
    },
    {
      "epoch": 4.824,
      "grad_norm": 181.18711853027344,
      "learning_rate": 3.9200000000000004e-05,
      "loss": -111.9468,
      "step": 60300
    },
    {
      "epoch": 4.8248,
      "grad_norm": 44.96407699584961,
      "learning_rate": 3.917333333333333e-05,
      "loss": -111.0852,
      "step": 60310
    },
    {
      "epoch": 4.8256,
      "grad_norm": 38.72077560424805,
      "learning_rate": 3.914666666666667e-05,
      "loss": -111.1696,
      "step": 60320
    },
    {
      "epoch": 4.8264,
      "grad_norm": 25.531978607177734,
      "learning_rate": 3.912e-05,
      "loss": -112.4236,
      "step": 60330
    },
    {
      "epoch": 4.8272,
      "grad_norm": 44.63655471801758,
      "learning_rate": 3.9093333333333334e-05,
      "loss": -111.4164,
      "step": 60340
    },
    {
      "epoch": 4.828,
      "grad_norm": 32.21699142456055,
      "learning_rate": 3.906666666666667e-05,
      "loss": -110.6949,
      "step": 60350
    },
    {
      "epoch": 4.8288,
      "grad_norm": 37.91206359863281,
      "learning_rate": 3.9040000000000006e-05,
      "loss": -111.6123,
      "step": 60360
    },
    {
      "epoch": 4.8296,
      "grad_norm": 72.17826843261719,
      "learning_rate": 3.9013333333333335e-05,
      "loss": -111.0339,
      "step": 60370
    },
    {
      "epoch": 4.8304,
      "grad_norm": 33.00209426879883,
      "learning_rate": 3.8986666666666664e-05,
      "loss": -112.233,
      "step": 60380
    },
    {
      "epoch": 4.8312,
      "grad_norm": 24.644853591918945,
      "learning_rate": 3.896e-05,
      "loss": -110.3297,
      "step": 60390
    },
    {
      "epoch": 4.832,
      "grad_norm": 39.68497848510742,
      "learning_rate": 3.8933333333333336e-05,
      "loss": -111.7467,
      "step": 60400
    },
    {
      "epoch": 4.8328,
      "grad_norm": 1794.5726318359375,
      "learning_rate": 3.890666666666667e-05,
      "loss": -112.0574,
      "step": 60410
    },
    {
      "epoch": 4.8336,
      "grad_norm": 57.339935302734375,
      "learning_rate": 3.888e-05,
      "loss": -111.9795,
      "step": 60420
    },
    {
      "epoch": 4.8344000000000005,
      "grad_norm": 33.004364013671875,
      "learning_rate": 3.8853333333333336e-05,
      "loss": -111.7246,
      "step": 60430
    },
    {
      "epoch": 4.8352,
      "grad_norm": 33.80083084106445,
      "learning_rate": 3.8826666666666665e-05,
      "loss": -111.3792,
      "step": 60440
    },
    {
      "epoch": 4.836,
      "grad_norm": 28.42594337463379,
      "learning_rate": 3.88e-05,
      "loss": -112.1138,
      "step": 60450
    },
    {
      "epoch": 4.8368,
      "grad_norm": 53.06708526611328,
      "learning_rate": 3.877333333333334e-05,
      "loss": -111.2968,
      "step": 60460
    },
    {
      "epoch": 4.8376,
      "grad_norm": 74.20356750488281,
      "learning_rate": 3.874666666666667e-05,
      "loss": -111.4077,
      "step": 60470
    },
    {
      "epoch": 4.8384,
      "grad_norm": 126.18436431884766,
      "learning_rate": 3.872e-05,
      "loss": -111.7891,
      "step": 60480
    },
    {
      "epoch": 4.8392,
      "grad_norm": 33.21914291381836,
      "learning_rate": 3.869333333333334e-05,
      "loss": -110.9374,
      "step": 60490
    },
    {
      "epoch": 4.84,
      "grad_norm": 84.92353057861328,
      "learning_rate": 3.866666666666667e-05,
      "loss": -112.6355,
      "step": 60500
    },
    {
      "epoch": 4.8408,
      "grad_norm": 103.02251434326172,
      "learning_rate": 3.864e-05,
      "loss": -111.6877,
      "step": 60510
    },
    {
      "epoch": 4.8416,
      "grad_norm": 138.85791015625,
      "learning_rate": 3.861333333333333e-05,
      "loss": -112.0741,
      "step": 60520
    },
    {
      "epoch": 4.8424,
      "grad_norm": 29.544538497924805,
      "learning_rate": 3.858666666666667e-05,
      "loss": -110.8554,
      "step": 60530
    },
    {
      "epoch": 4.8431999999999995,
      "grad_norm": 90.34093475341797,
      "learning_rate": 3.8560000000000004e-05,
      "loss": -111.4609,
      "step": 60540
    },
    {
      "epoch": 4.844,
      "grad_norm": 33.18837356567383,
      "learning_rate": 3.853333333333334e-05,
      "loss": -110.7852,
      "step": 60550
    },
    {
      "epoch": 4.8448,
      "grad_norm": 143.72686767578125,
      "learning_rate": 3.850666666666667e-05,
      "loss": -111.6632,
      "step": 60560
    },
    {
      "epoch": 4.8456,
      "grad_norm": 21.99270248413086,
      "learning_rate": 3.848e-05,
      "loss": -111.0898,
      "step": 60570
    },
    {
      "epoch": 4.8464,
      "grad_norm": 162.08001708984375,
      "learning_rate": 3.845333333333333e-05,
      "loss": -111.9048,
      "step": 60580
    },
    {
      "epoch": 4.8472,
      "grad_norm": 78.71591186523438,
      "learning_rate": 3.842666666666667e-05,
      "loss": -111.7753,
      "step": 60590
    },
    {
      "epoch": 4.848,
      "grad_norm": 26.416851043701172,
      "learning_rate": 3.8400000000000005e-05,
      "loss": -110.7283,
      "step": 60600
    },
    {
      "epoch": 4.8488,
      "grad_norm": 416.7861328125,
      "learning_rate": 3.8373333333333334e-05,
      "loss": -111.9298,
      "step": 60610
    },
    {
      "epoch": 4.8496,
      "grad_norm": 109.34310913085938,
      "learning_rate": 3.834666666666667e-05,
      "loss": -112.4059,
      "step": 60620
    },
    {
      "epoch": 4.8504000000000005,
      "grad_norm": 99.8427505493164,
      "learning_rate": 3.832e-05,
      "loss": -112.4801,
      "step": 60630
    },
    {
      "epoch": 4.8512,
      "grad_norm": 28.510095596313477,
      "learning_rate": 3.8293333333333335e-05,
      "loss": -111.0904,
      "step": 60640
    },
    {
      "epoch": 4.852,
      "grad_norm": 71.31388092041016,
      "learning_rate": 3.8266666666666664e-05,
      "loss": -112.0057,
      "step": 60650
    },
    {
      "epoch": 4.8528,
      "grad_norm": 27.15410804748535,
      "learning_rate": 3.8240000000000007e-05,
      "loss": -111.1105,
      "step": 60660
    },
    {
      "epoch": 4.8536,
      "grad_norm": 68.21385955810547,
      "learning_rate": 3.8213333333333336e-05,
      "loss": -110.3249,
      "step": 60670
    },
    {
      "epoch": 4.8544,
      "grad_norm": 38.39316940307617,
      "learning_rate": 3.818666666666667e-05,
      "loss": -111.5574,
      "step": 60680
    },
    {
      "epoch": 4.8552,
      "grad_norm": 56.9407844543457,
      "learning_rate": 3.816e-05,
      "loss": -112.2883,
      "step": 60690
    },
    {
      "epoch": 4.856,
      "grad_norm": 35.3904914855957,
      "learning_rate": 3.8133333333333336e-05,
      "loss": -112.024,
      "step": 60700
    },
    {
      "epoch": 4.8568,
      "grad_norm": 153.8427734375,
      "learning_rate": 3.8106666666666665e-05,
      "loss": -112.4885,
      "step": 60710
    },
    {
      "epoch": 4.8576,
      "grad_norm": 135.82261657714844,
      "learning_rate": 3.808e-05,
      "loss": -112.1077,
      "step": 60720
    },
    {
      "epoch": 4.8584,
      "grad_norm": 60.399322509765625,
      "learning_rate": 3.805333333333334e-05,
      "loss": -111.4023,
      "step": 60730
    },
    {
      "epoch": 4.8591999999999995,
      "grad_norm": 65.33353424072266,
      "learning_rate": 3.8026666666666666e-05,
      "loss": -111.5221,
      "step": 60740
    },
    {
      "epoch": 4.86,
      "grad_norm": 135.98126220703125,
      "learning_rate": 3.8e-05,
      "loss": -111.6212,
      "step": 60750
    },
    {
      "epoch": 4.8608,
      "grad_norm": 42.100982666015625,
      "learning_rate": 3.797333333333333e-05,
      "loss": -111.5129,
      "step": 60760
    },
    {
      "epoch": 4.8616,
      "grad_norm": 37.775753021240234,
      "learning_rate": 3.794666666666667e-05,
      "loss": -111.5675,
      "step": 60770
    },
    {
      "epoch": 4.8624,
      "grad_norm": 27.52837562561035,
      "learning_rate": 3.792e-05,
      "loss": -111.4985,
      "step": 60780
    },
    {
      "epoch": 4.8632,
      "grad_norm": 109.65003204345703,
      "learning_rate": 3.789333333333334e-05,
      "loss": -111.5452,
      "step": 60790
    },
    {
      "epoch": 4.864,
      "grad_norm": 272.7781677246094,
      "learning_rate": 3.786666666666667e-05,
      "loss": -111.6445,
      "step": 60800
    },
    {
      "epoch": 4.8648,
      "grad_norm": 56.64102554321289,
      "learning_rate": 3.7840000000000004e-05,
      "loss": -111.9214,
      "step": 60810
    },
    {
      "epoch": 4.8656,
      "grad_norm": 698.8661499023438,
      "learning_rate": 3.781333333333333e-05,
      "loss": -110.4634,
      "step": 60820
    },
    {
      "epoch": 4.8664,
      "grad_norm": 35.27336120605469,
      "learning_rate": 3.778666666666667e-05,
      "loss": -111.6081,
      "step": 60830
    },
    {
      "epoch": 4.8672,
      "grad_norm": 56.45442581176758,
      "learning_rate": 3.776e-05,
      "loss": -111.3485,
      "step": 60840
    },
    {
      "epoch": 4.868,
      "grad_norm": 144.5518798828125,
      "learning_rate": 3.773333333333334e-05,
      "loss": -111.9227,
      "step": 60850
    },
    {
      "epoch": 4.8688,
      "grad_norm": 97.5689697265625,
      "learning_rate": 3.770666666666667e-05,
      "loss": -112.2183,
      "step": 60860
    },
    {
      "epoch": 4.8696,
      "grad_norm": 34.44091796875,
      "learning_rate": 3.7680000000000005e-05,
      "loss": -111.5218,
      "step": 60870
    },
    {
      "epoch": 4.8704,
      "grad_norm": 30.16456413269043,
      "learning_rate": 3.7653333333333334e-05,
      "loss": -112.3451,
      "step": 60880
    },
    {
      "epoch": 4.8712,
      "grad_norm": 86.45645904541016,
      "learning_rate": 3.762666666666667e-05,
      "loss": -110.9604,
      "step": 60890
    },
    {
      "epoch": 4.872,
      "grad_norm": 23.52385711669922,
      "learning_rate": 3.76e-05,
      "loss": -111.6598,
      "step": 60900
    },
    {
      "epoch": 4.8728,
      "grad_norm": 44.01474380493164,
      "learning_rate": 3.7573333333333335e-05,
      "loss": -111.6229,
      "step": 60910
    },
    {
      "epoch": 4.8736,
      "grad_norm": 35.486637115478516,
      "learning_rate": 3.754666666666667e-05,
      "loss": -111.7219,
      "step": 60920
    },
    {
      "epoch": 4.8744,
      "grad_norm": 31.09576416015625,
      "learning_rate": 3.752e-05,
      "loss": -111.3305,
      "step": 60930
    },
    {
      "epoch": 4.8751999999999995,
      "grad_norm": 30.32515525817871,
      "learning_rate": 3.7493333333333336e-05,
      "loss": -111.4085,
      "step": 60940
    },
    {
      "epoch": 4.876,
      "grad_norm": 34.795082092285156,
      "learning_rate": 3.7466666666666665e-05,
      "loss": -111.2094,
      "step": 60950
    },
    {
      "epoch": 4.8768,
      "grad_norm": 55.08423614501953,
      "learning_rate": 3.744e-05,
      "loss": -111.4994,
      "step": 60960
    },
    {
      "epoch": 4.8776,
      "grad_norm": 100.03563690185547,
      "learning_rate": 3.7413333333333337e-05,
      "loss": -111.481,
      "step": 60970
    },
    {
      "epoch": 4.8784,
      "grad_norm": 46.01087951660156,
      "learning_rate": 3.738666666666667e-05,
      "loss": -111.4576,
      "step": 60980
    },
    {
      "epoch": 4.8792,
      "grad_norm": 81.33399200439453,
      "learning_rate": 3.736e-05,
      "loss": -111.668,
      "step": 60990
    },
    {
      "epoch": 4.88,
      "grad_norm": 43.942440032958984,
      "learning_rate": 3.733333333333334e-05,
      "loss": -111.0483,
      "step": 61000
    },
    {
      "epoch": 4.8808,
      "grad_norm": 36.85304260253906,
      "learning_rate": 3.7306666666666666e-05,
      "loss": -111.6391,
      "step": 61010
    },
    {
      "epoch": 4.8816,
      "grad_norm": 76.09779357910156,
      "learning_rate": 3.728e-05,
      "loss": -112.2823,
      "step": 61020
    },
    {
      "epoch": 4.8824,
      "grad_norm": 68.35135650634766,
      "learning_rate": 3.725333333333333e-05,
      "loss": -111.528,
      "step": 61030
    },
    {
      "epoch": 4.8832,
      "grad_norm": 64.90045928955078,
      "learning_rate": 3.7226666666666674e-05,
      "loss": -112.4267,
      "step": 61040
    },
    {
      "epoch": 4.884,
      "grad_norm": 31.72066879272461,
      "learning_rate": 3.72e-05,
      "loss": -112.3001,
      "step": 61050
    },
    {
      "epoch": 4.8848,
      "grad_norm": 40.12834548950195,
      "learning_rate": 3.717333333333334e-05,
      "loss": -111.4852,
      "step": 61060
    },
    {
      "epoch": 4.8856,
      "grad_norm": 49.05461883544922,
      "learning_rate": 3.714666666666667e-05,
      "loss": -112.2079,
      "step": 61070
    },
    {
      "epoch": 4.8864,
      "grad_norm": 162.19427490234375,
      "learning_rate": 3.712e-05,
      "loss": -112.187,
      "step": 61080
    },
    {
      "epoch": 4.8872,
      "grad_norm": 132.43081665039062,
      "learning_rate": 3.709333333333333e-05,
      "loss": -112.0394,
      "step": 61090
    },
    {
      "epoch": 4.888,
      "grad_norm": 66.17499542236328,
      "learning_rate": 3.706666666666667e-05,
      "loss": -111.6373,
      "step": 61100
    },
    {
      "epoch": 4.8888,
      "grad_norm": 59.74773025512695,
      "learning_rate": 3.7040000000000005e-05,
      "loss": -111.057,
      "step": 61110
    },
    {
      "epoch": 4.8896,
      "grad_norm": 78.22360229492188,
      "learning_rate": 3.7013333333333334e-05,
      "loss": -111.6985,
      "step": 61120
    },
    {
      "epoch": 4.8904,
      "grad_norm": 63.26586151123047,
      "learning_rate": 3.698666666666667e-05,
      "loss": -111.4748,
      "step": 61130
    },
    {
      "epoch": 4.8911999999999995,
      "grad_norm": 49.97197723388672,
      "learning_rate": 3.696e-05,
      "loss": -111.0728,
      "step": 61140
    },
    {
      "epoch": 4.892,
      "grad_norm": 89.73973083496094,
      "learning_rate": 3.6933333333333334e-05,
      "loss": -110.9502,
      "step": 61150
    },
    {
      "epoch": 4.8928,
      "grad_norm": 86.899658203125,
      "learning_rate": 3.690666666666667e-05,
      "loss": -111.5374,
      "step": 61160
    },
    {
      "epoch": 4.8936,
      "grad_norm": 31.66904067993164,
      "learning_rate": 3.6880000000000006e-05,
      "loss": -110.3549,
      "step": 61170
    },
    {
      "epoch": 4.8944,
      "grad_norm": 56.094520568847656,
      "learning_rate": 3.6853333333333335e-05,
      "loss": -111.7731,
      "step": 61180
    },
    {
      "epoch": 4.8952,
      "grad_norm": 78.44845581054688,
      "learning_rate": 3.682666666666667e-05,
      "loss": -111.0388,
      "step": 61190
    },
    {
      "epoch": 4.896,
      "grad_norm": 21.418676376342773,
      "learning_rate": 3.68e-05,
      "loss": -111.3069,
      "step": 61200
    },
    {
      "epoch": 4.8968,
      "grad_norm": 78.78868103027344,
      "learning_rate": 3.6773333333333336e-05,
      "loss": -111.311,
      "step": 61210
    },
    {
      "epoch": 4.8976,
      "grad_norm": 59.612091064453125,
      "learning_rate": 3.6746666666666665e-05,
      "loss": -112.1625,
      "step": 61220
    },
    {
      "epoch": 4.8984,
      "grad_norm": 122.7593002319336,
      "learning_rate": 3.672000000000001e-05,
      "loss": -112.2948,
      "step": 61230
    },
    {
      "epoch": 4.8992,
      "grad_norm": 162.623291015625,
      "learning_rate": 3.669333333333334e-05,
      "loss": -112.1127,
      "step": 61240
    },
    {
      "epoch": 4.9,
      "grad_norm": 31.297847747802734,
      "learning_rate": 3.6666666666666666e-05,
      "loss": -111.3083,
      "step": 61250
    },
    {
      "epoch": 4.9008,
      "grad_norm": 271.4771423339844,
      "learning_rate": 3.664e-05,
      "loss": -111.7292,
      "step": 61260
    },
    {
      "epoch": 4.9016,
      "grad_norm": 49.980003356933594,
      "learning_rate": 3.661333333333333e-05,
      "loss": -111.3321,
      "step": 61270
    },
    {
      "epoch": 4.9024,
      "grad_norm": 129.01840209960938,
      "learning_rate": 3.6586666666666666e-05,
      "loss": -111.3417,
      "step": 61280
    },
    {
      "epoch": 4.9032,
      "grad_norm": 1593.242431640625,
      "learning_rate": 3.656e-05,
      "loss": -111.0716,
      "step": 61290
    },
    {
      "epoch": 4.904,
      "grad_norm": 27.639982223510742,
      "learning_rate": 3.653333333333334e-05,
      "loss": -111.3633,
      "step": 61300
    },
    {
      "epoch": 4.9048,
      "grad_norm": 59.19316864013672,
      "learning_rate": 3.650666666666667e-05,
      "loss": -111.6833,
      "step": 61310
    },
    {
      "epoch": 4.9056,
      "grad_norm": 49.80926513671875,
      "learning_rate": 3.648e-05,
      "loss": -112.1873,
      "step": 61320
    },
    {
      "epoch": 4.9064,
      "grad_norm": 27.856508255004883,
      "learning_rate": 3.645333333333333e-05,
      "loss": -111.3351,
      "step": 61330
    },
    {
      "epoch": 4.9072,
      "grad_norm": 110.71226501464844,
      "learning_rate": 3.642666666666667e-05,
      "loss": -111.3829,
      "step": 61340
    },
    {
      "epoch": 4.908,
      "grad_norm": 53.69419860839844,
      "learning_rate": 3.6400000000000004e-05,
      "loss": -111.9069,
      "step": 61350
    },
    {
      "epoch": 4.9088,
      "grad_norm": 86.90731811523438,
      "learning_rate": 3.637333333333334e-05,
      "loss": -111.9518,
      "step": 61360
    },
    {
      "epoch": 4.9096,
      "grad_norm": 57.510738372802734,
      "learning_rate": 3.634666666666667e-05,
      "loss": -111.0531,
      "step": 61370
    },
    {
      "epoch": 4.9104,
      "grad_norm": 211.74539184570312,
      "learning_rate": 3.6320000000000005e-05,
      "loss": -111.7081,
      "step": 61380
    },
    {
      "epoch": 4.9112,
      "grad_norm": 576.8167114257812,
      "learning_rate": 3.6293333333333334e-05,
      "loss": -112.0624,
      "step": 61390
    },
    {
      "epoch": 4.912,
      "grad_norm": 35.58741760253906,
      "learning_rate": 3.626666666666667e-05,
      "loss": -111.5532,
      "step": 61400
    },
    {
      "epoch": 4.9128,
      "grad_norm": 25.13129997253418,
      "learning_rate": 3.624e-05,
      "loss": -112.425,
      "step": 61410
    },
    {
      "epoch": 4.9136,
      "grad_norm": 50.07550048828125,
      "learning_rate": 3.6213333333333334e-05,
      "loss": -111.9994,
      "step": 61420
    },
    {
      "epoch": 4.9144,
      "grad_norm": 54.081787109375,
      "learning_rate": 3.618666666666667e-05,
      "loss": -111.5641,
      "step": 61430
    },
    {
      "epoch": 4.9152000000000005,
      "grad_norm": 25.83567237854004,
      "learning_rate": 3.616e-05,
      "loss": -112.2028,
      "step": 61440
    },
    {
      "epoch": 4.916,
      "grad_norm": 43.08897018432617,
      "learning_rate": 3.6133333333333335e-05,
      "loss": -112.0712,
      "step": 61450
    },
    {
      "epoch": 4.9168,
      "grad_norm": 129.8297882080078,
      "learning_rate": 3.6106666666666664e-05,
      "loss": -111.0371,
      "step": 61460
    },
    {
      "epoch": 4.9176,
      "grad_norm": 45.79728698730469,
      "learning_rate": 3.608e-05,
      "loss": -111.4757,
      "step": 61470
    },
    {
      "epoch": 4.9184,
      "grad_norm": 93.20085906982422,
      "learning_rate": 3.6053333333333336e-05,
      "loss": -112.1561,
      "step": 61480
    },
    {
      "epoch": 4.9192,
      "grad_norm": 48.5576057434082,
      "learning_rate": 3.602666666666667e-05,
      "loss": -110.7439,
      "step": 61490
    },
    {
      "epoch": 4.92,
      "grad_norm": 52.52164840698242,
      "learning_rate": 3.6e-05,
      "loss": -111.633,
      "step": 61500
    },
    {
      "epoch": 4.9208,
      "grad_norm": 68.9629898071289,
      "learning_rate": 3.597333333333334e-05,
      "loss": -111.7413,
      "step": 61510
    },
    {
      "epoch": 4.9216,
      "grad_norm": 26.59780502319336,
      "learning_rate": 3.5946666666666666e-05,
      "loss": -111.8265,
      "step": 61520
    },
    {
      "epoch": 4.9224,
      "grad_norm": 88.6764144897461,
      "learning_rate": 3.592e-05,
      "loss": -112.4622,
      "step": 61530
    },
    {
      "epoch": 4.9232,
      "grad_norm": 215.17886352539062,
      "learning_rate": 3.589333333333334e-05,
      "loss": -112.7988,
      "step": 61540
    },
    {
      "epoch": 4.924,
      "grad_norm": 26.559877395629883,
      "learning_rate": 3.586666666666667e-05,
      "loss": -110.5661,
      "step": 61550
    },
    {
      "epoch": 4.9248,
      "grad_norm": 132.54212951660156,
      "learning_rate": 3.584e-05,
      "loss": -111.0582,
      "step": 61560
    },
    {
      "epoch": 4.9256,
      "grad_norm": 84.63724517822266,
      "learning_rate": 3.581333333333334e-05,
      "loss": -110.7509,
      "step": 61570
    },
    {
      "epoch": 4.9264,
      "grad_norm": 62.64435958862305,
      "learning_rate": 3.578666666666667e-05,
      "loss": -111.4891,
      "step": 61580
    },
    {
      "epoch": 4.9272,
      "grad_norm": 93.08106231689453,
      "learning_rate": 3.5759999999999996e-05,
      "loss": -111.4189,
      "step": 61590
    },
    {
      "epoch": 4.928,
      "grad_norm": 47.43373489379883,
      "learning_rate": 3.573333333333333e-05,
      "loss": -112.0116,
      "step": 61600
    },
    {
      "epoch": 4.9288,
      "grad_norm": 32.934661865234375,
      "learning_rate": 3.570666666666667e-05,
      "loss": -111.3858,
      "step": 61610
    },
    {
      "epoch": 4.9296,
      "grad_norm": 189.37811279296875,
      "learning_rate": 3.5680000000000004e-05,
      "loss": -112.0287,
      "step": 61620
    },
    {
      "epoch": 4.9304,
      "grad_norm": 119.26508331298828,
      "learning_rate": 3.565333333333333e-05,
      "loss": -111.5854,
      "step": 61630
    },
    {
      "epoch": 4.9312000000000005,
      "grad_norm": 23.307809829711914,
      "learning_rate": 3.562666666666667e-05,
      "loss": -111.1117,
      "step": 61640
    },
    {
      "epoch": 4.932,
      "grad_norm": 37.821720123291016,
      "learning_rate": 3.56e-05,
      "loss": -112.0374,
      "step": 61650
    },
    {
      "epoch": 4.9328,
      "grad_norm": 29.458471298217773,
      "learning_rate": 3.5573333333333334e-05,
      "loss": -111.7439,
      "step": 61660
    },
    {
      "epoch": 4.9336,
      "grad_norm": 30.77006721496582,
      "learning_rate": 3.554666666666667e-05,
      "loss": -111.6707,
      "step": 61670
    },
    {
      "epoch": 4.9344,
      "grad_norm": 46.94171905517578,
      "learning_rate": 3.5520000000000006e-05,
      "loss": -111.2604,
      "step": 61680
    },
    {
      "epoch": 4.9352,
      "grad_norm": 64.23593139648438,
      "learning_rate": 3.5493333333333335e-05,
      "loss": -110.5707,
      "step": 61690
    },
    {
      "epoch": 4.936,
      "grad_norm": 32.654075622558594,
      "learning_rate": 3.546666666666667e-05,
      "loss": -111.1047,
      "step": 61700
    },
    {
      "epoch": 4.9368,
      "grad_norm": 25.231000900268555,
      "learning_rate": 3.544e-05,
      "loss": -112.5281,
      "step": 61710
    },
    {
      "epoch": 4.9376,
      "grad_norm": 71.56059265136719,
      "learning_rate": 3.5413333333333335e-05,
      "loss": -111.3319,
      "step": 61720
    },
    {
      "epoch": 4.9384,
      "grad_norm": 73.45298767089844,
      "learning_rate": 3.538666666666667e-05,
      "loss": -111.8605,
      "step": 61730
    },
    {
      "epoch": 4.9392,
      "grad_norm": 64.0487060546875,
      "learning_rate": 3.536000000000001e-05,
      "loss": -112.0354,
      "step": 61740
    },
    {
      "epoch": 4.9399999999999995,
      "grad_norm": 94.81363677978516,
      "learning_rate": 3.5333333333333336e-05,
      "loss": -111.5136,
      "step": 61750
    },
    {
      "epoch": 4.9408,
      "grad_norm": 267.0841064453125,
      "learning_rate": 3.5306666666666665e-05,
      "loss": -111.5526,
      "step": 61760
    },
    {
      "epoch": 4.9416,
      "grad_norm": 55.28675842285156,
      "learning_rate": 3.528e-05,
      "loss": -111.849,
      "step": 61770
    },
    {
      "epoch": 4.9424,
      "grad_norm": 38.078857421875,
      "learning_rate": 3.525333333333333e-05,
      "loss": -111.2002,
      "step": 61780
    },
    {
      "epoch": 4.9432,
      "grad_norm": 103.9393310546875,
      "learning_rate": 3.5226666666666666e-05,
      "loss": -112.2921,
      "step": 61790
    },
    {
      "epoch": 4.944,
      "grad_norm": 107.44639587402344,
      "learning_rate": 3.52e-05,
      "loss": -112.1655,
      "step": 61800
    },
    {
      "epoch": 4.9448,
      "grad_norm": 52.194969177246094,
      "learning_rate": 3.517333333333334e-05,
      "loss": -111.5155,
      "step": 61810
    },
    {
      "epoch": 4.9456,
      "grad_norm": 75.2642593383789,
      "learning_rate": 3.514666666666667e-05,
      "loss": -111.8392,
      "step": 61820
    },
    {
      "epoch": 4.9464,
      "grad_norm": 37.272605895996094,
      "learning_rate": 3.512e-05,
      "loss": -112.4189,
      "step": 61830
    },
    {
      "epoch": 4.9472000000000005,
      "grad_norm": 55.06951904296875,
      "learning_rate": 3.509333333333333e-05,
      "loss": -111.976,
      "step": 61840
    },
    {
      "epoch": 4.948,
      "grad_norm": 47.73903274536133,
      "learning_rate": 3.506666666666667e-05,
      "loss": -110.6896,
      "step": 61850
    },
    {
      "epoch": 4.9488,
      "grad_norm": 131.26524353027344,
      "learning_rate": 3.504e-05,
      "loss": -111.0483,
      "step": 61860
    },
    {
      "epoch": 4.9496,
      "grad_norm": 22.760202407836914,
      "learning_rate": 3.501333333333334e-05,
      "loss": -112.3959,
      "step": 61870
    },
    {
      "epoch": 4.9504,
      "grad_norm": 80.22664642333984,
      "learning_rate": 3.498666666666667e-05,
      "loss": -110.8216,
      "step": 61880
    },
    {
      "epoch": 4.9512,
      "grad_norm": 36.0639762878418,
      "learning_rate": 3.4960000000000004e-05,
      "loss": -112.2061,
      "step": 61890
    },
    {
      "epoch": 4.952,
      "grad_norm": 81.84066772460938,
      "learning_rate": 3.493333333333333e-05,
      "loss": -111.8894,
      "step": 61900
    },
    {
      "epoch": 4.9528,
      "grad_norm": 54.09641647338867,
      "learning_rate": 3.490666666666667e-05,
      "loss": -111.297,
      "step": 61910
    },
    {
      "epoch": 4.9536,
      "grad_norm": 22.51938247680664,
      "learning_rate": 3.4880000000000005e-05,
      "loss": -111.3579,
      "step": 61920
    },
    {
      "epoch": 4.9544,
      "grad_norm": 124.5826644897461,
      "learning_rate": 3.4853333333333334e-05,
      "loss": -111.2761,
      "step": 61930
    },
    {
      "epoch": 4.9552,
      "grad_norm": 45.307525634765625,
      "learning_rate": 3.482666666666667e-05,
      "loss": -111.8455,
      "step": 61940
    },
    {
      "epoch": 4.9559999999999995,
      "grad_norm": 111.02659606933594,
      "learning_rate": 3.48e-05,
      "loss": -111.3094,
      "step": 61950
    },
    {
      "epoch": 4.9568,
      "grad_norm": 43.25931930541992,
      "learning_rate": 3.4773333333333335e-05,
      "loss": -111.4157,
      "step": 61960
    },
    {
      "epoch": 4.9576,
      "grad_norm": 80.0516357421875,
      "learning_rate": 3.4746666666666664e-05,
      "loss": -112.323,
      "step": 61970
    },
    {
      "epoch": 4.9584,
      "grad_norm": 71.67086029052734,
      "learning_rate": 3.472e-05,
      "loss": -111.7124,
      "step": 61980
    },
    {
      "epoch": 4.9592,
      "grad_norm": 64.14671325683594,
      "learning_rate": 3.4693333333333335e-05,
      "loss": -112.1554,
      "step": 61990
    },
    {
      "epoch": 4.96,
      "grad_norm": 31.42597770690918,
      "learning_rate": 3.466666666666667e-05,
      "loss": -111.957,
      "step": 62000
    },
    {
      "epoch": 4.9608,
      "grad_norm": 25.113426208496094,
      "learning_rate": 3.464e-05,
      "loss": -111.7625,
      "step": 62010
    },
    {
      "epoch": 4.9616,
      "grad_norm": 101.0354995727539,
      "learning_rate": 3.4613333333333336e-05,
      "loss": -111.6399,
      "step": 62020
    },
    {
      "epoch": 4.9624,
      "grad_norm": 75.3880615234375,
      "learning_rate": 3.4586666666666665e-05,
      "loss": -112.2316,
      "step": 62030
    },
    {
      "epoch": 4.9632,
      "grad_norm": 52.48059844970703,
      "learning_rate": 3.456e-05,
      "loss": -111.4611,
      "step": 62040
    },
    {
      "epoch": 4.964,
      "grad_norm": 74.74168395996094,
      "learning_rate": 3.453333333333334e-05,
      "loss": -112.0509,
      "step": 62050
    },
    {
      "epoch": 4.9648,
      "grad_norm": 66.73234558105469,
      "learning_rate": 3.450666666666667e-05,
      "loss": -111.8166,
      "step": 62060
    },
    {
      "epoch": 4.9656,
      "grad_norm": 87.57311248779297,
      "learning_rate": 3.448e-05,
      "loss": -111.5136,
      "step": 62070
    },
    {
      "epoch": 4.9664,
      "grad_norm": 29.08595085144043,
      "learning_rate": 3.445333333333334e-05,
      "loss": -110.5248,
      "step": 62080
    },
    {
      "epoch": 4.9672,
      "grad_norm": 58.97145080566406,
      "learning_rate": 3.442666666666667e-05,
      "loss": -112.1214,
      "step": 62090
    },
    {
      "epoch": 4.968,
      "grad_norm": 248.5063934326172,
      "learning_rate": 3.4399999999999996e-05,
      "loss": -111.7799,
      "step": 62100
    },
    {
      "epoch": 4.9688,
      "grad_norm": 41.86231994628906,
      "learning_rate": 3.437333333333334e-05,
      "loss": -111.8069,
      "step": 62110
    },
    {
      "epoch": 4.9696,
      "grad_norm": 49.14693069458008,
      "learning_rate": 3.434666666666667e-05,
      "loss": -111.6441,
      "step": 62120
    },
    {
      "epoch": 4.9704,
      "grad_norm": 70.27332305908203,
      "learning_rate": 3.4320000000000003e-05,
      "loss": -110.9763,
      "step": 62130
    },
    {
      "epoch": 4.9712,
      "grad_norm": 54.3537712097168,
      "learning_rate": 3.429333333333333e-05,
      "loss": -111.5873,
      "step": 62140
    },
    {
      "epoch": 4.9719999999999995,
      "grad_norm": 36.46175765991211,
      "learning_rate": 3.426666666666667e-05,
      "loss": -111.8295,
      "step": 62150
    },
    {
      "epoch": 4.9728,
      "grad_norm": 100.4912338256836,
      "learning_rate": 3.424e-05,
      "loss": -111.9323,
      "step": 62160
    },
    {
      "epoch": 4.9736,
      "grad_norm": 77.29116821289062,
      "learning_rate": 3.421333333333333e-05,
      "loss": -111.7874,
      "step": 62170
    },
    {
      "epoch": 4.9744,
      "grad_norm": 46.86902618408203,
      "learning_rate": 3.418666666666667e-05,
      "loss": -111.66,
      "step": 62180
    },
    {
      "epoch": 4.9752,
      "grad_norm": 24.637771606445312,
      "learning_rate": 3.4160000000000005e-05,
      "loss": -111.3314,
      "step": 62190
    },
    {
      "epoch": 4.976,
      "grad_norm": 431.21380615234375,
      "learning_rate": 3.4133333333333334e-05,
      "loss": -111.3155,
      "step": 62200
    },
    {
      "epoch": 4.9768,
      "grad_norm": 125.52493286132812,
      "learning_rate": 3.410666666666667e-05,
      "loss": -111.7086,
      "step": 62210
    },
    {
      "epoch": 4.9776,
      "grad_norm": 238.9517364501953,
      "learning_rate": 3.408e-05,
      "loss": -112.454,
      "step": 62220
    },
    {
      "epoch": 4.9784,
      "grad_norm": 236.14114379882812,
      "learning_rate": 3.4053333333333335e-05,
      "loss": -112.26,
      "step": 62230
    },
    {
      "epoch": 4.9792,
      "grad_norm": 79.27980041503906,
      "learning_rate": 3.402666666666667e-05,
      "loss": -111.2818,
      "step": 62240
    },
    {
      "epoch": 4.98,
      "grad_norm": 47.870208740234375,
      "learning_rate": 3.4000000000000007e-05,
      "loss": -111.2901,
      "step": 62250
    },
    {
      "epoch": 4.9808,
      "grad_norm": 61.102420806884766,
      "learning_rate": 3.3973333333333336e-05,
      "loss": -111.1217,
      "step": 62260
    },
    {
      "epoch": 4.9816,
      "grad_norm": 55.157691955566406,
      "learning_rate": 3.394666666666667e-05,
      "loss": -111.7059,
      "step": 62270
    },
    {
      "epoch": 4.9824,
      "grad_norm": 39.46124267578125,
      "learning_rate": 3.392e-05,
      "loss": -111.3063,
      "step": 62280
    },
    {
      "epoch": 4.9832,
      "grad_norm": 197.0595245361328,
      "learning_rate": 3.389333333333333e-05,
      "loss": -111.6281,
      "step": 62290
    },
    {
      "epoch": 4.984,
      "grad_norm": 37.60219192504883,
      "learning_rate": 3.3866666666666665e-05,
      "loss": -111.0575,
      "step": 62300
    },
    {
      "epoch": 4.9848,
      "grad_norm": 27.535953521728516,
      "learning_rate": 3.384e-05,
      "loss": -112.1579,
      "step": 62310
    },
    {
      "epoch": 4.9856,
      "grad_norm": 46.950008392333984,
      "learning_rate": 3.381333333333334e-05,
      "loss": -111.8359,
      "step": 62320
    },
    {
      "epoch": 4.9864,
      "grad_norm": 75.27835845947266,
      "learning_rate": 3.3786666666666666e-05,
      "loss": -112.139,
      "step": 62330
    },
    {
      "epoch": 4.9872,
      "grad_norm": 39.071537017822266,
      "learning_rate": 3.376e-05,
      "loss": -110.9013,
      "step": 62340
    },
    {
      "epoch": 4.9879999999999995,
      "grad_norm": 33.743263244628906,
      "learning_rate": 3.373333333333333e-05,
      "loss": -111.849,
      "step": 62350
    },
    {
      "epoch": 4.9888,
      "grad_norm": 25.127412796020508,
      "learning_rate": 3.370666666666667e-05,
      "loss": -111.7424,
      "step": 62360
    },
    {
      "epoch": 4.9896,
      "grad_norm": 38.26283645629883,
      "learning_rate": 3.368e-05,
      "loss": -111.3022,
      "step": 62370
    },
    {
      "epoch": 4.9904,
      "grad_norm": 42.23159408569336,
      "learning_rate": 3.365333333333334e-05,
      "loss": -111.1613,
      "step": 62380
    },
    {
      "epoch": 4.9912,
      "grad_norm": 27.353256225585938,
      "learning_rate": 3.362666666666667e-05,
      "loss": -111.3178,
      "step": 62390
    },
    {
      "epoch": 4.992,
      "grad_norm": 95.26262664794922,
      "learning_rate": 3.3600000000000004e-05,
      "loss": -111.186,
      "step": 62400
    },
    {
      "epoch": 4.9928,
      "grad_norm": 50.250003814697266,
      "learning_rate": 3.357333333333333e-05,
      "loss": -111.8319,
      "step": 62410
    },
    {
      "epoch": 4.9936,
      "grad_norm": 36.588111877441406,
      "learning_rate": 3.354666666666667e-05,
      "loss": -111.6627,
      "step": 62420
    },
    {
      "epoch": 4.9944,
      "grad_norm": 55.56379699707031,
      "learning_rate": 3.3520000000000004e-05,
      "loss": -111.5904,
      "step": 62430
    },
    {
      "epoch": 4.9952,
      "grad_norm": 92.29998779296875,
      "learning_rate": 3.349333333333334e-05,
      "loss": -111.3524,
      "step": 62440
    },
    {
      "epoch": 4.996,
      "grad_norm": 40.975975036621094,
      "learning_rate": 3.346666666666667e-05,
      "loss": -111.1024,
      "step": 62450
    },
    {
      "epoch": 4.9968,
      "grad_norm": 81.70484161376953,
      "learning_rate": 3.344e-05,
      "loss": -111.3725,
      "step": 62460
    },
    {
      "epoch": 4.9976,
      "grad_norm": 76.0218276977539,
      "learning_rate": 3.3413333333333334e-05,
      "loss": -111.0437,
      "step": 62470
    },
    {
      "epoch": 4.9984,
      "grad_norm": 50.90594482421875,
      "learning_rate": 3.338666666666666e-05,
      "loss": -111.487,
      "step": 62480
    },
    {
      "epoch": 4.9992,
      "grad_norm": 38.191184997558594,
      "learning_rate": 3.336e-05,
      "loss": -111.845,
      "step": 62490
    },
    {
      "epoch": 5.0,
      "grad_norm": 85.0262222290039,
      "learning_rate": 3.3333333333333335e-05,
      "loss": -112.3102,
      "step": 62500
    },
    {
      "epoch": 5.0008,
      "grad_norm": 73.137939453125,
      "learning_rate": 3.330666666666667e-05,
      "loss": -111.9922,
      "step": 62510
    },
    {
      "epoch": 5.0016,
      "grad_norm": 35.35255432128906,
      "learning_rate": 3.328e-05,
      "loss": -111.9933,
      "step": 62520
    },
    {
      "epoch": 5.0024,
      "grad_norm": 156.87965393066406,
      "learning_rate": 3.3253333333333336e-05,
      "loss": -111.9248,
      "step": 62530
    },
    {
      "epoch": 5.0032,
      "grad_norm": 24.886442184448242,
      "learning_rate": 3.3226666666666665e-05,
      "loss": -112.0625,
      "step": 62540
    },
    {
      "epoch": 5.004,
      "grad_norm": 67.7888412475586,
      "learning_rate": 3.32e-05,
      "loss": -112.3586,
      "step": 62550
    },
    {
      "epoch": 5.0048,
      "grad_norm": 29.074172973632812,
      "learning_rate": 3.3173333333333336e-05,
      "loss": -111.6368,
      "step": 62560
    },
    {
      "epoch": 5.0056,
      "grad_norm": 83.7547607421875,
      "learning_rate": 3.314666666666667e-05,
      "loss": -111.837,
      "step": 62570
    },
    {
      "epoch": 5.0064,
      "grad_norm": 31.29510498046875,
      "learning_rate": 3.312e-05,
      "loss": -110.9943,
      "step": 62580
    },
    {
      "epoch": 5.0072,
      "grad_norm": 83.2486343383789,
      "learning_rate": 3.309333333333334e-05,
      "loss": -111.4781,
      "step": 62590
    },
    {
      "epoch": 5.008,
      "grad_norm": 98.36214447021484,
      "learning_rate": 3.3066666666666666e-05,
      "loss": -111.1491,
      "step": 62600
    },
    {
      "epoch": 5.0088,
      "grad_norm": 80.66224670410156,
      "learning_rate": 3.304e-05,
      "loss": -111.8572,
      "step": 62610
    },
    {
      "epoch": 5.0096,
      "grad_norm": 67.77206420898438,
      "learning_rate": 3.301333333333334e-05,
      "loss": -112.0359,
      "step": 62620
    },
    {
      "epoch": 5.0104,
      "grad_norm": 41.28418731689453,
      "learning_rate": 3.298666666666667e-05,
      "loss": -111.7691,
      "step": 62630
    },
    {
      "epoch": 5.0112,
      "grad_norm": 67.84014892578125,
      "learning_rate": 3.296e-05,
      "loss": -112.7005,
      "step": 62640
    },
    {
      "epoch": 5.012,
      "grad_norm": 29.756376266479492,
      "learning_rate": 3.293333333333333e-05,
      "loss": -111.8071,
      "step": 62650
    },
    {
      "epoch": 5.0128,
      "grad_norm": 39.535823822021484,
      "learning_rate": 3.290666666666667e-05,
      "loss": -111.8395,
      "step": 62660
    },
    {
      "epoch": 5.0136,
      "grad_norm": 68.09544372558594,
      "learning_rate": 3.288e-05,
      "loss": -111.9223,
      "step": 62670
    },
    {
      "epoch": 5.0144,
      "grad_norm": 36.675968170166016,
      "learning_rate": 3.285333333333333e-05,
      "loss": -111.6528,
      "step": 62680
    },
    {
      "epoch": 5.0152,
      "grad_norm": 29.091089248657227,
      "learning_rate": 3.282666666666667e-05,
      "loss": -111.5848,
      "step": 62690
    },
    {
      "epoch": 5.016,
      "grad_norm": 65.4836654663086,
      "learning_rate": 3.2800000000000004e-05,
      "loss": -111.5091,
      "step": 62700
    },
    {
      "epoch": 5.0168,
      "grad_norm": 87.90404510498047,
      "learning_rate": 3.2773333333333334e-05,
      "loss": -111.7229,
      "step": 62710
    },
    {
      "epoch": 5.0176,
      "grad_norm": 41.683677673339844,
      "learning_rate": 3.274666666666667e-05,
      "loss": -111.0539,
      "step": 62720
    },
    {
      "epoch": 5.0184,
      "grad_norm": 29.927888870239258,
      "learning_rate": 3.272e-05,
      "loss": -111.3683,
      "step": 62730
    },
    {
      "epoch": 5.0192,
      "grad_norm": 32.57502746582031,
      "learning_rate": 3.2693333333333334e-05,
      "loss": -111.6429,
      "step": 62740
    },
    {
      "epoch": 5.02,
      "grad_norm": 84.30674743652344,
      "learning_rate": 3.266666666666667e-05,
      "loss": -111.4593,
      "step": 62750
    },
    {
      "epoch": 5.0208,
      "grad_norm": 203.30685424804688,
      "learning_rate": 3.2640000000000006e-05,
      "loss": -111.0646,
      "step": 62760
    },
    {
      "epoch": 5.0216,
      "grad_norm": 70.84395599365234,
      "learning_rate": 3.2613333333333335e-05,
      "loss": -111.6012,
      "step": 62770
    },
    {
      "epoch": 5.0224,
      "grad_norm": 74.23603057861328,
      "learning_rate": 3.258666666666667e-05,
      "loss": -111.8107,
      "step": 62780
    },
    {
      "epoch": 5.0232,
      "grad_norm": 65.7898941040039,
      "learning_rate": 3.256e-05,
      "loss": -111.7868,
      "step": 62790
    },
    {
      "epoch": 5.024,
      "grad_norm": 25.35407066345215,
      "learning_rate": 3.253333333333333e-05,
      "loss": -111.5404,
      "step": 62800
    },
    {
      "epoch": 5.0248,
      "grad_norm": 47.69444274902344,
      "learning_rate": 3.250666666666667e-05,
      "loss": -111.8615,
      "step": 62810
    },
    {
      "epoch": 5.0256,
      "grad_norm": 22.29322624206543,
      "learning_rate": 3.248e-05,
      "loss": -111.5459,
      "step": 62820
    },
    {
      "epoch": 5.0264,
      "grad_norm": 28.864194869995117,
      "learning_rate": 3.2453333333333337e-05,
      "loss": -111.7764,
      "step": 62830
    },
    {
      "epoch": 5.0272,
      "grad_norm": 25.674097061157227,
      "learning_rate": 3.2426666666666666e-05,
      "loss": -112.5474,
      "step": 62840
    },
    {
      "epoch": 5.028,
      "grad_norm": 83.15371704101562,
      "learning_rate": 3.24e-05,
      "loss": -112.1499,
      "step": 62850
    },
    {
      "epoch": 5.0288,
      "grad_norm": 28.59939193725586,
      "learning_rate": 3.237333333333333e-05,
      "loss": -110.6344,
      "step": 62860
    },
    {
      "epoch": 5.0296,
      "grad_norm": 55.78223419189453,
      "learning_rate": 3.2346666666666666e-05,
      "loss": -111.8193,
      "step": 62870
    },
    {
      "epoch": 5.0304,
      "grad_norm": 106.31189727783203,
      "learning_rate": 3.232e-05,
      "loss": -111.1343,
      "step": 62880
    },
    {
      "epoch": 5.0312,
      "grad_norm": 40.86790466308594,
      "learning_rate": 3.229333333333334e-05,
      "loss": -111.8732,
      "step": 62890
    },
    {
      "epoch": 5.032,
      "grad_norm": 48.168800354003906,
      "learning_rate": 3.226666666666667e-05,
      "loss": -111.9176,
      "step": 62900
    },
    {
      "epoch": 5.0328,
      "grad_norm": 90.05598449707031,
      "learning_rate": 3.224e-05,
      "loss": -111.7151,
      "step": 62910
    },
    {
      "epoch": 5.0336,
      "grad_norm": 31.237415313720703,
      "learning_rate": 3.221333333333333e-05,
      "loss": -111.1553,
      "step": 62920
    },
    {
      "epoch": 5.0344,
      "grad_norm": 31.04222869873047,
      "learning_rate": 3.218666666666667e-05,
      "loss": -111.4791,
      "step": 62930
    },
    {
      "epoch": 5.0352,
      "grad_norm": 46.72949981689453,
      "learning_rate": 3.2160000000000004e-05,
      "loss": -112.4458,
      "step": 62940
    },
    {
      "epoch": 5.036,
      "grad_norm": 87.04901123046875,
      "learning_rate": 3.213333333333334e-05,
      "loss": -111.7868,
      "step": 62950
    },
    {
      "epoch": 5.0368,
      "grad_norm": 31.733701705932617,
      "learning_rate": 3.210666666666667e-05,
      "loss": -111.418,
      "step": 62960
    },
    {
      "epoch": 5.0376,
      "grad_norm": 97.40608215332031,
      "learning_rate": 3.208e-05,
      "loss": -112.5113,
      "step": 62970
    },
    {
      "epoch": 5.0384,
      "grad_norm": 54.516178131103516,
      "learning_rate": 3.2053333333333334e-05,
      "loss": -111.313,
      "step": 62980
    },
    {
      "epoch": 5.0392,
      "grad_norm": 53.40764236450195,
      "learning_rate": 3.202666666666666e-05,
      "loss": -111.3949,
      "step": 62990
    },
    {
      "epoch": 5.04,
      "grad_norm": 100.68968963623047,
      "learning_rate": 3.2000000000000005e-05,
      "loss": -111.634,
      "step": 63000
    },
    {
      "epoch": 5.0408,
      "grad_norm": 44.348270416259766,
      "learning_rate": 3.1973333333333334e-05,
      "loss": -111.7997,
      "step": 63010
    },
    {
      "epoch": 5.0416,
      "grad_norm": 70.34959411621094,
      "learning_rate": 3.194666666666667e-05,
      "loss": -112.0159,
      "step": 63020
    },
    {
      "epoch": 5.0424,
      "grad_norm": 125.98197174072266,
      "learning_rate": 3.192e-05,
      "loss": -110.955,
      "step": 63030
    },
    {
      "epoch": 5.0432,
      "grad_norm": 116.09711456298828,
      "learning_rate": 3.1893333333333335e-05,
      "loss": -112.9031,
      "step": 63040
    },
    {
      "epoch": 5.044,
      "grad_norm": 31.252544403076172,
      "learning_rate": 3.1866666666666664e-05,
      "loss": -111.4767,
      "step": 63050
    },
    {
      "epoch": 5.0448,
      "grad_norm": 46.082984924316406,
      "learning_rate": 3.184e-05,
      "loss": -112.02,
      "step": 63060
    },
    {
      "epoch": 5.0456,
      "grad_norm": 40.05887985229492,
      "learning_rate": 3.1813333333333336e-05,
      "loss": -111.1439,
      "step": 63070
    },
    {
      "epoch": 5.0464,
      "grad_norm": 23.112247467041016,
      "learning_rate": 3.178666666666667e-05,
      "loss": -111.4601,
      "step": 63080
    },
    {
      "epoch": 5.0472,
      "grad_norm": 35.79068374633789,
      "learning_rate": 3.176e-05,
      "loss": -111.9297,
      "step": 63090
    },
    {
      "epoch": 5.048,
      "grad_norm": 45.42887496948242,
      "learning_rate": 3.173333333333334e-05,
      "loss": -111.3453,
      "step": 63100
    },
    {
      "epoch": 5.0488,
      "grad_norm": 38.31078338623047,
      "learning_rate": 3.1706666666666666e-05,
      "loss": -111.7862,
      "step": 63110
    },
    {
      "epoch": 5.0496,
      "grad_norm": 22.245433807373047,
      "learning_rate": 3.168e-05,
      "loss": -111.195,
      "step": 63120
    },
    {
      "epoch": 5.0504,
      "grad_norm": 104.96839904785156,
      "learning_rate": 3.165333333333334e-05,
      "loss": -112.043,
      "step": 63130
    },
    {
      "epoch": 5.0512,
      "grad_norm": 130.10516357421875,
      "learning_rate": 3.1626666666666667e-05,
      "loss": -112.3869,
      "step": 63140
    },
    {
      "epoch": 5.052,
      "grad_norm": 55.5014762878418,
      "learning_rate": 3.16e-05,
      "loss": -111.4381,
      "step": 63150
    },
    {
      "epoch": 5.0528,
      "grad_norm": 68.42589569091797,
      "learning_rate": 3.157333333333333e-05,
      "loss": -111.8335,
      "step": 63160
    },
    {
      "epoch": 5.0536,
      "grad_norm": 36.679649353027344,
      "learning_rate": 3.154666666666667e-05,
      "loss": -111.3584,
      "step": 63170
    },
    {
      "epoch": 5.0544,
      "grad_norm": 82.07955169677734,
      "learning_rate": 3.1519999999999996e-05,
      "loss": -111.0515,
      "step": 63180
    },
    {
      "epoch": 5.0552,
      "grad_norm": 58.73980712890625,
      "learning_rate": 3.149333333333334e-05,
      "loss": -111.5192,
      "step": 63190
    },
    {
      "epoch": 5.056,
      "grad_norm": 32.0208740234375,
      "learning_rate": 3.146666666666667e-05,
      "loss": -111.0756,
      "step": 63200
    },
    {
      "epoch": 5.0568,
      "grad_norm": 95.91219329833984,
      "learning_rate": 3.1440000000000004e-05,
      "loss": -111.7983,
      "step": 63210
    },
    {
      "epoch": 5.0576,
      "grad_norm": 90.13426208496094,
      "learning_rate": 3.141333333333333e-05,
      "loss": -111.2948,
      "step": 63220
    },
    {
      "epoch": 5.0584,
      "grad_norm": 106.76852416992188,
      "learning_rate": 3.138666666666667e-05,
      "loss": -111.6568,
      "step": 63230
    },
    {
      "epoch": 5.0592,
      "grad_norm": 26.800567626953125,
      "learning_rate": 3.136e-05,
      "loss": -111.9057,
      "step": 63240
    },
    {
      "epoch": 5.06,
      "grad_norm": 41.32603073120117,
      "learning_rate": 3.1333333333333334e-05,
      "loss": -111.1297,
      "step": 63250
    },
    {
      "epoch": 5.0608,
      "grad_norm": 81.1845932006836,
      "learning_rate": 3.130666666666667e-05,
      "loss": -112.0777,
      "step": 63260
    },
    {
      "epoch": 5.0616,
      "grad_norm": 52.99931335449219,
      "learning_rate": 3.1280000000000005e-05,
      "loss": -111.3711,
      "step": 63270
    },
    {
      "epoch": 5.0624,
      "grad_norm": 133.069091796875,
      "learning_rate": 3.1253333333333335e-05,
      "loss": -111.6148,
      "step": 63280
    },
    {
      "epoch": 5.0632,
      "grad_norm": 31.588298797607422,
      "learning_rate": 3.122666666666667e-05,
      "loss": -112.1445,
      "step": 63290
    },
    {
      "epoch": 5.064,
      "grad_norm": 81.35443115234375,
      "learning_rate": 3.12e-05,
      "loss": -112.4346,
      "step": 63300
    },
    {
      "epoch": 5.0648,
      "grad_norm": 29.04691505432129,
      "learning_rate": 3.1173333333333335e-05,
      "loss": -111.6203,
      "step": 63310
    },
    {
      "epoch": 5.0656,
      "grad_norm": 69.09415435791016,
      "learning_rate": 3.114666666666667e-05,
      "loss": -111.9534,
      "step": 63320
    },
    {
      "epoch": 5.0664,
      "grad_norm": 101.42385864257812,
      "learning_rate": 3.112e-05,
      "loss": -111.1782,
      "step": 63330
    },
    {
      "epoch": 5.0672,
      "grad_norm": 36.49594497680664,
      "learning_rate": 3.1093333333333336e-05,
      "loss": -111.4503,
      "step": 63340
    },
    {
      "epoch": 5.068,
      "grad_norm": 99.03304290771484,
      "learning_rate": 3.1066666666666665e-05,
      "loss": -112.1125,
      "step": 63350
    },
    {
      "epoch": 5.0688,
      "grad_norm": 68.05488586425781,
      "learning_rate": 3.104e-05,
      "loss": -111.8046,
      "step": 63360
    },
    {
      "epoch": 5.0696,
      "grad_norm": 24.651823043823242,
      "learning_rate": 3.101333333333333e-05,
      "loss": -111.8613,
      "step": 63370
    },
    {
      "epoch": 5.0704,
      "grad_norm": 115.70323181152344,
      "learning_rate": 3.098666666666667e-05,
      "loss": -112.1732,
      "step": 63380
    },
    {
      "epoch": 5.0712,
      "grad_norm": 48.16162872314453,
      "learning_rate": 3.096e-05,
      "loss": -110.8528,
      "step": 63390
    },
    {
      "epoch": 5.072,
      "grad_norm": 144.64012145996094,
      "learning_rate": 3.093333333333334e-05,
      "loss": -111.6758,
      "step": 63400
    },
    {
      "epoch": 5.0728,
      "grad_norm": 30.872411727905273,
      "learning_rate": 3.090666666666667e-05,
      "loss": -111.0729,
      "step": 63410
    },
    {
      "epoch": 5.0736,
      "grad_norm": 88.72674560546875,
      "learning_rate": 3.088e-05,
      "loss": -112.0371,
      "step": 63420
    },
    {
      "epoch": 5.0744,
      "grad_norm": 58.05028533935547,
      "learning_rate": 3.085333333333333e-05,
      "loss": -111.6089,
      "step": 63430
    },
    {
      "epoch": 5.0752,
      "grad_norm": 51.534271240234375,
      "learning_rate": 3.082666666666667e-05,
      "loss": -111.8135,
      "step": 63440
    },
    {
      "epoch": 5.076,
      "grad_norm": 153.08297729492188,
      "learning_rate": 3.08e-05,
      "loss": -112.1144,
      "step": 63450
    },
    {
      "epoch": 5.0768,
      "grad_norm": 102.87752532958984,
      "learning_rate": 3.077333333333334e-05,
      "loss": -111.2747,
      "step": 63460
    },
    {
      "epoch": 5.0776,
      "grad_norm": 56.74894332885742,
      "learning_rate": 3.074666666666667e-05,
      "loss": -111.7414,
      "step": 63470
    },
    {
      "epoch": 5.0784,
      "grad_norm": 43.439910888671875,
      "learning_rate": 3.072e-05,
      "loss": -112.1376,
      "step": 63480
    },
    {
      "epoch": 5.0792,
      "grad_norm": 54.10203170776367,
      "learning_rate": 3.069333333333333e-05,
      "loss": -112.1193,
      "step": 63490
    },
    {
      "epoch": 5.08,
      "grad_norm": 32.18984603881836,
      "learning_rate": 3.066666666666667e-05,
      "loss": -111.1008,
      "step": 63500
    },
    {
      "epoch": 5.0808,
      "grad_norm": 38.18913650512695,
      "learning_rate": 3.0640000000000005e-05,
      "loss": -111.7864,
      "step": 63510
    },
    {
      "epoch": 5.0816,
      "grad_norm": 79.2017822265625,
      "learning_rate": 3.0613333333333334e-05,
      "loss": -112.9235,
      "step": 63520
    },
    {
      "epoch": 5.0824,
      "grad_norm": 31.79387664794922,
      "learning_rate": 3.058666666666667e-05,
      "loss": -111.5561,
      "step": 63530
    },
    {
      "epoch": 5.0832,
      "grad_norm": 19.80623435974121,
      "learning_rate": 3.056e-05,
      "loss": -111.5178,
      "step": 63540
    },
    {
      "epoch": 5.084,
      "grad_norm": 78.09381866455078,
      "learning_rate": 3.0533333333333335e-05,
      "loss": -111.8342,
      "step": 63550
    },
    {
      "epoch": 5.0848,
      "grad_norm": 39.79607391357422,
      "learning_rate": 3.0506666666666667e-05,
      "loss": -111.5391,
      "step": 63560
    },
    {
      "epoch": 5.0856,
      "grad_norm": 27.892261505126953,
      "learning_rate": 3.0480000000000003e-05,
      "loss": -111.072,
      "step": 63570
    },
    {
      "epoch": 5.0864,
      "grad_norm": 55.77228927612305,
      "learning_rate": 3.0453333333333335e-05,
      "loss": -112.2031,
      "step": 63580
    },
    {
      "epoch": 5.0872,
      "grad_norm": 36.78946304321289,
      "learning_rate": 3.042666666666667e-05,
      "loss": -110.7009,
      "step": 63590
    },
    {
      "epoch": 5.088,
      "grad_norm": 87.74752807617188,
      "learning_rate": 3.04e-05,
      "loss": -112.0335,
      "step": 63600
    },
    {
      "epoch": 5.0888,
      "grad_norm": 36.82449722290039,
      "learning_rate": 3.0373333333333336e-05,
      "loss": -111.102,
      "step": 63610
    },
    {
      "epoch": 5.0896,
      "grad_norm": 24.157575607299805,
      "learning_rate": 3.034666666666667e-05,
      "loss": -112.1488,
      "step": 63620
    },
    {
      "epoch": 5.0904,
      "grad_norm": 29.174922943115234,
      "learning_rate": 3.0320000000000004e-05,
      "loss": -111.1242,
      "step": 63630
    },
    {
      "epoch": 5.0912,
      "grad_norm": 26.32572364807129,
      "learning_rate": 3.0293333333333334e-05,
      "loss": -111.7091,
      "step": 63640
    },
    {
      "epoch": 5.092,
      "grad_norm": 133.79464721679688,
      "learning_rate": 3.0266666666666666e-05,
      "loss": -111.6751,
      "step": 63650
    },
    {
      "epoch": 5.0928,
      "grad_norm": 72.53020477294922,
      "learning_rate": 3.0240000000000002e-05,
      "loss": -111.0748,
      "step": 63660
    },
    {
      "epoch": 5.0936,
      "grad_norm": 37.79368591308594,
      "learning_rate": 3.021333333333333e-05,
      "loss": -111.6673,
      "step": 63670
    },
    {
      "epoch": 5.0944,
      "grad_norm": 108.26782989501953,
      "learning_rate": 3.018666666666667e-05,
      "loss": -111.7617,
      "step": 63680
    },
    {
      "epoch": 5.0952,
      "grad_norm": 158.94479370117188,
      "learning_rate": 3.016e-05,
      "loss": -110.7638,
      "step": 63690
    },
    {
      "epoch": 5.096,
      "grad_norm": 37.17876434326172,
      "learning_rate": 3.0133333333333335e-05,
      "loss": -111.5135,
      "step": 63700
    },
    {
      "epoch": 5.0968,
      "grad_norm": 30.08966636657715,
      "learning_rate": 3.0106666666666668e-05,
      "loss": -111.5983,
      "step": 63710
    },
    {
      "epoch": 5.0976,
      "grad_norm": 82.1041030883789,
      "learning_rate": 3.0080000000000003e-05,
      "loss": -111.028,
      "step": 63720
    },
    {
      "epoch": 5.0984,
      "grad_norm": 29.05533218383789,
      "learning_rate": 3.0053333333333332e-05,
      "loss": -112.0694,
      "step": 63730
    },
    {
      "epoch": 5.0992,
      "grad_norm": 76.22550964355469,
      "learning_rate": 3.0026666666666668e-05,
      "loss": -111.7326,
      "step": 63740
    },
    {
      "epoch": 5.1,
      "grad_norm": 34.63150405883789,
      "learning_rate": 3e-05,
      "loss": -112.5904,
      "step": 63750
    },
    {
      "epoch": 5.1008,
      "grad_norm": 61.02642822265625,
      "learning_rate": 2.9973333333333337e-05,
      "loss": -111.7319,
      "step": 63760
    },
    {
      "epoch": 5.1016,
      "grad_norm": 34.70741653442383,
      "learning_rate": 2.9946666666666666e-05,
      "loss": -111.2925,
      "step": 63770
    },
    {
      "epoch": 5.1024,
      "grad_norm": 38.381622314453125,
      "learning_rate": 2.9920000000000005e-05,
      "loss": -111.4894,
      "step": 63780
    },
    {
      "epoch": 5.1032,
      "grad_norm": 87.63340759277344,
      "learning_rate": 2.9893333333333334e-05,
      "loss": -110.7182,
      "step": 63790
    },
    {
      "epoch": 5.104,
      "grad_norm": 91.99158477783203,
      "learning_rate": 2.986666666666667e-05,
      "loss": -111.2478,
      "step": 63800
    },
    {
      "epoch": 5.1048,
      "grad_norm": 23.573253631591797,
      "learning_rate": 2.9840000000000002e-05,
      "loss": -112.2176,
      "step": 63810
    },
    {
      "epoch": 5.1056,
      "grad_norm": 90.94365692138672,
      "learning_rate": 2.981333333333333e-05,
      "loss": -112.377,
      "step": 63820
    },
    {
      "epoch": 5.1064,
      "grad_norm": 57.472686767578125,
      "learning_rate": 2.9786666666666667e-05,
      "loss": -112.2699,
      "step": 63830
    },
    {
      "epoch": 5.1072,
      "grad_norm": 22.895275115966797,
      "learning_rate": 2.976e-05,
      "loss": -111.3605,
      "step": 63840
    },
    {
      "epoch": 5.108,
      "grad_norm": 33.68161392211914,
      "learning_rate": 2.9733333333333336e-05,
      "loss": -111.6979,
      "step": 63850
    },
    {
      "epoch": 5.1088,
      "grad_norm": 191.6911163330078,
      "learning_rate": 2.9706666666666665e-05,
      "loss": -112.3279,
      "step": 63860
    },
    {
      "epoch": 5.1096,
      "grad_norm": 83.65350341796875,
      "learning_rate": 2.9680000000000004e-05,
      "loss": -112.0933,
      "step": 63870
    },
    {
      "epoch": 5.1104,
      "grad_norm": 50.19844436645508,
      "learning_rate": 2.9653333333333333e-05,
      "loss": -111.6026,
      "step": 63880
    },
    {
      "epoch": 5.1112,
      "grad_norm": 79.3764877319336,
      "learning_rate": 2.962666666666667e-05,
      "loss": -111.3683,
      "step": 63890
    },
    {
      "epoch": 5.112,
      "grad_norm": 50.05437469482422,
      "learning_rate": 2.96e-05,
      "loss": -111.1289,
      "step": 63900
    },
    {
      "epoch": 5.1128,
      "grad_norm": 177.11024475097656,
      "learning_rate": 2.9573333333333337e-05,
      "loss": -111.5401,
      "step": 63910
    },
    {
      "epoch": 5.1136,
      "grad_norm": 168.0508270263672,
      "learning_rate": 2.9546666666666666e-05,
      "loss": -111.0664,
      "step": 63920
    },
    {
      "epoch": 5.1144,
      "grad_norm": 125.32665252685547,
      "learning_rate": 2.9520000000000002e-05,
      "loss": -111.7343,
      "step": 63930
    },
    {
      "epoch": 5.1152,
      "grad_norm": 68.58784484863281,
      "learning_rate": 2.9493333333333334e-05,
      "loss": -111.9406,
      "step": 63940
    },
    {
      "epoch": 5.116,
      "grad_norm": 108.49311065673828,
      "learning_rate": 2.946666666666667e-05,
      "loss": -111.2958,
      "step": 63950
    },
    {
      "epoch": 5.1168,
      "grad_norm": 45.99770736694336,
      "learning_rate": 2.944e-05,
      "loss": -111.9824,
      "step": 63960
    },
    {
      "epoch": 5.1176,
      "grad_norm": 76.25299072265625,
      "learning_rate": 2.941333333333334e-05,
      "loss": -112.0423,
      "step": 63970
    },
    {
      "epoch": 5.1184,
      "grad_norm": 181.902587890625,
      "learning_rate": 2.9386666666666668e-05,
      "loss": -112.8847,
      "step": 63980
    },
    {
      "epoch": 5.1192,
      "grad_norm": 139.80380249023438,
      "learning_rate": 2.9360000000000003e-05,
      "loss": -111.4775,
      "step": 63990
    },
    {
      "epoch": 5.12,
      "grad_norm": 33.702598571777344,
      "learning_rate": 2.9333333333333336e-05,
      "loss": -111.2419,
      "step": 64000
    },
    {
      "epoch": 5.1208,
      "grad_norm": 27.87488555908203,
      "learning_rate": 2.9306666666666665e-05,
      "loss": -111.9726,
      "step": 64010
    },
    {
      "epoch": 5.1216,
      "grad_norm": 63.928314208984375,
      "learning_rate": 2.928e-05,
      "loss": -112.1444,
      "step": 64020
    },
    {
      "epoch": 5.1224,
      "grad_norm": 92.24724578857422,
      "learning_rate": 2.9253333333333333e-05,
      "loss": -111.2863,
      "step": 64030
    },
    {
      "epoch": 5.1232,
      "grad_norm": 57.39370346069336,
      "learning_rate": 2.922666666666667e-05,
      "loss": -111.8067,
      "step": 64040
    },
    {
      "epoch": 5.124,
      "grad_norm": 95.0966796875,
      "learning_rate": 2.9199999999999998e-05,
      "loss": -111.7154,
      "step": 64050
    },
    {
      "epoch": 5.1248,
      "grad_norm": 21.55095100402832,
      "learning_rate": 2.9173333333333337e-05,
      "loss": -110.9601,
      "step": 64060
    },
    {
      "epoch": 5.1256,
      "grad_norm": 74.64370727539062,
      "learning_rate": 2.9146666666666667e-05,
      "loss": -111.6797,
      "step": 64070
    },
    {
      "epoch": 5.1264,
      "grad_norm": 30.600337982177734,
      "learning_rate": 2.9120000000000002e-05,
      "loss": -112.0527,
      "step": 64080
    },
    {
      "epoch": 5.1272,
      "grad_norm": 42.65707015991211,
      "learning_rate": 2.9093333333333335e-05,
      "loss": -111.1432,
      "step": 64090
    },
    {
      "epoch": 5.128,
      "grad_norm": 24.839935302734375,
      "learning_rate": 2.906666666666667e-05,
      "loss": -111.6442,
      "step": 64100
    },
    {
      "epoch": 5.1288,
      "grad_norm": 25.807886123657227,
      "learning_rate": 2.904e-05,
      "loss": -111.6592,
      "step": 64110
    },
    {
      "epoch": 5.1296,
      "grad_norm": 32.26789855957031,
      "learning_rate": 2.9013333333333336e-05,
      "loss": -111.8836,
      "step": 64120
    },
    {
      "epoch": 5.1304,
      "grad_norm": 117.8953628540039,
      "learning_rate": 2.8986666666666668e-05,
      "loss": -111.6923,
      "step": 64130
    },
    {
      "epoch": 5.1312,
      "grad_norm": 103.82511901855469,
      "learning_rate": 2.8960000000000004e-05,
      "loss": -111.6656,
      "step": 64140
    },
    {
      "epoch": 5.132,
      "grad_norm": 164.8478240966797,
      "learning_rate": 2.8933333333333333e-05,
      "loss": -112.1346,
      "step": 64150
    },
    {
      "epoch": 5.1328,
      "grad_norm": 39.632999420166016,
      "learning_rate": 2.8906666666666672e-05,
      "loss": -111.1509,
      "step": 64160
    },
    {
      "epoch": 5.1336,
      "grad_norm": 18.325668334960938,
      "learning_rate": 2.888e-05,
      "loss": -112.4148,
      "step": 64170
    },
    {
      "epoch": 5.1344,
      "grad_norm": 153.28118896484375,
      "learning_rate": 2.8853333333333334e-05,
      "loss": -111.5676,
      "step": 64180
    },
    {
      "epoch": 5.1352,
      "grad_norm": 30.48521614074707,
      "learning_rate": 2.882666666666667e-05,
      "loss": -110.9986,
      "step": 64190
    },
    {
      "epoch": 5.136,
      "grad_norm": 125.61566925048828,
      "learning_rate": 2.88e-05,
      "loss": -112.0223,
      "step": 64200
    },
    {
      "epoch": 5.1368,
      "grad_norm": 35.84251022338867,
      "learning_rate": 2.8773333333333335e-05,
      "loss": -111.7239,
      "step": 64210
    },
    {
      "epoch": 5.1376,
      "grad_norm": 116.3993148803711,
      "learning_rate": 2.8746666666666667e-05,
      "loss": -111.3999,
      "step": 64220
    },
    {
      "epoch": 5.1384,
      "grad_norm": 118.59333038330078,
      "learning_rate": 2.8720000000000003e-05,
      "loss": -111.8799,
      "step": 64230
    },
    {
      "epoch": 5.1392,
      "grad_norm": 28.389841079711914,
      "learning_rate": 2.8693333333333332e-05,
      "loss": -110.8288,
      "step": 64240
    },
    {
      "epoch": 5.14,
      "grad_norm": 23.304790496826172,
      "learning_rate": 2.8666666666666668e-05,
      "loss": -111.8381,
      "step": 64250
    },
    {
      "epoch": 5.1408,
      "grad_norm": 55.983158111572266,
      "learning_rate": 2.864e-05,
      "loss": -111.7781,
      "step": 64260
    },
    {
      "epoch": 5.1416,
      "grad_norm": 31.465328216552734,
      "learning_rate": 2.8613333333333336e-05,
      "loss": -111.4729,
      "step": 64270
    },
    {
      "epoch": 5.1424,
      "grad_norm": 150.30686950683594,
      "learning_rate": 2.858666666666667e-05,
      "loss": -111.2977,
      "step": 64280
    },
    {
      "epoch": 5.1432,
      "grad_norm": 76.7112808227539,
      "learning_rate": 2.8560000000000004e-05,
      "loss": -111.5469,
      "step": 64290
    },
    {
      "epoch": 5.144,
      "grad_norm": 39.50564193725586,
      "learning_rate": 2.8533333333333333e-05,
      "loss": -112.4358,
      "step": 64300
    },
    {
      "epoch": 5.1448,
      "grad_norm": 29.70137596130371,
      "learning_rate": 2.850666666666667e-05,
      "loss": -111.8052,
      "step": 64310
    },
    {
      "epoch": 5.1456,
      "grad_norm": 62.5245246887207,
      "learning_rate": 2.8480000000000002e-05,
      "loss": -111.7948,
      "step": 64320
    },
    {
      "epoch": 5.1464,
      "grad_norm": 122.4981918334961,
      "learning_rate": 2.8453333333333338e-05,
      "loss": -111.3588,
      "step": 64330
    },
    {
      "epoch": 5.1472,
      "grad_norm": 34.34566116333008,
      "learning_rate": 2.8426666666666667e-05,
      "loss": -111.1147,
      "step": 64340
    },
    {
      "epoch": 5.148,
      "grad_norm": 96.03656768798828,
      "learning_rate": 2.84e-05,
      "loss": -111.1799,
      "step": 64350
    },
    {
      "epoch": 5.1488,
      "grad_norm": 44.50577926635742,
      "learning_rate": 2.8373333333333335e-05,
      "loss": -111.6894,
      "step": 64360
    },
    {
      "epoch": 5.1496,
      "grad_norm": 41.54701614379883,
      "learning_rate": 2.8346666666666667e-05,
      "loss": -111.4132,
      "step": 64370
    },
    {
      "epoch": 5.1504,
      "grad_norm": 25.973770141601562,
      "learning_rate": 2.8320000000000003e-05,
      "loss": -111.6498,
      "step": 64380
    },
    {
      "epoch": 5.1512,
      "grad_norm": 39.985198974609375,
      "learning_rate": 2.8293333333333332e-05,
      "loss": -112.0561,
      "step": 64390
    },
    {
      "epoch": 5.152,
      "grad_norm": 72.86275482177734,
      "learning_rate": 2.8266666666666668e-05,
      "loss": -110.5929,
      "step": 64400
    },
    {
      "epoch": 5.1528,
      "grad_norm": 454.326171875,
      "learning_rate": 2.824e-05,
      "loss": -111.4505,
      "step": 64410
    },
    {
      "epoch": 5.1536,
      "grad_norm": 36.81442642211914,
      "learning_rate": 2.8213333333333337e-05,
      "loss": -111.6652,
      "step": 64420
    },
    {
      "epoch": 5.1544,
      "grad_norm": 19.607044219970703,
      "learning_rate": 2.8186666666666666e-05,
      "loss": -111.5455,
      "step": 64430
    },
    {
      "epoch": 5.1552,
      "grad_norm": 46.95908737182617,
      "learning_rate": 2.816e-05,
      "loss": -112.1914,
      "step": 64440
    },
    {
      "epoch": 5.156,
      "grad_norm": 38.20607376098633,
      "learning_rate": 2.8133333333333334e-05,
      "loss": -112.5427,
      "step": 64450
    },
    {
      "epoch": 5.1568,
      "grad_norm": 29.531108856201172,
      "learning_rate": 2.810666666666667e-05,
      "loss": -112.1679,
      "step": 64460
    },
    {
      "epoch": 5.1576,
      "grad_norm": 33.434879302978516,
      "learning_rate": 2.8080000000000002e-05,
      "loss": -112.2827,
      "step": 64470
    },
    {
      "epoch": 5.1584,
      "grad_norm": 89.91595458984375,
      "learning_rate": 2.8053333333333338e-05,
      "loss": -111.1894,
      "step": 64480
    },
    {
      "epoch": 5.1592,
      "grad_norm": 55.556400299072266,
      "learning_rate": 2.8026666666666667e-05,
      "loss": -112.3689,
      "step": 64490
    },
    {
      "epoch": 5.16,
      "grad_norm": 24.936521530151367,
      "learning_rate": 2.8000000000000003e-05,
      "loss": -111.8911,
      "step": 64500
    },
    {
      "epoch": 5.1608,
      "grad_norm": 46.71086502075195,
      "learning_rate": 2.7973333333333335e-05,
      "loss": -111.5859,
      "step": 64510
    },
    {
      "epoch": 5.1616,
      "grad_norm": 23.647329330444336,
      "learning_rate": 2.7946666666666664e-05,
      "loss": -111.925,
      "step": 64520
    },
    {
      "epoch": 5.1624,
      "grad_norm": 27.922222137451172,
      "learning_rate": 2.792e-05,
      "loss": -112.1134,
      "step": 64530
    },
    {
      "epoch": 5.1632,
      "grad_norm": 34.575782775878906,
      "learning_rate": 2.7893333333333333e-05,
      "loss": -111.76,
      "step": 64540
    },
    {
      "epoch": 5.164,
      "grad_norm": 77.81819152832031,
      "learning_rate": 2.786666666666667e-05,
      "loss": -111.6561,
      "step": 64550
    },
    {
      "epoch": 5.1648,
      "grad_norm": 51.315460205078125,
      "learning_rate": 2.7839999999999998e-05,
      "loss": -111.2619,
      "step": 64560
    },
    {
      "epoch": 5.1656,
      "grad_norm": 65.03192138671875,
      "learning_rate": 2.7813333333333337e-05,
      "loss": -111.3999,
      "step": 64570
    },
    {
      "epoch": 5.1664,
      "grad_norm": 66.7926025390625,
      "learning_rate": 2.7786666666666666e-05,
      "loss": -111.9221,
      "step": 64580
    },
    {
      "epoch": 5.1672,
      "grad_norm": 72.67485046386719,
      "learning_rate": 2.7760000000000002e-05,
      "loss": -111.2613,
      "step": 64590
    },
    {
      "epoch": 5.168,
      "grad_norm": 26.466224670410156,
      "learning_rate": 2.7733333333333334e-05,
      "loss": -111.0766,
      "step": 64600
    },
    {
      "epoch": 5.1688,
      "grad_norm": 85.7043685913086,
      "learning_rate": 2.770666666666667e-05,
      "loss": -111.2838,
      "step": 64610
    },
    {
      "epoch": 5.1696,
      "grad_norm": 67.17073822021484,
      "learning_rate": 2.768e-05,
      "loss": -112.3082,
      "step": 64620
    },
    {
      "epoch": 5.1704,
      "grad_norm": 75.19210815429688,
      "learning_rate": 2.7653333333333335e-05,
      "loss": -111.9979,
      "step": 64630
    },
    {
      "epoch": 5.1712,
      "grad_norm": 165.4531707763672,
      "learning_rate": 2.7626666666666668e-05,
      "loss": -111.3974,
      "step": 64640
    },
    {
      "epoch": 5.172,
      "grad_norm": 25.225618362426758,
      "learning_rate": 2.7600000000000003e-05,
      "loss": -112.1823,
      "step": 64650
    },
    {
      "epoch": 5.1728,
      "grad_norm": 122.42942810058594,
      "learning_rate": 2.7573333333333336e-05,
      "loss": -110.9314,
      "step": 64660
    },
    {
      "epoch": 5.1736,
      "grad_norm": 69.73755645751953,
      "learning_rate": 2.7546666666666672e-05,
      "loss": -112.466,
      "step": 64670
    },
    {
      "epoch": 5.1744,
      "grad_norm": 31.003860473632812,
      "learning_rate": 2.752e-05,
      "loss": -110.4139,
      "step": 64680
    },
    {
      "epoch": 5.1752,
      "grad_norm": 79.98360443115234,
      "learning_rate": 2.7493333333333333e-05,
      "loss": -111.8615,
      "step": 64690
    },
    {
      "epoch": 5.176,
      "grad_norm": 108.38040161132812,
      "learning_rate": 2.746666666666667e-05,
      "loss": -111.2662,
      "step": 64700
    },
    {
      "epoch": 5.1768,
      "grad_norm": 22.029531478881836,
      "learning_rate": 2.7439999999999998e-05,
      "loss": -111.193,
      "step": 64710
    },
    {
      "epoch": 5.1776,
      "grad_norm": 49.920799255371094,
      "learning_rate": 2.7413333333333334e-05,
      "loss": -111.9621,
      "step": 64720
    },
    {
      "epoch": 5.1784,
      "grad_norm": 81.20767211914062,
      "learning_rate": 2.7386666666666666e-05,
      "loss": -111.09,
      "step": 64730
    },
    {
      "epoch": 5.1792,
      "grad_norm": 54.970516204833984,
      "learning_rate": 2.7360000000000002e-05,
      "loss": -112.1104,
      "step": 64740
    },
    {
      "epoch": 5.18,
      "grad_norm": 223.3466033935547,
      "learning_rate": 2.733333333333333e-05,
      "loss": -111.9002,
      "step": 64750
    },
    {
      "epoch": 5.1808,
      "grad_norm": 42.848785400390625,
      "learning_rate": 2.730666666666667e-05,
      "loss": -112.0241,
      "step": 64760
    },
    {
      "epoch": 5.1816,
      "grad_norm": 64.1347427368164,
      "learning_rate": 2.728e-05,
      "loss": -112.1912,
      "step": 64770
    },
    {
      "epoch": 5.1824,
      "grad_norm": 57.06904220581055,
      "learning_rate": 2.7253333333333336e-05,
      "loss": -111.0578,
      "step": 64780
    },
    {
      "epoch": 5.1832,
      "grad_norm": 26.94729995727539,
      "learning_rate": 2.7226666666666668e-05,
      "loss": -111.4626,
      "step": 64790
    },
    {
      "epoch": 5.184,
      "grad_norm": 36.98765182495117,
      "learning_rate": 2.7200000000000004e-05,
      "loss": -111.5346,
      "step": 64800
    },
    {
      "epoch": 5.1848,
      "grad_norm": 293.8816223144531,
      "learning_rate": 2.7173333333333333e-05,
      "loss": -111.7272,
      "step": 64810
    },
    {
      "epoch": 5.1856,
      "grad_norm": 129.1682891845703,
      "learning_rate": 2.714666666666667e-05,
      "loss": -111.2874,
      "step": 64820
    },
    {
      "epoch": 5.1864,
      "grad_norm": 109.0289535522461,
      "learning_rate": 2.712e-05,
      "loss": -111.7509,
      "step": 64830
    },
    {
      "epoch": 5.1872,
      "grad_norm": 60.03441619873047,
      "learning_rate": 2.7093333333333337e-05,
      "loss": -111.8978,
      "step": 64840
    },
    {
      "epoch": 5.188,
      "grad_norm": 35.136253356933594,
      "learning_rate": 2.706666666666667e-05,
      "loss": -112.0218,
      "step": 64850
    },
    {
      "epoch": 5.1888,
      "grad_norm": 33.33271789550781,
      "learning_rate": 2.704e-05,
      "loss": -112.2126,
      "step": 64860
    },
    {
      "epoch": 5.1896,
      "grad_norm": 39.364776611328125,
      "learning_rate": 2.7013333333333334e-05,
      "loss": -111.4834,
      "step": 64870
    },
    {
      "epoch": 5.1904,
      "grad_norm": 36.18608093261719,
      "learning_rate": 2.6986666666666667e-05,
      "loss": -111.8143,
      "step": 64880
    },
    {
      "epoch": 5.1912,
      "grad_norm": 84.67395782470703,
      "learning_rate": 2.6960000000000003e-05,
      "loss": -110.4033,
      "step": 64890
    },
    {
      "epoch": 5.192,
      "grad_norm": 38.34058380126953,
      "learning_rate": 2.6933333333333332e-05,
      "loss": -110.804,
      "step": 64900
    },
    {
      "epoch": 5.1928,
      "grad_norm": 486.6622314453125,
      "learning_rate": 2.6906666666666668e-05,
      "loss": -112.9077,
      "step": 64910
    },
    {
      "epoch": 5.1936,
      "grad_norm": 38.51951217651367,
      "learning_rate": 2.688e-05,
      "loss": -112.2295,
      "step": 64920
    },
    {
      "epoch": 5.1944,
      "grad_norm": 29.195648193359375,
      "learning_rate": 2.6853333333333336e-05,
      "loss": -111.4846,
      "step": 64930
    },
    {
      "epoch": 5.1952,
      "grad_norm": 128.67568969726562,
      "learning_rate": 2.6826666666666665e-05,
      "loss": -110.6844,
      "step": 64940
    },
    {
      "epoch": 5.196,
      "grad_norm": 22.392553329467773,
      "learning_rate": 2.6800000000000004e-05,
      "loss": -111.6516,
      "step": 64950
    },
    {
      "epoch": 5.1968,
      "grad_norm": 47.230098724365234,
      "learning_rate": 2.6773333333333333e-05,
      "loss": -111.7141,
      "step": 64960
    },
    {
      "epoch": 5.1975999999999996,
      "grad_norm": 43.73088073730469,
      "learning_rate": 2.674666666666667e-05,
      "loss": -111.1009,
      "step": 64970
    },
    {
      "epoch": 5.1984,
      "grad_norm": 46.93552780151367,
      "learning_rate": 2.672e-05,
      "loss": -111.6186,
      "step": 64980
    },
    {
      "epoch": 5.1992,
      "grad_norm": 71.01200866699219,
      "learning_rate": 2.6693333333333338e-05,
      "loss": -111.4554,
      "step": 64990
    },
    {
      "epoch": 5.2,
      "grad_norm": 40.35475158691406,
      "learning_rate": 2.6666666666666667e-05,
      "loss": -111.3096,
      "step": 65000
    },
    {
      "epoch": 5.2008,
      "grad_norm": 36.2828369140625,
      "learning_rate": 2.6640000000000002e-05,
      "loss": -111.0904,
      "step": 65010
    },
    {
      "epoch": 5.2016,
      "grad_norm": 168.17306518554688,
      "learning_rate": 2.6613333333333335e-05,
      "loss": -111.6933,
      "step": 65020
    },
    {
      "epoch": 5.2024,
      "grad_norm": 95.8488998413086,
      "learning_rate": 2.6586666666666664e-05,
      "loss": -112.2104,
      "step": 65030
    },
    {
      "epoch": 5.2032,
      "grad_norm": 21.48423957824707,
      "learning_rate": 2.6560000000000003e-05,
      "loss": -110.7227,
      "step": 65040
    },
    {
      "epoch": 5.204,
      "grad_norm": 51.08409118652344,
      "learning_rate": 2.6533333333333332e-05,
      "loss": -111.6127,
      "step": 65050
    },
    {
      "epoch": 5.2048,
      "grad_norm": 489.8812255859375,
      "learning_rate": 2.6506666666666668e-05,
      "loss": -111.5435,
      "step": 65060
    },
    {
      "epoch": 5.2056000000000004,
      "grad_norm": 62.29971694946289,
      "learning_rate": 2.648e-05,
      "loss": -111.2998,
      "step": 65070
    },
    {
      "epoch": 5.2064,
      "grad_norm": 25.297788619995117,
      "learning_rate": 2.6453333333333336e-05,
      "loss": -111.7682,
      "step": 65080
    },
    {
      "epoch": 5.2072,
      "grad_norm": 37.85871124267578,
      "learning_rate": 2.6426666666666665e-05,
      "loss": -111.531,
      "step": 65090
    },
    {
      "epoch": 5.208,
      "grad_norm": 28.91947364807129,
      "learning_rate": 2.64e-05,
      "loss": -111.6917,
      "step": 65100
    },
    {
      "epoch": 5.2088,
      "grad_norm": 22.099308013916016,
      "learning_rate": 2.6373333333333334e-05,
      "loss": -111.1536,
      "step": 65110
    },
    {
      "epoch": 5.2096,
      "grad_norm": 24.772912979125977,
      "learning_rate": 2.634666666666667e-05,
      "loss": -110.7286,
      "step": 65120
    },
    {
      "epoch": 5.2104,
      "grad_norm": 30.166584014892578,
      "learning_rate": 2.632e-05,
      "loss": -111.1182,
      "step": 65130
    },
    {
      "epoch": 5.2112,
      "grad_norm": 70.26168060302734,
      "learning_rate": 2.6293333333333338e-05,
      "loss": -112.1142,
      "step": 65140
    },
    {
      "epoch": 5.212,
      "grad_norm": 99.74771118164062,
      "learning_rate": 2.6266666666666667e-05,
      "loss": -111.1117,
      "step": 65150
    },
    {
      "epoch": 5.2128,
      "grad_norm": 232.3817596435547,
      "learning_rate": 2.6240000000000003e-05,
      "loss": -111.8101,
      "step": 65160
    },
    {
      "epoch": 5.2136,
      "grad_norm": 36.396305084228516,
      "learning_rate": 2.6213333333333335e-05,
      "loss": -111.4358,
      "step": 65170
    },
    {
      "epoch": 5.2144,
      "grad_norm": 67.17765808105469,
      "learning_rate": 2.618666666666667e-05,
      "loss": -110.4044,
      "step": 65180
    },
    {
      "epoch": 5.2152,
      "grad_norm": 114.74335479736328,
      "learning_rate": 2.616e-05,
      "loss": -111.2361,
      "step": 65190
    },
    {
      "epoch": 5.216,
      "grad_norm": 72.13465118408203,
      "learning_rate": 2.6133333333333333e-05,
      "loss": -112.0206,
      "step": 65200
    },
    {
      "epoch": 5.2168,
      "grad_norm": 42.235477447509766,
      "learning_rate": 2.610666666666667e-05,
      "loss": -111.9788,
      "step": 65210
    },
    {
      "epoch": 5.2176,
      "grad_norm": 167.85287475585938,
      "learning_rate": 2.6079999999999998e-05,
      "loss": -112.0153,
      "step": 65220
    },
    {
      "epoch": 5.2184,
      "grad_norm": 182.68682861328125,
      "learning_rate": 2.6053333333333333e-05,
      "loss": -111.2454,
      "step": 65230
    },
    {
      "epoch": 5.2192,
      "grad_norm": 172.39254760742188,
      "learning_rate": 2.6026666666666666e-05,
      "loss": -112.1567,
      "step": 65240
    },
    {
      "epoch": 5.22,
      "grad_norm": 85.5120849609375,
      "learning_rate": 2.6000000000000002e-05,
      "loss": -111.5003,
      "step": 65250
    },
    {
      "epoch": 5.2208,
      "grad_norm": 83.68555450439453,
      "learning_rate": 2.5973333333333334e-05,
      "loss": -111.834,
      "step": 65260
    },
    {
      "epoch": 5.2216,
      "grad_norm": 20.600732803344727,
      "learning_rate": 2.594666666666667e-05,
      "loss": -111.7355,
      "step": 65270
    },
    {
      "epoch": 5.2224,
      "grad_norm": 143.2566680908203,
      "learning_rate": 2.592e-05,
      "loss": -111.8,
      "step": 65280
    },
    {
      "epoch": 5.2232,
      "grad_norm": 96.58638763427734,
      "learning_rate": 2.5893333333333335e-05,
      "loss": -111.5878,
      "step": 65290
    },
    {
      "epoch": 5.224,
      "grad_norm": 34.946044921875,
      "learning_rate": 2.5866666666666667e-05,
      "loss": -111.493,
      "step": 65300
    },
    {
      "epoch": 5.2248,
      "grad_norm": 64.61863708496094,
      "learning_rate": 2.5840000000000003e-05,
      "loss": -111.3406,
      "step": 65310
    },
    {
      "epoch": 5.2256,
      "grad_norm": 20.604206085205078,
      "learning_rate": 2.5813333333333332e-05,
      "loss": -111.5389,
      "step": 65320
    },
    {
      "epoch": 5.2264,
      "grad_norm": 102.0105209350586,
      "learning_rate": 2.578666666666667e-05,
      "loss": -111.744,
      "step": 65330
    },
    {
      "epoch": 5.2272,
      "grad_norm": 531.3712768554688,
      "learning_rate": 2.576e-05,
      "loss": -111.4862,
      "step": 65340
    },
    {
      "epoch": 5.228,
      "grad_norm": 38.50862503051758,
      "learning_rate": 2.5733333333333337e-05,
      "loss": -111.1331,
      "step": 65350
    },
    {
      "epoch": 5.2288,
      "grad_norm": 41.58224105834961,
      "learning_rate": 2.570666666666667e-05,
      "loss": -111.563,
      "step": 65360
    },
    {
      "epoch": 5.2296,
      "grad_norm": 25.765995025634766,
      "learning_rate": 2.5679999999999998e-05,
      "loss": -111.5516,
      "step": 65370
    },
    {
      "epoch": 5.2304,
      "grad_norm": 33.007320404052734,
      "learning_rate": 2.5653333333333334e-05,
      "loss": -111.0845,
      "step": 65380
    },
    {
      "epoch": 5.2312,
      "grad_norm": 132.7324981689453,
      "learning_rate": 2.5626666666666666e-05,
      "loss": -111.6439,
      "step": 65390
    },
    {
      "epoch": 5.232,
      "grad_norm": 65.28445434570312,
      "learning_rate": 2.5600000000000002e-05,
      "loss": -111.1768,
      "step": 65400
    },
    {
      "epoch": 5.2328,
      "grad_norm": 140.599365234375,
      "learning_rate": 2.557333333333333e-05,
      "loss": -111.0398,
      "step": 65410
    },
    {
      "epoch": 5.2336,
      "grad_norm": 29.688589096069336,
      "learning_rate": 2.5546666666666667e-05,
      "loss": -110.5619,
      "step": 65420
    },
    {
      "epoch": 5.2344,
      "grad_norm": 24.202407836914062,
      "learning_rate": 2.552e-05,
      "loss": -111.5958,
      "step": 65430
    },
    {
      "epoch": 5.2352,
      "grad_norm": 76.43067169189453,
      "learning_rate": 2.5493333333333335e-05,
      "loss": -111.4234,
      "step": 65440
    },
    {
      "epoch": 5.236,
      "grad_norm": 26.1241455078125,
      "learning_rate": 2.5466666666666668e-05,
      "loss": -110.8828,
      "step": 65450
    },
    {
      "epoch": 5.2368,
      "grad_norm": 23.765647888183594,
      "learning_rate": 2.5440000000000004e-05,
      "loss": -112.4187,
      "step": 65460
    },
    {
      "epoch": 5.2376,
      "grad_norm": 83.4732437133789,
      "learning_rate": 2.5413333333333333e-05,
      "loss": -110.8538,
      "step": 65470
    },
    {
      "epoch": 5.2384,
      "grad_norm": 72.46990966796875,
      "learning_rate": 2.538666666666667e-05,
      "loss": -112.1103,
      "step": 65480
    },
    {
      "epoch": 5.2392,
      "grad_norm": 19.096311569213867,
      "learning_rate": 2.536e-05,
      "loss": -110.879,
      "step": 65490
    },
    {
      "epoch": 5.24,
      "grad_norm": 39.47367477416992,
      "learning_rate": 2.5333333333333337e-05,
      "loss": -111.5327,
      "step": 65500
    },
    {
      "epoch": 5.2408,
      "grad_norm": 103.35806274414062,
      "learning_rate": 2.5306666666666666e-05,
      "loss": -112.4771,
      "step": 65510
    },
    {
      "epoch": 5.2416,
      "grad_norm": 23.37135124206543,
      "learning_rate": 2.5280000000000005e-05,
      "loss": -111.4945,
      "step": 65520
    },
    {
      "epoch": 5.2424,
      "grad_norm": 30.44171714782715,
      "learning_rate": 2.5253333333333334e-05,
      "loss": -112.0002,
      "step": 65530
    },
    {
      "epoch": 5.2432,
      "grad_norm": 37.128719329833984,
      "learning_rate": 2.5226666666666663e-05,
      "loss": -111.7043,
      "step": 65540
    },
    {
      "epoch": 5.244,
      "grad_norm": 53.848548889160156,
      "learning_rate": 2.5200000000000003e-05,
      "loss": -111.0157,
      "step": 65550
    },
    {
      "epoch": 5.2448,
      "grad_norm": 63.01992416381836,
      "learning_rate": 2.5173333333333332e-05,
      "loss": -111.4831,
      "step": 65560
    },
    {
      "epoch": 5.2456,
      "grad_norm": 118.1848373413086,
      "learning_rate": 2.5146666666666668e-05,
      "loss": -111.2709,
      "step": 65570
    },
    {
      "epoch": 5.2464,
      "grad_norm": 126.04234313964844,
      "learning_rate": 2.512e-05,
      "loss": -111.4688,
      "step": 65580
    },
    {
      "epoch": 5.2472,
      "grad_norm": 289.2445373535156,
      "learning_rate": 2.5093333333333336e-05,
      "loss": -112.472,
      "step": 65590
    },
    {
      "epoch": 5.248,
      "grad_norm": 37.219818115234375,
      "learning_rate": 2.5066666666666665e-05,
      "loss": -111.4008,
      "step": 65600
    },
    {
      "epoch": 5.2488,
      "grad_norm": 28.783428192138672,
      "learning_rate": 2.504e-05,
      "loss": -111.6759,
      "step": 65610
    },
    {
      "epoch": 5.2496,
      "grad_norm": 84.78997802734375,
      "learning_rate": 2.5013333333333333e-05,
      "loss": -111.0201,
      "step": 65620
    },
    {
      "epoch": 5.2504,
      "grad_norm": 205.1084747314453,
      "learning_rate": 2.4986666666666666e-05,
      "loss": -111.7484,
      "step": 65630
    },
    {
      "epoch": 5.2512,
      "grad_norm": 26.461565017700195,
      "learning_rate": 2.496e-05,
      "loss": -111.7219,
      "step": 65640
    },
    {
      "epoch": 5.252,
      "grad_norm": 31.67420196533203,
      "learning_rate": 2.4933333333333334e-05,
      "loss": -111.5298,
      "step": 65650
    },
    {
      "epoch": 5.2528,
      "grad_norm": 84.2206802368164,
      "learning_rate": 2.4906666666666666e-05,
      "loss": -111.5425,
      "step": 65660
    },
    {
      "epoch": 5.2536,
      "grad_norm": 25.432876586914062,
      "learning_rate": 2.488e-05,
      "loss": -111.9627,
      "step": 65670
    },
    {
      "epoch": 5.2544,
      "grad_norm": 194.3101043701172,
      "learning_rate": 2.4853333333333335e-05,
      "loss": -111.3889,
      "step": 65680
    },
    {
      "epoch": 5.2552,
      "grad_norm": 60.620330810546875,
      "learning_rate": 2.4826666666666667e-05,
      "loss": -111.9167,
      "step": 65690
    },
    {
      "epoch": 5.256,
      "grad_norm": 43.898502349853516,
      "learning_rate": 2.48e-05,
      "loss": -110.7757,
      "step": 65700
    },
    {
      "epoch": 5.2568,
      "grad_norm": 59.115379333496094,
      "learning_rate": 2.4773333333333336e-05,
      "loss": -111.1622,
      "step": 65710
    },
    {
      "epoch": 5.2576,
      "grad_norm": 80.97405242919922,
      "learning_rate": 2.4746666666666668e-05,
      "loss": -110.6338,
      "step": 65720
    },
    {
      "epoch": 5.2584,
      "grad_norm": 25.401290893554688,
      "learning_rate": 2.472e-05,
      "loss": -111.6335,
      "step": 65730
    },
    {
      "epoch": 5.2592,
      "grad_norm": 178.0750732421875,
      "learning_rate": 2.4693333333333336e-05,
      "loss": -112.0276,
      "step": 65740
    },
    {
      "epoch": 5.26,
      "grad_norm": 45.31189727783203,
      "learning_rate": 2.466666666666667e-05,
      "loss": -111.7355,
      "step": 65750
    },
    {
      "epoch": 5.2608,
      "grad_norm": 117.23706817626953,
      "learning_rate": 2.464e-05,
      "loss": -111.2502,
      "step": 65760
    },
    {
      "epoch": 5.2616,
      "grad_norm": 97.50311279296875,
      "learning_rate": 2.4613333333333337e-05,
      "loss": -112.0586,
      "step": 65770
    },
    {
      "epoch": 5.2624,
      "grad_norm": 199.7410888671875,
      "learning_rate": 2.458666666666667e-05,
      "loss": -112.3114,
      "step": 65780
    },
    {
      "epoch": 5.2632,
      "grad_norm": 65.1082992553711,
      "learning_rate": 2.4560000000000002e-05,
      "loss": -112.3148,
      "step": 65790
    },
    {
      "epoch": 5.264,
      "grad_norm": 69.72083282470703,
      "learning_rate": 2.4533333333333334e-05,
      "loss": -112.3731,
      "step": 65800
    },
    {
      "epoch": 5.2648,
      "grad_norm": 42.69963836669922,
      "learning_rate": 2.4506666666666667e-05,
      "loss": -112.2794,
      "step": 65810
    },
    {
      "epoch": 5.2656,
      "grad_norm": 54.81565475463867,
      "learning_rate": 2.448e-05,
      "loss": -111.9779,
      "step": 65820
    },
    {
      "epoch": 5.2664,
      "grad_norm": 120.26478576660156,
      "learning_rate": 2.4453333333333335e-05,
      "loss": -113.2948,
      "step": 65830
    },
    {
      "epoch": 5.2672,
      "grad_norm": 57.885536193847656,
      "learning_rate": 2.4426666666666668e-05,
      "loss": -111.5008,
      "step": 65840
    },
    {
      "epoch": 5.268,
      "grad_norm": 72.4232406616211,
      "learning_rate": 2.44e-05,
      "loss": -111.68,
      "step": 65850
    },
    {
      "epoch": 5.2688,
      "grad_norm": 106.3668212890625,
      "learning_rate": 2.4373333333333333e-05,
      "loss": -111.9804,
      "step": 65860
    },
    {
      "epoch": 5.2696,
      "grad_norm": 90.9414291381836,
      "learning_rate": 2.434666666666667e-05,
      "loss": -112.3727,
      "step": 65870
    },
    {
      "epoch": 5.2704,
      "grad_norm": 76.96560668945312,
      "learning_rate": 2.432e-05,
      "loss": -111.3288,
      "step": 65880
    },
    {
      "epoch": 5.2712,
      "grad_norm": 29.25106430053711,
      "learning_rate": 2.4293333333333333e-05,
      "loss": -112.2587,
      "step": 65890
    },
    {
      "epoch": 5.272,
      "grad_norm": 34.94984817504883,
      "learning_rate": 2.426666666666667e-05,
      "loss": -111.7948,
      "step": 65900
    },
    {
      "epoch": 5.2728,
      "grad_norm": 75.80695343017578,
      "learning_rate": 2.4240000000000002e-05,
      "loss": -111.2626,
      "step": 65910
    },
    {
      "epoch": 5.2736,
      "grad_norm": 32.84162521362305,
      "learning_rate": 2.4213333333333334e-05,
      "loss": -112.5878,
      "step": 65920
    },
    {
      "epoch": 5.2744,
      "grad_norm": 53.46485900878906,
      "learning_rate": 2.418666666666667e-05,
      "loss": -111.0545,
      "step": 65930
    },
    {
      "epoch": 5.2752,
      "grad_norm": 79.99897003173828,
      "learning_rate": 2.4160000000000002e-05,
      "loss": -111.7491,
      "step": 65940
    },
    {
      "epoch": 5.276,
      "grad_norm": 161.91746520996094,
      "learning_rate": 2.4133333333333335e-05,
      "loss": -112.0979,
      "step": 65950
    },
    {
      "epoch": 5.2768,
      "grad_norm": 109.74299621582031,
      "learning_rate": 2.4106666666666667e-05,
      "loss": -111.2884,
      "step": 65960
    },
    {
      "epoch": 5.2776,
      "grad_norm": 34.197757720947266,
      "learning_rate": 2.408e-05,
      "loss": -111.9065,
      "step": 65970
    },
    {
      "epoch": 5.2783999999999995,
      "grad_norm": 29.394582748413086,
      "learning_rate": 2.4053333333333332e-05,
      "loss": -111.9144,
      "step": 65980
    },
    {
      "epoch": 5.2792,
      "grad_norm": 45.98009490966797,
      "learning_rate": 2.4026666666666668e-05,
      "loss": -111.1422,
      "step": 65990
    },
    {
      "epoch": 5.28,
      "grad_norm": 87.02323913574219,
      "learning_rate": 2.4e-05,
      "loss": -111.7579,
      "step": 66000
    },
    {
      "epoch": 5.2808,
      "grad_norm": 39.25952911376953,
      "learning_rate": 2.3973333333333333e-05,
      "loss": -111.0987,
      "step": 66010
    },
    {
      "epoch": 5.2816,
      "grad_norm": 24.660924911499023,
      "learning_rate": 2.394666666666667e-05,
      "loss": -111.2184,
      "step": 66020
    },
    {
      "epoch": 5.2824,
      "grad_norm": 63.654300689697266,
      "learning_rate": 2.392e-05,
      "loss": -111.1046,
      "step": 66030
    },
    {
      "epoch": 5.2832,
      "grad_norm": 65.40003204345703,
      "learning_rate": 2.3893333333333334e-05,
      "loss": -111.2085,
      "step": 66040
    },
    {
      "epoch": 5.284,
      "grad_norm": 61.36299514770508,
      "learning_rate": 2.3866666666666666e-05,
      "loss": -111.9424,
      "step": 66050
    },
    {
      "epoch": 5.2848,
      "grad_norm": 44.01233673095703,
      "learning_rate": 2.3840000000000002e-05,
      "loss": -111.8027,
      "step": 66060
    },
    {
      "epoch": 5.2856,
      "grad_norm": 66.81580352783203,
      "learning_rate": 2.3813333333333335e-05,
      "loss": -110.9844,
      "step": 66070
    },
    {
      "epoch": 5.2864,
      "grad_norm": 27.340024948120117,
      "learning_rate": 2.3786666666666667e-05,
      "loss": -110.9371,
      "step": 66080
    },
    {
      "epoch": 5.2872,
      "grad_norm": 27.15751075744629,
      "learning_rate": 2.3760000000000003e-05,
      "loss": -111.3106,
      "step": 66090
    },
    {
      "epoch": 5.288,
      "grad_norm": 32.38457107543945,
      "learning_rate": 2.3733333333333335e-05,
      "loss": -112.0057,
      "step": 66100
    },
    {
      "epoch": 5.2888,
      "grad_norm": 113.24844360351562,
      "learning_rate": 2.3706666666666668e-05,
      "loss": -111.7006,
      "step": 66110
    },
    {
      "epoch": 5.2896,
      "grad_norm": 35.24260330200195,
      "learning_rate": 2.3680000000000004e-05,
      "loss": -111.5421,
      "step": 66120
    },
    {
      "epoch": 5.2904,
      "grad_norm": 59.09162139892578,
      "learning_rate": 2.3653333333333336e-05,
      "loss": -112.3458,
      "step": 66130
    },
    {
      "epoch": 5.2912,
      "grad_norm": 130.13711547851562,
      "learning_rate": 2.362666666666667e-05,
      "loss": -112.5226,
      "step": 66140
    },
    {
      "epoch": 5.292,
      "grad_norm": 46.95500564575195,
      "learning_rate": 2.36e-05,
      "loss": -111.5353,
      "step": 66150
    },
    {
      "epoch": 5.2928,
      "grad_norm": 85.56002807617188,
      "learning_rate": 2.3573333333333334e-05,
      "loss": -111.0487,
      "step": 66160
    },
    {
      "epoch": 5.2936,
      "grad_norm": 100.0167236328125,
      "learning_rate": 2.3546666666666666e-05,
      "loss": -111.4925,
      "step": 66170
    },
    {
      "epoch": 5.2943999999999996,
      "grad_norm": 23.904983520507812,
      "learning_rate": 2.3520000000000002e-05,
      "loss": -111.3952,
      "step": 66180
    },
    {
      "epoch": 5.2952,
      "grad_norm": 20.93275260925293,
      "learning_rate": 2.3493333333333334e-05,
      "loss": -111.5667,
      "step": 66190
    },
    {
      "epoch": 5.296,
      "grad_norm": 89.49679565429688,
      "learning_rate": 2.3466666666666667e-05,
      "loss": -111.7638,
      "step": 66200
    },
    {
      "epoch": 5.2968,
      "grad_norm": 84.5982666015625,
      "learning_rate": 2.344e-05,
      "loss": -111.2905,
      "step": 66210
    },
    {
      "epoch": 5.2976,
      "grad_norm": 156.85279846191406,
      "learning_rate": 2.3413333333333335e-05,
      "loss": -111.7602,
      "step": 66220
    },
    {
      "epoch": 5.2984,
      "grad_norm": 65.89851379394531,
      "learning_rate": 2.3386666666666668e-05,
      "loss": -111.9702,
      "step": 66230
    },
    {
      "epoch": 5.2992,
      "grad_norm": 78.27400207519531,
      "learning_rate": 2.336e-05,
      "loss": -111.6164,
      "step": 66240
    },
    {
      "epoch": 5.3,
      "grad_norm": 38.28557586669922,
      "learning_rate": 2.3333333333333336e-05,
      "loss": -112.3062,
      "step": 66250
    },
    {
      "epoch": 5.3008,
      "grad_norm": 125.6771011352539,
      "learning_rate": 2.3306666666666668e-05,
      "loss": -111.3797,
      "step": 66260
    },
    {
      "epoch": 5.3016,
      "grad_norm": 35.50764846801758,
      "learning_rate": 2.328e-05,
      "loss": -111.9844,
      "step": 66270
    },
    {
      "epoch": 5.3024000000000004,
      "grad_norm": 33.44981384277344,
      "learning_rate": 2.3253333333333337e-05,
      "loss": -112.2943,
      "step": 66280
    },
    {
      "epoch": 5.3032,
      "grad_norm": 115.07704162597656,
      "learning_rate": 2.322666666666667e-05,
      "loss": -110.8647,
      "step": 66290
    },
    {
      "epoch": 5.304,
      "grad_norm": 40.42144775390625,
      "learning_rate": 2.32e-05,
      "loss": -111.9083,
      "step": 66300
    },
    {
      "epoch": 5.3048,
      "grad_norm": 115.07817840576172,
      "learning_rate": 2.3173333333333337e-05,
      "loss": -111.6192,
      "step": 66310
    },
    {
      "epoch": 5.3056,
      "grad_norm": 63.21601486206055,
      "learning_rate": 2.3146666666666666e-05,
      "loss": -111.5449,
      "step": 66320
    },
    {
      "epoch": 5.3064,
      "grad_norm": 22.083847045898438,
      "learning_rate": 2.312e-05,
      "loss": -110.845,
      "step": 66330
    },
    {
      "epoch": 5.3072,
      "grad_norm": 41.44627380371094,
      "learning_rate": 2.3093333333333335e-05,
      "loss": -111.5623,
      "step": 66340
    },
    {
      "epoch": 5.308,
      "grad_norm": 42.115928649902344,
      "learning_rate": 2.3066666666666667e-05,
      "loss": -111.8042,
      "step": 66350
    },
    {
      "epoch": 5.3088,
      "grad_norm": 28.67963409423828,
      "learning_rate": 2.304e-05,
      "loss": -111.5905,
      "step": 66360
    },
    {
      "epoch": 5.3096,
      "grad_norm": 65.30806732177734,
      "learning_rate": 2.3013333333333335e-05,
      "loss": -111.0432,
      "step": 66370
    },
    {
      "epoch": 5.3104,
      "grad_norm": 78.95685577392578,
      "learning_rate": 2.2986666666666668e-05,
      "loss": -111.4864,
      "step": 66380
    },
    {
      "epoch": 5.3112,
      "grad_norm": 124.97992706298828,
      "learning_rate": 2.296e-05,
      "loss": -111.7892,
      "step": 66390
    },
    {
      "epoch": 5.312,
      "grad_norm": 28.782930374145508,
      "learning_rate": 2.2933333333333333e-05,
      "loss": -111.4064,
      "step": 66400
    },
    {
      "epoch": 5.3128,
      "grad_norm": 17.13593864440918,
      "learning_rate": 2.290666666666667e-05,
      "loss": -112.4919,
      "step": 66410
    },
    {
      "epoch": 5.3136,
      "grad_norm": 25.291555404663086,
      "learning_rate": 2.288e-05,
      "loss": -110.9694,
      "step": 66420
    },
    {
      "epoch": 5.3144,
      "grad_norm": 63.073204040527344,
      "learning_rate": 2.2853333333333334e-05,
      "loss": -111.6736,
      "step": 66430
    },
    {
      "epoch": 5.3152,
      "grad_norm": 23.217023849487305,
      "learning_rate": 2.282666666666667e-05,
      "loss": -111.9804,
      "step": 66440
    },
    {
      "epoch": 5.316,
      "grad_norm": 49.52686309814453,
      "learning_rate": 2.2800000000000002e-05,
      "loss": -111.4,
      "step": 66450
    },
    {
      "epoch": 5.3168,
      "grad_norm": 28.237871170043945,
      "learning_rate": 2.2773333333333334e-05,
      "loss": -111.4531,
      "step": 66460
    },
    {
      "epoch": 5.3176,
      "grad_norm": 47.153709411621094,
      "learning_rate": 2.274666666666667e-05,
      "loss": -111.7519,
      "step": 66470
    },
    {
      "epoch": 5.3184000000000005,
      "grad_norm": 76.28508758544922,
      "learning_rate": 2.2720000000000003e-05,
      "loss": -111.2522,
      "step": 66480
    },
    {
      "epoch": 5.3192,
      "grad_norm": 39.06922912597656,
      "learning_rate": 2.2693333333333332e-05,
      "loss": -112.0006,
      "step": 66490
    },
    {
      "epoch": 5.32,
      "grad_norm": 100.30020904541016,
      "learning_rate": 2.2666666666666668e-05,
      "loss": -112.0881,
      "step": 66500
    },
    {
      "epoch": 5.3208,
      "grad_norm": 52.060489654541016,
      "learning_rate": 2.264e-05,
      "loss": -112.2339,
      "step": 66510
    },
    {
      "epoch": 5.3216,
      "grad_norm": 29.389087677001953,
      "learning_rate": 2.2613333333333333e-05,
      "loss": -111.7444,
      "step": 66520
    },
    {
      "epoch": 5.3224,
      "grad_norm": 36.43290328979492,
      "learning_rate": 2.258666666666667e-05,
      "loss": -110.9536,
      "step": 66530
    },
    {
      "epoch": 5.3232,
      "grad_norm": 69.05606079101562,
      "learning_rate": 2.256e-05,
      "loss": -111.7028,
      "step": 66540
    },
    {
      "epoch": 5.324,
      "grad_norm": 52.687408447265625,
      "learning_rate": 2.2533333333333333e-05,
      "loss": -111.2754,
      "step": 66550
    },
    {
      "epoch": 5.3248,
      "grad_norm": 31.596229553222656,
      "learning_rate": 2.250666666666667e-05,
      "loss": -111.9774,
      "step": 66560
    },
    {
      "epoch": 5.3256,
      "grad_norm": 64.66386413574219,
      "learning_rate": 2.248e-05,
      "loss": -112.8193,
      "step": 66570
    },
    {
      "epoch": 5.3264,
      "grad_norm": 130.1302032470703,
      "learning_rate": 2.2453333333333334e-05,
      "loss": -111.7964,
      "step": 66580
    },
    {
      "epoch": 5.3272,
      "grad_norm": 83.59651947021484,
      "learning_rate": 2.2426666666666667e-05,
      "loss": -112.233,
      "step": 66590
    },
    {
      "epoch": 5.328,
      "grad_norm": 94.7680892944336,
      "learning_rate": 2.2400000000000002e-05,
      "loss": -111.5919,
      "step": 66600
    },
    {
      "epoch": 5.3288,
      "grad_norm": 77.2882308959961,
      "learning_rate": 2.2373333333333335e-05,
      "loss": -111.4646,
      "step": 66610
    },
    {
      "epoch": 5.3296,
      "grad_norm": 73.44257354736328,
      "learning_rate": 2.2346666666666667e-05,
      "loss": -112.1239,
      "step": 66620
    },
    {
      "epoch": 5.3304,
      "grad_norm": 51.42107009887695,
      "learning_rate": 2.2320000000000003e-05,
      "loss": -112.0345,
      "step": 66630
    },
    {
      "epoch": 5.3312,
      "grad_norm": 30.340200424194336,
      "learning_rate": 2.2293333333333336e-05,
      "loss": -111.7506,
      "step": 66640
    },
    {
      "epoch": 5.332,
      "grad_norm": 87.46504974365234,
      "learning_rate": 2.2266666666666668e-05,
      "loss": -111.404,
      "step": 66650
    },
    {
      "epoch": 5.3328,
      "grad_norm": 624.66015625,
      "learning_rate": 2.224e-05,
      "loss": -112.6853,
      "step": 66660
    },
    {
      "epoch": 5.3336,
      "grad_norm": 40.99657440185547,
      "learning_rate": 2.2213333333333333e-05,
      "loss": -111.8779,
      "step": 66670
    },
    {
      "epoch": 5.3344,
      "grad_norm": 66.07609558105469,
      "learning_rate": 2.2186666666666665e-05,
      "loss": -111.7711,
      "step": 66680
    },
    {
      "epoch": 5.3352,
      "grad_norm": 75.15989685058594,
      "learning_rate": 2.216e-05,
      "loss": -111.9757,
      "step": 66690
    },
    {
      "epoch": 5.336,
      "grad_norm": 50.201778411865234,
      "learning_rate": 2.2133333333333334e-05,
      "loss": -111.5423,
      "step": 66700
    },
    {
      "epoch": 5.3368,
      "grad_norm": 47.0042839050293,
      "learning_rate": 2.2106666666666666e-05,
      "loss": -112.2668,
      "step": 66710
    },
    {
      "epoch": 5.3376,
      "grad_norm": 38.103546142578125,
      "learning_rate": 2.2080000000000002e-05,
      "loss": -111.8269,
      "step": 66720
    },
    {
      "epoch": 5.3384,
      "grad_norm": 39.77019500732422,
      "learning_rate": 2.2053333333333335e-05,
      "loss": -111.0338,
      "step": 66730
    },
    {
      "epoch": 5.3392,
      "grad_norm": 124.03136444091797,
      "learning_rate": 2.2026666666666667e-05,
      "loss": -112.1535,
      "step": 66740
    },
    {
      "epoch": 5.34,
      "grad_norm": 205.7227325439453,
      "learning_rate": 2.2000000000000003e-05,
      "loss": -111.8709,
      "step": 66750
    },
    {
      "epoch": 5.3408,
      "grad_norm": 80.75493621826172,
      "learning_rate": 2.1973333333333335e-05,
      "loss": -111.7491,
      "step": 66760
    },
    {
      "epoch": 5.3416,
      "grad_norm": 131.77940368652344,
      "learning_rate": 2.1946666666666668e-05,
      "loss": -112.0844,
      "step": 66770
    },
    {
      "epoch": 5.3424,
      "grad_norm": 102.51337432861328,
      "learning_rate": 2.192e-05,
      "loss": -111.8609,
      "step": 66780
    },
    {
      "epoch": 5.3432,
      "grad_norm": 136.98635864257812,
      "learning_rate": 2.1893333333333336e-05,
      "loss": -111.1796,
      "step": 66790
    },
    {
      "epoch": 5.344,
      "grad_norm": 29.553884506225586,
      "learning_rate": 2.186666666666667e-05,
      "loss": -112.1962,
      "step": 66800
    },
    {
      "epoch": 5.3448,
      "grad_norm": 28.858722686767578,
      "learning_rate": 2.184e-05,
      "loss": -112.6172,
      "step": 66810
    },
    {
      "epoch": 5.3456,
      "grad_norm": 84.32720947265625,
      "learning_rate": 2.1813333333333337e-05,
      "loss": -111.7682,
      "step": 66820
    },
    {
      "epoch": 5.3464,
      "grad_norm": 160.18161010742188,
      "learning_rate": 2.1786666666666666e-05,
      "loss": -111.7368,
      "step": 66830
    },
    {
      "epoch": 5.3472,
      "grad_norm": 81.94694519042969,
      "learning_rate": 2.176e-05,
      "loss": -111.7637,
      "step": 66840
    },
    {
      "epoch": 5.348,
      "grad_norm": 25.02089500427246,
      "learning_rate": 2.1733333333333334e-05,
      "loss": -110.965,
      "step": 66850
    },
    {
      "epoch": 5.3488,
      "grad_norm": 76.1573257446289,
      "learning_rate": 2.1706666666666667e-05,
      "loss": -111.2008,
      "step": 66860
    },
    {
      "epoch": 5.3496,
      "grad_norm": 71.1374740600586,
      "learning_rate": 2.168e-05,
      "loss": -112.2297,
      "step": 66870
    },
    {
      "epoch": 5.3504,
      "grad_norm": 103.8370590209961,
      "learning_rate": 2.1653333333333335e-05,
      "loss": -111.6033,
      "step": 66880
    },
    {
      "epoch": 5.3512,
      "grad_norm": 51.46300506591797,
      "learning_rate": 2.1626666666666667e-05,
      "loss": -111.3383,
      "step": 66890
    },
    {
      "epoch": 5.352,
      "grad_norm": 21.66303253173828,
      "learning_rate": 2.16e-05,
      "loss": -111.7426,
      "step": 66900
    },
    {
      "epoch": 5.3528,
      "grad_norm": 81.84064483642578,
      "learning_rate": 2.1573333333333336e-05,
      "loss": -112.7959,
      "step": 66910
    },
    {
      "epoch": 5.3536,
      "grad_norm": 193.25186157226562,
      "learning_rate": 2.1546666666666668e-05,
      "loss": -110.4253,
      "step": 66920
    },
    {
      "epoch": 5.3544,
      "grad_norm": 118.16753387451172,
      "learning_rate": 2.152e-05,
      "loss": -112.2131,
      "step": 66930
    },
    {
      "epoch": 5.3552,
      "grad_norm": 33.15745544433594,
      "learning_rate": 2.1493333333333333e-05,
      "loss": -111.0546,
      "step": 66940
    },
    {
      "epoch": 5.356,
      "grad_norm": 50.11179733276367,
      "learning_rate": 2.146666666666667e-05,
      "loss": -112.3034,
      "step": 66950
    },
    {
      "epoch": 5.3568,
      "grad_norm": 35.955413818359375,
      "learning_rate": 2.144e-05,
      "loss": -112.2964,
      "step": 66960
    },
    {
      "epoch": 5.3576,
      "grad_norm": 95.5596923828125,
      "learning_rate": 2.1413333333333334e-05,
      "loss": -111.6107,
      "step": 66970
    },
    {
      "epoch": 5.3584,
      "grad_norm": 32.91535186767578,
      "learning_rate": 2.138666666666667e-05,
      "loss": -111.4628,
      "step": 66980
    },
    {
      "epoch": 5.3592,
      "grad_norm": 42.94283676147461,
      "learning_rate": 2.1360000000000002e-05,
      "loss": -111.7408,
      "step": 66990
    },
    {
      "epoch": 5.36,
      "grad_norm": 87.51985168457031,
      "learning_rate": 2.1333333333333335e-05,
      "loss": -111.2679,
      "step": 67000
    },
    {
      "epoch": 5.3608,
      "grad_norm": 30.300935745239258,
      "learning_rate": 2.1306666666666667e-05,
      "loss": -111.412,
      "step": 67010
    },
    {
      "epoch": 5.3616,
      "grad_norm": 209.5414276123047,
      "learning_rate": 2.128e-05,
      "loss": -111.1602,
      "step": 67020
    },
    {
      "epoch": 5.3624,
      "grad_norm": 43.51801300048828,
      "learning_rate": 2.1253333333333332e-05,
      "loss": -111.0604,
      "step": 67030
    },
    {
      "epoch": 5.3632,
      "grad_norm": 82.87786102294922,
      "learning_rate": 2.1226666666666668e-05,
      "loss": -112.0986,
      "step": 67040
    },
    {
      "epoch": 5.364,
      "grad_norm": 61.49623489379883,
      "learning_rate": 2.12e-05,
      "loss": -112.262,
      "step": 67050
    },
    {
      "epoch": 5.3648,
      "grad_norm": 23.39051628112793,
      "learning_rate": 2.1173333333333333e-05,
      "loss": -111.3898,
      "step": 67060
    },
    {
      "epoch": 5.3656,
      "grad_norm": 56.909706115722656,
      "learning_rate": 2.114666666666667e-05,
      "loss": -112.4823,
      "step": 67070
    },
    {
      "epoch": 5.3664,
      "grad_norm": 98.16780853271484,
      "learning_rate": 2.112e-05,
      "loss": -112.5001,
      "step": 67080
    },
    {
      "epoch": 5.3672,
      "grad_norm": 131.3043975830078,
      "learning_rate": 2.1093333333333334e-05,
      "loss": -111.9743,
      "step": 67090
    },
    {
      "epoch": 5.368,
      "grad_norm": 88.18838500976562,
      "learning_rate": 2.106666666666667e-05,
      "loss": -111.431,
      "step": 67100
    },
    {
      "epoch": 5.3688,
      "grad_norm": 128.6486053466797,
      "learning_rate": 2.1040000000000002e-05,
      "loss": -111.3155,
      "step": 67110
    },
    {
      "epoch": 5.3696,
      "grad_norm": 34.246437072753906,
      "learning_rate": 2.1013333333333334e-05,
      "loss": -110.7696,
      "step": 67120
    },
    {
      "epoch": 5.3704,
      "grad_norm": 34.81199645996094,
      "learning_rate": 2.0986666666666667e-05,
      "loss": -112.233,
      "step": 67130
    },
    {
      "epoch": 5.3712,
      "grad_norm": 32.660194396972656,
      "learning_rate": 2.0960000000000003e-05,
      "loss": -111.6916,
      "step": 67140
    },
    {
      "epoch": 5.372,
      "grad_norm": 60.79617691040039,
      "learning_rate": 2.0933333333333335e-05,
      "loss": -111.3442,
      "step": 67150
    },
    {
      "epoch": 5.3728,
      "grad_norm": 99.67438507080078,
      "learning_rate": 2.0906666666666668e-05,
      "loss": -111.1178,
      "step": 67160
    },
    {
      "epoch": 5.3736,
      "grad_norm": 38.676673889160156,
      "learning_rate": 2.0880000000000003e-05,
      "loss": -111.8282,
      "step": 67170
    },
    {
      "epoch": 5.3744,
      "grad_norm": 688.0435791015625,
      "learning_rate": 2.0853333333333332e-05,
      "loss": -111.412,
      "step": 67180
    },
    {
      "epoch": 5.3751999999999995,
      "grad_norm": 54.72941970825195,
      "learning_rate": 2.0826666666666665e-05,
      "loss": -112.1316,
      "step": 67190
    },
    {
      "epoch": 5.376,
      "grad_norm": 92.73409271240234,
      "learning_rate": 2.08e-05,
      "loss": -111.1199,
      "step": 67200
    },
    {
      "epoch": 5.3768,
      "grad_norm": 29.612382888793945,
      "learning_rate": 2.0773333333333333e-05,
      "loss": -111.7656,
      "step": 67210
    },
    {
      "epoch": 5.3776,
      "grad_norm": 82.2630844116211,
      "learning_rate": 2.0746666666666666e-05,
      "loss": -111.7359,
      "step": 67220
    },
    {
      "epoch": 5.3784,
      "grad_norm": 31.711225509643555,
      "learning_rate": 2.072e-05,
      "loss": -110.5321,
      "step": 67230
    },
    {
      "epoch": 5.3792,
      "grad_norm": 134.17315673828125,
      "learning_rate": 2.0693333333333334e-05,
      "loss": -112.1828,
      "step": 67240
    },
    {
      "epoch": 5.38,
      "grad_norm": 37.125858306884766,
      "learning_rate": 2.0666666666666666e-05,
      "loss": -112.7825,
      "step": 67250
    },
    {
      "epoch": 5.3808,
      "grad_norm": 72.66775512695312,
      "learning_rate": 2.0640000000000002e-05,
      "loss": -111.7291,
      "step": 67260
    },
    {
      "epoch": 5.3816,
      "grad_norm": 23.08667755126953,
      "learning_rate": 2.0613333333333335e-05,
      "loss": -112.6027,
      "step": 67270
    },
    {
      "epoch": 5.3824,
      "grad_norm": 120.36856079101562,
      "learning_rate": 2.0586666666666667e-05,
      "loss": -111.4551,
      "step": 67280
    },
    {
      "epoch": 5.3832,
      "grad_norm": 83.41309356689453,
      "learning_rate": 2.0560000000000003e-05,
      "loss": -111.7601,
      "step": 67290
    },
    {
      "epoch": 5.384,
      "grad_norm": 127.93894958496094,
      "learning_rate": 2.0533333333333336e-05,
      "loss": -111.2548,
      "step": 67300
    },
    {
      "epoch": 5.3848,
      "grad_norm": 37.08546447753906,
      "learning_rate": 2.0506666666666668e-05,
      "loss": -111.7835,
      "step": 67310
    },
    {
      "epoch": 5.3856,
      "grad_norm": 154.8335723876953,
      "learning_rate": 2.048e-05,
      "loss": -111.6326,
      "step": 67320
    },
    {
      "epoch": 5.3864,
      "grad_norm": 25.110248565673828,
      "learning_rate": 2.0453333333333336e-05,
      "loss": -111.3222,
      "step": 67330
    },
    {
      "epoch": 5.3872,
      "grad_norm": 35.9945182800293,
      "learning_rate": 2.042666666666667e-05,
      "loss": -111.4279,
      "step": 67340
    },
    {
      "epoch": 5.388,
      "grad_norm": 27.10045051574707,
      "learning_rate": 2.04e-05,
      "loss": -111.3321,
      "step": 67350
    },
    {
      "epoch": 5.3888,
      "grad_norm": 97.5973892211914,
      "learning_rate": 2.0373333333333334e-05,
      "loss": -112.0037,
      "step": 67360
    },
    {
      "epoch": 5.3896,
      "grad_norm": 39.151885986328125,
      "learning_rate": 2.0346666666666666e-05,
      "loss": -111.1045,
      "step": 67370
    },
    {
      "epoch": 5.3904,
      "grad_norm": 23.122968673706055,
      "learning_rate": 2.032e-05,
      "loss": -111.5738,
      "step": 67380
    },
    {
      "epoch": 5.3911999999999995,
      "grad_norm": 43.6396369934082,
      "learning_rate": 2.0293333333333334e-05,
      "loss": -111.5543,
      "step": 67390
    },
    {
      "epoch": 5.392,
      "grad_norm": 26.071453094482422,
      "learning_rate": 2.0266666666666667e-05,
      "loss": -111.7664,
      "step": 67400
    },
    {
      "epoch": 5.3928,
      "grad_norm": 24.27828025817871,
      "learning_rate": 2.024e-05,
      "loss": -111.5922,
      "step": 67410
    },
    {
      "epoch": 5.3936,
      "grad_norm": 46.466468811035156,
      "learning_rate": 2.0213333333333335e-05,
      "loss": -112.2444,
      "step": 67420
    },
    {
      "epoch": 5.3944,
      "grad_norm": 197.78237915039062,
      "learning_rate": 2.0186666666666668e-05,
      "loss": -112.0531,
      "step": 67430
    },
    {
      "epoch": 5.3952,
      "grad_norm": 78.54965209960938,
      "learning_rate": 2.016e-05,
      "loss": -111.2615,
      "step": 67440
    },
    {
      "epoch": 5.396,
      "grad_norm": 40.42570495605469,
      "learning_rate": 2.0133333333333336e-05,
      "loss": -111.8471,
      "step": 67450
    },
    {
      "epoch": 5.3968,
      "grad_norm": 73.7267837524414,
      "learning_rate": 2.010666666666667e-05,
      "loss": -111.9308,
      "step": 67460
    },
    {
      "epoch": 5.3976,
      "grad_norm": 68.94605255126953,
      "learning_rate": 2.008e-05,
      "loss": -111.6405,
      "step": 67470
    },
    {
      "epoch": 5.3984,
      "grad_norm": 71.70026397705078,
      "learning_rate": 2.0053333333333337e-05,
      "loss": -111.1683,
      "step": 67480
    },
    {
      "epoch": 5.3992,
      "grad_norm": 35.25434494018555,
      "learning_rate": 2.002666666666667e-05,
      "loss": -110.8005,
      "step": 67490
    },
    {
      "epoch": 5.4,
      "grad_norm": 84.43868255615234,
      "learning_rate": 2e-05,
      "loss": -111.5499,
      "step": 67500
    },
    {
      "epoch": 5.4008,
      "grad_norm": 47.4162712097168,
      "learning_rate": 1.9973333333333334e-05,
      "loss": -111.4089,
      "step": 67510
    },
    {
      "epoch": 5.4016,
      "grad_norm": 76.8758544921875,
      "learning_rate": 1.9946666666666667e-05,
      "loss": -111.9,
      "step": 67520
    },
    {
      "epoch": 5.4024,
      "grad_norm": 48.98451614379883,
      "learning_rate": 1.992e-05,
      "loss": -112.7203,
      "step": 67530
    },
    {
      "epoch": 5.4032,
      "grad_norm": 21.707839965820312,
      "learning_rate": 1.9893333333333335e-05,
      "loss": -111.1632,
      "step": 67540
    },
    {
      "epoch": 5.404,
      "grad_norm": 139.1539764404297,
      "learning_rate": 1.9866666666666667e-05,
      "loss": -112.3019,
      "step": 67550
    },
    {
      "epoch": 5.4048,
      "grad_norm": 47.45221710205078,
      "learning_rate": 1.984e-05,
      "loss": -111.173,
      "step": 67560
    },
    {
      "epoch": 5.4056,
      "grad_norm": 65.09221649169922,
      "learning_rate": 1.9813333333333332e-05,
      "loss": -112.092,
      "step": 67570
    },
    {
      "epoch": 5.4064,
      "grad_norm": 131.5304412841797,
      "learning_rate": 1.9786666666666668e-05,
      "loss": -112.8928,
      "step": 67580
    },
    {
      "epoch": 5.4072,
      "grad_norm": 32.1087646484375,
      "learning_rate": 1.976e-05,
      "loss": -111.4281,
      "step": 67590
    },
    {
      "epoch": 5.408,
      "grad_norm": 63.37485885620117,
      "learning_rate": 1.9733333333333333e-05,
      "loss": -112.1363,
      "step": 67600
    },
    {
      "epoch": 5.4088,
      "grad_norm": 29.619670867919922,
      "learning_rate": 1.970666666666667e-05,
      "loss": -111.8044,
      "step": 67610
    },
    {
      "epoch": 5.4096,
      "grad_norm": 65.99105072021484,
      "learning_rate": 1.968e-05,
      "loss": -111.9644,
      "step": 67620
    },
    {
      "epoch": 5.4104,
      "grad_norm": 65.52171325683594,
      "learning_rate": 1.9653333333333334e-05,
      "loss": -111.1245,
      "step": 67630
    },
    {
      "epoch": 5.4112,
      "grad_norm": 99.39545440673828,
      "learning_rate": 1.962666666666667e-05,
      "loss": -112.2366,
      "step": 67640
    },
    {
      "epoch": 5.412,
      "grad_norm": 97.82339477539062,
      "learning_rate": 1.9600000000000002e-05,
      "loss": -110.9964,
      "step": 67650
    },
    {
      "epoch": 5.4128,
      "grad_norm": 32.049285888671875,
      "learning_rate": 1.9573333333333335e-05,
      "loss": -111.431,
      "step": 67660
    },
    {
      "epoch": 5.4136,
      "grad_norm": 59.35361099243164,
      "learning_rate": 1.9546666666666667e-05,
      "loss": -111.1144,
      "step": 67670
    },
    {
      "epoch": 5.4144,
      "grad_norm": 49.466068267822266,
      "learning_rate": 1.9520000000000003e-05,
      "loss": -110.7544,
      "step": 67680
    },
    {
      "epoch": 5.4152000000000005,
      "grad_norm": 57.92668533325195,
      "learning_rate": 1.9493333333333332e-05,
      "loss": -111.3976,
      "step": 67690
    },
    {
      "epoch": 5.416,
      "grad_norm": 129.45025634765625,
      "learning_rate": 1.9466666666666668e-05,
      "loss": -111.2403,
      "step": 67700
    },
    {
      "epoch": 5.4168,
      "grad_norm": 20.937021255493164,
      "learning_rate": 1.944e-05,
      "loss": -112.3199,
      "step": 67710
    },
    {
      "epoch": 5.4176,
      "grad_norm": 35.22175979614258,
      "learning_rate": 1.9413333333333333e-05,
      "loss": -111.3256,
      "step": 67720
    },
    {
      "epoch": 5.4184,
      "grad_norm": 34.87516403198242,
      "learning_rate": 1.938666666666667e-05,
      "loss": -111.5228,
      "step": 67730
    },
    {
      "epoch": 5.4192,
      "grad_norm": 65.30726623535156,
      "learning_rate": 1.936e-05,
      "loss": -111.6828,
      "step": 67740
    },
    {
      "epoch": 5.42,
      "grad_norm": 105.90608215332031,
      "learning_rate": 1.9333333333333333e-05,
      "loss": -111.694,
      "step": 67750
    },
    {
      "epoch": 5.4208,
      "grad_norm": 51.06536102294922,
      "learning_rate": 1.9306666666666666e-05,
      "loss": -111.8283,
      "step": 67760
    },
    {
      "epoch": 5.4216,
      "grad_norm": 44.639530181884766,
      "learning_rate": 1.9280000000000002e-05,
      "loss": -112.2226,
      "step": 67770
    },
    {
      "epoch": 5.4224,
      "grad_norm": 68.43268585205078,
      "learning_rate": 1.9253333333333334e-05,
      "loss": -112.6537,
      "step": 67780
    },
    {
      "epoch": 5.4232,
      "grad_norm": 32.000545501708984,
      "learning_rate": 1.9226666666666667e-05,
      "loss": -111.5981,
      "step": 67790
    },
    {
      "epoch": 5.424,
      "grad_norm": 31.91661262512207,
      "learning_rate": 1.9200000000000003e-05,
      "loss": -111.4212,
      "step": 67800
    },
    {
      "epoch": 5.4248,
      "grad_norm": 81.07584381103516,
      "learning_rate": 1.9173333333333335e-05,
      "loss": -111.1594,
      "step": 67810
    },
    {
      "epoch": 5.4256,
      "grad_norm": 145.1483917236328,
      "learning_rate": 1.9146666666666667e-05,
      "loss": -113.4961,
      "step": 67820
    },
    {
      "epoch": 5.4264,
      "grad_norm": 98.5909194946289,
      "learning_rate": 1.9120000000000003e-05,
      "loss": -111.7071,
      "step": 67830
    },
    {
      "epoch": 5.4272,
      "grad_norm": 237.60733032226562,
      "learning_rate": 1.9093333333333336e-05,
      "loss": -111.8503,
      "step": 67840
    },
    {
      "epoch": 5.428,
      "grad_norm": 54.66102600097656,
      "learning_rate": 1.9066666666666668e-05,
      "loss": -111.8711,
      "step": 67850
    },
    {
      "epoch": 5.4288,
      "grad_norm": 70.1168441772461,
      "learning_rate": 1.904e-05,
      "loss": -112.1152,
      "step": 67860
    },
    {
      "epoch": 5.4296,
      "grad_norm": 107.63581848144531,
      "learning_rate": 1.9013333333333333e-05,
      "loss": -111.807,
      "step": 67870
    },
    {
      "epoch": 5.4304,
      "grad_norm": 46.10236358642578,
      "learning_rate": 1.8986666666666666e-05,
      "loss": -111.629,
      "step": 67880
    },
    {
      "epoch": 5.4312000000000005,
      "grad_norm": 20.51361656188965,
      "learning_rate": 1.896e-05,
      "loss": -112.0607,
      "step": 67890
    },
    {
      "epoch": 5.432,
      "grad_norm": 16.54216766357422,
      "learning_rate": 1.8933333333333334e-05,
      "loss": -111.5018,
      "step": 67900
    },
    {
      "epoch": 5.4328,
      "grad_norm": 71.56415557861328,
      "learning_rate": 1.8906666666666666e-05,
      "loss": -111.3797,
      "step": 67910
    },
    {
      "epoch": 5.4336,
      "grad_norm": 56.61095428466797,
      "learning_rate": 1.888e-05,
      "loss": -111.6262,
      "step": 67920
    },
    {
      "epoch": 5.4344,
      "grad_norm": 65.1837387084961,
      "learning_rate": 1.8853333333333335e-05,
      "loss": -111.5205,
      "step": 67930
    },
    {
      "epoch": 5.4352,
      "grad_norm": 24.821924209594727,
      "learning_rate": 1.8826666666666667e-05,
      "loss": -112.0024,
      "step": 67940
    },
    {
      "epoch": 5.436,
      "grad_norm": 60.51036071777344,
      "learning_rate": 1.88e-05,
      "loss": -111.7786,
      "step": 67950
    },
    {
      "epoch": 5.4368,
      "grad_norm": 24.329116821289062,
      "learning_rate": 1.8773333333333335e-05,
      "loss": -112.0178,
      "step": 67960
    },
    {
      "epoch": 5.4376,
      "grad_norm": 59.94759750366211,
      "learning_rate": 1.8746666666666668e-05,
      "loss": -112.3304,
      "step": 67970
    },
    {
      "epoch": 5.4384,
      "grad_norm": 30.913646697998047,
      "learning_rate": 1.872e-05,
      "loss": -110.5593,
      "step": 67980
    },
    {
      "epoch": 5.4392,
      "grad_norm": 23.756166458129883,
      "learning_rate": 1.8693333333333336e-05,
      "loss": -111.8484,
      "step": 67990
    },
    {
      "epoch": 5.44,
      "grad_norm": 24.64979362487793,
      "learning_rate": 1.866666666666667e-05,
      "loss": -111.5718,
      "step": 68000
    },
    {
      "epoch": 5.4408,
      "grad_norm": 177.43679809570312,
      "learning_rate": 1.864e-05,
      "loss": -111.9058,
      "step": 68010
    },
    {
      "epoch": 5.4416,
      "grad_norm": 73.13427734375,
      "learning_rate": 1.8613333333333337e-05,
      "loss": -111.6987,
      "step": 68020
    },
    {
      "epoch": 5.4424,
      "grad_norm": 90.86801147460938,
      "learning_rate": 1.858666666666667e-05,
      "loss": -112.3929,
      "step": 68030
    },
    {
      "epoch": 5.4432,
      "grad_norm": 38.486331939697266,
      "learning_rate": 1.856e-05,
      "loss": -110.9911,
      "step": 68040
    },
    {
      "epoch": 5.444,
      "grad_norm": 51.2582893371582,
      "learning_rate": 1.8533333333333334e-05,
      "loss": -111.5517,
      "step": 68050
    },
    {
      "epoch": 5.4448,
      "grad_norm": 38.64320755004883,
      "learning_rate": 1.8506666666666667e-05,
      "loss": -111.9816,
      "step": 68060
    },
    {
      "epoch": 5.4456,
      "grad_norm": 39.888343811035156,
      "learning_rate": 1.848e-05,
      "loss": -110.6156,
      "step": 68070
    },
    {
      "epoch": 5.4464,
      "grad_norm": 85.27632141113281,
      "learning_rate": 1.8453333333333335e-05,
      "loss": -112.0678,
      "step": 68080
    },
    {
      "epoch": 5.4472,
      "grad_norm": 57.830196380615234,
      "learning_rate": 1.8426666666666668e-05,
      "loss": -111.431,
      "step": 68090
    },
    {
      "epoch": 5.448,
      "grad_norm": 123.71533966064453,
      "learning_rate": 1.84e-05,
      "loss": -111.6573,
      "step": 68100
    },
    {
      "epoch": 5.4488,
      "grad_norm": 25.155963897705078,
      "learning_rate": 1.8373333333333332e-05,
      "loss": -110.7977,
      "step": 68110
    },
    {
      "epoch": 5.4496,
      "grad_norm": 40.817596435546875,
      "learning_rate": 1.834666666666667e-05,
      "loss": -111.1883,
      "step": 68120
    },
    {
      "epoch": 5.4504,
      "grad_norm": 50.699676513671875,
      "learning_rate": 1.832e-05,
      "loss": -111.1305,
      "step": 68130
    },
    {
      "epoch": 5.4512,
      "grad_norm": 35.8002815246582,
      "learning_rate": 1.8293333333333333e-05,
      "loss": -111.7017,
      "step": 68140
    },
    {
      "epoch": 5.452,
      "grad_norm": 36.45201110839844,
      "learning_rate": 1.826666666666667e-05,
      "loss": -111.6753,
      "step": 68150
    },
    {
      "epoch": 5.4528,
      "grad_norm": 97.72393035888672,
      "learning_rate": 1.824e-05,
      "loss": -111.5051,
      "step": 68160
    },
    {
      "epoch": 5.4536,
      "grad_norm": 40.42351531982422,
      "learning_rate": 1.8213333333333334e-05,
      "loss": -110.7236,
      "step": 68170
    },
    {
      "epoch": 5.4544,
      "grad_norm": 36.20058822631836,
      "learning_rate": 1.818666666666667e-05,
      "loss": -111.5231,
      "step": 68180
    },
    {
      "epoch": 5.4552,
      "grad_norm": 79.32506561279297,
      "learning_rate": 1.8160000000000002e-05,
      "loss": -111.7568,
      "step": 68190
    },
    {
      "epoch": 5.456,
      "grad_norm": 57.127750396728516,
      "learning_rate": 1.8133333333333335e-05,
      "loss": -111.8661,
      "step": 68200
    },
    {
      "epoch": 5.4568,
      "grad_norm": 75.12247467041016,
      "learning_rate": 1.8106666666666667e-05,
      "loss": -111.1672,
      "step": 68210
    },
    {
      "epoch": 5.4576,
      "grad_norm": 52.50681686401367,
      "learning_rate": 1.808e-05,
      "loss": -111.8431,
      "step": 68220
    },
    {
      "epoch": 5.4584,
      "grad_norm": 96.88510131835938,
      "learning_rate": 1.8053333333333332e-05,
      "loss": -112.1676,
      "step": 68230
    },
    {
      "epoch": 5.4592,
      "grad_norm": 80.7186508178711,
      "learning_rate": 1.8026666666666668e-05,
      "loss": -110.9161,
      "step": 68240
    },
    {
      "epoch": 5.46,
      "grad_norm": 63.393577575683594,
      "learning_rate": 1.8e-05,
      "loss": -111.9681,
      "step": 68250
    },
    {
      "epoch": 5.4608,
      "grad_norm": 35.69322204589844,
      "learning_rate": 1.7973333333333333e-05,
      "loss": -111.6913,
      "step": 68260
    },
    {
      "epoch": 5.4616,
      "grad_norm": 27.988048553466797,
      "learning_rate": 1.794666666666667e-05,
      "loss": -112.0268,
      "step": 68270
    },
    {
      "epoch": 5.4624,
      "grad_norm": 91.42620086669922,
      "learning_rate": 1.792e-05,
      "loss": -111.7415,
      "step": 68280
    },
    {
      "epoch": 5.4632,
      "grad_norm": 57.62809371948242,
      "learning_rate": 1.7893333333333334e-05,
      "loss": -112.235,
      "step": 68290
    },
    {
      "epoch": 5.464,
      "grad_norm": 202.69871520996094,
      "learning_rate": 1.7866666666666666e-05,
      "loss": -112.4942,
      "step": 68300
    },
    {
      "epoch": 5.4648,
      "grad_norm": 65.1165771484375,
      "learning_rate": 1.7840000000000002e-05,
      "loss": -110.4116,
      "step": 68310
    },
    {
      "epoch": 5.4656,
      "grad_norm": 29.414966583251953,
      "learning_rate": 1.7813333333333334e-05,
      "loss": -110.8597,
      "step": 68320
    },
    {
      "epoch": 5.4664,
      "grad_norm": 105.45086669921875,
      "learning_rate": 1.7786666666666667e-05,
      "loss": -112.0522,
      "step": 68330
    },
    {
      "epoch": 5.4672,
      "grad_norm": 150.67185974121094,
      "learning_rate": 1.7760000000000003e-05,
      "loss": -111.883,
      "step": 68340
    },
    {
      "epoch": 5.468,
      "grad_norm": 73.53426361083984,
      "learning_rate": 1.7733333333333335e-05,
      "loss": -111.8201,
      "step": 68350
    },
    {
      "epoch": 5.4688,
      "grad_norm": 189.5615997314453,
      "learning_rate": 1.7706666666666668e-05,
      "loss": -111.2362,
      "step": 68360
    },
    {
      "epoch": 5.4696,
      "grad_norm": 81.65409851074219,
      "learning_rate": 1.7680000000000004e-05,
      "loss": -110.92,
      "step": 68370
    },
    {
      "epoch": 5.4704,
      "grad_norm": 159.47512817382812,
      "learning_rate": 1.7653333333333333e-05,
      "loss": -111.048,
      "step": 68380
    },
    {
      "epoch": 5.4712,
      "grad_norm": 32.41484832763672,
      "learning_rate": 1.7626666666666665e-05,
      "loss": -111.764,
      "step": 68390
    },
    {
      "epoch": 5.4719999999999995,
      "grad_norm": 20.421152114868164,
      "learning_rate": 1.76e-05,
      "loss": -112.3685,
      "step": 68400
    },
    {
      "epoch": 5.4728,
      "grad_norm": 50.36018371582031,
      "learning_rate": 1.7573333333333333e-05,
      "loss": -111.359,
      "step": 68410
    },
    {
      "epoch": 5.4736,
      "grad_norm": 62.485042572021484,
      "learning_rate": 1.7546666666666666e-05,
      "loss": -112.1129,
      "step": 68420
    },
    {
      "epoch": 5.4744,
      "grad_norm": 32.79338455200195,
      "learning_rate": 1.752e-05,
      "loss": -112.1004,
      "step": 68430
    },
    {
      "epoch": 5.4752,
      "grad_norm": 135.14727783203125,
      "learning_rate": 1.7493333333333334e-05,
      "loss": -110.9081,
      "step": 68440
    },
    {
      "epoch": 5.476,
      "grad_norm": 36.97482681274414,
      "learning_rate": 1.7466666666666667e-05,
      "loss": -111.8094,
      "step": 68450
    },
    {
      "epoch": 5.4768,
      "grad_norm": 28.32308006286621,
      "learning_rate": 1.7440000000000002e-05,
      "loss": -111.8197,
      "step": 68460
    },
    {
      "epoch": 5.4776,
      "grad_norm": 96.57772064208984,
      "learning_rate": 1.7413333333333335e-05,
      "loss": -111.5738,
      "step": 68470
    },
    {
      "epoch": 5.4784,
      "grad_norm": 66.1665267944336,
      "learning_rate": 1.7386666666666667e-05,
      "loss": -112.4688,
      "step": 68480
    },
    {
      "epoch": 5.4792,
      "grad_norm": 59.55647277832031,
      "learning_rate": 1.736e-05,
      "loss": -112.0215,
      "step": 68490
    },
    {
      "epoch": 5.48,
      "grad_norm": 71.20048522949219,
      "learning_rate": 1.7333333333333336e-05,
      "loss": -111.9784,
      "step": 68500
    },
    {
      "epoch": 5.4808,
      "grad_norm": 29.67290496826172,
      "learning_rate": 1.7306666666666668e-05,
      "loss": -112.6489,
      "step": 68510
    },
    {
      "epoch": 5.4816,
      "grad_norm": 81.61811065673828,
      "learning_rate": 1.728e-05,
      "loss": -112.1479,
      "step": 68520
    },
    {
      "epoch": 5.4824,
      "grad_norm": 28.56791877746582,
      "learning_rate": 1.7253333333333336e-05,
      "loss": -111.686,
      "step": 68530
    },
    {
      "epoch": 5.4832,
      "grad_norm": 118.08125305175781,
      "learning_rate": 1.722666666666667e-05,
      "loss": -112.2771,
      "step": 68540
    },
    {
      "epoch": 5.484,
      "grad_norm": 80.2896499633789,
      "learning_rate": 1.7199999999999998e-05,
      "loss": -111.3962,
      "step": 68550
    },
    {
      "epoch": 5.4848,
      "grad_norm": 27.709306716918945,
      "learning_rate": 1.7173333333333334e-05,
      "loss": -111.2068,
      "step": 68560
    },
    {
      "epoch": 5.4856,
      "grad_norm": 55.21160125732422,
      "learning_rate": 1.7146666666666666e-05,
      "loss": -111.6055,
      "step": 68570
    },
    {
      "epoch": 5.4864,
      "grad_norm": 40.136383056640625,
      "learning_rate": 1.712e-05,
      "loss": -111.5789,
      "step": 68580
    },
    {
      "epoch": 5.4872,
      "grad_norm": 59.16823959350586,
      "learning_rate": 1.7093333333333335e-05,
      "loss": -112.195,
      "step": 68590
    },
    {
      "epoch": 5.4879999999999995,
      "grad_norm": 175.0003204345703,
      "learning_rate": 1.7066666666666667e-05,
      "loss": -112.4629,
      "step": 68600
    },
    {
      "epoch": 5.4888,
      "grad_norm": 30.37311553955078,
      "learning_rate": 1.704e-05,
      "loss": -110.6959,
      "step": 68610
    },
    {
      "epoch": 5.4896,
      "grad_norm": 36.67295455932617,
      "learning_rate": 1.7013333333333335e-05,
      "loss": -111.7819,
      "step": 68620
    },
    {
      "epoch": 5.4904,
      "grad_norm": 23.98482322692871,
      "learning_rate": 1.6986666666666668e-05,
      "loss": -111.762,
      "step": 68630
    },
    {
      "epoch": 5.4912,
      "grad_norm": 34.93697738647461,
      "learning_rate": 1.696e-05,
      "loss": -111.8233,
      "step": 68640
    },
    {
      "epoch": 5.492,
      "grad_norm": 171.81492614746094,
      "learning_rate": 1.6933333333333333e-05,
      "loss": -112.0779,
      "step": 68650
    },
    {
      "epoch": 5.4928,
      "grad_norm": 157.95840454101562,
      "learning_rate": 1.690666666666667e-05,
      "loss": -111.4077,
      "step": 68660
    },
    {
      "epoch": 5.4936,
      "grad_norm": 99.5262680053711,
      "learning_rate": 1.688e-05,
      "loss": -111.0468,
      "step": 68670
    },
    {
      "epoch": 5.4944,
      "grad_norm": 55.201454162597656,
      "learning_rate": 1.6853333333333333e-05,
      "loss": -111.5396,
      "step": 68680
    },
    {
      "epoch": 5.4952,
      "grad_norm": 117.5082015991211,
      "learning_rate": 1.682666666666667e-05,
      "loss": -111.4355,
      "step": 68690
    },
    {
      "epoch": 5.496,
      "grad_norm": 106.020263671875,
      "learning_rate": 1.6800000000000002e-05,
      "loss": -112.0151,
      "step": 68700
    },
    {
      "epoch": 5.4968,
      "grad_norm": 26.49448013305664,
      "learning_rate": 1.6773333333333334e-05,
      "loss": -111.0361,
      "step": 68710
    },
    {
      "epoch": 5.4976,
      "grad_norm": 52.261680603027344,
      "learning_rate": 1.674666666666667e-05,
      "loss": -111.4923,
      "step": 68720
    },
    {
      "epoch": 5.4984,
      "grad_norm": 26.33257484436035,
      "learning_rate": 1.672e-05,
      "loss": -112.0036,
      "step": 68730
    },
    {
      "epoch": 5.4992,
      "grad_norm": 60.12884521484375,
      "learning_rate": 1.669333333333333e-05,
      "loss": -111.1826,
      "step": 68740
    },
    {
      "epoch": 5.5,
      "grad_norm": 31.12901496887207,
      "learning_rate": 1.6666666666666667e-05,
      "loss": -111.6936,
      "step": 68750
    },
    {
      "epoch": 5.5008,
      "grad_norm": 102.18682098388672,
      "learning_rate": 1.664e-05,
      "loss": -111.7327,
      "step": 68760
    },
    {
      "epoch": 5.5016,
      "grad_norm": 45.37184524536133,
      "learning_rate": 1.6613333333333332e-05,
      "loss": -111.1813,
      "step": 68770
    },
    {
      "epoch": 5.5024,
      "grad_norm": 46.25507354736328,
      "learning_rate": 1.6586666666666668e-05,
      "loss": -111.5664,
      "step": 68780
    },
    {
      "epoch": 5.5032,
      "grad_norm": 32.592445373535156,
      "learning_rate": 1.656e-05,
      "loss": -111.4139,
      "step": 68790
    },
    {
      "epoch": 5.504,
      "grad_norm": 32.69837188720703,
      "learning_rate": 1.6533333333333333e-05,
      "loss": -112.2984,
      "step": 68800
    },
    {
      "epoch": 5.5048,
      "grad_norm": 127.06077575683594,
      "learning_rate": 1.650666666666667e-05,
      "loss": -112.2689,
      "step": 68810
    },
    {
      "epoch": 5.5056,
      "grad_norm": 34.91169738769531,
      "learning_rate": 1.648e-05,
      "loss": -112.1419,
      "step": 68820
    },
    {
      "epoch": 5.5064,
      "grad_norm": 64.44405364990234,
      "learning_rate": 1.6453333333333334e-05,
      "loss": -111.9636,
      "step": 68830
    },
    {
      "epoch": 5.5072,
      "grad_norm": 112.03723907470703,
      "learning_rate": 1.6426666666666666e-05,
      "loss": -112.1615,
      "step": 68840
    },
    {
      "epoch": 5.508,
      "grad_norm": 24.895116806030273,
      "learning_rate": 1.6400000000000002e-05,
      "loss": -111.1284,
      "step": 68850
    },
    {
      "epoch": 5.5088,
      "grad_norm": 57.60801315307617,
      "learning_rate": 1.6373333333333335e-05,
      "loss": -110.8708,
      "step": 68860
    },
    {
      "epoch": 5.5096,
      "grad_norm": 35.119136810302734,
      "learning_rate": 1.6346666666666667e-05,
      "loss": -111.7857,
      "step": 68870
    },
    {
      "epoch": 5.5104,
      "grad_norm": 59.751461029052734,
      "learning_rate": 1.6320000000000003e-05,
      "loss": -111.7545,
      "step": 68880
    },
    {
      "epoch": 5.5112,
      "grad_norm": 100.31719207763672,
      "learning_rate": 1.6293333333333335e-05,
      "loss": -111.8238,
      "step": 68890
    },
    {
      "epoch": 5.5120000000000005,
      "grad_norm": 74.24754333496094,
      "learning_rate": 1.6266666666666665e-05,
      "loss": -112.3257,
      "step": 68900
    },
    {
      "epoch": 5.5128,
      "grad_norm": 21.173452377319336,
      "learning_rate": 1.624e-05,
      "loss": -111.7701,
      "step": 68910
    },
    {
      "epoch": 5.5136,
      "grad_norm": 92.58707427978516,
      "learning_rate": 1.6213333333333333e-05,
      "loss": -111.4799,
      "step": 68920
    },
    {
      "epoch": 5.5144,
      "grad_norm": 71.66548156738281,
      "learning_rate": 1.6186666666666665e-05,
      "loss": -112.7137,
      "step": 68930
    },
    {
      "epoch": 5.5152,
      "grad_norm": 87.40142059326172,
      "learning_rate": 1.616e-05,
      "loss": -111.7262,
      "step": 68940
    },
    {
      "epoch": 5.516,
      "grad_norm": 35.48957443237305,
      "learning_rate": 1.6133333333333334e-05,
      "loss": -111.2384,
      "step": 68950
    },
    {
      "epoch": 5.5168,
      "grad_norm": 26.580419540405273,
      "learning_rate": 1.6106666666666666e-05,
      "loss": -112.5364,
      "step": 68960
    },
    {
      "epoch": 5.5176,
      "grad_norm": 65.5691146850586,
      "learning_rate": 1.6080000000000002e-05,
      "loss": -111.8539,
      "step": 68970
    },
    {
      "epoch": 5.5184,
      "grad_norm": 77.71659851074219,
      "learning_rate": 1.6053333333333334e-05,
      "loss": -110.9034,
      "step": 68980
    },
    {
      "epoch": 5.5192,
      "grad_norm": 23.498720169067383,
      "learning_rate": 1.6026666666666667e-05,
      "loss": -112.2375,
      "step": 68990
    },
    {
      "epoch": 5.52,
      "grad_norm": 49.15813064575195,
      "learning_rate": 1.6000000000000003e-05,
      "loss": -111.8181,
      "step": 69000
    },
    {
      "epoch": 5.5208,
      "grad_norm": 23.137928009033203,
      "learning_rate": 1.5973333333333335e-05,
      "loss": -111.4173,
      "step": 69010
    },
    {
      "epoch": 5.5216,
      "grad_norm": 33.99637222290039,
      "learning_rate": 1.5946666666666668e-05,
      "loss": -112.5488,
      "step": 69020
    },
    {
      "epoch": 5.5224,
      "grad_norm": 144.07032775878906,
      "learning_rate": 1.592e-05,
      "loss": -111.8254,
      "step": 69030
    },
    {
      "epoch": 5.5232,
      "grad_norm": 40.760929107666016,
      "learning_rate": 1.5893333333333336e-05,
      "loss": -111.0856,
      "step": 69040
    },
    {
      "epoch": 5.524,
      "grad_norm": 32.66090393066406,
      "learning_rate": 1.586666666666667e-05,
      "loss": -112.0594,
      "step": 69050
    },
    {
      "epoch": 5.5248,
      "grad_norm": 22.990535736083984,
      "learning_rate": 1.584e-05,
      "loss": -112.2519,
      "step": 69060
    },
    {
      "epoch": 5.5256,
      "grad_norm": 75.52699279785156,
      "learning_rate": 1.5813333333333333e-05,
      "loss": -110.8852,
      "step": 69070
    },
    {
      "epoch": 5.5264,
      "grad_norm": 101.00823974609375,
      "learning_rate": 1.5786666666666666e-05,
      "loss": -111.9275,
      "step": 69080
    },
    {
      "epoch": 5.5272,
      "grad_norm": 27.9279727935791,
      "learning_rate": 1.5759999999999998e-05,
      "loss": -110.9893,
      "step": 69090
    },
    {
      "epoch": 5.5280000000000005,
      "grad_norm": 38.42121505737305,
      "learning_rate": 1.5733333333333334e-05,
      "loss": -111.9103,
      "step": 69100
    },
    {
      "epoch": 5.5288,
      "grad_norm": 49.69571304321289,
      "learning_rate": 1.5706666666666666e-05,
      "loss": -111.692,
      "step": 69110
    },
    {
      "epoch": 5.5296,
      "grad_norm": 33.013038635253906,
      "learning_rate": 1.568e-05,
      "loss": -111.7734,
      "step": 69120
    },
    {
      "epoch": 5.5304,
      "grad_norm": 90.07987976074219,
      "learning_rate": 1.5653333333333335e-05,
      "loss": -111.7778,
      "step": 69130
    },
    {
      "epoch": 5.5312,
      "grad_norm": 40.32518768310547,
      "learning_rate": 1.5626666666666667e-05,
      "loss": -111.728,
      "step": 69140
    },
    {
      "epoch": 5.532,
      "grad_norm": 102.81720733642578,
      "learning_rate": 1.56e-05,
      "loss": -111.5974,
      "step": 69150
    },
    {
      "epoch": 5.5328,
      "grad_norm": 38.595523834228516,
      "learning_rate": 1.5573333333333336e-05,
      "loss": -111.5334,
      "step": 69160
    },
    {
      "epoch": 5.5336,
      "grad_norm": 100.9291000366211,
      "learning_rate": 1.5546666666666668e-05,
      "loss": -111.9613,
      "step": 69170
    },
    {
      "epoch": 5.5344,
      "grad_norm": 95.30126190185547,
      "learning_rate": 1.552e-05,
      "loss": -112.7789,
      "step": 69180
    },
    {
      "epoch": 5.5352,
      "grad_norm": 38.17194366455078,
      "learning_rate": 1.5493333333333336e-05,
      "loss": -111.7617,
      "step": 69190
    },
    {
      "epoch": 5.536,
      "grad_norm": 142.622802734375,
      "learning_rate": 1.546666666666667e-05,
      "loss": -111.7732,
      "step": 69200
    },
    {
      "epoch": 5.5368,
      "grad_norm": 38.4038200378418,
      "learning_rate": 1.544e-05,
      "loss": -112.2202,
      "step": 69210
    },
    {
      "epoch": 5.5376,
      "grad_norm": 78.76185607910156,
      "learning_rate": 1.5413333333333334e-05,
      "loss": -112.0826,
      "step": 69220
    },
    {
      "epoch": 5.5384,
      "grad_norm": 50.068416595458984,
      "learning_rate": 1.538666666666667e-05,
      "loss": -111.8501,
      "step": 69230
    },
    {
      "epoch": 5.5392,
      "grad_norm": 34.287750244140625,
      "learning_rate": 1.536e-05,
      "loss": -111.2727,
      "step": 69240
    },
    {
      "epoch": 5.54,
      "grad_norm": 32.16214370727539,
      "learning_rate": 1.5333333333333334e-05,
      "loss": -111.8718,
      "step": 69250
    },
    {
      "epoch": 5.5408,
      "grad_norm": 50.8958740234375,
      "learning_rate": 1.5306666666666667e-05,
      "loss": -111.2363,
      "step": 69260
    },
    {
      "epoch": 5.5416,
      "grad_norm": 71.83555603027344,
      "learning_rate": 1.528e-05,
      "loss": -111.2384,
      "step": 69270
    },
    {
      "epoch": 5.5424,
      "grad_norm": 26.749082565307617,
      "learning_rate": 1.5253333333333334e-05,
      "loss": -111.4715,
      "step": 69280
    },
    {
      "epoch": 5.5432,
      "grad_norm": 87.86678314208984,
      "learning_rate": 1.5226666666666668e-05,
      "loss": -111.6542,
      "step": 69290
    },
    {
      "epoch": 5.5440000000000005,
      "grad_norm": 36.12192916870117,
      "learning_rate": 1.52e-05,
      "loss": -111.712,
      "step": 69300
    },
    {
      "epoch": 5.5448,
      "grad_norm": 107.48977661132812,
      "learning_rate": 1.5173333333333334e-05,
      "loss": -112.4033,
      "step": 69310
    },
    {
      "epoch": 5.5456,
      "grad_norm": 116.53645324707031,
      "learning_rate": 1.5146666666666667e-05,
      "loss": -110.7888,
      "step": 69320
    },
    {
      "epoch": 5.5464,
      "grad_norm": 83.04782104492188,
      "learning_rate": 1.5120000000000001e-05,
      "loss": -112.4137,
      "step": 69330
    },
    {
      "epoch": 5.5472,
      "grad_norm": 24.87535858154297,
      "learning_rate": 1.5093333333333335e-05,
      "loss": -110.5118,
      "step": 69340
    },
    {
      "epoch": 5.548,
      "grad_norm": 123.68293762207031,
      "learning_rate": 1.5066666666666668e-05,
      "loss": -112.3227,
      "step": 69350
    },
    {
      "epoch": 5.5488,
      "grad_norm": 35.62331008911133,
      "learning_rate": 1.5040000000000002e-05,
      "loss": -110.793,
      "step": 69360
    },
    {
      "epoch": 5.5496,
      "grad_norm": 47.704498291015625,
      "learning_rate": 1.5013333333333334e-05,
      "loss": -111.238,
      "step": 69370
    },
    {
      "epoch": 5.5504,
      "grad_norm": 40.8498649597168,
      "learning_rate": 1.4986666666666668e-05,
      "loss": -111.2686,
      "step": 69380
    },
    {
      "epoch": 5.5512,
      "grad_norm": 23.4052734375,
      "learning_rate": 1.4960000000000002e-05,
      "loss": -111.5803,
      "step": 69390
    },
    {
      "epoch": 5.552,
      "grad_norm": 60.328983306884766,
      "learning_rate": 1.4933333333333335e-05,
      "loss": -111.7891,
      "step": 69400
    },
    {
      "epoch": 5.5527999999999995,
      "grad_norm": 35.040489196777344,
      "learning_rate": 1.4906666666666666e-05,
      "loss": -112.2334,
      "step": 69410
    },
    {
      "epoch": 5.5536,
      "grad_norm": 97.9154281616211,
      "learning_rate": 1.488e-05,
      "loss": -112.146,
      "step": 69420
    },
    {
      "epoch": 5.5544,
      "grad_norm": 77.976806640625,
      "learning_rate": 1.4853333333333332e-05,
      "loss": -111.3212,
      "step": 69430
    },
    {
      "epoch": 5.5552,
      "grad_norm": 80.99010467529297,
      "learning_rate": 1.4826666666666666e-05,
      "loss": -112.1511,
      "step": 69440
    },
    {
      "epoch": 5.556,
      "grad_norm": 63.58628463745117,
      "learning_rate": 1.48e-05,
      "loss": -110.9824,
      "step": 69450
    },
    {
      "epoch": 5.5568,
      "grad_norm": 29.663694381713867,
      "learning_rate": 1.4773333333333333e-05,
      "loss": -111.6657,
      "step": 69460
    },
    {
      "epoch": 5.5576,
      "grad_norm": 48.87995910644531,
      "learning_rate": 1.4746666666666667e-05,
      "loss": -110.9995,
      "step": 69470
    },
    {
      "epoch": 5.5584,
      "grad_norm": 71.87922668457031,
      "learning_rate": 1.472e-05,
      "loss": -111.7634,
      "step": 69480
    },
    {
      "epoch": 5.5592,
      "grad_norm": 703.7327880859375,
      "learning_rate": 1.4693333333333334e-05,
      "loss": -111.0347,
      "step": 69490
    },
    {
      "epoch": 5.5600000000000005,
      "grad_norm": 104.50252532958984,
      "learning_rate": 1.4666666666666668e-05,
      "loss": -112.0327,
      "step": 69500
    },
    {
      "epoch": 5.5608,
      "grad_norm": 60.4525260925293,
      "learning_rate": 1.464e-05,
      "loss": -110.7658,
      "step": 69510
    },
    {
      "epoch": 5.5616,
      "grad_norm": 84.73430633544922,
      "learning_rate": 1.4613333333333335e-05,
      "loss": -111.4746,
      "step": 69520
    },
    {
      "epoch": 5.5624,
      "grad_norm": 81.99267578125,
      "learning_rate": 1.4586666666666669e-05,
      "loss": -110.7794,
      "step": 69530
    },
    {
      "epoch": 5.5632,
      "grad_norm": 77.28307342529297,
      "learning_rate": 1.4560000000000001e-05,
      "loss": -111.2466,
      "step": 69540
    },
    {
      "epoch": 5.564,
      "grad_norm": 25.229890823364258,
      "learning_rate": 1.4533333333333335e-05,
      "loss": -111.059,
      "step": 69550
    },
    {
      "epoch": 5.5648,
      "grad_norm": 55.47150802612305,
      "learning_rate": 1.4506666666666668e-05,
      "loss": -110.9755,
      "step": 69560
    },
    {
      "epoch": 5.5656,
      "grad_norm": 79.00592041015625,
      "learning_rate": 1.4480000000000002e-05,
      "loss": -111.7038,
      "step": 69570
    },
    {
      "epoch": 5.5664,
      "grad_norm": 47.915321350097656,
      "learning_rate": 1.4453333333333336e-05,
      "loss": -111.9013,
      "step": 69580
    },
    {
      "epoch": 5.5672,
      "grad_norm": 32.34528732299805,
      "learning_rate": 1.4426666666666667e-05,
      "loss": -112.0195,
      "step": 69590
    },
    {
      "epoch": 5.568,
      "grad_norm": 34.145687103271484,
      "learning_rate": 1.44e-05,
      "loss": -111.4305,
      "step": 69600
    },
    {
      "epoch": 5.5687999999999995,
      "grad_norm": 21.37508773803711,
      "learning_rate": 1.4373333333333334e-05,
      "loss": -111.611,
      "step": 69610
    },
    {
      "epoch": 5.5696,
      "grad_norm": 42.89743423461914,
      "learning_rate": 1.4346666666666666e-05,
      "loss": -111.1893,
      "step": 69620
    },
    {
      "epoch": 5.5704,
      "grad_norm": 105.93148040771484,
      "learning_rate": 1.432e-05,
      "loss": -111.8769,
      "step": 69630
    },
    {
      "epoch": 5.5712,
      "grad_norm": 32.557899475097656,
      "learning_rate": 1.4293333333333334e-05,
      "loss": -111.6951,
      "step": 69640
    },
    {
      "epoch": 5.572,
      "grad_norm": 28.261783599853516,
      "learning_rate": 1.4266666666666667e-05,
      "loss": -112.2406,
      "step": 69650
    },
    {
      "epoch": 5.5728,
      "grad_norm": 42.01789855957031,
      "learning_rate": 1.4240000000000001e-05,
      "loss": -111.4197,
      "step": 69660
    },
    {
      "epoch": 5.5736,
      "grad_norm": 56.24189758300781,
      "learning_rate": 1.4213333333333333e-05,
      "loss": -112.9125,
      "step": 69670
    },
    {
      "epoch": 5.5744,
      "grad_norm": 20.654956817626953,
      "learning_rate": 1.4186666666666667e-05,
      "loss": -112.4632,
      "step": 69680
    },
    {
      "epoch": 5.5752,
      "grad_norm": 82.60107421875,
      "learning_rate": 1.4160000000000002e-05,
      "loss": -111.4112,
      "step": 69690
    },
    {
      "epoch": 5.576,
      "grad_norm": 79.15211486816406,
      "learning_rate": 1.4133333333333334e-05,
      "loss": -112.1351,
      "step": 69700
    },
    {
      "epoch": 5.5768,
      "grad_norm": 148.56857299804688,
      "learning_rate": 1.4106666666666668e-05,
      "loss": -111.5786,
      "step": 69710
    },
    {
      "epoch": 5.5776,
      "grad_norm": 22.813318252563477,
      "learning_rate": 1.408e-05,
      "loss": -111.4955,
      "step": 69720
    },
    {
      "epoch": 5.5784,
      "grad_norm": 24.924177169799805,
      "learning_rate": 1.4053333333333335e-05,
      "loss": -111.0386,
      "step": 69730
    },
    {
      "epoch": 5.5792,
      "grad_norm": 71.38786315917969,
      "learning_rate": 1.4026666666666669e-05,
      "loss": -112.0515,
      "step": 69740
    },
    {
      "epoch": 5.58,
      "grad_norm": 63.72560119628906,
      "learning_rate": 1.4000000000000001e-05,
      "loss": -111.541,
      "step": 69750
    },
    {
      "epoch": 5.5808,
      "grad_norm": 71.85591888427734,
      "learning_rate": 1.3973333333333332e-05,
      "loss": -112.5897,
      "step": 69760
    },
    {
      "epoch": 5.5816,
      "grad_norm": 38.39816665649414,
      "learning_rate": 1.3946666666666666e-05,
      "loss": -111.1718,
      "step": 69770
    },
    {
      "epoch": 5.5824,
      "grad_norm": 59.617286682128906,
      "learning_rate": 1.3919999999999999e-05,
      "loss": -111.6286,
      "step": 69780
    },
    {
      "epoch": 5.5832,
      "grad_norm": 32.119693756103516,
      "learning_rate": 1.3893333333333333e-05,
      "loss": -111.2142,
      "step": 69790
    },
    {
      "epoch": 5.584,
      "grad_norm": 57.60295104980469,
      "learning_rate": 1.3866666666666667e-05,
      "loss": -112.5003,
      "step": 69800
    },
    {
      "epoch": 5.5847999999999995,
      "grad_norm": 47.60063934326172,
      "learning_rate": 1.384e-05,
      "loss": -111.6026,
      "step": 69810
    },
    {
      "epoch": 5.5856,
      "grad_norm": 108.504150390625,
      "learning_rate": 1.3813333333333334e-05,
      "loss": -111.664,
      "step": 69820
    },
    {
      "epoch": 5.5864,
      "grad_norm": 19.800920486450195,
      "learning_rate": 1.3786666666666668e-05,
      "loss": -111.9744,
      "step": 69830
    },
    {
      "epoch": 5.5872,
      "grad_norm": 25.24541473388672,
      "learning_rate": 1.376e-05,
      "loss": -110.9837,
      "step": 69840
    },
    {
      "epoch": 5.588,
      "grad_norm": 58.99275588989258,
      "learning_rate": 1.3733333333333335e-05,
      "loss": -112.388,
      "step": 69850
    },
    {
      "epoch": 5.5888,
      "grad_norm": 61.49441146850586,
      "learning_rate": 1.3706666666666667e-05,
      "loss": -111.7896,
      "step": 69860
    },
    {
      "epoch": 5.5896,
      "grad_norm": 24.207822799682617,
      "learning_rate": 1.3680000000000001e-05,
      "loss": -112.1893,
      "step": 69870
    },
    {
      "epoch": 5.5904,
      "grad_norm": 50.36983108520508,
      "learning_rate": 1.3653333333333335e-05,
      "loss": -110.8764,
      "step": 69880
    },
    {
      "epoch": 5.5912,
      "grad_norm": 87.9930419921875,
      "learning_rate": 1.3626666666666668e-05,
      "loss": -111.8336,
      "step": 69890
    },
    {
      "epoch": 5.592,
      "grad_norm": 234.7482452392578,
      "learning_rate": 1.3600000000000002e-05,
      "loss": -112.4742,
      "step": 69900
    },
    {
      "epoch": 5.5928,
      "grad_norm": 57.162052154541016,
      "learning_rate": 1.3573333333333334e-05,
      "loss": -112.2475,
      "step": 69910
    },
    {
      "epoch": 5.5936,
      "grad_norm": 164.76229858398438,
      "learning_rate": 1.3546666666666669e-05,
      "loss": -112.0957,
      "step": 69920
    },
    {
      "epoch": 5.5944,
      "grad_norm": 87.42638397216797,
      "learning_rate": 1.352e-05,
      "loss": -112.3949,
      "step": 69930
    },
    {
      "epoch": 5.5952,
      "grad_norm": 121.23118591308594,
      "learning_rate": 1.3493333333333333e-05,
      "loss": -111.7093,
      "step": 69940
    },
    {
      "epoch": 5.596,
      "grad_norm": 24.188806533813477,
      "learning_rate": 1.3466666666666666e-05,
      "loss": -111.9538,
      "step": 69950
    },
    {
      "epoch": 5.5968,
      "grad_norm": 125.4893569946289,
      "learning_rate": 1.344e-05,
      "loss": -111.415,
      "step": 69960
    },
    {
      "epoch": 5.5976,
      "grad_norm": 59.80226135253906,
      "learning_rate": 1.3413333333333333e-05,
      "loss": -111.4124,
      "step": 69970
    },
    {
      "epoch": 5.5984,
      "grad_norm": 55.59223175048828,
      "learning_rate": 1.3386666666666667e-05,
      "loss": -112.1353,
      "step": 69980
    },
    {
      "epoch": 5.5992,
      "grad_norm": 25.076677322387695,
      "learning_rate": 1.336e-05,
      "loss": -112.2122,
      "step": 69990
    },
    {
      "epoch": 5.6,
      "grad_norm": 24.5349063873291,
      "learning_rate": 1.3333333333333333e-05,
      "loss": -111.8324,
      "step": 70000
    },
    {
      "epoch": 5.6008,
      "grad_norm": 60.18870544433594,
      "learning_rate": 1.3306666666666667e-05,
      "loss": -110.6418,
      "step": 70010
    },
    {
      "epoch": 5.6016,
      "grad_norm": 79.7582015991211,
      "learning_rate": 1.3280000000000002e-05,
      "loss": -110.99,
      "step": 70020
    },
    {
      "epoch": 5.6024,
      "grad_norm": 92.22747802734375,
      "learning_rate": 1.3253333333333334e-05,
      "loss": -112.2958,
      "step": 70030
    },
    {
      "epoch": 5.6032,
      "grad_norm": 113.3522720336914,
      "learning_rate": 1.3226666666666668e-05,
      "loss": -110.4475,
      "step": 70040
    },
    {
      "epoch": 5.604,
      "grad_norm": 29.14257049560547,
      "learning_rate": 1.32e-05,
      "loss": -111.4557,
      "step": 70050
    },
    {
      "epoch": 5.6048,
      "grad_norm": 61.21010971069336,
      "learning_rate": 1.3173333333333335e-05,
      "loss": -110.656,
      "step": 70060
    },
    {
      "epoch": 5.6056,
      "grad_norm": 194.79336547851562,
      "learning_rate": 1.3146666666666669e-05,
      "loss": -112.6367,
      "step": 70070
    },
    {
      "epoch": 5.6064,
      "grad_norm": 69.2978286743164,
      "learning_rate": 1.3120000000000001e-05,
      "loss": -111.2533,
      "step": 70080
    },
    {
      "epoch": 5.6072,
      "grad_norm": 44.79297637939453,
      "learning_rate": 1.3093333333333336e-05,
      "loss": -111.7908,
      "step": 70090
    },
    {
      "epoch": 5.608,
      "grad_norm": 79.452880859375,
      "learning_rate": 1.3066666666666666e-05,
      "loss": -112.0761,
      "step": 70100
    },
    {
      "epoch": 5.6088000000000005,
      "grad_norm": 39.90227127075195,
      "learning_rate": 1.3039999999999999e-05,
      "loss": -111.3158,
      "step": 70110
    },
    {
      "epoch": 5.6096,
      "grad_norm": 141.5267333984375,
      "learning_rate": 1.3013333333333333e-05,
      "loss": -112.1436,
      "step": 70120
    },
    {
      "epoch": 5.6104,
      "grad_norm": 21.318796157836914,
      "learning_rate": 1.2986666666666667e-05,
      "loss": -111.7247,
      "step": 70130
    },
    {
      "epoch": 5.6112,
      "grad_norm": 64.51054382324219,
      "learning_rate": 1.296e-05,
      "loss": -112.475,
      "step": 70140
    },
    {
      "epoch": 5.612,
      "grad_norm": 67.67874908447266,
      "learning_rate": 1.2933333333333334e-05,
      "loss": -111.9069,
      "step": 70150
    },
    {
      "epoch": 5.6128,
      "grad_norm": 96.03839111328125,
      "learning_rate": 1.2906666666666666e-05,
      "loss": -111.5215,
      "step": 70160
    },
    {
      "epoch": 5.6136,
      "grad_norm": 91.66649627685547,
      "learning_rate": 1.288e-05,
      "loss": -112.2431,
      "step": 70170
    },
    {
      "epoch": 5.6144,
      "grad_norm": 83.57369995117188,
      "learning_rate": 1.2853333333333335e-05,
      "loss": -111.8874,
      "step": 70180
    },
    {
      "epoch": 5.6152,
      "grad_norm": 63.933448791503906,
      "learning_rate": 1.2826666666666667e-05,
      "loss": -111.4372,
      "step": 70190
    },
    {
      "epoch": 5.616,
      "grad_norm": 37.437522888183594,
      "learning_rate": 1.2800000000000001e-05,
      "loss": -111.3891,
      "step": 70200
    },
    {
      "epoch": 5.6168,
      "grad_norm": 70.6709976196289,
      "learning_rate": 1.2773333333333334e-05,
      "loss": -111.2882,
      "step": 70210
    },
    {
      "epoch": 5.6176,
      "grad_norm": 110.50062561035156,
      "learning_rate": 1.2746666666666668e-05,
      "loss": -111.9837,
      "step": 70220
    },
    {
      "epoch": 5.6184,
      "grad_norm": 61.279788970947266,
      "learning_rate": 1.2720000000000002e-05,
      "loss": -112.2438,
      "step": 70230
    },
    {
      "epoch": 5.6192,
      "grad_norm": 27.86107635498047,
      "learning_rate": 1.2693333333333334e-05,
      "loss": -111.256,
      "step": 70240
    },
    {
      "epoch": 5.62,
      "grad_norm": 67.26803588867188,
      "learning_rate": 1.2666666666666668e-05,
      "loss": -111.1591,
      "step": 70250
    },
    {
      "epoch": 5.6208,
      "grad_norm": 76.48310089111328,
      "learning_rate": 1.2640000000000003e-05,
      "loss": -112.4103,
      "step": 70260
    },
    {
      "epoch": 5.6216,
      "grad_norm": 19.156723022460938,
      "learning_rate": 1.2613333333333332e-05,
      "loss": -111.4423,
      "step": 70270
    },
    {
      "epoch": 5.6224,
      "grad_norm": 110.35240936279297,
      "learning_rate": 1.2586666666666666e-05,
      "loss": -111.6383,
      "step": 70280
    },
    {
      "epoch": 5.6232,
      "grad_norm": 23.77409553527832,
      "learning_rate": 1.256e-05,
      "loss": -111.7659,
      "step": 70290
    },
    {
      "epoch": 5.624,
      "grad_norm": 45.00505447387695,
      "learning_rate": 1.2533333333333332e-05,
      "loss": -112.314,
      "step": 70300
    },
    {
      "epoch": 5.6248000000000005,
      "grad_norm": 63.748451232910156,
      "learning_rate": 1.2506666666666667e-05,
      "loss": -112.8365,
      "step": 70310
    },
    {
      "epoch": 5.6256,
      "grad_norm": 44.02418899536133,
      "learning_rate": 1.248e-05,
      "loss": -111.4557,
      "step": 70320
    },
    {
      "epoch": 5.6264,
      "grad_norm": 220.91831970214844,
      "learning_rate": 1.2453333333333333e-05,
      "loss": -112.7365,
      "step": 70330
    },
    {
      "epoch": 5.6272,
      "grad_norm": 103.89094543457031,
      "learning_rate": 1.2426666666666667e-05,
      "loss": -112.2491,
      "step": 70340
    },
    {
      "epoch": 5.628,
      "grad_norm": 155.94744873046875,
      "learning_rate": 1.24e-05,
      "loss": -111.3573,
      "step": 70350
    },
    {
      "epoch": 5.6288,
      "grad_norm": 125.5416488647461,
      "learning_rate": 1.2373333333333334e-05,
      "loss": -111.3864,
      "step": 70360
    },
    {
      "epoch": 5.6296,
      "grad_norm": 69.06786346435547,
      "learning_rate": 1.2346666666666668e-05,
      "loss": -111.2576,
      "step": 70370
    },
    {
      "epoch": 5.6304,
      "grad_norm": 28.175518035888672,
      "learning_rate": 1.232e-05,
      "loss": -111.5479,
      "step": 70380
    },
    {
      "epoch": 5.6312,
      "grad_norm": 135.41189575195312,
      "learning_rate": 1.2293333333333335e-05,
      "loss": -112.2089,
      "step": 70390
    },
    {
      "epoch": 5.632,
      "grad_norm": 45.9867057800293,
      "learning_rate": 1.2266666666666667e-05,
      "loss": -111.5276,
      "step": 70400
    },
    {
      "epoch": 5.6328,
      "grad_norm": 21.553327560424805,
      "learning_rate": 1.224e-05,
      "loss": -111.3987,
      "step": 70410
    },
    {
      "epoch": 5.6336,
      "grad_norm": 36.32859420776367,
      "learning_rate": 1.2213333333333334e-05,
      "loss": -111.2849,
      "step": 70420
    },
    {
      "epoch": 5.6344,
      "grad_norm": 38.16727066040039,
      "learning_rate": 1.2186666666666666e-05,
      "loss": -111.6135,
      "step": 70430
    },
    {
      "epoch": 5.6352,
      "grad_norm": 152.20465087890625,
      "learning_rate": 1.216e-05,
      "loss": -112.5785,
      "step": 70440
    },
    {
      "epoch": 5.636,
      "grad_norm": 67.90289306640625,
      "learning_rate": 1.2133333333333335e-05,
      "loss": -111.9836,
      "step": 70450
    },
    {
      "epoch": 5.6368,
      "grad_norm": 43.51253128051758,
      "learning_rate": 1.2106666666666667e-05,
      "loss": -111.9878,
      "step": 70460
    },
    {
      "epoch": 5.6376,
      "grad_norm": 79.9322738647461,
      "learning_rate": 1.2080000000000001e-05,
      "loss": -112.3999,
      "step": 70470
    },
    {
      "epoch": 5.6384,
      "grad_norm": 58.0561408996582,
      "learning_rate": 1.2053333333333334e-05,
      "loss": -111.9117,
      "step": 70480
    },
    {
      "epoch": 5.6392,
      "grad_norm": 69.82233428955078,
      "learning_rate": 1.2026666666666666e-05,
      "loss": -112.0673,
      "step": 70490
    },
    {
      "epoch": 5.64,
      "grad_norm": 98.45467376708984,
      "learning_rate": 1.2e-05,
      "loss": -111.116,
      "step": 70500
    },
    {
      "epoch": 5.6408000000000005,
      "grad_norm": 28.103153228759766,
      "learning_rate": 1.1973333333333334e-05,
      "loss": -111.6789,
      "step": 70510
    },
    {
      "epoch": 5.6416,
      "grad_norm": 19.866117477416992,
      "learning_rate": 1.1946666666666667e-05,
      "loss": -110.5184,
      "step": 70520
    },
    {
      "epoch": 5.6424,
      "grad_norm": 26.132930755615234,
      "learning_rate": 1.1920000000000001e-05,
      "loss": -110.7876,
      "step": 70530
    },
    {
      "epoch": 5.6432,
      "grad_norm": 52.684593200683594,
      "learning_rate": 1.1893333333333334e-05,
      "loss": -111.5081,
      "step": 70540
    },
    {
      "epoch": 5.644,
      "grad_norm": 63.1025505065918,
      "learning_rate": 1.1866666666666668e-05,
      "loss": -112.2974,
      "step": 70550
    },
    {
      "epoch": 5.6448,
      "grad_norm": 172.6476287841797,
      "learning_rate": 1.1840000000000002e-05,
      "loss": -111.494,
      "step": 70560
    },
    {
      "epoch": 5.6456,
      "grad_norm": 25.05528450012207,
      "learning_rate": 1.1813333333333334e-05,
      "loss": -111.5871,
      "step": 70570
    },
    {
      "epoch": 5.6464,
      "grad_norm": 17.650075912475586,
      "learning_rate": 1.1786666666666667e-05,
      "loss": -112.3112,
      "step": 70580
    },
    {
      "epoch": 5.6472,
      "grad_norm": 50.40153884887695,
      "learning_rate": 1.1760000000000001e-05,
      "loss": -110.8933,
      "step": 70590
    },
    {
      "epoch": 5.648,
      "grad_norm": 51.577720642089844,
      "learning_rate": 1.1733333333333333e-05,
      "loss": -112.2947,
      "step": 70600
    },
    {
      "epoch": 5.6488,
      "grad_norm": 74.89605712890625,
      "learning_rate": 1.1706666666666668e-05,
      "loss": -111.4081,
      "step": 70610
    },
    {
      "epoch": 5.6495999999999995,
      "grad_norm": 31.78060531616211,
      "learning_rate": 1.168e-05,
      "loss": -112.1567,
      "step": 70620
    },
    {
      "epoch": 5.6504,
      "grad_norm": 71.25574493408203,
      "learning_rate": 1.1653333333333334e-05,
      "loss": -111.1231,
      "step": 70630
    },
    {
      "epoch": 5.6512,
      "grad_norm": 71.56239318847656,
      "learning_rate": 1.1626666666666668e-05,
      "loss": -112.1314,
      "step": 70640
    },
    {
      "epoch": 5.652,
      "grad_norm": 23.408422470092773,
      "learning_rate": 1.16e-05,
      "loss": -111.4091,
      "step": 70650
    },
    {
      "epoch": 5.6528,
      "grad_norm": 104.80184936523438,
      "learning_rate": 1.1573333333333333e-05,
      "loss": -111.2888,
      "step": 70660
    },
    {
      "epoch": 5.6536,
      "grad_norm": 97.3060531616211,
      "learning_rate": 1.1546666666666667e-05,
      "loss": -110.9283,
      "step": 70670
    },
    {
      "epoch": 5.6544,
      "grad_norm": 36.1008186340332,
      "learning_rate": 1.152e-05,
      "loss": -111.9347,
      "step": 70680
    },
    {
      "epoch": 5.6552,
      "grad_norm": 78.0914077758789,
      "learning_rate": 1.1493333333333334e-05,
      "loss": -111.1827,
      "step": 70690
    },
    {
      "epoch": 5.656,
      "grad_norm": 21.699691772460938,
      "learning_rate": 1.1466666666666666e-05,
      "loss": -112.094,
      "step": 70700
    },
    {
      "epoch": 5.6568000000000005,
      "grad_norm": 16.206727981567383,
      "learning_rate": 1.144e-05,
      "loss": -111.646,
      "step": 70710
    },
    {
      "epoch": 5.6576,
      "grad_norm": 121.69095611572266,
      "learning_rate": 1.1413333333333335e-05,
      "loss": -112.207,
      "step": 70720
    },
    {
      "epoch": 5.6584,
      "grad_norm": 49.83572006225586,
      "learning_rate": 1.1386666666666667e-05,
      "loss": -111.4662,
      "step": 70730
    },
    {
      "epoch": 5.6592,
      "grad_norm": 26.503488540649414,
      "learning_rate": 1.1360000000000001e-05,
      "loss": -111.5668,
      "step": 70740
    },
    {
      "epoch": 5.66,
      "grad_norm": 148.70205688476562,
      "learning_rate": 1.1333333333333334e-05,
      "loss": -111.9923,
      "step": 70750
    },
    {
      "epoch": 5.6608,
      "grad_norm": 40.99514389038086,
      "learning_rate": 1.1306666666666666e-05,
      "loss": -111.6886,
      "step": 70760
    },
    {
      "epoch": 5.6616,
      "grad_norm": 68.8444595336914,
      "learning_rate": 1.128e-05,
      "loss": -110.7113,
      "step": 70770
    },
    {
      "epoch": 5.6624,
      "grad_norm": 74.01443481445312,
      "learning_rate": 1.1253333333333335e-05,
      "loss": -111.7172,
      "step": 70780
    },
    {
      "epoch": 5.6632,
      "grad_norm": 83.77941131591797,
      "learning_rate": 1.1226666666666667e-05,
      "loss": -112.3935,
      "step": 70790
    },
    {
      "epoch": 5.664,
      "grad_norm": 68.47968292236328,
      "learning_rate": 1.1200000000000001e-05,
      "loss": -111.1879,
      "step": 70800
    },
    {
      "epoch": 5.6648,
      "grad_norm": 331.3680419921875,
      "learning_rate": 1.1173333333333334e-05,
      "loss": -112.1503,
      "step": 70810
    },
    {
      "epoch": 5.6655999999999995,
      "grad_norm": 24.260082244873047,
      "learning_rate": 1.1146666666666668e-05,
      "loss": -111.9707,
      "step": 70820
    },
    {
      "epoch": 5.6664,
      "grad_norm": 30.220664978027344,
      "learning_rate": 1.112e-05,
      "loss": -111.6363,
      "step": 70830
    },
    {
      "epoch": 5.6672,
      "grad_norm": 76.84513854980469,
      "learning_rate": 1.1093333333333333e-05,
      "loss": -111.498,
      "step": 70840
    },
    {
      "epoch": 5.668,
      "grad_norm": 19.299556732177734,
      "learning_rate": 1.1066666666666667e-05,
      "loss": -111.7205,
      "step": 70850
    },
    {
      "epoch": 5.6688,
      "grad_norm": 87.33555603027344,
      "learning_rate": 1.1040000000000001e-05,
      "loss": -111.6987,
      "step": 70860
    },
    {
      "epoch": 5.6696,
      "grad_norm": 22.559099197387695,
      "learning_rate": 1.1013333333333333e-05,
      "loss": -111.2996,
      "step": 70870
    },
    {
      "epoch": 5.6704,
      "grad_norm": 58.91057205200195,
      "learning_rate": 1.0986666666666668e-05,
      "loss": -111.6871,
      "step": 70880
    },
    {
      "epoch": 5.6712,
      "grad_norm": 69.49870300292969,
      "learning_rate": 1.096e-05,
      "loss": -111.5338,
      "step": 70890
    },
    {
      "epoch": 5.672,
      "grad_norm": 87.7457275390625,
      "learning_rate": 1.0933333333333334e-05,
      "loss": -112.5838,
      "step": 70900
    },
    {
      "epoch": 5.6728,
      "grad_norm": 33.793418884277344,
      "learning_rate": 1.0906666666666668e-05,
      "loss": -111.6025,
      "step": 70910
    },
    {
      "epoch": 5.6736,
      "grad_norm": 72.72403717041016,
      "learning_rate": 1.088e-05,
      "loss": -111.8824,
      "step": 70920
    },
    {
      "epoch": 5.6744,
      "grad_norm": 63.05779266357422,
      "learning_rate": 1.0853333333333333e-05,
      "loss": -110.4818,
      "step": 70930
    },
    {
      "epoch": 5.6752,
      "grad_norm": 78.87920379638672,
      "learning_rate": 1.0826666666666667e-05,
      "loss": -110.7985,
      "step": 70940
    },
    {
      "epoch": 5.676,
      "grad_norm": 19.44866943359375,
      "learning_rate": 1.08e-05,
      "loss": -112.2621,
      "step": 70950
    },
    {
      "epoch": 5.6768,
      "grad_norm": 41.48891830444336,
      "learning_rate": 1.0773333333333334e-05,
      "loss": -111.7299,
      "step": 70960
    },
    {
      "epoch": 5.6776,
      "grad_norm": 146.17063903808594,
      "learning_rate": 1.0746666666666667e-05,
      "loss": -112.4434,
      "step": 70970
    },
    {
      "epoch": 5.6784,
      "grad_norm": 22.525753021240234,
      "learning_rate": 1.072e-05,
      "loss": -111.1929,
      "step": 70980
    },
    {
      "epoch": 5.6792,
      "grad_norm": 132.00399780273438,
      "learning_rate": 1.0693333333333335e-05,
      "loss": -112.3874,
      "step": 70990
    },
    {
      "epoch": 5.68,
      "grad_norm": 109.52530670166016,
      "learning_rate": 1.0666666666666667e-05,
      "loss": -112.5828,
      "step": 71000
    },
    {
      "epoch": 5.6808,
      "grad_norm": 34.49880599975586,
      "learning_rate": 1.064e-05,
      "loss": -111.5945,
      "step": 71010
    },
    {
      "epoch": 5.6815999999999995,
      "grad_norm": 64.77808380126953,
      "learning_rate": 1.0613333333333334e-05,
      "loss": -111.576,
      "step": 71020
    },
    {
      "epoch": 5.6824,
      "grad_norm": 21.12203025817871,
      "learning_rate": 1.0586666666666666e-05,
      "loss": -111.4138,
      "step": 71030
    },
    {
      "epoch": 5.6832,
      "grad_norm": 26.559354782104492,
      "learning_rate": 1.056e-05,
      "loss": -112.6706,
      "step": 71040
    },
    {
      "epoch": 5.684,
      "grad_norm": 84.78101348876953,
      "learning_rate": 1.0533333333333335e-05,
      "loss": -112.6269,
      "step": 71050
    },
    {
      "epoch": 5.6848,
      "grad_norm": 47.53315734863281,
      "learning_rate": 1.0506666666666667e-05,
      "loss": -111.6541,
      "step": 71060
    },
    {
      "epoch": 5.6856,
      "grad_norm": 19.231595993041992,
      "learning_rate": 1.0480000000000001e-05,
      "loss": -112.577,
      "step": 71070
    },
    {
      "epoch": 5.6864,
      "grad_norm": 46.56272888183594,
      "learning_rate": 1.0453333333333334e-05,
      "loss": -112.1872,
      "step": 71080
    },
    {
      "epoch": 5.6872,
      "grad_norm": 28.55661392211914,
      "learning_rate": 1.0426666666666666e-05,
      "loss": -111.1461,
      "step": 71090
    },
    {
      "epoch": 5.688,
      "grad_norm": 112.16924285888672,
      "learning_rate": 1.04e-05,
      "loss": -112.739,
      "step": 71100
    },
    {
      "epoch": 5.6888,
      "grad_norm": 26.743080139160156,
      "learning_rate": 1.0373333333333333e-05,
      "loss": -110.5333,
      "step": 71110
    },
    {
      "epoch": 5.6896,
      "grad_norm": 73.1022720336914,
      "learning_rate": 1.0346666666666667e-05,
      "loss": -112.3125,
      "step": 71120
    },
    {
      "epoch": 5.6904,
      "grad_norm": 31.470121383666992,
      "learning_rate": 1.0320000000000001e-05,
      "loss": -111.0049,
      "step": 71130
    },
    {
      "epoch": 5.6912,
      "grad_norm": 78.49388885498047,
      "learning_rate": 1.0293333333333334e-05,
      "loss": -112.4623,
      "step": 71140
    },
    {
      "epoch": 5.692,
      "grad_norm": 33.03801345825195,
      "learning_rate": 1.0266666666666668e-05,
      "loss": -111.984,
      "step": 71150
    },
    {
      "epoch": 5.6928,
      "grad_norm": 46.77047348022461,
      "learning_rate": 1.024e-05,
      "loss": -111.3713,
      "step": 71160
    },
    {
      "epoch": 5.6936,
      "grad_norm": 54.65943908691406,
      "learning_rate": 1.0213333333333334e-05,
      "loss": -110.9639,
      "step": 71170
    },
    {
      "epoch": 5.6944,
      "grad_norm": 25.062000274658203,
      "learning_rate": 1.0186666666666667e-05,
      "loss": -111.4681,
      "step": 71180
    },
    {
      "epoch": 5.6952,
      "grad_norm": 28.865602493286133,
      "learning_rate": 1.016e-05,
      "loss": -111.7191,
      "step": 71190
    },
    {
      "epoch": 5.696,
      "grad_norm": 152.79388427734375,
      "learning_rate": 1.0133333333333333e-05,
      "loss": -112.1226,
      "step": 71200
    },
    {
      "epoch": 5.6968,
      "grad_norm": 141.99578857421875,
      "learning_rate": 1.0106666666666668e-05,
      "loss": -111.9479,
      "step": 71210
    },
    {
      "epoch": 5.6975999999999996,
      "grad_norm": 60.84661102294922,
      "learning_rate": 1.008e-05,
      "loss": -111.817,
      "step": 71220
    },
    {
      "epoch": 5.6984,
      "grad_norm": 78.12669372558594,
      "learning_rate": 1.0053333333333334e-05,
      "loss": -112.4311,
      "step": 71230
    },
    {
      "epoch": 5.6992,
      "grad_norm": 80.08434295654297,
      "learning_rate": 1.0026666666666668e-05,
      "loss": -111.525,
      "step": 71240
    },
    {
      "epoch": 5.7,
      "grad_norm": 31.240541458129883,
      "learning_rate": 1e-05,
      "loss": -111.5367,
      "step": 71250
    },
    {
      "epoch": 5.7008,
      "grad_norm": 177.448486328125,
      "learning_rate": 9.973333333333333e-06,
      "loss": -111.9129,
      "step": 71260
    },
    {
      "epoch": 5.7016,
      "grad_norm": 93.06562805175781,
      "learning_rate": 9.946666666666667e-06,
      "loss": -112.2544,
      "step": 71270
    },
    {
      "epoch": 5.7024,
      "grad_norm": 97.30902862548828,
      "learning_rate": 9.92e-06,
      "loss": -112.4289,
      "step": 71280
    },
    {
      "epoch": 5.7032,
      "grad_norm": 173.5636444091797,
      "learning_rate": 9.893333333333334e-06,
      "loss": -111.4724,
      "step": 71290
    },
    {
      "epoch": 5.704,
      "grad_norm": 60.8831672668457,
      "learning_rate": 9.866666666666667e-06,
      "loss": -111.0786,
      "step": 71300
    },
    {
      "epoch": 5.7048,
      "grad_norm": 99.54838562011719,
      "learning_rate": 9.84e-06,
      "loss": -111.3611,
      "step": 71310
    },
    {
      "epoch": 5.7056000000000004,
      "grad_norm": 21.678958892822266,
      "learning_rate": 9.813333333333335e-06,
      "loss": -110.8863,
      "step": 71320
    },
    {
      "epoch": 5.7064,
      "grad_norm": 26.58781623840332,
      "learning_rate": 9.786666666666667e-06,
      "loss": -111.1665,
      "step": 71330
    },
    {
      "epoch": 5.7072,
      "grad_norm": 127.06111907958984,
      "learning_rate": 9.760000000000001e-06,
      "loss": -111.8416,
      "step": 71340
    },
    {
      "epoch": 5.708,
      "grad_norm": 28.65445327758789,
      "learning_rate": 9.733333333333334e-06,
      "loss": -111.688,
      "step": 71350
    },
    {
      "epoch": 5.7088,
      "grad_norm": 130.7289276123047,
      "learning_rate": 9.706666666666666e-06,
      "loss": -111.5772,
      "step": 71360
    },
    {
      "epoch": 5.7096,
      "grad_norm": 65.75437927246094,
      "learning_rate": 9.68e-06,
      "loss": -111.3142,
      "step": 71370
    },
    {
      "epoch": 5.7104,
      "grad_norm": 101.88448333740234,
      "learning_rate": 9.653333333333333e-06,
      "loss": -111.8744,
      "step": 71380
    },
    {
      "epoch": 5.7112,
      "grad_norm": 68.58374786376953,
      "learning_rate": 9.626666666666667e-06,
      "loss": -110.7479,
      "step": 71390
    },
    {
      "epoch": 5.712,
      "grad_norm": 76.89351654052734,
      "learning_rate": 9.600000000000001e-06,
      "loss": -111.0033,
      "step": 71400
    },
    {
      "epoch": 5.7128,
      "grad_norm": 26.462438583374023,
      "learning_rate": 9.573333333333334e-06,
      "loss": -112.1301,
      "step": 71410
    },
    {
      "epoch": 5.7136,
      "grad_norm": 55.88518142700195,
      "learning_rate": 9.546666666666668e-06,
      "loss": -112.0765,
      "step": 71420
    },
    {
      "epoch": 5.7144,
      "grad_norm": 41.33401870727539,
      "learning_rate": 9.52e-06,
      "loss": -111.7572,
      "step": 71430
    },
    {
      "epoch": 5.7152,
      "grad_norm": 80.9271469116211,
      "learning_rate": 9.493333333333333e-06,
      "loss": -111.6553,
      "step": 71440
    },
    {
      "epoch": 5.716,
      "grad_norm": 57.477169036865234,
      "learning_rate": 9.466666666666667e-06,
      "loss": -112.0533,
      "step": 71450
    },
    {
      "epoch": 5.7168,
      "grad_norm": 51.24724578857422,
      "learning_rate": 9.44e-06,
      "loss": -111.5741,
      "step": 71460
    },
    {
      "epoch": 5.7176,
      "grad_norm": 92.48139190673828,
      "learning_rate": 9.413333333333334e-06,
      "loss": -111.2651,
      "step": 71470
    },
    {
      "epoch": 5.7184,
      "grad_norm": 91.13207244873047,
      "learning_rate": 9.386666666666668e-06,
      "loss": -111.1841,
      "step": 71480
    },
    {
      "epoch": 5.7192,
      "grad_norm": 40.762149810791016,
      "learning_rate": 9.36e-06,
      "loss": -111.2296,
      "step": 71490
    },
    {
      "epoch": 5.72,
      "grad_norm": 55.435855865478516,
      "learning_rate": 9.333333333333334e-06,
      "loss": -112.5358,
      "step": 71500
    },
    {
      "epoch": 5.7208,
      "grad_norm": 62.97678756713867,
      "learning_rate": 9.306666666666668e-06,
      "loss": -112.7518,
      "step": 71510
    },
    {
      "epoch": 5.7216000000000005,
      "grad_norm": 154.9428253173828,
      "learning_rate": 9.28e-06,
      "loss": -112.3753,
      "step": 71520
    },
    {
      "epoch": 5.7224,
      "grad_norm": 68.41905212402344,
      "learning_rate": 9.253333333333333e-06,
      "loss": -111.9375,
      "step": 71530
    },
    {
      "epoch": 5.7232,
      "grad_norm": 17.770294189453125,
      "learning_rate": 9.226666666666668e-06,
      "loss": -111.2707,
      "step": 71540
    },
    {
      "epoch": 5.724,
      "grad_norm": 115.18036651611328,
      "learning_rate": 9.2e-06,
      "loss": -111.2289,
      "step": 71550
    },
    {
      "epoch": 5.7248,
      "grad_norm": 90.9887924194336,
      "learning_rate": 9.173333333333334e-06,
      "loss": -111.8448,
      "step": 71560
    },
    {
      "epoch": 5.7256,
      "grad_norm": 51.89883804321289,
      "learning_rate": 9.146666666666667e-06,
      "loss": -111.7432,
      "step": 71570
    },
    {
      "epoch": 5.7264,
      "grad_norm": 138.3179931640625,
      "learning_rate": 9.12e-06,
      "loss": -111.2734,
      "step": 71580
    },
    {
      "epoch": 5.7272,
      "grad_norm": 88.01046752929688,
      "learning_rate": 9.093333333333335e-06,
      "loss": -112.5992,
      "step": 71590
    },
    {
      "epoch": 5.728,
      "grad_norm": 44.54677200317383,
      "learning_rate": 9.066666666666667e-06,
      "loss": -113.2083,
      "step": 71600
    },
    {
      "epoch": 5.7288,
      "grad_norm": 57.28642272949219,
      "learning_rate": 9.04e-06,
      "loss": -110.5464,
      "step": 71610
    },
    {
      "epoch": 5.7296,
      "grad_norm": 82.50463104248047,
      "learning_rate": 9.013333333333334e-06,
      "loss": -112.6652,
      "step": 71620
    },
    {
      "epoch": 5.7304,
      "grad_norm": 18.173099517822266,
      "learning_rate": 8.986666666666666e-06,
      "loss": -111.6274,
      "step": 71630
    },
    {
      "epoch": 5.7312,
      "grad_norm": 49.30545425415039,
      "learning_rate": 8.96e-06,
      "loss": -111.9046,
      "step": 71640
    },
    {
      "epoch": 5.732,
      "grad_norm": 68.66949462890625,
      "learning_rate": 8.933333333333333e-06,
      "loss": -112.0172,
      "step": 71650
    },
    {
      "epoch": 5.7328,
      "grad_norm": 39.553916931152344,
      "learning_rate": 8.906666666666667e-06,
      "loss": -111.7275,
      "step": 71660
    },
    {
      "epoch": 5.7336,
      "grad_norm": 21.672740936279297,
      "learning_rate": 8.880000000000001e-06,
      "loss": -111.4468,
      "step": 71670
    },
    {
      "epoch": 5.7344,
      "grad_norm": 71.17216491699219,
      "learning_rate": 8.853333333333334e-06,
      "loss": -111.6453,
      "step": 71680
    },
    {
      "epoch": 5.7352,
      "grad_norm": 37.60847091674805,
      "learning_rate": 8.826666666666666e-06,
      "loss": -111.5683,
      "step": 71690
    },
    {
      "epoch": 5.736,
      "grad_norm": 29.59254264831543,
      "learning_rate": 8.8e-06,
      "loss": -111.4822,
      "step": 71700
    },
    {
      "epoch": 5.7368,
      "grad_norm": 106.38169860839844,
      "learning_rate": 8.773333333333333e-06,
      "loss": -111.388,
      "step": 71710
    },
    {
      "epoch": 5.7376000000000005,
      "grad_norm": 82.38835906982422,
      "learning_rate": 8.746666666666667e-06,
      "loss": -111.0483,
      "step": 71720
    },
    {
      "epoch": 5.7384,
      "grad_norm": 79.0066146850586,
      "learning_rate": 8.720000000000001e-06,
      "loss": -112.0783,
      "step": 71730
    },
    {
      "epoch": 5.7392,
      "grad_norm": 57.41919708251953,
      "learning_rate": 8.693333333333334e-06,
      "loss": -111.7269,
      "step": 71740
    },
    {
      "epoch": 5.74,
      "grad_norm": 565.7081909179688,
      "learning_rate": 8.666666666666668e-06,
      "loss": -110.9958,
      "step": 71750
    },
    {
      "epoch": 5.7408,
      "grad_norm": 106.81935119628906,
      "learning_rate": 8.64e-06,
      "loss": -111.1932,
      "step": 71760
    },
    {
      "epoch": 5.7416,
      "grad_norm": 40.26320266723633,
      "learning_rate": 8.613333333333334e-06,
      "loss": -111.6285,
      "step": 71770
    },
    {
      "epoch": 5.7424,
      "grad_norm": 97.3204116821289,
      "learning_rate": 8.586666666666667e-06,
      "loss": -111.8682,
      "step": 71780
    },
    {
      "epoch": 5.7432,
      "grad_norm": 20.846845626831055,
      "learning_rate": 8.56e-06,
      "loss": -111.7799,
      "step": 71790
    },
    {
      "epoch": 5.744,
      "grad_norm": 92.65086364746094,
      "learning_rate": 8.533333333333334e-06,
      "loss": -111.4951,
      "step": 71800
    },
    {
      "epoch": 5.7448,
      "grad_norm": 146.59036254882812,
      "learning_rate": 8.506666666666668e-06,
      "loss": -111.0747,
      "step": 71810
    },
    {
      "epoch": 5.7456,
      "grad_norm": 22.797025680541992,
      "learning_rate": 8.48e-06,
      "loss": -111.1075,
      "step": 71820
    },
    {
      "epoch": 5.7463999999999995,
      "grad_norm": 62.698970794677734,
      "learning_rate": 8.453333333333334e-06,
      "loss": -111.1116,
      "step": 71830
    },
    {
      "epoch": 5.7472,
      "grad_norm": 60.17219924926758,
      "learning_rate": 8.426666666666667e-06,
      "loss": -110.6735,
      "step": 71840
    },
    {
      "epoch": 5.748,
      "grad_norm": 26.670188903808594,
      "learning_rate": 8.400000000000001e-06,
      "loss": -111.2071,
      "step": 71850
    },
    {
      "epoch": 5.7488,
      "grad_norm": 61.37948989868164,
      "learning_rate": 8.373333333333335e-06,
      "loss": -111.4529,
      "step": 71860
    },
    {
      "epoch": 5.7496,
      "grad_norm": 22.85689353942871,
      "learning_rate": 8.346666666666666e-06,
      "loss": -110.8789,
      "step": 71870
    },
    {
      "epoch": 5.7504,
      "grad_norm": 88.41447448730469,
      "learning_rate": 8.32e-06,
      "loss": -111.6174,
      "step": 71880
    },
    {
      "epoch": 5.7512,
      "grad_norm": 89.98355102539062,
      "learning_rate": 8.293333333333334e-06,
      "loss": -112.0882,
      "step": 71890
    },
    {
      "epoch": 5.752,
      "grad_norm": 51.04314041137695,
      "learning_rate": 8.266666666666667e-06,
      "loss": -111.6378,
      "step": 71900
    },
    {
      "epoch": 5.7528,
      "grad_norm": 39.15953826904297,
      "learning_rate": 8.24e-06,
      "loss": -111.9551,
      "step": 71910
    },
    {
      "epoch": 5.7536000000000005,
      "grad_norm": 24.89791488647461,
      "learning_rate": 8.213333333333333e-06,
      "loss": -111.3381,
      "step": 71920
    },
    {
      "epoch": 5.7544,
      "grad_norm": 55.27672576904297,
      "learning_rate": 8.186666666666667e-06,
      "loss": -111.8159,
      "step": 71930
    },
    {
      "epoch": 5.7552,
      "grad_norm": 29.494911193847656,
      "learning_rate": 8.160000000000001e-06,
      "loss": -111.4398,
      "step": 71940
    },
    {
      "epoch": 5.756,
      "grad_norm": 110.20662689208984,
      "learning_rate": 8.133333333333332e-06,
      "loss": -112.1132,
      "step": 71950
    },
    {
      "epoch": 5.7568,
      "grad_norm": 95.01740264892578,
      "learning_rate": 8.106666666666666e-06,
      "loss": -111.0438,
      "step": 71960
    },
    {
      "epoch": 5.7576,
      "grad_norm": 30.746540069580078,
      "learning_rate": 8.08e-06,
      "loss": -111.3735,
      "step": 71970
    },
    {
      "epoch": 5.7584,
      "grad_norm": 77.63096618652344,
      "learning_rate": 8.053333333333333e-06,
      "loss": -111.5378,
      "step": 71980
    },
    {
      "epoch": 5.7592,
      "grad_norm": 25.830703735351562,
      "learning_rate": 8.026666666666667e-06,
      "loss": -110.9366,
      "step": 71990
    },
    {
      "epoch": 5.76,
      "grad_norm": 46.22283935546875,
      "learning_rate": 8.000000000000001e-06,
      "loss": -111.4176,
      "step": 72000
    },
    {
      "epoch": 5.7608,
      "grad_norm": 22.8710994720459,
      "learning_rate": 7.973333333333334e-06,
      "loss": -110.4614,
      "step": 72010
    },
    {
      "epoch": 5.7616,
      "grad_norm": 75.29308319091797,
      "learning_rate": 7.946666666666668e-06,
      "loss": -111.1005,
      "step": 72020
    },
    {
      "epoch": 5.7623999999999995,
      "grad_norm": 21.980592727661133,
      "learning_rate": 7.92e-06,
      "loss": -112.4034,
      "step": 72030
    },
    {
      "epoch": 5.7632,
      "grad_norm": 158.70382690429688,
      "learning_rate": 7.893333333333333e-06,
      "loss": -111.9002,
      "step": 72040
    },
    {
      "epoch": 5.764,
      "grad_norm": 77.873046875,
      "learning_rate": 7.866666666666667e-06,
      "loss": -111.7827,
      "step": 72050
    },
    {
      "epoch": 5.7648,
      "grad_norm": 18.592914581298828,
      "learning_rate": 7.84e-06,
      "loss": -111.9471,
      "step": 72060
    },
    {
      "epoch": 5.7656,
      "grad_norm": 143.2858123779297,
      "learning_rate": 7.813333333333334e-06,
      "loss": -111.3433,
      "step": 72070
    },
    {
      "epoch": 5.7664,
      "grad_norm": 96.32042694091797,
      "learning_rate": 7.786666666666668e-06,
      "loss": -111.6066,
      "step": 72080
    },
    {
      "epoch": 5.7672,
      "grad_norm": 20.483158111572266,
      "learning_rate": 7.76e-06,
      "loss": -111.5079,
      "step": 72090
    },
    {
      "epoch": 5.768,
      "grad_norm": 177.9475860595703,
      "learning_rate": 7.733333333333334e-06,
      "loss": -111.4719,
      "step": 72100
    },
    {
      "epoch": 5.7688,
      "grad_norm": 140.23660278320312,
      "learning_rate": 7.706666666666667e-06,
      "loss": -112.1018,
      "step": 72110
    },
    {
      "epoch": 5.7696,
      "grad_norm": 82.73748779296875,
      "learning_rate": 7.68e-06,
      "loss": -110.9562,
      "step": 72120
    },
    {
      "epoch": 5.7704,
      "grad_norm": 31.866575241088867,
      "learning_rate": 7.653333333333333e-06,
      "loss": -111.0358,
      "step": 72130
    },
    {
      "epoch": 5.7712,
      "grad_norm": 31.92256736755371,
      "learning_rate": 7.626666666666667e-06,
      "loss": -111.1412,
      "step": 72140
    },
    {
      "epoch": 5.772,
      "grad_norm": 19.644329071044922,
      "learning_rate": 7.6e-06,
      "loss": -112.0811,
      "step": 72150
    },
    {
      "epoch": 5.7728,
      "grad_norm": 19.666406631469727,
      "learning_rate": 7.573333333333333e-06,
      "loss": -111.5312,
      "step": 72160
    },
    {
      "epoch": 5.7736,
      "grad_norm": 27.11986541748047,
      "learning_rate": 7.5466666666666675e-06,
      "loss": -111.1534,
      "step": 72170
    },
    {
      "epoch": 5.7744,
      "grad_norm": 24.696096420288086,
      "learning_rate": 7.520000000000001e-06,
      "loss": -111.3073,
      "step": 72180
    },
    {
      "epoch": 5.7752,
      "grad_norm": 23.217924118041992,
      "learning_rate": 7.493333333333334e-06,
      "loss": -111.3826,
      "step": 72190
    },
    {
      "epoch": 5.776,
      "grad_norm": 18.87702178955078,
      "learning_rate": 7.4666666666666675e-06,
      "loss": -111.4471,
      "step": 72200
    },
    {
      "epoch": 5.7768,
      "grad_norm": 72.60083770751953,
      "learning_rate": 7.44e-06,
      "loss": -111.79,
      "step": 72210
    },
    {
      "epoch": 5.7776,
      "grad_norm": 72.34418487548828,
      "learning_rate": 7.413333333333333e-06,
      "loss": -112.6041,
      "step": 72220
    },
    {
      "epoch": 5.7783999999999995,
      "grad_norm": 68.649169921875,
      "learning_rate": 7.3866666666666665e-06,
      "loss": -111.8644,
      "step": 72230
    },
    {
      "epoch": 5.7792,
      "grad_norm": 73.54646301269531,
      "learning_rate": 7.36e-06,
      "loss": -111.3608,
      "step": 72240
    },
    {
      "epoch": 5.78,
      "grad_norm": 25.333911895751953,
      "learning_rate": 7.333333333333334e-06,
      "loss": -111.4096,
      "step": 72250
    },
    {
      "epoch": 5.7808,
      "grad_norm": 26.242019653320312,
      "learning_rate": 7.306666666666667e-06,
      "loss": -111.5372,
      "step": 72260
    },
    {
      "epoch": 5.7816,
      "grad_norm": 86.98553466796875,
      "learning_rate": 7.280000000000001e-06,
      "loss": -112.0576,
      "step": 72270
    },
    {
      "epoch": 5.7824,
      "grad_norm": 81.18262481689453,
      "learning_rate": 7.253333333333334e-06,
      "loss": -111.6714,
      "step": 72280
    },
    {
      "epoch": 5.7832,
      "grad_norm": 83.25410461425781,
      "learning_rate": 7.226666666666668e-06,
      "loss": -111.5921,
      "step": 72290
    },
    {
      "epoch": 5.784,
      "grad_norm": 90.35126495361328,
      "learning_rate": 7.2e-06,
      "loss": -112.3416,
      "step": 72300
    },
    {
      "epoch": 5.7848,
      "grad_norm": 22.01058578491211,
      "learning_rate": 7.173333333333333e-06,
      "loss": -111.7887,
      "step": 72310
    },
    {
      "epoch": 5.7856,
      "grad_norm": 95.92498016357422,
      "learning_rate": 7.146666666666667e-06,
      "loss": -112.5055,
      "step": 72320
    },
    {
      "epoch": 5.7864,
      "grad_norm": 42.32450866699219,
      "learning_rate": 7.1200000000000004e-06,
      "loss": -112.7015,
      "step": 72330
    },
    {
      "epoch": 5.7872,
      "grad_norm": 30.432373046875,
      "learning_rate": 7.093333333333334e-06,
      "loss": -111.5052,
      "step": 72340
    },
    {
      "epoch": 5.788,
      "grad_norm": 42.32001876831055,
      "learning_rate": 7.066666666666667e-06,
      "loss": -111.467,
      "step": 72350
    },
    {
      "epoch": 5.7888,
      "grad_norm": 94.82320404052734,
      "learning_rate": 7.04e-06,
      "loss": -111.9762,
      "step": 72360
    },
    {
      "epoch": 5.7896,
      "grad_norm": 19.36079216003418,
      "learning_rate": 7.0133333333333345e-06,
      "loss": -112.5067,
      "step": 72370
    },
    {
      "epoch": 5.7904,
      "grad_norm": 49.483646392822266,
      "learning_rate": 6.986666666666666e-06,
      "loss": -111.8951,
      "step": 72380
    },
    {
      "epoch": 5.7912,
      "grad_norm": 86.16222381591797,
      "learning_rate": 6.9599999999999994e-06,
      "loss": -112.3433,
      "step": 72390
    },
    {
      "epoch": 5.792,
      "grad_norm": 73.95800018310547,
      "learning_rate": 6.933333333333334e-06,
      "loss": -110.9177,
      "step": 72400
    },
    {
      "epoch": 5.7928,
      "grad_norm": 70.48713684082031,
      "learning_rate": 6.906666666666667e-06,
      "loss": -111.1799,
      "step": 72410
    },
    {
      "epoch": 5.7936,
      "grad_norm": 649.0944213867188,
      "learning_rate": 6.88e-06,
      "loss": -112.3627,
      "step": 72420
    },
    {
      "epoch": 5.7943999999999996,
      "grad_norm": 83.85833740234375,
      "learning_rate": 6.8533333333333335e-06,
      "loss": -111.644,
      "step": 72430
    },
    {
      "epoch": 5.7952,
      "grad_norm": 16.591615676879883,
      "learning_rate": 6.826666666666668e-06,
      "loss": -112.4615,
      "step": 72440
    },
    {
      "epoch": 5.796,
      "grad_norm": 29.81719970703125,
      "learning_rate": 6.800000000000001e-06,
      "loss": -110.921,
      "step": 72450
    },
    {
      "epoch": 5.7968,
      "grad_norm": 124.40901184082031,
      "learning_rate": 6.773333333333334e-06,
      "loss": -112.2523,
      "step": 72460
    },
    {
      "epoch": 5.7976,
      "grad_norm": 106.60533905029297,
      "learning_rate": 6.746666666666667e-06,
      "loss": -112.6397,
      "step": 72470
    },
    {
      "epoch": 5.7984,
      "grad_norm": 56.9722900390625,
      "learning_rate": 6.72e-06,
      "loss": -112.1613,
      "step": 72480
    },
    {
      "epoch": 5.7992,
      "grad_norm": 25.26401710510254,
      "learning_rate": 6.693333333333333e-06,
      "loss": -111.0717,
      "step": 72490
    },
    {
      "epoch": 5.8,
      "grad_norm": 84.72855377197266,
      "learning_rate": 6.666666666666667e-06,
      "loss": -110.8989,
      "step": 72500
    },
    {
      "epoch": 5.8008,
      "grad_norm": 67.19038391113281,
      "learning_rate": 6.640000000000001e-06,
      "loss": -111.6034,
      "step": 72510
    },
    {
      "epoch": 5.8016,
      "grad_norm": 57.76591491699219,
      "learning_rate": 6.613333333333334e-06,
      "loss": -111.5498,
      "step": 72520
    },
    {
      "epoch": 5.8024000000000004,
      "grad_norm": 25.2582950592041,
      "learning_rate": 6.586666666666667e-06,
      "loss": -111.2901,
      "step": 72530
    },
    {
      "epoch": 5.8032,
      "grad_norm": 38.755828857421875,
      "learning_rate": 6.560000000000001e-06,
      "loss": -111.2853,
      "step": 72540
    },
    {
      "epoch": 5.804,
      "grad_norm": 38.55706787109375,
      "learning_rate": 6.533333333333333e-06,
      "loss": -111.2861,
      "step": 72550
    },
    {
      "epoch": 5.8048,
      "grad_norm": 45.043190002441406,
      "learning_rate": 6.5066666666666665e-06,
      "loss": -111.0847,
      "step": 72560
    },
    {
      "epoch": 5.8056,
      "grad_norm": 210.8513641357422,
      "learning_rate": 6.48e-06,
      "loss": -111.9278,
      "step": 72570
    },
    {
      "epoch": 5.8064,
      "grad_norm": 29.62493133544922,
      "learning_rate": 6.453333333333333e-06,
      "loss": -111.8112,
      "step": 72580
    },
    {
      "epoch": 5.8072,
      "grad_norm": 92.10706329345703,
      "learning_rate": 6.426666666666667e-06,
      "loss": -111.9459,
      "step": 72590
    },
    {
      "epoch": 5.808,
      "grad_norm": 78.39421081542969,
      "learning_rate": 6.4000000000000006e-06,
      "loss": -111.7489,
      "step": 72600
    },
    {
      "epoch": 5.8088,
      "grad_norm": 73.1885986328125,
      "learning_rate": 6.373333333333334e-06,
      "loss": -112.26,
      "step": 72610
    },
    {
      "epoch": 5.8096,
      "grad_norm": 21.872373580932617,
      "learning_rate": 6.346666666666667e-06,
      "loss": -112.5858,
      "step": 72620
    },
    {
      "epoch": 5.8104,
      "grad_norm": 30.603281021118164,
      "learning_rate": 6.320000000000001e-06,
      "loss": -111.6232,
      "step": 72630
    },
    {
      "epoch": 5.8112,
      "grad_norm": 48.73505783081055,
      "learning_rate": 6.293333333333333e-06,
      "loss": -111.8798,
      "step": 72640
    },
    {
      "epoch": 5.812,
      "grad_norm": 62.818790435791016,
      "learning_rate": 6.266666666666666e-06,
      "loss": -111.2455,
      "step": 72650
    },
    {
      "epoch": 5.8128,
      "grad_norm": 24.274450302124023,
      "learning_rate": 6.24e-06,
      "loss": -112.9096,
      "step": 72660
    },
    {
      "epoch": 5.8136,
      "grad_norm": 85.73258972167969,
      "learning_rate": 6.213333333333334e-06,
      "loss": -112.1194,
      "step": 72670
    },
    {
      "epoch": 5.8144,
      "grad_norm": 43.67940902709961,
      "learning_rate": 6.186666666666667e-06,
      "loss": -112.2754,
      "step": 72680
    },
    {
      "epoch": 5.8152,
      "grad_norm": 99.32147979736328,
      "learning_rate": 6.16e-06,
      "loss": -111.0937,
      "step": 72690
    },
    {
      "epoch": 5.816,
      "grad_norm": 31.513681411743164,
      "learning_rate": 6.133333333333334e-06,
      "loss": -111.4122,
      "step": 72700
    },
    {
      "epoch": 5.8168,
      "grad_norm": 27.491992950439453,
      "learning_rate": 6.106666666666667e-06,
      "loss": -111.0923,
      "step": 72710
    },
    {
      "epoch": 5.8176,
      "grad_norm": 54.658050537109375,
      "learning_rate": 6.08e-06,
      "loss": -112.1161,
      "step": 72720
    },
    {
      "epoch": 5.8184000000000005,
      "grad_norm": 21.220191955566406,
      "learning_rate": 6.0533333333333335e-06,
      "loss": -111.3813,
      "step": 72730
    },
    {
      "epoch": 5.8192,
      "grad_norm": 72.69924926757812,
      "learning_rate": 6.026666666666667e-06,
      "loss": -111.3357,
      "step": 72740
    },
    {
      "epoch": 5.82,
      "grad_norm": 24.59537124633789,
      "learning_rate": 6e-06,
      "loss": -110.6727,
      "step": 72750
    },
    {
      "epoch": 5.8208,
      "grad_norm": 38.635345458984375,
      "learning_rate": 5.9733333333333335e-06,
      "loss": -111.1575,
      "step": 72760
    },
    {
      "epoch": 5.8216,
      "grad_norm": 79.87769317626953,
      "learning_rate": 5.946666666666667e-06,
      "loss": -110.6366,
      "step": 72770
    },
    {
      "epoch": 5.8224,
      "grad_norm": 27.509666442871094,
      "learning_rate": 5.920000000000001e-06,
      "loss": -112.4692,
      "step": 72780
    },
    {
      "epoch": 5.8232,
      "grad_norm": 65.97962188720703,
      "learning_rate": 5.893333333333333e-06,
      "loss": -111.8997,
      "step": 72790
    },
    {
      "epoch": 5.824,
      "grad_norm": 94.12165069580078,
      "learning_rate": 5.866666666666667e-06,
      "loss": -112.218,
      "step": 72800
    },
    {
      "epoch": 5.8248,
      "grad_norm": 23.260555267333984,
      "learning_rate": 5.84e-06,
      "loss": -111.2309,
      "step": 72810
    },
    {
      "epoch": 5.8256,
      "grad_norm": 70.22710418701172,
      "learning_rate": 5.813333333333334e-06,
      "loss": -111.9474,
      "step": 72820
    },
    {
      "epoch": 5.8264,
      "grad_norm": 58.006961822509766,
      "learning_rate": 5.786666666666667e-06,
      "loss": -111.8325,
      "step": 72830
    },
    {
      "epoch": 5.8272,
      "grad_norm": 26.62361717224121,
      "learning_rate": 5.76e-06,
      "loss": -112.0471,
      "step": 72840
    },
    {
      "epoch": 5.828,
      "grad_norm": 39.83012008666992,
      "learning_rate": 5.733333333333333e-06,
      "loss": -112.1491,
      "step": 72850
    },
    {
      "epoch": 5.8288,
      "grad_norm": 17.80729866027832,
      "learning_rate": 5.706666666666667e-06,
      "loss": -111.7406,
      "step": 72860
    },
    {
      "epoch": 5.8296,
      "grad_norm": 60.845458984375,
      "learning_rate": 5.680000000000001e-06,
      "loss": -112.0062,
      "step": 72870
    },
    {
      "epoch": 5.8304,
      "grad_norm": 237.12832641601562,
      "learning_rate": 5.653333333333333e-06,
      "loss": -112.1182,
      "step": 72880
    },
    {
      "epoch": 5.8312,
      "grad_norm": 48.64613723754883,
      "learning_rate": 5.626666666666667e-06,
      "loss": -112.1139,
      "step": 72890
    },
    {
      "epoch": 5.832,
      "grad_norm": 40.535308837890625,
      "learning_rate": 5.600000000000001e-06,
      "loss": -111.4016,
      "step": 72900
    },
    {
      "epoch": 5.8328,
      "grad_norm": 109.86463928222656,
      "learning_rate": 5.573333333333334e-06,
      "loss": -111.3405,
      "step": 72910
    },
    {
      "epoch": 5.8336,
      "grad_norm": 17.23680877685547,
      "learning_rate": 5.546666666666666e-06,
      "loss": -111.9161,
      "step": 72920
    },
    {
      "epoch": 5.8344000000000005,
      "grad_norm": 27.23832893371582,
      "learning_rate": 5.5200000000000005e-06,
      "loss": -111.9491,
      "step": 72930
    },
    {
      "epoch": 5.8352,
      "grad_norm": 53.529457092285156,
      "learning_rate": 5.493333333333334e-06,
      "loss": -111.2893,
      "step": 72940
    },
    {
      "epoch": 5.836,
      "grad_norm": 126.04017639160156,
      "learning_rate": 5.466666666666667e-06,
      "loss": -111.5913,
      "step": 72950
    },
    {
      "epoch": 5.8368,
      "grad_norm": 18.580581665039062,
      "learning_rate": 5.44e-06,
      "loss": -111.8731,
      "step": 72960
    },
    {
      "epoch": 5.8376,
      "grad_norm": 71.09546661376953,
      "learning_rate": 5.413333333333334e-06,
      "loss": -111.0343,
      "step": 72970
    },
    {
      "epoch": 5.8384,
      "grad_norm": 22.25267791748047,
      "learning_rate": 5.386666666666667e-06,
      "loss": -111.6051,
      "step": 72980
    },
    {
      "epoch": 5.8392,
      "grad_norm": 41.49065399169922,
      "learning_rate": 5.36e-06,
      "loss": -111.247,
      "step": 72990
    },
    {
      "epoch": 5.84,
      "grad_norm": 18.22176742553711,
      "learning_rate": 5.333333333333334e-06,
      "loss": -111.2926,
      "step": 73000
    },
    {
      "epoch": 5.8408,
      "grad_norm": 25.283174514770508,
      "learning_rate": 5.306666666666667e-06,
      "loss": -112.4401,
      "step": 73010
    },
    {
      "epoch": 5.8416,
      "grad_norm": 35.47785186767578,
      "learning_rate": 5.28e-06,
      "loss": -111.0708,
      "step": 73020
    },
    {
      "epoch": 5.8424,
      "grad_norm": 115.90275573730469,
      "learning_rate": 5.2533333333333336e-06,
      "loss": -111.5416,
      "step": 73030
    },
    {
      "epoch": 5.8431999999999995,
      "grad_norm": 75.61045837402344,
      "learning_rate": 5.226666666666667e-06,
      "loss": -111.4267,
      "step": 73040
    },
    {
      "epoch": 5.844,
      "grad_norm": 24.132532119750977,
      "learning_rate": 5.2e-06,
      "loss": -112.2676,
      "step": 73050
    },
    {
      "epoch": 5.8448,
      "grad_norm": 55.39638900756836,
      "learning_rate": 5.1733333333333335e-06,
      "loss": -111.4466,
      "step": 73060
    },
    {
      "epoch": 5.8456,
      "grad_norm": 35.62018585205078,
      "learning_rate": 5.146666666666667e-06,
      "loss": -111.3736,
      "step": 73070
    },
    {
      "epoch": 5.8464,
      "grad_norm": 25.755949020385742,
      "learning_rate": 5.12e-06,
      "loss": -112.0246,
      "step": 73080
    },
    {
      "epoch": 5.8472,
      "grad_norm": 40.05693435668945,
      "learning_rate": 5.093333333333333e-06,
      "loss": -111.1858,
      "step": 73090
    },
    {
      "epoch": 5.848,
      "grad_norm": 59.105316162109375,
      "learning_rate": 5.066666666666667e-06,
      "loss": -112.6592,
      "step": 73100
    },
    {
      "epoch": 5.8488,
      "grad_norm": 20.186222076416016,
      "learning_rate": 5.04e-06,
      "loss": -111.6643,
      "step": 73110
    },
    {
      "epoch": 5.8496,
      "grad_norm": 37.448219299316406,
      "learning_rate": 5.013333333333334e-06,
      "loss": -111.8404,
      "step": 73120
    },
    {
      "epoch": 5.8504000000000005,
      "grad_norm": 62.89640808105469,
      "learning_rate": 4.986666666666667e-06,
      "loss": -111.529,
      "step": 73130
    },
    {
      "epoch": 5.8512,
      "grad_norm": 103.14151000976562,
      "learning_rate": 4.96e-06,
      "loss": -111.8432,
      "step": 73140
    },
    {
      "epoch": 5.852,
      "grad_norm": 108.70745849609375,
      "learning_rate": 4.933333333333333e-06,
      "loss": -111.6005,
      "step": 73150
    },
    {
      "epoch": 5.8528,
      "grad_norm": 28.260440826416016,
      "learning_rate": 4.906666666666667e-06,
      "loss": -111.5455,
      "step": 73160
    },
    {
      "epoch": 5.8536,
      "grad_norm": 25.18068504333496,
      "learning_rate": 4.880000000000001e-06,
      "loss": -111.9161,
      "step": 73170
    },
    {
      "epoch": 5.8544,
      "grad_norm": 59.829742431640625,
      "learning_rate": 4.853333333333333e-06,
      "loss": -112.108,
      "step": 73180
    },
    {
      "epoch": 5.8552,
      "grad_norm": 60.391090393066406,
      "learning_rate": 4.8266666666666665e-06,
      "loss": -111.9929,
      "step": 73190
    },
    {
      "epoch": 5.856,
      "grad_norm": 18.644100189208984,
      "learning_rate": 4.800000000000001e-06,
      "loss": -111.5857,
      "step": 73200
    },
    {
      "epoch": 5.8568,
      "grad_norm": 52.92498779296875,
      "learning_rate": 4.773333333333334e-06,
      "loss": -111.5475,
      "step": 73210
    },
    {
      "epoch": 5.8576,
      "grad_norm": 47.84403610229492,
      "learning_rate": 4.746666666666666e-06,
      "loss": -112.2966,
      "step": 73220
    },
    {
      "epoch": 5.8584,
      "grad_norm": 48.60337829589844,
      "learning_rate": 4.72e-06,
      "loss": -111.6933,
      "step": 73230
    },
    {
      "epoch": 5.8591999999999995,
      "grad_norm": 65.43896484375,
      "learning_rate": 4.693333333333334e-06,
      "loss": -111.8092,
      "step": 73240
    },
    {
      "epoch": 5.86,
      "grad_norm": 50.17241668701172,
      "learning_rate": 4.666666666666667e-06,
      "loss": -112.5331,
      "step": 73250
    },
    {
      "epoch": 5.8608,
      "grad_norm": 98.79424285888672,
      "learning_rate": 4.64e-06,
      "loss": -111.4785,
      "step": 73260
    },
    {
      "epoch": 5.8616,
      "grad_norm": 81.34821319580078,
      "learning_rate": 4.613333333333334e-06,
      "loss": -112.2577,
      "step": 73270
    },
    {
      "epoch": 5.8624,
      "grad_norm": 18.878202438354492,
      "learning_rate": 4.586666666666667e-06,
      "loss": -112.0395,
      "step": 73280
    },
    {
      "epoch": 5.8632,
      "grad_norm": 88.7695541381836,
      "learning_rate": 4.56e-06,
      "loss": -111.5963,
      "step": 73290
    },
    {
      "epoch": 5.864,
      "grad_norm": 38.412010192871094,
      "learning_rate": 4.533333333333334e-06,
      "loss": -111.7481,
      "step": 73300
    },
    {
      "epoch": 5.8648,
      "grad_norm": 32.40101623535156,
      "learning_rate": 4.506666666666667e-06,
      "loss": -111.6655,
      "step": 73310
    },
    {
      "epoch": 5.8656,
      "grad_norm": 20.081087112426758,
      "learning_rate": 4.48e-06,
      "loss": -111.4396,
      "step": 73320
    },
    {
      "epoch": 5.8664,
      "grad_norm": 63.7404670715332,
      "learning_rate": 4.453333333333334e-06,
      "loss": -112.3876,
      "step": 73330
    },
    {
      "epoch": 5.8672,
      "grad_norm": 19.3247013092041,
      "learning_rate": 4.426666666666667e-06,
      "loss": -110.96,
      "step": 73340
    },
    {
      "epoch": 5.868,
      "grad_norm": 43.6422004699707,
      "learning_rate": 4.4e-06,
      "loss": -111.6742,
      "step": 73350
    },
    {
      "epoch": 5.8688,
      "grad_norm": 102.42727661132812,
      "learning_rate": 4.3733333333333335e-06,
      "loss": -112.1279,
      "step": 73360
    },
    {
      "epoch": 5.8696,
      "grad_norm": 176.7458038330078,
      "learning_rate": 4.346666666666667e-06,
      "loss": -112.0187,
      "step": 73370
    },
    {
      "epoch": 5.8704,
      "grad_norm": 41.98746109008789,
      "learning_rate": 4.32e-06,
      "loss": -111.9714,
      "step": 73380
    },
    {
      "epoch": 5.8712,
      "grad_norm": 44.53990173339844,
      "learning_rate": 4.2933333333333334e-06,
      "loss": -111.6387,
      "step": 73390
    },
    {
      "epoch": 5.872,
      "grad_norm": 121.11481475830078,
      "learning_rate": 4.266666666666667e-06,
      "loss": -112.9186,
      "step": 73400
    },
    {
      "epoch": 5.8728,
      "grad_norm": 27.70139503479004,
      "learning_rate": 4.24e-06,
      "loss": -111.7591,
      "step": 73410
    },
    {
      "epoch": 5.8736,
      "grad_norm": 52.418087005615234,
      "learning_rate": 4.213333333333333e-06,
      "loss": -111.599,
      "step": 73420
    },
    {
      "epoch": 5.8744,
      "grad_norm": 73.39385223388672,
      "learning_rate": 4.1866666666666675e-06,
      "loss": -110.7241,
      "step": 73430
    },
    {
      "epoch": 5.8751999999999995,
      "grad_norm": 159.89903259277344,
      "learning_rate": 4.16e-06,
      "loss": -111.8814,
      "step": 73440
    },
    {
      "epoch": 5.876,
      "grad_norm": 78.79834747314453,
      "learning_rate": 4.133333333333333e-06,
      "loss": -111.9925,
      "step": 73450
    },
    {
      "epoch": 5.8768,
      "grad_norm": 53.895538330078125,
      "learning_rate": 4.106666666666667e-06,
      "loss": -111.1956,
      "step": 73460
    },
    {
      "epoch": 5.8776,
      "grad_norm": 71.13493347167969,
      "learning_rate": 4.080000000000001e-06,
      "loss": -111.5674,
      "step": 73470
    },
    {
      "epoch": 5.8784,
      "grad_norm": 81.74551391601562,
      "learning_rate": 4.053333333333333e-06,
      "loss": -111.0182,
      "step": 73480
    },
    {
      "epoch": 5.8792,
      "grad_norm": 107.35135650634766,
      "learning_rate": 4.0266666666666665e-06,
      "loss": -111.5818,
      "step": 73490
    },
    {
      "epoch": 5.88,
      "grad_norm": 30.432710647583008,
      "learning_rate": 4.000000000000001e-06,
      "loss": -111.4593,
      "step": 73500
    },
    {
      "epoch": 5.8808,
      "grad_norm": 76.33025360107422,
      "learning_rate": 3.973333333333334e-06,
      "loss": -112.386,
      "step": 73510
    },
    {
      "epoch": 5.8816,
      "grad_norm": 38.20899200439453,
      "learning_rate": 3.9466666666666664e-06,
      "loss": -111.8017,
      "step": 73520
    },
    {
      "epoch": 5.8824,
      "grad_norm": 17.251094818115234,
      "learning_rate": 3.92e-06,
      "loss": -112.0792,
      "step": 73530
    },
    {
      "epoch": 5.8832,
      "grad_norm": 33.169677734375,
      "learning_rate": 3.893333333333334e-06,
      "loss": -112.1062,
      "step": 73540
    },
    {
      "epoch": 5.884,
      "grad_norm": 22.585248947143555,
      "learning_rate": 3.866666666666667e-06,
      "loss": -111.1925,
      "step": 73550
    },
    {
      "epoch": 5.8848,
      "grad_norm": 160.44114685058594,
      "learning_rate": 3.84e-06,
      "loss": -111.8587,
      "step": 73560
    },
    {
      "epoch": 5.8856,
      "grad_norm": 22.738666534423828,
      "learning_rate": 3.8133333333333334e-06,
      "loss": -112.3653,
      "step": 73570
    },
    {
      "epoch": 5.8864,
      "grad_norm": 33.465641021728516,
      "learning_rate": 3.7866666666666667e-06,
      "loss": -111.5877,
      "step": 73580
    },
    {
      "epoch": 5.8872,
      "grad_norm": 73.83168029785156,
      "learning_rate": 3.7600000000000004e-06,
      "loss": -111.4348,
      "step": 73590
    },
    {
      "epoch": 5.888,
      "grad_norm": 56.20138168334961,
      "learning_rate": 3.7333333333333337e-06,
      "loss": -111.6314,
      "step": 73600
    },
    {
      "epoch": 5.8888,
      "grad_norm": 19.589012145996094,
      "learning_rate": 3.7066666666666666e-06,
      "loss": -112.4636,
      "step": 73610
    },
    {
      "epoch": 5.8896,
      "grad_norm": 39.1446533203125,
      "learning_rate": 3.68e-06,
      "loss": -111.8658,
      "step": 73620
    },
    {
      "epoch": 5.8904,
      "grad_norm": 37.828304290771484,
      "learning_rate": 3.6533333333333336e-06,
      "loss": -111.3395,
      "step": 73630
    },
    {
      "epoch": 5.8911999999999995,
      "grad_norm": 32.3147087097168,
      "learning_rate": 3.626666666666667e-06,
      "loss": -110.7666,
      "step": 73640
    },
    {
      "epoch": 5.892,
      "grad_norm": 15.65506649017334,
      "learning_rate": 3.6e-06,
      "loss": -111.3679,
      "step": 73650
    },
    {
      "epoch": 5.8928,
      "grad_norm": 98.17374420166016,
      "learning_rate": 3.5733333333333336e-06,
      "loss": -111.55,
      "step": 73660
    },
    {
      "epoch": 5.8936,
      "grad_norm": 64.82001495361328,
      "learning_rate": 3.546666666666667e-06,
      "loss": -111.6292,
      "step": 73670
    },
    {
      "epoch": 5.8944,
      "grad_norm": 54.9179801940918,
      "learning_rate": 3.52e-06,
      "loss": -111.1718,
      "step": 73680
    },
    {
      "epoch": 5.8952,
      "grad_norm": 34.2821159362793,
      "learning_rate": 3.493333333333333e-06,
      "loss": -110.9993,
      "step": 73690
    },
    {
      "epoch": 5.896,
      "grad_norm": 20.281362533569336,
      "learning_rate": 3.466666666666667e-06,
      "loss": -112.0903,
      "step": 73700
    },
    {
      "epoch": 5.8968,
      "grad_norm": 21.515520095825195,
      "learning_rate": 3.44e-06,
      "loss": -111.6715,
      "step": 73710
    },
    {
      "epoch": 5.8976,
      "grad_norm": 43.57079315185547,
      "learning_rate": 3.413333333333334e-06,
      "loss": -112.2332,
      "step": 73720
    },
    {
      "epoch": 5.8984,
      "grad_norm": 20.406909942626953,
      "learning_rate": 3.386666666666667e-06,
      "loss": -111.1296,
      "step": 73730
    },
    {
      "epoch": 5.8992,
      "grad_norm": 32.081932067871094,
      "learning_rate": 3.36e-06,
      "loss": -111.8061,
      "step": 73740
    },
    {
      "epoch": 5.9,
      "grad_norm": 197.54965209960938,
      "learning_rate": 3.3333333333333333e-06,
      "loss": -112.0611,
      "step": 73750
    },
    {
      "epoch": 5.9008,
      "grad_norm": 57.90144348144531,
      "learning_rate": 3.306666666666667e-06,
      "loss": -112.092,
      "step": 73760
    },
    {
      "epoch": 5.9016,
      "grad_norm": 70.8945541381836,
      "learning_rate": 3.2800000000000004e-06,
      "loss": -113.0554,
      "step": 73770
    },
    {
      "epoch": 5.9024,
      "grad_norm": 16.117658615112305,
      "learning_rate": 3.2533333333333332e-06,
      "loss": -111.5057,
      "step": 73780
    },
    {
      "epoch": 5.9032,
      "grad_norm": 113.6676025390625,
      "learning_rate": 3.2266666666666665e-06,
      "loss": -111.3296,
      "step": 73790
    },
    {
      "epoch": 5.904,
      "grad_norm": 24.147808074951172,
      "learning_rate": 3.2000000000000003e-06,
      "loss": -111.8586,
      "step": 73800
    },
    {
      "epoch": 5.9048,
      "grad_norm": 41.4808349609375,
      "learning_rate": 3.1733333333333336e-06,
      "loss": -111.184,
      "step": 73810
    },
    {
      "epoch": 5.9056,
      "grad_norm": 45.71424865722656,
      "learning_rate": 3.1466666666666665e-06,
      "loss": -111.4969,
      "step": 73820
    },
    {
      "epoch": 5.9064,
      "grad_norm": 38.087677001953125,
      "learning_rate": 3.12e-06,
      "loss": -112.0725,
      "step": 73830
    },
    {
      "epoch": 5.9072,
      "grad_norm": 65.1022720336914,
      "learning_rate": 3.0933333333333335e-06,
      "loss": -113.2207,
      "step": 73840
    },
    {
      "epoch": 5.908,
      "grad_norm": 13.995373725891113,
      "learning_rate": 3.066666666666667e-06,
      "loss": -111.0841,
      "step": 73850
    },
    {
      "epoch": 5.9088,
      "grad_norm": 134.55697631835938,
      "learning_rate": 3.04e-06,
      "loss": -111.7335,
      "step": 73860
    },
    {
      "epoch": 5.9096,
      "grad_norm": 45.2320556640625,
      "learning_rate": 3.0133333333333334e-06,
      "loss": -111.6416,
      "step": 73870
    },
    {
      "epoch": 5.9104,
      "grad_norm": 18.70293426513672,
      "learning_rate": 2.9866666666666667e-06,
      "loss": -111.6636,
      "step": 73880
    },
    {
      "epoch": 5.9112,
      "grad_norm": 52.3991584777832,
      "learning_rate": 2.9600000000000005e-06,
      "loss": -111.273,
      "step": 73890
    },
    {
      "epoch": 5.912,
      "grad_norm": 50.1571044921875,
      "learning_rate": 2.9333333333333333e-06,
      "loss": -111.7528,
      "step": 73900
    },
    {
      "epoch": 5.9128,
      "grad_norm": 48.65962600708008,
      "learning_rate": 2.906666666666667e-06,
      "loss": -112.0729,
      "step": 73910
    },
    {
      "epoch": 5.9136,
      "grad_norm": 24.56183624267578,
      "learning_rate": 2.88e-06,
      "loss": -111.1592,
      "step": 73920
    },
    {
      "epoch": 5.9144,
      "grad_norm": 20.55864715576172,
      "learning_rate": 2.8533333333333337e-06,
      "loss": -111.4547,
      "step": 73930
    },
    {
      "epoch": 5.9152000000000005,
      "grad_norm": 104.57828521728516,
      "learning_rate": 2.8266666666666666e-06,
      "loss": -111.8414,
      "step": 73940
    },
    {
      "epoch": 5.916,
      "grad_norm": 164.83169555664062,
      "learning_rate": 2.8000000000000003e-06,
      "loss": -111.5383,
      "step": 73950
    },
    {
      "epoch": 5.9168,
      "grad_norm": 94.5776596069336,
      "learning_rate": 2.773333333333333e-06,
      "loss": -110.6526,
      "step": 73960
    },
    {
      "epoch": 5.9176,
      "grad_norm": 23.030567169189453,
      "learning_rate": 2.746666666666667e-06,
      "loss": -111.5827,
      "step": 73970
    },
    {
      "epoch": 5.9184,
      "grad_norm": 28.720909118652344,
      "learning_rate": 2.72e-06,
      "loss": -111.356,
      "step": 73980
    },
    {
      "epoch": 5.9192,
      "grad_norm": 34.919166564941406,
      "learning_rate": 2.6933333333333335e-06,
      "loss": -112.0757,
      "step": 73990
    },
    {
      "epoch": 5.92,
      "grad_norm": 21.392858505249023,
      "learning_rate": 2.666666666666667e-06,
      "loss": -112.2575,
      "step": 74000
    },
    {
      "epoch": 5.9208,
      "grad_norm": 41.070518493652344,
      "learning_rate": 2.64e-06,
      "loss": -111.9194,
      "step": 74010
    },
    {
      "epoch": 5.9216,
      "grad_norm": 18.72520637512207,
      "learning_rate": 2.6133333333333334e-06,
      "loss": -111.5705,
      "step": 74020
    },
    {
      "epoch": 5.9224,
      "grad_norm": 30.127567291259766,
      "learning_rate": 2.5866666666666667e-06,
      "loss": -111.4163,
      "step": 74030
    },
    {
      "epoch": 5.9232,
      "grad_norm": 37.037147521972656,
      "learning_rate": 2.56e-06,
      "loss": -111.4925,
      "step": 74040
    },
    {
      "epoch": 5.924,
      "grad_norm": 32.28618621826172,
      "learning_rate": 2.5333333333333334e-06,
      "loss": -111.4096,
      "step": 74050
    },
    {
      "epoch": 5.9248,
      "grad_norm": 113.2923812866211,
      "learning_rate": 2.506666666666667e-06,
      "loss": -112.0651,
      "step": 74060
    },
    {
      "epoch": 5.9256,
      "grad_norm": 36.86178207397461,
      "learning_rate": 2.48e-06,
      "loss": -111.5587,
      "step": 74070
    },
    {
      "epoch": 5.9264,
      "grad_norm": 15.742378234863281,
      "learning_rate": 2.4533333333333337e-06,
      "loss": -112.0997,
      "step": 74080
    },
    {
      "epoch": 5.9272,
      "grad_norm": 120.89720916748047,
      "learning_rate": 2.4266666666666666e-06,
      "loss": -112.4342,
      "step": 74090
    },
    {
      "epoch": 5.928,
      "grad_norm": 40.39210891723633,
      "learning_rate": 2.4000000000000003e-06,
      "loss": -111.8172,
      "step": 74100
    },
    {
      "epoch": 5.9288,
      "grad_norm": 71.47187805175781,
      "learning_rate": 2.373333333333333e-06,
      "loss": -112.3398,
      "step": 74110
    },
    {
      "epoch": 5.9296,
      "grad_norm": 58.80186080932617,
      "learning_rate": 2.346666666666667e-06,
      "loss": -111.8754,
      "step": 74120
    },
    {
      "epoch": 5.9304,
      "grad_norm": 32.31513595581055,
      "learning_rate": 2.32e-06,
      "loss": -111.8635,
      "step": 74130
    },
    {
      "epoch": 5.9312000000000005,
      "grad_norm": 86.32351684570312,
      "learning_rate": 2.2933333333333335e-06,
      "loss": -111.0746,
      "step": 74140
    },
    {
      "epoch": 5.932,
      "grad_norm": 31.097808837890625,
      "learning_rate": 2.266666666666667e-06,
      "loss": -110.8288,
      "step": 74150
    },
    {
      "epoch": 5.9328,
      "grad_norm": 106.35977172851562,
      "learning_rate": 2.24e-06,
      "loss": -111.3285,
      "step": 74160
    },
    {
      "epoch": 5.9336,
      "grad_norm": 110.82061004638672,
      "learning_rate": 2.2133333333333335e-06,
      "loss": -112.2239,
      "step": 74170
    },
    {
      "epoch": 5.9344,
      "grad_norm": 57.93716812133789,
      "learning_rate": 2.1866666666666668e-06,
      "loss": -111.344,
      "step": 74180
    },
    {
      "epoch": 5.9352,
      "grad_norm": 72.43824768066406,
      "learning_rate": 2.16e-06,
      "loss": -111.407,
      "step": 74190
    },
    {
      "epoch": 5.936,
      "grad_norm": 64.485107421875,
      "learning_rate": 2.1333333333333334e-06,
      "loss": -111.9997,
      "step": 74200
    },
    {
      "epoch": 5.9368,
      "grad_norm": 94.31233215332031,
      "learning_rate": 2.1066666666666667e-06,
      "loss": -112.546,
      "step": 74210
    },
    {
      "epoch": 5.9376,
      "grad_norm": 54.91444396972656,
      "learning_rate": 2.08e-06,
      "loss": -110.7259,
      "step": 74220
    },
    {
      "epoch": 5.9384,
      "grad_norm": 72.59071350097656,
      "learning_rate": 2.0533333333333333e-06,
      "loss": -112.219,
      "step": 74230
    },
    {
      "epoch": 5.9392,
      "grad_norm": 18.11734962463379,
      "learning_rate": 2.0266666666666666e-06,
      "loss": -110.9852,
      "step": 74240
    },
    {
      "epoch": 5.9399999999999995,
      "grad_norm": 61.70806121826172,
      "learning_rate": 2.0000000000000003e-06,
      "loss": -111.5106,
      "step": 74250
    },
    {
      "epoch": 5.9408,
      "grad_norm": 119.91546630859375,
      "learning_rate": 1.9733333333333332e-06,
      "loss": -112.6369,
      "step": 74260
    },
    {
      "epoch": 5.9416,
      "grad_norm": 23.930824279785156,
      "learning_rate": 1.946666666666667e-06,
      "loss": -111.8679,
      "step": 74270
    },
    {
      "epoch": 5.9424,
      "grad_norm": 40.207733154296875,
      "learning_rate": 1.92e-06,
      "loss": -112.5079,
      "step": 74280
    },
    {
      "epoch": 5.9432,
      "grad_norm": 76.54997253417969,
      "learning_rate": 1.8933333333333333e-06,
      "loss": -110.5134,
      "step": 74290
    },
    {
      "epoch": 5.944,
      "grad_norm": 22.463619232177734,
      "learning_rate": 1.8666666666666669e-06,
      "loss": -112.0588,
      "step": 74300
    },
    {
      "epoch": 5.9448,
      "grad_norm": 59.00147247314453,
      "learning_rate": 1.84e-06,
      "loss": -111.0575,
      "step": 74310
    },
    {
      "epoch": 5.9456,
      "grad_norm": 89.8331069946289,
      "learning_rate": 1.8133333333333335e-06,
      "loss": -112.2108,
      "step": 74320
    },
    {
      "epoch": 5.9464,
      "grad_norm": 140.92239379882812,
      "learning_rate": 1.7866666666666668e-06,
      "loss": -110.7635,
      "step": 74330
    },
    {
      "epoch": 5.9472000000000005,
      "grad_norm": 60.6170654296875,
      "learning_rate": 1.76e-06,
      "loss": -112.0962,
      "step": 74340
    },
    {
      "epoch": 5.948,
      "grad_norm": 28.18981170654297,
      "learning_rate": 1.7333333333333334e-06,
      "loss": -112.3244,
      "step": 74350
    },
    {
      "epoch": 5.9488,
      "grad_norm": 20.132814407348633,
      "learning_rate": 1.706666666666667e-06,
      "loss": -111.6276,
      "step": 74360
    },
    {
      "epoch": 5.9496,
      "grad_norm": 29.78340721130371,
      "learning_rate": 1.68e-06,
      "loss": -111.537,
      "step": 74370
    },
    {
      "epoch": 5.9504,
      "grad_norm": 16.328784942626953,
      "learning_rate": 1.6533333333333335e-06,
      "loss": -111.9623,
      "step": 74380
    },
    {
      "epoch": 5.9512,
      "grad_norm": 46.99602127075195,
      "learning_rate": 1.6266666666666666e-06,
      "loss": -111.5015,
      "step": 74390
    },
    {
      "epoch": 5.952,
      "grad_norm": 29.190370559692383,
      "learning_rate": 1.6000000000000001e-06,
      "loss": -111.8521,
      "step": 74400
    },
    {
      "epoch": 5.9528,
      "grad_norm": 19.530982971191406,
      "learning_rate": 1.5733333333333332e-06,
      "loss": -111.6828,
      "step": 74410
    },
    {
      "epoch": 5.9536,
      "grad_norm": 26.887678146362305,
      "learning_rate": 1.5466666666666668e-06,
      "loss": -111.5971,
      "step": 74420
    },
    {
      "epoch": 5.9544,
      "grad_norm": 76.56488800048828,
      "learning_rate": 1.52e-06,
      "loss": -112.3548,
      "step": 74430
    },
    {
      "epoch": 5.9552,
      "grad_norm": 19.58315658569336,
      "learning_rate": 1.4933333333333334e-06,
      "loss": -111.3101,
      "step": 74440
    },
    {
      "epoch": 5.9559999999999995,
      "grad_norm": 17.699460983276367,
      "learning_rate": 1.4666666666666667e-06,
      "loss": -112.0799,
      "step": 74450
    },
    {
      "epoch": 5.9568,
      "grad_norm": 109.79362487792969,
      "learning_rate": 1.44e-06,
      "loss": -112.4602,
      "step": 74460
    },
    {
      "epoch": 5.9576,
      "grad_norm": 28.886253356933594,
      "learning_rate": 1.4133333333333333e-06,
      "loss": -111.7261,
      "step": 74470
    },
    {
      "epoch": 5.9584,
      "grad_norm": 85.98223876953125,
      "learning_rate": 1.3866666666666666e-06,
      "loss": -111.7691,
      "step": 74480
    },
    {
      "epoch": 5.9592,
      "grad_norm": 137.52989196777344,
      "learning_rate": 1.36e-06,
      "loss": -111.2487,
      "step": 74490
    },
    {
      "epoch": 5.96,
      "grad_norm": 53.624568939208984,
      "learning_rate": 1.3333333333333334e-06,
      "loss": -111.7387,
      "step": 74500
    },
    {
      "epoch": 5.9608,
      "grad_norm": 71.22077941894531,
      "learning_rate": 1.3066666666666667e-06,
      "loss": -111.5872,
      "step": 74510
    },
    {
      "epoch": 5.9616,
      "grad_norm": 48.631248474121094,
      "learning_rate": 1.28e-06,
      "loss": -111.914,
      "step": 74520
    },
    {
      "epoch": 5.9624,
      "grad_norm": 25.637672424316406,
      "learning_rate": 1.2533333333333335e-06,
      "loss": -110.6865,
      "step": 74530
    },
    {
      "epoch": 5.9632,
      "grad_norm": 48.76054000854492,
      "learning_rate": 1.2266666666666669e-06,
      "loss": -111.9109,
      "step": 74540
    },
    {
      "epoch": 5.964,
      "grad_norm": 20.63703155517578,
      "learning_rate": 1.2000000000000002e-06,
      "loss": -110.4854,
      "step": 74550
    },
    {
      "epoch": 5.9648,
      "grad_norm": 31.86101722717285,
      "learning_rate": 1.1733333333333335e-06,
      "loss": -111.0525,
      "step": 74560
    },
    {
      "epoch": 5.9656,
      "grad_norm": 28.28958511352539,
      "learning_rate": 1.1466666666666668e-06,
      "loss": -111.3409,
      "step": 74570
    },
    {
      "epoch": 5.9664,
      "grad_norm": 27.324220657348633,
      "learning_rate": 1.12e-06,
      "loss": -111.2003,
      "step": 74580
    },
    {
      "epoch": 5.9672,
      "grad_norm": 61.387142181396484,
      "learning_rate": 1.0933333333333334e-06,
      "loss": -111.8519,
      "step": 74590
    },
    {
      "epoch": 5.968,
      "grad_norm": 29.56605339050293,
      "learning_rate": 1.0666666666666667e-06,
      "loss": -110.4243,
      "step": 74600
    },
    {
      "epoch": 5.9688,
      "grad_norm": 13.42680835723877,
      "learning_rate": 1.04e-06,
      "loss": -111.3742,
      "step": 74610
    },
    {
      "epoch": 5.9696,
      "grad_norm": 78.4583740234375,
      "learning_rate": 1.0133333333333333e-06,
      "loss": -111.3303,
      "step": 74620
    },
    {
      "epoch": 5.9704,
      "grad_norm": 23.42034339904785,
      "learning_rate": 9.866666666666666e-07,
      "loss": -111.9898,
      "step": 74630
    },
    {
      "epoch": 5.9712,
      "grad_norm": 23.452106475830078,
      "learning_rate": 9.6e-07,
      "loss": -111.9928,
      "step": 74640
    },
    {
      "epoch": 5.9719999999999995,
      "grad_norm": 87.2040786743164,
      "learning_rate": 9.333333333333334e-07,
      "loss": -111.9493,
      "step": 74650
    },
    {
      "epoch": 5.9728,
      "grad_norm": 29.25286102294922,
      "learning_rate": 9.066666666666667e-07,
      "loss": -111.3703,
      "step": 74660
    },
    {
      "epoch": 5.9736,
      "grad_norm": 43.54849624633789,
      "learning_rate": 8.8e-07,
      "loss": -111.6524,
      "step": 74670
    },
    {
      "epoch": 5.9744,
      "grad_norm": 26.945720672607422,
      "learning_rate": 8.533333333333335e-07,
      "loss": -111.3762,
      "step": 74680
    },
    {
      "epoch": 5.9752,
      "grad_norm": 26.125747680664062,
      "learning_rate": 8.266666666666668e-07,
      "loss": -111.3628,
      "step": 74690
    },
    {
      "epoch": 5.976,
      "grad_norm": 37.60407638549805,
      "learning_rate": 8.000000000000001e-07,
      "loss": -112.17,
      "step": 74700
    },
    {
      "epoch": 5.9768,
      "grad_norm": 23.6527042388916,
      "learning_rate": 7.733333333333334e-07,
      "loss": -112.1874,
      "step": 74710
    },
    {
      "epoch": 5.9776,
      "grad_norm": 21.563796997070312,
      "learning_rate": 7.466666666666667e-07,
      "loss": -111.5007,
      "step": 74720
    },
    {
      "epoch": 5.9784,
      "grad_norm": 22.725229263305664,
      "learning_rate": 7.2e-07,
      "loss": -112.6632,
      "step": 74730
    },
    {
      "epoch": 5.9792,
      "grad_norm": 91.88726806640625,
      "learning_rate": 6.933333333333333e-07,
      "loss": -111.702,
      "step": 74740
    },
    {
      "epoch": 5.98,
      "grad_norm": 115.60990142822266,
      "learning_rate": 6.666666666666667e-07,
      "loss": -112.293,
      "step": 74750
    },
    {
      "epoch": 5.9808,
      "grad_norm": 33.29826354980469,
      "learning_rate": 6.4e-07,
      "loss": -111.0675,
      "step": 74760
    },
    {
      "epoch": 5.9816,
      "grad_norm": 72.41614532470703,
      "learning_rate": 6.133333333333334e-07,
      "loss": -111.6407,
      "step": 74770
    },
    {
      "epoch": 5.9824,
      "grad_norm": 66.2393569946289,
      "learning_rate": 5.866666666666667e-07,
      "loss": -111.7976,
      "step": 74780
    },
    {
      "epoch": 5.9832,
      "grad_norm": 35.51952362060547,
      "learning_rate": 5.6e-07,
      "loss": -112.3914,
      "step": 74790
    },
    {
      "epoch": 5.984,
      "grad_norm": 24.412752151489258,
      "learning_rate": 5.333333333333333e-07,
      "loss": -111.8382,
      "step": 74800
    },
    {
      "epoch": 5.9848,
      "grad_norm": 67.61270904541016,
      "learning_rate": 5.066666666666667e-07,
      "loss": -110.8984,
      "step": 74810
    },
    {
      "epoch": 5.9856,
      "grad_norm": 135.23211669921875,
      "learning_rate": 4.8e-07,
      "loss": -111.1099,
      "step": 74820
    },
    {
      "epoch": 5.9864,
      "grad_norm": 48.35858917236328,
      "learning_rate": 4.5333333333333337e-07,
      "loss": -112.0425,
      "step": 74830
    },
    {
      "epoch": 5.9872,
      "grad_norm": 54.88655471801758,
      "learning_rate": 4.2666666666666673e-07,
      "loss": -112.188,
      "step": 74840
    },
    {
      "epoch": 5.9879999999999995,
      "grad_norm": 19.142484664916992,
      "learning_rate": 4.0000000000000003e-07,
      "loss": -111.8578,
      "step": 74850
    },
    {
      "epoch": 5.9888,
      "grad_norm": 17.948352813720703,
      "learning_rate": 3.7333333333333334e-07,
      "loss": -111.7052,
      "step": 74860
    },
    {
      "epoch": 5.9896,
      "grad_norm": 75.2696762084961,
      "learning_rate": 3.4666666666666665e-07,
      "loss": -111.1327,
      "step": 74870
    },
    {
      "epoch": 5.9904,
      "grad_norm": 346.64984130859375,
      "learning_rate": 3.2e-07,
      "loss": -111.8925,
      "step": 74880
    },
    {
      "epoch": 5.9912,
      "grad_norm": 27.758615493774414,
      "learning_rate": 2.9333333333333337e-07,
      "loss": -111.0253,
      "step": 74890
    },
    {
      "epoch": 5.992,
      "grad_norm": 23.7025146484375,
      "learning_rate": 2.6666666666666667e-07,
      "loss": -111.8995,
      "step": 74900
    },
    {
      "epoch": 5.9928,
      "grad_norm": 16.068748474121094,
      "learning_rate": 2.4e-07,
      "loss": -111.6586,
      "step": 74910
    },
    {
      "epoch": 5.9936,
      "grad_norm": 32.715572357177734,
      "learning_rate": 2.1333333333333336e-07,
      "loss": -111.9819,
      "step": 74920
    },
    {
      "epoch": 5.9944,
      "grad_norm": 135.54083251953125,
      "learning_rate": 1.8666666666666667e-07,
      "loss": -110.8363,
      "step": 74930
    },
    {
      "epoch": 5.9952,
      "grad_norm": 64.27992248535156,
      "learning_rate": 1.6e-07,
      "loss": -110.2472,
      "step": 74940
    },
    {
      "epoch": 5.996,
      "grad_norm": 24.217864990234375,
      "learning_rate": 1.3333333333333334e-07,
      "loss": -110.5543,
      "step": 74950
    },
    {
      "epoch": 5.9968,
      "grad_norm": 50.70524215698242,
      "learning_rate": 1.0666666666666668e-07,
      "loss": -110.9756,
      "step": 74960
    },
    {
      "epoch": 5.9976,
      "grad_norm": 26.68126106262207,
      "learning_rate": 8e-08,
      "loss": -111.5079,
      "step": 74970
    },
    {
      "epoch": 5.9984,
      "grad_norm": 43.4404182434082,
      "learning_rate": 5.333333333333334e-08,
      "loss": -111.2113,
      "step": 74980
    },
    {
      "epoch": 5.9992,
      "grad_norm": 59.032203674316406,
      "learning_rate": 2.666666666666667e-08,
      "loss": -112.006,
      "step": 74990
    },
    {
      "epoch": 6.0,
      "grad_norm": 40.644569396972656,
      "learning_rate": 0.0,
      "loss": -112.0856,
      "step": 75000
    }
  ],
  "logging_steps": 10,
  "max_steps": 75000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.48818315264e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
