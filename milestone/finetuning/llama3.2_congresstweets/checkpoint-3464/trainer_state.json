{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 3464,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005773672055427252,
      "grad_norm": 5.394704818725586,
      "learning_rate": 0.0001996150885296382,
      "loss": 8.6714,
      "step": 10
    },
    {
      "epoch": 0.011547344110854504,
      "grad_norm": 3.4730312824249268,
      "learning_rate": 0.00019923017705927637,
      "loss": 7.2437,
      "step": 20
    },
    {
      "epoch": 0.017321016166281754,
      "grad_norm": 3.893934488296509,
      "learning_rate": 0.00019884526558891456,
      "loss": 6.465,
      "step": 30
    },
    {
      "epoch": 0.023094688221709007,
      "grad_norm": 4.380390644073486,
      "learning_rate": 0.00019846035411855275,
      "loss": 5.6951,
      "step": 40
    },
    {
      "epoch": 0.028868360277136258,
      "grad_norm": 4.522777557373047,
      "learning_rate": 0.00019807544264819095,
      "loss": 5.2066,
      "step": 50
    },
    {
      "epoch": 0.03464203233256351,
      "grad_norm": 4.840507984161377,
      "learning_rate": 0.0001976905311778291,
      "loss": 4.8981,
      "step": 60
    },
    {
      "epoch": 0.04041570438799076,
      "grad_norm": 4.835920333862305,
      "learning_rate": 0.00019730561970746728,
      "loss": 4.7627,
      "step": 70
    },
    {
      "epoch": 0.046189376443418015,
      "grad_norm": 4.378875255584717,
      "learning_rate": 0.00019692070823710547,
      "loss": 4.5861,
      "step": 80
    },
    {
      "epoch": 0.05196304849884527,
      "grad_norm": 5.315417289733887,
      "learning_rate": 0.00019653579676674366,
      "loss": 4.6405,
      "step": 90
    },
    {
      "epoch": 0.057736720554272515,
      "grad_norm": 5.264034748077393,
      "learning_rate": 0.00019615088529638183,
      "loss": 4.4227,
      "step": 100
    },
    {
      "epoch": 0.06351039260969978,
      "grad_norm": 5.934943675994873,
      "learning_rate": 0.00019576597382602002,
      "loss": 4.281,
      "step": 110
    },
    {
      "epoch": 0.06928406466512702,
      "grad_norm": 5.277578830718994,
      "learning_rate": 0.0001953810623556582,
      "loss": 4.245,
      "step": 120
    },
    {
      "epoch": 0.07505773672055427,
      "grad_norm": 6.1827311515808105,
      "learning_rate": 0.00019499615088529638,
      "loss": 4.2031,
      "step": 130
    },
    {
      "epoch": 0.08083140877598152,
      "grad_norm": 6.071866512298584,
      "learning_rate": 0.00019461123941493457,
      "loss": 4.0355,
      "step": 140
    },
    {
      "epoch": 0.08660508083140878,
      "grad_norm": 5.955367565155029,
      "learning_rate": 0.00019422632794457276,
      "loss": 4.2058,
      "step": 150
    },
    {
      "epoch": 0.09237875288683603,
      "grad_norm": 7.339695453643799,
      "learning_rate": 0.00019384141647421096,
      "loss": 3.9436,
      "step": 160
    },
    {
      "epoch": 0.09815242494226328,
      "grad_norm": 7.817234516143799,
      "learning_rate": 0.00019345650500384912,
      "loss": 4.0025,
      "step": 170
    },
    {
      "epoch": 0.10392609699769054,
      "grad_norm": 6.88603401184082,
      "learning_rate": 0.00019307159353348731,
      "loss": 3.7127,
      "step": 180
    },
    {
      "epoch": 0.10969976905311778,
      "grad_norm": 7.098567485809326,
      "learning_rate": 0.0001926866820631255,
      "loss": 3.9622,
      "step": 190
    },
    {
      "epoch": 0.11547344110854503,
      "grad_norm": 8.652942657470703,
      "learning_rate": 0.00019230177059276367,
      "loss": 3.9132,
      "step": 200
    },
    {
      "epoch": 0.12124711316397228,
      "grad_norm": 6.916898727416992,
      "learning_rate": 0.00019191685912240184,
      "loss": 3.8211,
      "step": 210
    },
    {
      "epoch": 0.12702078521939955,
      "grad_norm": 7.401280879974365,
      "learning_rate": 0.00019153194765204003,
      "loss": 3.5514,
      "step": 220
    },
    {
      "epoch": 0.13279445727482678,
      "grad_norm": 8.693737983703613,
      "learning_rate": 0.00019114703618167822,
      "loss": 3.7771,
      "step": 230
    },
    {
      "epoch": 0.13856812933025403,
      "grad_norm": 7.338130950927734,
      "learning_rate": 0.00019076212471131642,
      "loss": 3.6583,
      "step": 240
    },
    {
      "epoch": 0.14434180138568128,
      "grad_norm": 8.718976020812988,
      "learning_rate": 0.00019037721324095458,
      "loss": 3.6294,
      "step": 250
    },
    {
      "epoch": 0.15011547344110854,
      "grad_norm": 7.55558967590332,
      "learning_rate": 0.00018999230177059277,
      "loss": 3.8247,
      "step": 260
    },
    {
      "epoch": 0.1558891454965358,
      "grad_norm": 7.241472244262695,
      "learning_rate": 0.00018960739030023097,
      "loss": 3.5355,
      "step": 270
    },
    {
      "epoch": 0.16166281755196305,
      "grad_norm": 6.417301654815674,
      "learning_rate": 0.00018922247882986913,
      "loss": 3.411,
      "step": 280
    },
    {
      "epoch": 0.1674364896073903,
      "grad_norm": 8.2581205368042,
      "learning_rate": 0.00018883756735950732,
      "loss": 3.5768,
      "step": 290
    },
    {
      "epoch": 0.17321016166281755,
      "grad_norm": 8.448038101196289,
      "learning_rate": 0.00018845265588914552,
      "loss": 3.565,
      "step": 300
    },
    {
      "epoch": 0.1789838337182448,
      "grad_norm": 8.656046867370605,
      "learning_rate": 0.0001880677444187837,
      "loss": 3.5709,
      "step": 310
    },
    {
      "epoch": 0.18475750577367206,
      "grad_norm": 7.887160301208496,
      "learning_rate": 0.00018768283294842188,
      "loss": 3.685,
      "step": 320
    },
    {
      "epoch": 0.1905311778290993,
      "grad_norm": 6.965436935424805,
      "learning_rate": 0.00018729792147806004,
      "loss": 3.5234,
      "step": 330
    },
    {
      "epoch": 0.19630484988452657,
      "grad_norm": 8.347277641296387,
      "learning_rate": 0.00018691301000769823,
      "loss": 3.6024,
      "step": 340
    },
    {
      "epoch": 0.20207852193995382,
      "grad_norm": 7.537250518798828,
      "learning_rate": 0.00018652809853733643,
      "loss": 3.6247,
      "step": 350
    },
    {
      "epoch": 0.20785219399538107,
      "grad_norm": 8.786406517028809,
      "learning_rate": 0.0001861431870669746,
      "loss": 3.3722,
      "step": 360
    },
    {
      "epoch": 0.21362586605080833,
      "grad_norm": 7.203778266906738,
      "learning_rate": 0.00018575827559661278,
      "loss": 3.6391,
      "step": 370
    },
    {
      "epoch": 0.21939953810623555,
      "grad_norm": 9.22874927520752,
      "learning_rate": 0.00018537336412625098,
      "loss": 3.5066,
      "step": 380
    },
    {
      "epoch": 0.2251732101616628,
      "grad_norm": 8.191043853759766,
      "learning_rate": 0.00018498845265588914,
      "loss": 3.4407,
      "step": 390
    },
    {
      "epoch": 0.23094688221709006,
      "grad_norm": 7.641911029815674,
      "learning_rate": 0.00018460354118552733,
      "loss": 3.1996,
      "step": 400
    },
    {
      "epoch": 0.23672055427251731,
      "grad_norm": 9.566222190856934,
      "learning_rate": 0.00018421862971516553,
      "loss": 3.3934,
      "step": 410
    },
    {
      "epoch": 0.24249422632794457,
      "grad_norm": 7.945310115814209,
      "learning_rate": 0.00018383371824480372,
      "loss": 3.326,
      "step": 420
    },
    {
      "epoch": 0.24826789838337182,
      "grad_norm": 8.2669095993042,
      "learning_rate": 0.00018344880677444189,
      "loss": 3.5322,
      "step": 430
    },
    {
      "epoch": 0.2540415704387991,
      "grad_norm": 8.592453956604004,
      "learning_rate": 0.00018306389530408008,
      "loss": 3.3081,
      "step": 440
    },
    {
      "epoch": 0.25981524249422633,
      "grad_norm": 8.346524238586426,
      "learning_rate": 0.00018267898383371827,
      "loss": 3.3102,
      "step": 450
    },
    {
      "epoch": 0.26558891454965355,
      "grad_norm": 9.478904724121094,
      "learning_rate": 0.00018229407236335644,
      "loss": 3.4564,
      "step": 460
    },
    {
      "epoch": 0.27136258660508084,
      "grad_norm": 7.440075874328613,
      "learning_rate": 0.0001819091608929946,
      "loss": 3.2939,
      "step": 470
    },
    {
      "epoch": 0.27713625866050806,
      "grad_norm": 8.318315505981445,
      "learning_rate": 0.0001815242494226328,
      "loss": 3.2928,
      "step": 480
    },
    {
      "epoch": 0.28290993071593534,
      "grad_norm": 9.04977035522461,
      "learning_rate": 0.000181139337952271,
      "loss": 3.3172,
      "step": 490
    },
    {
      "epoch": 0.28868360277136257,
      "grad_norm": 8.041167259216309,
      "learning_rate": 0.00018075442648190918,
      "loss": 3.0484,
      "step": 500
    },
    {
      "epoch": 0.29445727482678985,
      "grad_norm": 8.6192045211792,
      "learning_rate": 0.00018036951501154735,
      "loss": 3.1508,
      "step": 510
    },
    {
      "epoch": 0.3002309468822171,
      "grad_norm": 9.185245513916016,
      "learning_rate": 0.00017998460354118554,
      "loss": 3.2003,
      "step": 520
    },
    {
      "epoch": 0.30600461893764436,
      "grad_norm": 8.312360763549805,
      "learning_rate": 0.00017959969207082373,
      "loss": 3.2038,
      "step": 530
    },
    {
      "epoch": 0.3117782909930716,
      "grad_norm": 6.778886318206787,
      "learning_rate": 0.0001792147806004619,
      "loss": 3.2793,
      "step": 540
    },
    {
      "epoch": 0.31755196304849886,
      "grad_norm": 8.312743186950684,
      "learning_rate": 0.0001788298691301001,
      "loss": 3.1832,
      "step": 550
    },
    {
      "epoch": 0.3233256351039261,
      "grad_norm": 7.591973781585693,
      "learning_rate": 0.00017844495765973828,
      "loss": 3.3254,
      "step": 560
    },
    {
      "epoch": 0.32909930715935337,
      "grad_norm": 7.106895923614502,
      "learning_rate": 0.00017806004618937647,
      "loss": 3.3196,
      "step": 570
    },
    {
      "epoch": 0.3348729792147806,
      "grad_norm": 8.881444931030273,
      "learning_rate": 0.0001776751347190146,
      "loss": 3.1369,
      "step": 580
    },
    {
      "epoch": 0.3406466512702079,
      "grad_norm": 9.960836410522461,
      "learning_rate": 0.0001772902232486528,
      "loss": 3.3956,
      "step": 590
    },
    {
      "epoch": 0.3464203233256351,
      "grad_norm": 8.207932472229004,
      "learning_rate": 0.000176905311778291,
      "loss": 3.0367,
      "step": 600
    },
    {
      "epoch": 0.35219399538106233,
      "grad_norm": 8.844594955444336,
      "learning_rate": 0.0001765204003079292,
      "loss": 3.1611,
      "step": 610
    },
    {
      "epoch": 0.3579676674364896,
      "grad_norm": 7.542003631591797,
      "learning_rate": 0.00017613548883756736,
      "loss": 3.0364,
      "step": 620
    },
    {
      "epoch": 0.36374133949191684,
      "grad_norm": 8.014111518859863,
      "learning_rate": 0.00017575057736720555,
      "loss": 3.0187,
      "step": 630
    },
    {
      "epoch": 0.3695150115473441,
      "grad_norm": 8.073080062866211,
      "learning_rate": 0.00017536566589684374,
      "loss": 3.1393,
      "step": 640
    },
    {
      "epoch": 0.37528868360277134,
      "grad_norm": 10.870834350585938,
      "learning_rate": 0.0001749807544264819,
      "loss": 3.0188,
      "step": 650
    },
    {
      "epoch": 0.3810623556581986,
      "grad_norm": 9.344916343688965,
      "learning_rate": 0.0001745958429561201,
      "loss": 3.0437,
      "step": 660
    },
    {
      "epoch": 0.38683602771362585,
      "grad_norm": 8.113601684570312,
      "learning_rate": 0.0001742109314857583,
      "loss": 3.106,
      "step": 670
    },
    {
      "epoch": 0.39260969976905313,
      "grad_norm": 10.25108814239502,
      "learning_rate": 0.00017382602001539648,
      "loss": 3.2052,
      "step": 680
    },
    {
      "epoch": 0.39838337182448036,
      "grad_norm": 9.311566352844238,
      "learning_rate": 0.00017344110854503465,
      "loss": 3.1162,
      "step": 690
    },
    {
      "epoch": 0.40415704387990764,
      "grad_norm": 14.020576477050781,
      "learning_rate": 0.00017305619707467284,
      "loss": 3.0816,
      "step": 700
    },
    {
      "epoch": 0.40993071593533487,
      "grad_norm": 10.787164688110352,
      "learning_rate": 0.000172671285604311,
      "loss": 3.1705,
      "step": 710
    },
    {
      "epoch": 0.41570438799076215,
      "grad_norm": 9.375713348388672,
      "learning_rate": 0.0001722863741339492,
      "loss": 3.2293,
      "step": 720
    },
    {
      "epoch": 0.4214780600461894,
      "grad_norm": 10.990602493286133,
      "learning_rate": 0.00017190146266358737,
      "loss": 2.8857,
      "step": 730
    },
    {
      "epoch": 0.42725173210161665,
      "grad_norm": 8.547743797302246,
      "learning_rate": 0.00017151655119322556,
      "loss": 3.0561,
      "step": 740
    },
    {
      "epoch": 0.4330254041570439,
      "grad_norm": 10.058903694152832,
      "learning_rate": 0.00017113163972286375,
      "loss": 2.7991,
      "step": 750
    },
    {
      "epoch": 0.4387990762124711,
      "grad_norm": 9.026091575622559,
      "learning_rate": 0.00017074672825250194,
      "loss": 3.0996,
      "step": 760
    },
    {
      "epoch": 0.4445727482678984,
      "grad_norm": 8.321759223937988,
      "learning_rate": 0.0001703618167821401,
      "loss": 3.2741,
      "step": 770
    },
    {
      "epoch": 0.4503464203233256,
      "grad_norm": 9.777701377868652,
      "learning_rate": 0.0001699769053117783,
      "loss": 3.1041,
      "step": 780
    },
    {
      "epoch": 0.4561200923787529,
      "grad_norm": 10.58922004699707,
      "learning_rate": 0.0001695919938414165,
      "loss": 3.149,
      "step": 790
    },
    {
      "epoch": 0.4618937644341801,
      "grad_norm": 7.897780418395996,
      "learning_rate": 0.00016920708237105466,
      "loss": 2.866,
      "step": 800
    },
    {
      "epoch": 0.4676674364896074,
      "grad_norm": 9.255369186401367,
      "learning_rate": 0.00016882217090069285,
      "loss": 3.1137,
      "step": 810
    },
    {
      "epoch": 0.47344110854503463,
      "grad_norm": 13.70384407043457,
      "learning_rate": 0.00016843725943033105,
      "loss": 3.0324,
      "step": 820
    },
    {
      "epoch": 0.4792147806004619,
      "grad_norm": 8.303434371948242,
      "learning_rate": 0.00016805234795996924,
      "loss": 3.0504,
      "step": 830
    },
    {
      "epoch": 0.48498845265588914,
      "grad_norm": 9.037691116333008,
      "learning_rate": 0.00016766743648960738,
      "loss": 2.929,
      "step": 840
    },
    {
      "epoch": 0.4907621247113164,
      "grad_norm": 9.314058303833008,
      "learning_rate": 0.00016728252501924557,
      "loss": 3.1306,
      "step": 850
    },
    {
      "epoch": 0.49653579676674364,
      "grad_norm": 9.7821044921875,
      "learning_rate": 0.00016689761354888376,
      "loss": 3.0,
      "step": 860
    },
    {
      "epoch": 0.5023094688221709,
      "grad_norm": 9.12315845489502,
      "learning_rate": 0.00016651270207852195,
      "loss": 2.9745,
      "step": 870
    },
    {
      "epoch": 0.5080831408775982,
      "grad_norm": 9.299561500549316,
      "learning_rate": 0.00016612779060816012,
      "loss": 3.0482,
      "step": 880
    },
    {
      "epoch": 0.5138568129330254,
      "grad_norm": 8.13341999053955,
      "learning_rate": 0.0001657428791377983,
      "loss": 3.0078,
      "step": 890
    },
    {
      "epoch": 0.5196304849884527,
      "grad_norm": 8.452197074890137,
      "learning_rate": 0.0001653579676674365,
      "loss": 2.8831,
      "step": 900
    },
    {
      "epoch": 0.5254041570438799,
      "grad_norm": 8.27440071105957,
      "learning_rate": 0.00016497305619707467,
      "loss": 3.0739,
      "step": 910
    },
    {
      "epoch": 0.5311778290993071,
      "grad_norm": 8.096866607666016,
      "learning_rate": 0.00016458814472671286,
      "loss": 2.9157,
      "step": 920
    },
    {
      "epoch": 0.5369515011547344,
      "grad_norm": 8.405878067016602,
      "learning_rate": 0.00016420323325635106,
      "loss": 2.9484,
      "step": 930
    },
    {
      "epoch": 0.5427251732101617,
      "grad_norm": 9.843050003051758,
      "learning_rate": 0.00016381832178598925,
      "loss": 2.96,
      "step": 940
    },
    {
      "epoch": 0.5484988452655889,
      "grad_norm": 9.297737121582031,
      "learning_rate": 0.00016343341031562741,
      "loss": 3.112,
      "step": 950
    },
    {
      "epoch": 0.5542725173210161,
      "grad_norm": 7.888749599456787,
      "learning_rate": 0.0001630484988452656,
      "loss": 2.7901,
      "step": 960
    },
    {
      "epoch": 0.5600461893764435,
      "grad_norm": 9.360055923461914,
      "learning_rate": 0.00016266358737490377,
      "loss": 2.9091,
      "step": 970
    },
    {
      "epoch": 0.5658198614318707,
      "grad_norm": 8.994209289550781,
      "learning_rate": 0.00016227867590454196,
      "loss": 2.912,
      "step": 980
    },
    {
      "epoch": 0.5715935334872979,
      "grad_norm": 8.85750675201416,
      "learning_rate": 0.00016189376443418013,
      "loss": 2.9235,
      "step": 990
    },
    {
      "epoch": 0.5773672055427251,
      "grad_norm": 8.940287590026855,
      "learning_rate": 0.00016150885296381832,
      "loss": 2.911,
      "step": 1000
    },
    {
      "epoch": 0.5831408775981525,
      "grad_norm": 8.587447166442871,
      "learning_rate": 0.00016112394149345652,
      "loss": 2.9678,
      "step": 1010
    },
    {
      "epoch": 0.5889145496535797,
      "grad_norm": 8.103964805603027,
      "learning_rate": 0.00016073903002309468,
      "loss": 2.9688,
      "step": 1020
    },
    {
      "epoch": 0.5946882217090069,
      "grad_norm": 10.637654304504395,
      "learning_rate": 0.00016035411855273287,
      "loss": 2.8337,
      "step": 1030
    },
    {
      "epoch": 0.6004618937644342,
      "grad_norm": 10.733487129211426,
      "learning_rate": 0.00015996920708237107,
      "loss": 3.0565,
      "step": 1040
    },
    {
      "epoch": 0.6062355658198614,
      "grad_norm": 7.658048152923584,
      "learning_rate": 0.00015958429561200926,
      "loss": 2.8844,
      "step": 1050
    },
    {
      "epoch": 0.6120092378752887,
      "grad_norm": 7.902583122253418,
      "learning_rate": 0.00015919938414164742,
      "loss": 2.8983,
      "step": 1060
    },
    {
      "epoch": 0.6177829099307159,
      "grad_norm": 7.546911716461182,
      "learning_rate": 0.00015881447267128562,
      "loss": 3.0956,
      "step": 1070
    },
    {
      "epoch": 0.6235565819861432,
      "grad_norm": 7.930085182189941,
      "learning_rate": 0.0001584295612009238,
      "loss": 2.9494,
      "step": 1080
    },
    {
      "epoch": 0.6293302540415704,
      "grad_norm": 8.84947681427002,
      "learning_rate": 0.000158044649730562,
      "loss": 2.6959,
      "step": 1090
    },
    {
      "epoch": 0.6351039260969977,
      "grad_norm": 7.434804916381836,
      "learning_rate": 0.00015765973826020014,
      "loss": 2.9805,
      "step": 1100
    },
    {
      "epoch": 0.640877598152425,
      "grad_norm": 9.552919387817383,
      "learning_rate": 0.00015727482678983833,
      "loss": 3.0191,
      "step": 1110
    },
    {
      "epoch": 0.6466512702078522,
      "grad_norm": 9.131082534790039,
      "learning_rate": 0.00015688991531947653,
      "loss": 2.9583,
      "step": 1120
    },
    {
      "epoch": 0.6524249422632794,
      "grad_norm": 10.913092613220215,
      "learning_rate": 0.00015650500384911472,
      "loss": 2.7228,
      "step": 1130
    },
    {
      "epoch": 0.6581986143187067,
      "grad_norm": 10.575553894042969,
      "learning_rate": 0.00015612009237875288,
      "loss": 2.862,
      "step": 1140
    },
    {
      "epoch": 0.663972286374134,
      "grad_norm": 9.00633716583252,
      "learning_rate": 0.00015573518090839108,
      "loss": 2.8878,
      "step": 1150
    },
    {
      "epoch": 0.6697459584295612,
      "grad_norm": 8.720050811767578,
      "learning_rate": 0.00015535026943802927,
      "loss": 2.887,
      "step": 1160
    },
    {
      "epoch": 0.6755196304849884,
      "grad_norm": 8.383231163024902,
      "learning_rate": 0.00015496535796766743,
      "loss": 2.7932,
      "step": 1170
    },
    {
      "epoch": 0.6812933025404158,
      "grad_norm": 8.585256576538086,
      "learning_rate": 0.00015458044649730563,
      "loss": 2.8003,
      "step": 1180
    },
    {
      "epoch": 0.687066974595843,
      "grad_norm": 8.998056411743164,
      "learning_rate": 0.00015419553502694382,
      "loss": 2.9106,
      "step": 1190
    },
    {
      "epoch": 0.6928406466512702,
      "grad_norm": 8.479927062988281,
      "learning_rate": 0.000153810623556582,
      "loss": 2.7588,
      "step": 1200
    },
    {
      "epoch": 0.6986143187066974,
      "grad_norm": 9.079183578491211,
      "learning_rate": 0.00015342571208622018,
      "loss": 2.9337,
      "step": 1210
    },
    {
      "epoch": 0.7043879907621247,
      "grad_norm": 8.46640396118164,
      "learning_rate": 0.00015304080061585837,
      "loss": 2.7174,
      "step": 1220
    },
    {
      "epoch": 0.710161662817552,
      "grad_norm": 9.807114601135254,
      "learning_rate": 0.00015265588914549654,
      "loss": 2.853,
      "step": 1230
    },
    {
      "epoch": 0.7159353348729792,
      "grad_norm": 9.31185245513916,
      "learning_rate": 0.00015227097767513473,
      "loss": 2.8017,
      "step": 1240
    },
    {
      "epoch": 0.7217090069284064,
      "grad_norm": 9.316097259521484,
      "learning_rate": 0.0001518860662047729,
      "loss": 3.0512,
      "step": 1250
    },
    {
      "epoch": 0.7274826789838337,
      "grad_norm": 7.982548236846924,
      "learning_rate": 0.0001515011547344111,
      "loss": 2.6752,
      "step": 1260
    },
    {
      "epoch": 0.733256351039261,
      "grad_norm": 8.987900733947754,
      "learning_rate": 0.00015111624326404928,
      "loss": 2.7623,
      "step": 1270
    },
    {
      "epoch": 0.7390300230946882,
      "grad_norm": 10.237798690795898,
      "learning_rate": 0.00015073133179368745,
      "loss": 2.9314,
      "step": 1280
    },
    {
      "epoch": 0.7448036951501155,
      "grad_norm": 13.888945579528809,
      "learning_rate": 0.00015034642032332564,
      "loss": 2.9313,
      "step": 1290
    },
    {
      "epoch": 0.7505773672055427,
      "grad_norm": 9.215217590332031,
      "learning_rate": 0.00014996150885296383,
      "loss": 2.7886,
      "step": 1300
    },
    {
      "epoch": 0.75635103926097,
      "grad_norm": 11.31294059753418,
      "learning_rate": 0.00014957659738260202,
      "loss": 2.6314,
      "step": 1310
    },
    {
      "epoch": 0.7621247113163973,
      "grad_norm": 9.509126663208008,
      "learning_rate": 0.0001491916859122402,
      "loss": 2.874,
      "step": 1320
    },
    {
      "epoch": 0.7678983833718245,
      "grad_norm": 9.685349464416504,
      "learning_rate": 0.00014880677444187838,
      "loss": 2.7729,
      "step": 1330
    },
    {
      "epoch": 0.7736720554272517,
      "grad_norm": 9.850232124328613,
      "learning_rate": 0.00014842186297151657,
      "loss": 2.8315,
      "step": 1340
    },
    {
      "epoch": 0.7794457274826789,
      "grad_norm": 8.744932174682617,
      "learning_rate": 0.00014803695150115474,
      "loss": 2.7662,
      "step": 1350
    },
    {
      "epoch": 0.7852193995381063,
      "grad_norm": 7.939655303955078,
      "learning_rate": 0.0001476520400307929,
      "loss": 2.7593,
      "step": 1360
    },
    {
      "epoch": 0.7909930715935335,
      "grad_norm": 8.76732349395752,
      "learning_rate": 0.0001472671285604311,
      "loss": 2.8323,
      "step": 1370
    },
    {
      "epoch": 0.7967667436489607,
      "grad_norm": 7.93919563293457,
      "learning_rate": 0.0001468822170900693,
      "loss": 2.7808,
      "step": 1380
    },
    {
      "epoch": 0.8025404157043879,
      "grad_norm": 7.96236515045166,
      "learning_rate": 0.00014649730561970748,
      "loss": 2.6179,
      "step": 1390
    },
    {
      "epoch": 0.8083140877598153,
      "grad_norm": 10.287138938903809,
      "learning_rate": 0.00014611239414934565,
      "loss": 2.4996,
      "step": 1400
    },
    {
      "epoch": 0.8140877598152425,
      "grad_norm": 8.51200008392334,
      "learning_rate": 0.00014572748267898384,
      "loss": 2.8944,
      "step": 1410
    },
    {
      "epoch": 0.8198614318706697,
      "grad_norm": 8.609482765197754,
      "learning_rate": 0.00014534257120862203,
      "loss": 2.6742,
      "step": 1420
    },
    {
      "epoch": 0.825635103926097,
      "grad_norm": 9.596278190612793,
      "learning_rate": 0.0001449576597382602,
      "loss": 2.7158,
      "step": 1430
    },
    {
      "epoch": 0.8314087759815243,
      "grad_norm": 8.424407005310059,
      "learning_rate": 0.0001445727482678984,
      "loss": 2.7521,
      "step": 1440
    },
    {
      "epoch": 0.8371824480369515,
      "grad_norm": 8.032039642333984,
      "learning_rate": 0.00014418783679753658,
      "loss": 2.8017,
      "step": 1450
    },
    {
      "epoch": 0.8429561200923787,
      "grad_norm": 8.772943496704102,
      "learning_rate": 0.00014380292532717478,
      "loss": 2.7348,
      "step": 1460
    },
    {
      "epoch": 0.848729792147806,
      "grad_norm": 8.333553314208984,
      "learning_rate": 0.00014341801385681294,
      "loss": 2.7912,
      "step": 1470
    },
    {
      "epoch": 0.8545034642032333,
      "grad_norm": 9.284392356872559,
      "learning_rate": 0.0001430331023864511,
      "loss": 2.6487,
      "step": 1480
    },
    {
      "epoch": 0.8602771362586605,
      "grad_norm": 8.720118522644043,
      "learning_rate": 0.0001426481909160893,
      "loss": 2.882,
      "step": 1490
    },
    {
      "epoch": 0.8660508083140878,
      "grad_norm": 8.868083953857422,
      "learning_rate": 0.0001422632794457275,
      "loss": 2.7102,
      "step": 1500
    },
    {
      "epoch": 0.871824480369515,
      "grad_norm": 8.957069396972656,
      "learning_rate": 0.00014187836797536566,
      "loss": 2.6844,
      "step": 1510
    },
    {
      "epoch": 0.8775981524249422,
      "grad_norm": 8.738401412963867,
      "learning_rate": 0.00014149345650500385,
      "loss": 2.6647,
      "step": 1520
    },
    {
      "epoch": 0.8833718244803695,
      "grad_norm": 8.677921295166016,
      "learning_rate": 0.00014110854503464204,
      "loss": 2.7292,
      "step": 1530
    },
    {
      "epoch": 0.8891454965357968,
      "grad_norm": 7.464322566986084,
      "learning_rate": 0.0001407236335642802,
      "loss": 2.7269,
      "step": 1540
    },
    {
      "epoch": 0.894919168591224,
      "grad_norm": 9.271146774291992,
      "learning_rate": 0.0001403387220939184,
      "loss": 2.5503,
      "step": 1550
    },
    {
      "epoch": 0.9006928406466512,
      "grad_norm": 9.176734924316406,
      "learning_rate": 0.0001399538106235566,
      "loss": 2.9021,
      "step": 1560
    },
    {
      "epoch": 0.9064665127020786,
      "grad_norm": 8.316612243652344,
      "learning_rate": 0.0001395688991531948,
      "loss": 2.8925,
      "step": 1570
    },
    {
      "epoch": 0.9122401847575058,
      "grad_norm": 9.754244804382324,
      "learning_rate": 0.00013918398768283295,
      "loss": 2.7477,
      "step": 1580
    },
    {
      "epoch": 0.918013856812933,
      "grad_norm": 8.789291381835938,
      "learning_rate": 0.00013879907621247115,
      "loss": 2.7109,
      "step": 1590
    },
    {
      "epoch": 0.9237875288683602,
      "grad_norm": 7.795121669769287,
      "learning_rate": 0.00013841416474210934,
      "loss": 2.5629,
      "step": 1600
    },
    {
      "epoch": 0.9295612009237876,
      "grad_norm": 8.099385261535645,
      "learning_rate": 0.0001380292532717475,
      "loss": 2.7569,
      "step": 1610
    },
    {
      "epoch": 0.9353348729792148,
      "grad_norm": 9.178093910217285,
      "learning_rate": 0.00013764434180138567,
      "loss": 2.7288,
      "step": 1620
    },
    {
      "epoch": 0.941108545034642,
      "grad_norm": 10.113588333129883,
      "learning_rate": 0.00013725943033102386,
      "loss": 2.6031,
      "step": 1630
    },
    {
      "epoch": 0.9468822170900693,
      "grad_norm": 9.05881404876709,
      "learning_rate": 0.00013687451886066205,
      "loss": 2.4539,
      "step": 1640
    },
    {
      "epoch": 0.9526558891454965,
      "grad_norm": 9.167428970336914,
      "learning_rate": 0.00013648960739030025,
      "loss": 2.7978,
      "step": 1650
    },
    {
      "epoch": 0.9584295612009238,
      "grad_norm": 9.07043743133545,
      "learning_rate": 0.0001361046959199384,
      "loss": 2.5501,
      "step": 1660
    },
    {
      "epoch": 0.964203233256351,
      "grad_norm": 11.918726921081543,
      "learning_rate": 0.0001357197844495766,
      "loss": 2.88,
      "step": 1670
    },
    {
      "epoch": 0.9699769053117783,
      "grad_norm": 7.876969337463379,
      "learning_rate": 0.0001353348729792148,
      "loss": 2.6113,
      "step": 1680
    },
    {
      "epoch": 0.9757505773672055,
      "grad_norm": 10.895013809204102,
      "learning_rate": 0.00013494996150885296,
      "loss": 2.5815,
      "step": 1690
    },
    {
      "epoch": 0.9815242494226328,
      "grad_norm": 7.77251672744751,
      "learning_rate": 0.00013456505003849116,
      "loss": 2.753,
      "step": 1700
    },
    {
      "epoch": 0.9872979214780601,
      "grad_norm": 7.4927802085876465,
      "learning_rate": 0.00013418013856812935,
      "loss": 2.6976,
      "step": 1710
    },
    {
      "epoch": 0.9930715935334873,
      "grad_norm": 8.56277084350586,
      "learning_rate": 0.00013379522709776754,
      "loss": 2.6333,
      "step": 1720
    },
    {
      "epoch": 0.9988452655889145,
      "grad_norm": 8.24951171875,
      "learning_rate": 0.0001334103156274057,
      "loss": 2.7264,
      "step": 1730
    },
    {
      "epoch": 1.0046189376443417,
      "grad_norm": 9.24658203125,
      "learning_rate": 0.00013302540415704387,
      "loss": 2.4829,
      "step": 1740
    },
    {
      "epoch": 1.010392609699769,
      "grad_norm": 9.16214370727539,
      "learning_rate": 0.00013264049268668206,
      "loss": 2.5877,
      "step": 1750
    },
    {
      "epoch": 1.0161662817551964,
      "grad_norm": 7.430916786193848,
      "learning_rate": 0.00013225558121632026,
      "loss": 2.6221,
      "step": 1760
    },
    {
      "epoch": 1.0219399538106235,
      "grad_norm": 9.413348197937012,
      "learning_rate": 0.00013187066974595842,
      "loss": 2.7271,
      "step": 1770
    },
    {
      "epoch": 1.0277136258660509,
      "grad_norm": 9.172768592834473,
      "learning_rate": 0.00013148575827559662,
      "loss": 2.8021,
      "step": 1780
    },
    {
      "epoch": 1.033487297921478,
      "grad_norm": 8.87837028503418,
      "learning_rate": 0.0001311008468052348,
      "loss": 2.7112,
      "step": 1790
    },
    {
      "epoch": 1.0392609699769053,
      "grad_norm": 9.664189338684082,
      "learning_rate": 0.00013071593533487297,
      "loss": 2.5397,
      "step": 1800
    },
    {
      "epoch": 1.0450346420323327,
      "grad_norm": 8.77734661102295,
      "learning_rate": 0.00013033102386451117,
      "loss": 2.4567,
      "step": 1810
    },
    {
      "epoch": 1.0508083140877598,
      "grad_norm": 9.090344429016113,
      "learning_rate": 0.00012994611239414936,
      "loss": 2.6046,
      "step": 1820
    },
    {
      "epoch": 1.056581986143187,
      "grad_norm": 7.635079860687256,
      "learning_rate": 0.00012956120092378755,
      "loss": 2.4716,
      "step": 1830
    },
    {
      "epoch": 1.0623556581986142,
      "grad_norm": 9.952165603637695,
      "learning_rate": 0.00012917628945342572,
      "loss": 2.6875,
      "step": 1840
    },
    {
      "epoch": 1.0681293302540416,
      "grad_norm": 8.49537467956543,
      "learning_rate": 0.0001287913779830639,
      "loss": 2.5017,
      "step": 1850
    },
    {
      "epoch": 1.073903002309469,
      "grad_norm": 7.781197547912598,
      "learning_rate": 0.0001284064665127021,
      "loss": 2.4388,
      "step": 1860
    },
    {
      "epoch": 1.079676674364896,
      "grad_norm": 8.343864440917969,
      "learning_rate": 0.00012802155504234027,
      "loss": 2.7712,
      "step": 1870
    },
    {
      "epoch": 1.0854503464203233,
      "grad_norm": 8.462321281433105,
      "learning_rate": 0.00012763664357197843,
      "loss": 2.6649,
      "step": 1880
    },
    {
      "epoch": 1.0912240184757507,
      "grad_norm": 8.992745399475098,
      "learning_rate": 0.00012725173210161663,
      "loss": 2.4892,
      "step": 1890
    },
    {
      "epoch": 1.0969976905311778,
      "grad_norm": 8.274604797363281,
      "learning_rate": 0.00012686682063125482,
      "loss": 2.6225,
      "step": 1900
    },
    {
      "epoch": 1.1027713625866051,
      "grad_norm": 9.759482383728027,
      "learning_rate": 0.00012648190916089298,
      "loss": 2.6145,
      "step": 1910
    },
    {
      "epoch": 1.1085450346420322,
      "grad_norm": 7.64008903503418,
      "learning_rate": 0.00012609699769053118,
      "loss": 2.6604,
      "step": 1920
    },
    {
      "epoch": 1.1143187066974596,
      "grad_norm": 8.513680458068848,
      "learning_rate": 0.00012571208622016937,
      "loss": 2.7751,
      "step": 1930
    },
    {
      "epoch": 1.120092378752887,
      "grad_norm": 8.46185302734375,
      "learning_rate": 0.00012532717474980756,
      "loss": 2.5044,
      "step": 1940
    },
    {
      "epoch": 1.125866050808314,
      "grad_norm": 8.20637035369873,
      "learning_rate": 0.00012494226327944573,
      "loss": 2.6011,
      "step": 1950
    },
    {
      "epoch": 1.1316397228637414,
      "grad_norm": 9.223947525024414,
      "learning_rate": 0.00012455735180908392,
      "loss": 2.5006,
      "step": 1960
    },
    {
      "epoch": 1.1374133949191685,
      "grad_norm": 8.651692390441895,
      "learning_rate": 0.0001241724403387221,
      "loss": 2.7963,
      "step": 1970
    },
    {
      "epoch": 1.1431870669745958,
      "grad_norm": 7.7018327713012695,
      "learning_rate": 0.0001237875288683603,
      "loss": 2.5664,
      "step": 1980
    },
    {
      "epoch": 1.1489607390300232,
      "grad_norm": 7.975354194641113,
      "learning_rate": 0.00012340261739799847,
      "loss": 2.3416,
      "step": 1990
    },
    {
      "epoch": 1.1547344110854503,
      "grad_norm": 11.210472106933594,
      "learning_rate": 0.00012301770592763664,
      "loss": 2.2884,
      "step": 2000
    },
    {
      "epoch": 1.1605080831408776,
      "grad_norm": 11.276636123657227,
      "learning_rate": 0.00012263279445727483,
      "loss": 2.5278,
      "step": 2010
    },
    {
      "epoch": 1.1662817551963047,
      "grad_norm": 8.560005187988281,
      "learning_rate": 0.00012224788298691302,
      "loss": 2.7721,
      "step": 2020
    },
    {
      "epoch": 1.172055427251732,
      "grad_norm": 9.169719696044922,
      "learning_rate": 0.0001218629715165512,
      "loss": 2.675,
      "step": 2030
    },
    {
      "epoch": 1.1778290993071594,
      "grad_norm": 7.831772804260254,
      "learning_rate": 0.00012147806004618938,
      "loss": 2.5532,
      "step": 2040
    },
    {
      "epoch": 1.1836027713625865,
      "grad_norm": 9.21294116973877,
      "learning_rate": 0.00012109314857582756,
      "loss": 2.8124,
      "step": 2050
    },
    {
      "epoch": 1.1893764434180139,
      "grad_norm": 9.069365501403809,
      "learning_rate": 0.00012070823710546575,
      "loss": 2.368,
      "step": 2060
    },
    {
      "epoch": 1.1951501154734412,
      "grad_norm": 9.117142677307129,
      "learning_rate": 0.00012032332563510393,
      "loss": 2.6958,
      "step": 2070
    },
    {
      "epoch": 1.2009237875288683,
      "grad_norm": 8.541726112365723,
      "learning_rate": 0.00011993841416474212,
      "loss": 2.4936,
      "step": 2080
    },
    {
      "epoch": 1.2066974595842956,
      "grad_norm": 8.449944496154785,
      "learning_rate": 0.0001195535026943803,
      "loss": 2.6342,
      "step": 2090
    },
    {
      "epoch": 1.212471131639723,
      "grad_norm": 10.706743240356445,
      "learning_rate": 0.0001191685912240185,
      "loss": 2.4696,
      "step": 2100
    },
    {
      "epoch": 1.21824480369515,
      "grad_norm": 10.4627103805542,
      "learning_rate": 0.00011878367975365667,
      "loss": 2.5266,
      "step": 2110
    },
    {
      "epoch": 1.2240184757505774,
      "grad_norm": 8.203376770019531,
      "learning_rate": 0.00011839876828329485,
      "loss": 2.365,
      "step": 2120
    },
    {
      "epoch": 1.2297921478060045,
      "grad_norm": 9.964241027832031,
      "learning_rate": 0.00011801385681293302,
      "loss": 2.5524,
      "step": 2130
    },
    {
      "epoch": 1.2355658198614319,
      "grad_norm": 7.1484527587890625,
      "learning_rate": 0.00011762894534257121,
      "loss": 2.5694,
      "step": 2140
    },
    {
      "epoch": 1.2413394919168592,
      "grad_norm": 9.594197273254395,
      "learning_rate": 0.00011724403387220939,
      "loss": 2.5092,
      "step": 2150
    },
    {
      "epoch": 1.2471131639722863,
      "grad_norm": 9.199195861816406,
      "learning_rate": 0.00011685912240184758,
      "loss": 2.6855,
      "step": 2160
    },
    {
      "epoch": 1.2528868360277137,
      "grad_norm": 10.671114921569824,
      "learning_rate": 0.00011647421093148576,
      "loss": 2.3859,
      "step": 2170
    },
    {
      "epoch": 1.2586605080831408,
      "grad_norm": 9.856082916259766,
      "learning_rate": 0.00011608929946112394,
      "loss": 2.608,
      "step": 2180
    },
    {
      "epoch": 1.2644341801385681,
      "grad_norm": 8.008997917175293,
      "learning_rate": 0.00011570438799076213,
      "loss": 2.5083,
      "step": 2190
    },
    {
      "epoch": 1.2702078521939955,
      "grad_norm": 8.262028694152832,
      "learning_rate": 0.00011531947652040031,
      "loss": 2.5187,
      "step": 2200
    },
    {
      "epoch": 1.2759815242494226,
      "grad_norm": 9.666763305664062,
      "learning_rate": 0.0001149345650500385,
      "loss": 2.6975,
      "step": 2210
    },
    {
      "epoch": 1.28175519630485,
      "grad_norm": 8.094937324523926,
      "learning_rate": 0.00011454965357967668,
      "loss": 2.6347,
      "step": 2220
    },
    {
      "epoch": 1.287528868360277,
      "grad_norm": 8.258230209350586,
      "learning_rate": 0.00011416474210931488,
      "loss": 2.5827,
      "step": 2230
    },
    {
      "epoch": 1.2933025404157044,
      "grad_norm": 9.399114608764648,
      "learning_rate": 0.00011377983063895306,
      "loss": 2.4694,
      "step": 2240
    },
    {
      "epoch": 1.2990762124711317,
      "grad_norm": 9.096034049987793,
      "learning_rate": 0.00011339491916859123,
      "loss": 2.5137,
      "step": 2250
    },
    {
      "epoch": 1.3048498845265588,
      "grad_norm": 11.590204238891602,
      "learning_rate": 0.0001130100076982294,
      "loss": 2.4539,
      "step": 2260
    },
    {
      "epoch": 1.3106235565819861,
      "grad_norm": 8.845701217651367,
      "learning_rate": 0.00011262509622786759,
      "loss": 2.3259,
      "step": 2270
    },
    {
      "epoch": 1.3163972286374133,
      "grad_norm": 8.609691619873047,
      "learning_rate": 0.00011224018475750577,
      "loss": 2.5909,
      "step": 2280
    },
    {
      "epoch": 1.3221709006928406,
      "grad_norm": 9.690180778503418,
      "learning_rate": 0.00011185527328714395,
      "loss": 2.4163,
      "step": 2290
    },
    {
      "epoch": 1.327944572748268,
      "grad_norm": 9.107685089111328,
      "learning_rate": 0.00011147036181678214,
      "loss": 2.5155,
      "step": 2300
    },
    {
      "epoch": 1.3337182448036953,
      "grad_norm": 10.383421897888184,
      "learning_rate": 0.00011108545034642032,
      "loss": 2.4374,
      "step": 2310
    },
    {
      "epoch": 1.3394919168591224,
      "grad_norm": 11.470712661743164,
      "learning_rate": 0.00011070053887605852,
      "loss": 2.4945,
      "step": 2320
    },
    {
      "epoch": 1.3452655889145497,
      "grad_norm": 8.274499893188477,
      "learning_rate": 0.0001103156274056967,
      "loss": 2.4822,
      "step": 2330
    },
    {
      "epoch": 1.3510392609699768,
      "grad_norm": 8.556744575500488,
      "learning_rate": 0.00010993071593533489,
      "loss": 2.4477,
      "step": 2340
    },
    {
      "epoch": 1.3568129330254042,
      "grad_norm": 8.479736328125,
      "learning_rate": 0.00010954580446497307,
      "loss": 2.542,
      "step": 2350
    },
    {
      "epoch": 1.3625866050808315,
      "grad_norm": 9.003548622131348,
      "learning_rate": 0.00010916089299461126,
      "loss": 2.4599,
      "step": 2360
    },
    {
      "epoch": 1.3683602771362586,
      "grad_norm": 9.846277236938477,
      "learning_rate": 0.00010877598152424944,
      "loss": 2.5516,
      "step": 2370
    },
    {
      "epoch": 1.374133949191686,
      "grad_norm": 9.283596992492676,
      "learning_rate": 0.0001083910700538876,
      "loss": 2.639,
      "step": 2380
    },
    {
      "epoch": 1.379907621247113,
      "grad_norm": 9.536718368530273,
      "learning_rate": 0.00010800615858352578,
      "loss": 2.4867,
      "step": 2390
    },
    {
      "epoch": 1.3856812933025404,
      "grad_norm": 10.992901802062988,
      "learning_rate": 0.00010762124711316398,
      "loss": 2.6151,
      "step": 2400
    },
    {
      "epoch": 1.3914549653579678,
      "grad_norm": 8.508752822875977,
      "learning_rate": 0.00010723633564280215,
      "loss": 2.5848,
      "step": 2410
    },
    {
      "epoch": 1.3972286374133949,
      "grad_norm": 8.125914573669434,
      "learning_rate": 0.00010685142417244033,
      "loss": 2.4738,
      "step": 2420
    },
    {
      "epoch": 1.4030023094688222,
      "grad_norm": 8.272932052612305,
      "learning_rate": 0.00010646651270207853,
      "loss": 2.5239,
      "step": 2430
    },
    {
      "epoch": 1.4087759815242493,
      "grad_norm": 8.534735679626465,
      "learning_rate": 0.0001060816012317167,
      "loss": 2.4603,
      "step": 2440
    },
    {
      "epoch": 1.4145496535796767,
      "grad_norm": 11.45752239227295,
      "learning_rate": 0.0001056966897613549,
      "loss": 2.6728,
      "step": 2450
    },
    {
      "epoch": 1.420323325635104,
      "grad_norm": 9.143529891967773,
      "learning_rate": 0.00010531177829099308,
      "loss": 2.7604,
      "step": 2460
    },
    {
      "epoch": 1.426096997690531,
      "grad_norm": 8.86199951171875,
      "learning_rate": 0.00010492686682063127,
      "loss": 2.6303,
      "step": 2470
    },
    {
      "epoch": 1.4318706697459584,
      "grad_norm": 7.97189474105835,
      "learning_rate": 0.00010454195535026945,
      "loss": 2.5733,
      "step": 2480
    },
    {
      "epoch": 1.4376443418013856,
      "grad_norm": 9.705642700195312,
      "learning_rate": 0.00010415704387990763,
      "loss": 2.4545,
      "step": 2490
    },
    {
      "epoch": 1.443418013856813,
      "grad_norm": 8.128824234008789,
      "learning_rate": 0.00010377213240954582,
      "loss": 2.3912,
      "step": 2500
    },
    {
      "epoch": 1.4491916859122402,
      "grad_norm": 9.82921028137207,
      "learning_rate": 0.00010338722093918399,
      "loss": 2.5384,
      "step": 2510
    },
    {
      "epoch": 1.4549653579676676,
      "grad_norm": 10.30091667175293,
      "learning_rate": 0.00010300230946882216,
      "loss": 2.668,
      "step": 2520
    },
    {
      "epoch": 1.4607390300230947,
      "grad_norm": 8.223875045776367,
      "learning_rate": 0.00010261739799846036,
      "loss": 2.3846,
      "step": 2530
    },
    {
      "epoch": 1.4665127020785218,
      "grad_norm": 7.759868621826172,
      "learning_rate": 0.00010223248652809854,
      "loss": 2.5899,
      "step": 2540
    },
    {
      "epoch": 1.4722863741339491,
      "grad_norm": 9.957391738891602,
      "learning_rate": 0.00010184757505773672,
      "loss": 2.5157,
      "step": 2550
    },
    {
      "epoch": 1.4780600461893765,
      "grad_norm": 10.128931999206543,
      "learning_rate": 0.00010146266358737491,
      "loss": 2.3852,
      "step": 2560
    },
    {
      "epoch": 1.4838337182448038,
      "grad_norm": 8.879491806030273,
      "learning_rate": 0.00010107775211701309,
      "loss": 2.3563,
      "step": 2570
    },
    {
      "epoch": 1.489607390300231,
      "grad_norm": 8.664847373962402,
      "learning_rate": 0.00010069284064665128,
      "loss": 2.4579,
      "step": 2580
    },
    {
      "epoch": 1.4953810623556583,
      "grad_norm": 8.58211612701416,
      "learning_rate": 0.00010030792917628946,
      "loss": 2.6852,
      "step": 2590
    },
    {
      "epoch": 1.5011547344110854,
      "grad_norm": 9.1126708984375,
      "learning_rate": 9.992301770592765e-05,
      "loss": 2.5872,
      "step": 2600
    },
    {
      "epoch": 1.5069284064665127,
      "grad_norm": 9.745171546936035,
      "learning_rate": 9.953810623556582e-05,
      "loss": 2.5385,
      "step": 2610
    },
    {
      "epoch": 1.51270207852194,
      "grad_norm": 9.952398300170898,
      "learning_rate": 9.915319476520401e-05,
      "loss": 2.4233,
      "step": 2620
    },
    {
      "epoch": 1.5184757505773672,
      "grad_norm": 8.043560981750488,
      "learning_rate": 9.876828329484219e-05,
      "loss": 2.3663,
      "step": 2630
    },
    {
      "epoch": 1.5242494226327945,
      "grad_norm": 8.670065879821777,
      "learning_rate": 9.838337182448038e-05,
      "loss": 2.5984,
      "step": 2640
    },
    {
      "epoch": 1.5300230946882216,
      "grad_norm": 9.301400184631348,
      "learning_rate": 9.799846035411856e-05,
      "loss": 2.6078,
      "step": 2650
    },
    {
      "epoch": 1.535796766743649,
      "grad_norm": 8.358722686767578,
      "learning_rate": 9.761354888375674e-05,
      "loss": 2.4307,
      "step": 2660
    },
    {
      "epoch": 1.5415704387990763,
      "grad_norm": 8.447524070739746,
      "learning_rate": 9.722863741339492e-05,
      "loss": 2.364,
      "step": 2670
    },
    {
      "epoch": 1.5473441108545036,
      "grad_norm": 8.386157989501953,
      "learning_rate": 9.68437259430331e-05,
      "loss": 2.4334,
      "step": 2680
    },
    {
      "epoch": 1.5531177829099307,
      "grad_norm": 8.634313583374023,
      "learning_rate": 9.645881447267129e-05,
      "loss": 2.3749,
      "step": 2690
    },
    {
      "epoch": 1.5588914549653579,
      "grad_norm": 7.804632663726807,
      "learning_rate": 9.607390300230947e-05,
      "loss": 2.4698,
      "step": 2700
    },
    {
      "epoch": 1.5646651270207852,
      "grad_norm": 7.421274662017822,
      "learning_rate": 9.568899153194766e-05,
      "loss": 2.3629,
      "step": 2710
    },
    {
      "epoch": 1.5704387990762125,
      "grad_norm": 7.7409138679504395,
      "learning_rate": 9.530408006158584e-05,
      "loss": 2.4672,
      "step": 2720
    },
    {
      "epoch": 1.5762124711316399,
      "grad_norm": 9.188861846923828,
      "learning_rate": 9.491916859122403e-05,
      "loss": 2.6192,
      "step": 2730
    },
    {
      "epoch": 1.581986143187067,
      "grad_norm": 10.271522521972656,
      "learning_rate": 9.45342571208622e-05,
      "loss": 2.6067,
      "step": 2740
    },
    {
      "epoch": 1.587759815242494,
      "grad_norm": 9.008621215820312,
      "learning_rate": 9.414934565050039e-05,
      "loss": 2.5816,
      "step": 2750
    },
    {
      "epoch": 1.5935334872979214,
      "grad_norm": 8.734715461730957,
      "learning_rate": 9.376443418013857e-05,
      "loss": 2.3701,
      "step": 2760
    },
    {
      "epoch": 1.5993071593533488,
      "grad_norm": 9.12325668334961,
      "learning_rate": 9.337952270977676e-05,
      "loss": 2.6472,
      "step": 2770
    },
    {
      "epoch": 1.605080831408776,
      "grad_norm": 9.171771049499512,
      "learning_rate": 9.299461123941494e-05,
      "loss": 2.4599,
      "step": 2780
    },
    {
      "epoch": 1.6108545034642032,
      "grad_norm": 8.045584678649902,
      "learning_rate": 9.260969976905312e-05,
      "loss": 2.4966,
      "step": 2790
    },
    {
      "epoch": 1.6166281755196303,
      "grad_norm": 7.750423431396484,
      "learning_rate": 9.22247882986913e-05,
      "loss": 2.5642,
      "step": 2800
    },
    {
      "epoch": 1.6224018475750577,
      "grad_norm": 8.44428539276123,
      "learning_rate": 9.183987682832948e-05,
      "loss": 2.4299,
      "step": 2810
    },
    {
      "epoch": 1.628175519630485,
      "grad_norm": 8.725118637084961,
      "learning_rate": 9.145496535796767e-05,
      "loss": 2.3604,
      "step": 2820
    },
    {
      "epoch": 1.6339491916859123,
      "grad_norm": 9.117457389831543,
      "learning_rate": 9.107005388760585e-05,
      "loss": 2.5358,
      "step": 2830
    },
    {
      "epoch": 1.6397228637413395,
      "grad_norm": 8.892126083374023,
      "learning_rate": 9.068514241724404e-05,
      "loss": 2.5039,
      "step": 2840
    },
    {
      "epoch": 1.6454965357967666,
      "grad_norm": 9.234062194824219,
      "learning_rate": 9.030023094688222e-05,
      "loss": 2.4334,
      "step": 2850
    },
    {
      "epoch": 1.651270207852194,
      "grad_norm": 8.635969161987305,
      "learning_rate": 8.991531947652042e-05,
      "loss": 2.3196,
      "step": 2860
    },
    {
      "epoch": 1.6570438799076213,
      "grad_norm": 7.562609672546387,
      "learning_rate": 8.953040800615858e-05,
      "loss": 2.4941,
      "step": 2870
    },
    {
      "epoch": 1.6628175519630486,
      "grad_norm": 8.91931438446045,
      "learning_rate": 8.914549653579677e-05,
      "loss": 2.4425,
      "step": 2880
    },
    {
      "epoch": 1.6685912240184757,
      "grad_norm": 9.848533630371094,
      "learning_rate": 8.876058506543495e-05,
      "loss": 2.6007,
      "step": 2890
    },
    {
      "epoch": 1.674364896073903,
      "grad_norm": 9.702886581420898,
      "learning_rate": 8.837567359507313e-05,
      "loss": 2.5782,
      "step": 2900
    },
    {
      "epoch": 1.6801385681293302,
      "grad_norm": 8.030735969543457,
      "learning_rate": 8.799076212471132e-05,
      "loss": 2.4816,
      "step": 2910
    },
    {
      "epoch": 1.6859122401847575,
      "grad_norm": 7.410764217376709,
      "learning_rate": 8.76058506543495e-05,
      "loss": 2.4258,
      "step": 2920
    },
    {
      "epoch": 1.6916859122401848,
      "grad_norm": 9.572863578796387,
      "learning_rate": 8.722093918398768e-05,
      "loss": 2.6096,
      "step": 2930
    },
    {
      "epoch": 1.6974595842956122,
      "grad_norm": 9.344860076904297,
      "learning_rate": 8.683602771362586e-05,
      "loss": 2.7062,
      "step": 2940
    },
    {
      "epoch": 1.7032332563510393,
      "grad_norm": 9.307522773742676,
      "learning_rate": 8.645111624326405e-05,
      "loss": 2.4351,
      "step": 2950
    },
    {
      "epoch": 1.7090069284064664,
      "grad_norm": 9.19092845916748,
      "learning_rate": 8.606620477290223e-05,
      "loss": 2.4183,
      "step": 2960
    },
    {
      "epoch": 1.7147806004618937,
      "grad_norm": 8.944852828979492,
      "learning_rate": 8.568129330254043e-05,
      "loss": 2.4596,
      "step": 2970
    },
    {
      "epoch": 1.720554272517321,
      "grad_norm": 9.64587688446045,
      "learning_rate": 8.52963818321786e-05,
      "loss": 2.5317,
      "step": 2980
    },
    {
      "epoch": 1.7263279445727484,
      "grad_norm": 7.593053817749023,
      "learning_rate": 8.49114703618168e-05,
      "loss": 2.4811,
      "step": 2990
    },
    {
      "epoch": 1.7321016166281755,
      "grad_norm": 9.21543025970459,
      "learning_rate": 8.452655889145496e-05,
      "loss": 2.4621,
      "step": 3000
    },
    {
      "epoch": 1.7378752886836026,
      "grad_norm": 7.785656452178955,
      "learning_rate": 8.414164742109316e-05,
      "loss": 2.2348,
      "step": 3010
    },
    {
      "epoch": 1.74364896073903,
      "grad_norm": 10.611766815185547,
      "learning_rate": 8.375673595073133e-05,
      "loss": 2.4586,
      "step": 3020
    },
    {
      "epoch": 1.7494226327944573,
      "grad_norm": 8.108084678649902,
      "learning_rate": 8.337182448036951e-05,
      "loss": 2.6478,
      "step": 3030
    },
    {
      "epoch": 1.7551963048498846,
      "grad_norm": 7.685645580291748,
      "learning_rate": 8.29869130100077e-05,
      "loss": 2.4134,
      "step": 3040
    },
    {
      "epoch": 1.7609699769053118,
      "grad_norm": 8.310053825378418,
      "learning_rate": 8.260200153964589e-05,
      "loss": 2.5307,
      "step": 3050
    },
    {
      "epoch": 1.7667436489607389,
      "grad_norm": 8.624102592468262,
      "learning_rate": 8.221709006928406e-05,
      "loss": 2.5827,
      "step": 3060
    },
    {
      "epoch": 1.7725173210161662,
      "grad_norm": 8.922110557556152,
      "learning_rate": 8.183217859892224e-05,
      "loss": 2.402,
      "step": 3070
    },
    {
      "epoch": 1.7782909930715936,
      "grad_norm": 8.209001541137695,
      "learning_rate": 8.144726712856044e-05,
      "loss": 2.5383,
      "step": 3080
    },
    {
      "epoch": 1.7840646651270209,
      "grad_norm": 8.551614761352539,
      "learning_rate": 8.106235565819862e-05,
      "loss": 2.3791,
      "step": 3090
    },
    {
      "epoch": 1.789838337182448,
      "grad_norm": 7.614059925079346,
      "learning_rate": 8.067744418783681e-05,
      "loss": 2.3868,
      "step": 3100
    },
    {
      "epoch": 1.7956120092378753,
      "grad_norm": 9.275861740112305,
      "learning_rate": 8.029253271747499e-05,
      "loss": 2.4859,
      "step": 3110
    },
    {
      "epoch": 1.8013856812933025,
      "grad_norm": 8.018239974975586,
      "learning_rate": 7.990762124711317e-05,
      "loss": 2.4015,
      "step": 3120
    },
    {
      "epoch": 1.8071593533487298,
      "grad_norm": 9.538363456726074,
      "learning_rate": 7.952270977675135e-05,
      "loss": 2.4717,
      "step": 3130
    },
    {
      "epoch": 1.8129330254041571,
      "grad_norm": 8.857864379882812,
      "learning_rate": 7.913779830638954e-05,
      "loss": 2.3504,
      "step": 3140
    },
    {
      "epoch": 1.8187066974595842,
      "grad_norm": 8.614221572875977,
      "learning_rate": 7.875288683602772e-05,
      "loss": 2.4247,
      "step": 3150
    },
    {
      "epoch": 1.8244803695150116,
      "grad_norm": 9.439332962036133,
      "learning_rate": 7.83679753656659e-05,
      "loss": 2.393,
      "step": 3160
    },
    {
      "epoch": 1.8302540415704387,
      "grad_norm": 8.581133842468262,
      "learning_rate": 7.798306389530409e-05,
      "loss": 2.4899,
      "step": 3170
    },
    {
      "epoch": 1.836027713625866,
      "grad_norm": 9.336601257324219,
      "learning_rate": 7.759815242494227e-05,
      "loss": 2.2805,
      "step": 3180
    },
    {
      "epoch": 1.8418013856812934,
      "grad_norm": 8.409008026123047,
      "learning_rate": 7.721324095458045e-05,
      "loss": 2.2994,
      "step": 3190
    },
    {
      "epoch": 1.8475750577367207,
      "grad_norm": 10.058119773864746,
      "learning_rate": 7.682832948421863e-05,
      "loss": 2.6576,
      "step": 3200
    },
    {
      "epoch": 1.8533487297921478,
      "grad_norm": 9.045417785644531,
      "learning_rate": 7.644341801385682e-05,
      "loss": 2.4462,
      "step": 3210
    },
    {
      "epoch": 1.859122401847575,
      "grad_norm": 7.002538204193115,
      "learning_rate": 7.6058506543495e-05,
      "loss": 2.2573,
      "step": 3220
    },
    {
      "epoch": 1.8648960739030023,
      "grad_norm": 9.247703552246094,
      "learning_rate": 7.567359507313319e-05,
      "loss": 2.5066,
      "step": 3230
    },
    {
      "epoch": 1.8706697459584296,
      "grad_norm": 8.77525520324707,
      "learning_rate": 7.528868360277137e-05,
      "loss": 2.403,
      "step": 3240
    },
    {
      "epoch": 1.876443418013857,
      "grad_norm": 8.448882102966309,
      "learning_rate": 7.490377213240955e-05,
      "loss": 2.4247,
      "step": 3250
    },
    {
      "epoch": 1.882217090069284,
      "grad_norm": 8.857186317443848,
      "learning_rate": 7.451886066204773e-05,
      "loss": 2.5719,
      "step": 3260
    },
    {
      "epoch": 1.8879907621247112,
      "grad_norm": 8.03472900390625,
      "learning_rate": 7.413394919168592e-05,
      "loss": 2.5166,
      "step": 3270
    },
    {
      "epoch": 1.8937644341801385,
      "grad_norm": 9.182045936584473,
      "learning_rate": 7.37490377213241e-05,
      "loss": 2.5758,
      "step": 3280
    },
    {
      "epoch": 1.8995381062355658,
      "grad_norm": 8.128011703491211,
      "learning_rate": 7.336412625096228e-05,
      "loss": 2.5568,
      "step": 3290
    },
    {
      "epoch": 1.9053117782909932,
      "grad_norm": 9.45842170715332,
      "learning_rate": 7.297921478060047e-05,
      "loss": 2.6712,
      "step": 3300
    },
    {
      "epoch": 1.9110854503464203,
      "grad_norm": 8.580302238464355,
      "learning_rate": 7.259430331023865e-05,
      "loss": 2.5071,
      "step": 3310
    },
    {
      "epoch": 1.9168591224018474,
      "grad_norm": 9.31057071685791,
      "learning_rate": 7.220939183987683e-05,
      "loss": 2.4531,
      "step": 3320
    },
    {
      "epoch": 1.9226327944572748,
      "grad_norm": 9.699182510375977,
      "learning_rate": 7.182448036951501e-05,
      "loss": 2.3996,
      "step": 3330
    },
    {
      "epoch": 1.928406466512702,
      "grad_norm": 9.756387710571289,
      "learning_rate": 7.14395688991532e-05,
      "loss": 2.3743,
      "step": 3340
    },
    {
      "epoch": 1.9341801385681294,
      "grad_norm": 9.384932518005371,
      "learning_rate": 7.105465742879138e-05,
      "loss": 2.6275,
      "step": 3350
    },
    {
      "epoch": 1.9399538106235565,
      "grad_norm": 8.993009567260742,
      "learning_rate": 7.066974595842957e-05,
      "loss": 2.5237,
      "step": 3360
    },
    {
      "epoch": 1.9457274826789839,
      "grad_norm": 7.388720989227295,
      "learning_rate": 7.028483448806775e-05,
      "loss": 2.3147,
      "step": 3370
    },
    {
      "epoch": 1.951501154734411,
      "grad_norm": 7.584265232086182,
      "learning_rate": 6.989992301770593e-05,
      "loss": 2.4746,
      "step": 3380
    },
    {
      "epoch": 1.9572748267898383,
      "grad_norm": 8.462037086486816,
      "learning_rate": 6.951501154734411e-05,
      "loss": 2.1451,
      "step": 3390
    },
    {
      "epoch": 1.9630484988452657,
      "grad_norm": 10.200286865234375,
      "learning_rate": 6.91301000769823e-05,
      "loss": 2.3138,
      "step": 3400
    },
    {
      "epoch": 1.968822170900693,
      "grad_norm": 9.290355682373047,
      "learning_rate": 6.874518860662048e-05,
      "loss": 2.6111,
      "step": 3410
    },
    {
      "epoch": 1.9745958429561201,
      "grad_norm": 10.428789138793945,
      "learning_rate": 6.836027713625866e-05,
      "loss": 2.3738,
      "step": 3420
    },
    {
      "epoch": 1.9803695150115472,
      "grad_norm": 8.34213638305664,
      "learning_rate": 6.797536566589685e-05,
      "loss": 2.4757,
      "step": 3430
    },
    {
      "epoch": 1.9861431870669746,
      "grad_norm": 9.156305313110352,
      "learning_rate": 6.759045419553503e-05,
      "loss": 2.3634,
      "step": 3440
    },
    {
      "epoch": 1.991916859122402,
      "grad_norm": 8.370559692382812,
      "learning_rate": 6.720554272517321e-05,
      "loss": 2.3167,
      "step": 3450
    },
    {
      "epoch": 1.9976905311778292,
      "grad_norm": 8.445372581481934,
      "learning_rate": 6.682063125481139e-05,
      "loss": 2.5459,
      "step": 3460
    }
  ],
  "logging_steps": 10,
  "max_steps": 5196,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.07279258599424e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
