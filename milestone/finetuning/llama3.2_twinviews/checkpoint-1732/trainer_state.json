{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1732,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005773672055427252,
      "grad_norm": 5.394704818725586,
      "learning_rate": 0.0001996150885296382,
      "loss": 8.6714,
      "step": 10
    },
    {
      "epoch": 0.011547344110854504,
      "grad_norm": 3.4730312824249268,
      "learning_rate": 0.00019923017705927637,
      "loss": 7.2437,
      "step": 20
    },
    {
      "epoch": 0.017321016166281754,
      "grad_norm": 3.893934488296509,
      "learning_rate": 0.00019884526558891456,
      "loss": 6.465,
      "step": 30
    },
    {
      "epoch": 0.023094688221709007,
      "grad_norm": 4.380390644073486,
      "learning_rate": 0.00019846035411855275,
      "loss": 5.6951,
      "step": 40
    },
    {
      "epoch": 0.028868360277136258,
      "grad_norm": 4.522777557373047,
      "learning_rate": 0.00019807544264819095,
      "loss": 5.2066,
      "step": 50
    },
    {
      "epoch": 0.03464203233256351,
      "grad_norm": 4.840507984161377,
      "learning_rate": 0.0001976905311778291,
      "loss": 4.8981,
      "step": 60
    },
    {
      "epoch": 0.04041570438799076,
      "grad_norm": 4.835920333862305,
      "learning_rate": 0.00019730561970746728,
      "loss": 4.7627,
      "step": 70
    },
    {
      "epoch": 0.046189376443418015,
      "grad_norm": 4.378875255584717,
      "learning_rate": 0.00019692070823710547,
      "loss": 4.5861,
      "step": 80
    },
    {
      "epoch": 0.05196304849884527,
      "grad_norm": 5.315417289733887,
      "learning_rate": 0.00019653579676674366,
      "loss": 4.6405,
      "step": 90
    },
    {
      "epoch": 0.057736720554272515,
      "grad_norm": 5.264034748077393,
      "learning_rate": 0.00019615088529638183,
      "loss": 4.4227,
      "step": 100
    },
    {
      "epoch": 0.06351039260969978,
      "grad_norm": 5.934943675994873,
      "learning_rate": 0.00019576597382602002,
      "loss": 4.281,
      "step": 110
    },
    {
      "epoch": 0.06928406466512702,
      "grad_norm": 5.277578830718994,
      "learning_rate": 0.0001953810623556582,
      "loss": 4.245,
      "step": 120
    },
    {
      "epoch": 0.07505773672055427,
      "grad_norm": 6.1827311515808105,
      "learning_rate": 0.00019499615088529638,
      "loss": 4.2031,
      "step": 130
    },
    {
      "epoch": 0.08083140877598152,
      "grad_norm": 6.071866512298584,
      "learning_rate": 0.00019461123941493457,
      "loss": 4.0355,
      "step": 140
    },
    {
      "epoch": 0.08660508083140878,
      "grad_norm": 5.955367565155029,
      "learning_rate": 0.00019422632794457276,
      "loss": 4.2058,
      "step": 150
    },
    {
      "epoch": 0.09237875288683603,
      "grad_norm": 7.339695453643799,
      "learning_rate": 0.00019384141647421096,
      "loss": 3.9436,
      "step": 160
    },
    {
      "epoch": 0.09815242494226328,
      "grad_norm": 7.817234516143799,
      "learning_rate": 0.00019345650500384912,
      "loss": 4.0025,
      "step": 170
    },
    {
      "epoch": 0.10392609699769054,
      "grad_norm": 6.88603401184082,
      "learning_rate": 0.00019307159353348731,
      "loss": 3.7127,
      "step": 180
    },
    {
      "epoch": 0.10969976905311778,
      "grad_norm": 7.098567485809326,
      "learning_rate": 0.0001926866820631255,
      "loss": 3.9622,
      "step": 190
    },
    {
      "epoch": 0.11547344110854503,
      "grad_norm": 8.652942657470703,
      "learning_rate": 0.00019230177059276367,
      "loss": 3.9132,
      "step": 200
    },
    {
      "epoch": 0.12124711316397228,
      "grad_norm": 6.916898727416992,
      "learning_rate": 0.00019191685912240184,
      "loss": 3.8211,
      "step": 210
    },
    {
      "epoch": 0.12702078521939955,
      "grad_norm": 7.401280879974365,
      "learning_rate": 0.00019153194765204003,
      "loss": 3.5514,
      "step": 220
    },
    {
      "epoch": 0.13279445727482678,
      "grad_norm": 8.693737983703613,
      "learning_rate": 0.00019114703618167822,
      "loss": 3.7771,
      "step": 230
    },
    {
      "epoch": 0.13856812933025403,
      "grad_norm": 7.338130950927734,
      "learning_rate": 0.00019076212471131642,
      "loss": 3.6583,
      "step": 240
    },
    {
      "epoch": 0.14434180138568128,
      "grad_norm": 8.718976020812988,
      "learning_rate": 0.00019037721324095458,
      "loss": 3.6294,
      "step": 250
    },
    {
      "epoch": 0.15011547344110854,
      "grad_norm": 7.55558967590332,
      "learning_rate": 0.00018999230177059277,
      "loss": 3.8247,
      "step": 260
    },
    {
      "epoch": 0.1558891454965358,
      "grad_norm": 7.241472244262695,
      "learning_rate": 0.00018960739030023097,
      "loss": 3.5355,
      "step": 270
    },
    {
      "epoch": 0.16166281755196305,
      "grad_norm": 6.417301654815674,
      "learning_rate": 0.00018922247882986913,
      "loss": 3.411,
      "step": 280
    },
    {
      "epoch": 0.1674364896073903,
      "grad_norm": 8.2581205368042,
      "learning_rate": 0.00018883756735950732,
      "loss": 3.5768,
      "step": 290
    },
    {
      "epoch": 0.17321016166281755,
      "grad_norm": 8.448038101196289,
      "learning_rate": 0.00018845265588914552,
      "loss": 3.565,
      "step": 300
    },
    {
      "epoch": 0.1789838337182448,
      "grad_norm": 8.656046867370605,
      "learning_rate": 0.0001880677444187837,
      "loss": 3.5709,
      "step": 310
    },
    {
      "epoch": 0.18475750577367206,
      "grad_norm": 7.887160301208496,
      "learning_rate": 0.00018768283294842188,
      "loss": 3.685,
      "step": 320
    },
    {
      "epoch": 0.1905311778290993,
      "grad_norm": 6.965436935424805,
      "learning_rate": 0.00018729792147806004,
      "loss": 3.5234,
      "step": 330
    },
    {
      "epoch": 0.19630484988452657,
      "grad_norm": 8.347277641296387,
      "learning_rate": 0.00018691301000769823,
      "loss": 3.6024,
      "step": 340
    },
    {
      "epoch": 0.20207852193995382,
      "grad_norm": 7.537250518798828,
      "learning_rate": 0.00018652809853733643,
      "loss": 3.6247,
      "step": 350
    },
    {
      "epoch": 0.20785219399538107,
      "grad_norm": 8.786406517028809,
      "learning_rate": 0.0001861431870669746,
      "loss": 3.3722,
      "step": 360
    },
    {
      "epoch": 0.21362586605080833,
      "grad_norm": 7.203778266906738,
      "learning_rate": 0.00018575827559661278,
      "loss": 3.6391,
      "step": 370
    },
    {
      "epoch": 0.21939953810623555,
      "grad_norm": 9.22874927520752,
      "learning_rate": 0.00018537336412625098,
      "loss": 3.5066,
      "step": 380
    },
    {
      "epoch": 0.2251732101616628,
      "grad_norm": 8.191043853759766,
      "learning_rate": 0.00018498845265588914,
      "loss": 3.4407,
      "step": 390
    },
    {
      "epoch": 0.23094688221709006,
      "grad_norm": 7.641911029815674,
      "learning_rate": 0.00018460354118552733,
      "loss": 3.1996,
      "step": 400
    },
    {
      "epoch": 0.23672055427251731,
      "grad_norm": 9.566222190856934,
      "learning_rate": 0.00018421862971516553,
      "loss": 3.3934,
      "step": 410
    },
    {
      "epoch": 0.24249422632794457,
      "grad_norm": 7.945310115814209,
      "learning_rate": 0.00018383371824480372,
      "loss": 3.326,
      "step": 420
    },
    {
      "epoch": 0.24826789838337182,
      "grad_norm": 8.2669095993042,
      "learning_rate": 0.00018344880677444189,
      "loss": 3.5322,
      "step": 430
    },
    {
      "epoch": 0.2540415704387991,
      "grad_norm": 8.592453956604004,
      "learning_rate": 0.00018306389530408008,
      "loss": 3.3081,
      "step": 440
    },
    {
      "epoch": 0.25981524249422633,
      "grad_norm": 8.346524238586426,
      "learning_rate": 0.00018267898383371827,
      "loss": 3.3102,
      "step": 450
    },
    {
      "epoch": 0.26558891454965355,
      "grad_norm": 9.478904724121094,
      "learning_rate": 0.00018229407236335644,
      "loss": 3.4564,
      "step": 460
    },
    {
      "epoch": 0.27136258660508084,
      "grad_norm": 7.440075874328613,
      "learning_rate": 0.0001819091608929946,
      "loss": 3.2939,
      "step": 470
    },
    {
      "epoch": 0.27713625866050806,
      "grad_norm": 8.318315505981445,
      "learning_rate": 0.0001815242494226328,
      "loss": 3.2928,
      "step": 480
    },
    {
      "epoch": 0.28290993071593534,
      "grad_norm": 9.04977035522461,
      "learning_rate": 0.000181139337952271,
      "loss": 3.3172,
      "step": 490
    },
    {
      "epoch": 0.28868360277136257,
      "grad_norm": 8.041167259216309,
      "learning_rate": 0.00018075442648190918,
      "loss": 3.0484,
      "step": 500
    },
    {
      "epoch": 0.29445727482678985,
      "grad_norm": 8.6192045211792,
      "learning_rate": 0.00018036951501154735,
      "loss": 3.1508,
      "step": 510
    },
    {
      "epoch": 0.3002309468822171,
      "grad_norm": 9.185245513916016,
      "learning_rate": 0.00017998460354118554,
      "loss": 3.2003,
      "step": 520
    },
    {
      "epoch": 0.30600461893764436,
      "grad_norm": 8.312360763549805,
      "learning_rate": 0.00017959969207082373,
      "loss": 3.2038,
      "step": 530
    },
    {
      "epoch": 0.3117782909930716,
      "grad_norm": 6.778886318206787,
      "learning_rate": 0.0001792147806004619,
      "loss": 3.2793,
      "step": 540
    },
    {
      "epoch": 0.31755196304849886,
      "grad_norm": 8.312743186950684,
      "learning_rate": 0.0001788298691301001,
      "loss": 3.1832,
      "step": 550
    },
    {
      "epoch": 0.3233256351039261,
      "grad_norm": 7.591973781585693,
      "learning_rate": 0.00017844495765973828,
      "loss": 3.3254,
      "step": 560
    },
    {
      "epoch": 0.32909930715935337,
      "grad_norm": 7.106895923614502,
      "learning_rate": 0.00017806004618937647,
      "loss": 3.3196,
      "step": 570
    },
    {
      "epoch": 0.3348729792147806,
      "grad_norm": 8.881444931030273,
      "learning_rate": 0.0001776751347190146,
      "loss": 3.1369,
      "step": 580
    },
    {
      "epoch": 0.3406466512702079,
      "grad_norm": 9.960836410522461,
      "learning_rate": 0.0001772902232486528,
      "loss": 3.3956,
      "step": 590
    },
    {
      "epoch": 0.3464203233256351,
      "grad_norm": 8.207932472229004,
      "learning_rate": 0.000176905311778291,
      "loss": 3.0367,
      "step": 600
    },
    {
      "epoch": 0.35219399538106233,
      "grad_norm": 8.844594955444336,
      "learning_rate": 0.0001765204003079292,
      "loss": 3.1611,
      "step": 610
    },
    {
      "epoch": 0.3579676674364896,
      "grad_norm": 7.542003631591797,
      "learning_rate": 0.00017613548883756736,
      "loss": 3.0364,
      "step": 620
    },
    {
      "epoch": 0.36374133949191684,
      "grad_norm": 8.014111518859863,
      "learning_rate": 0.00017575057736720555,
      "loss": 3.0187,
      "step": 630
    },
    {
      "epoch": 0.3695150115473441,
      "grad_norm": 8.073080062866211,
      "learning_rate": 0.00017536566589684374,
      "loss": 3.1393,
      "step": 640
    },
    {
      "epoch": 0.37528868360277134,
      "grad_norm": 10.870834350585938,
      "learning_rate": 0.0001749807544264819,
      "loss": 3.0188,
      "step": 650
    },
    {
      "epoch": 0.3810623556581986,
      "grad_norm": 9.344916343688965,
      "learning_rate": 0.0001745958429561201,
      "loss": 3.0437,
      "step": 660
    },
    {
      "epoch": 0.38683602771362585,
      "grad_norm": 8.113601684570312,
      "learning_rate": 0.0001742109314857583,
      "loss": 3.106,
      "step": 670
    },
    {
      "epoch": 0.39260969976905313,
      "grad_norm": 10.25108814239502,
      "learning_rate": 0.00017382602001539648,
      "loss": 3.2052,
      "step": 680
    },
    {
      "epoch": 0.39838337182448036,
      "grad_norm": 9.311566352844238,
      "learning_rate": 0.00017344110854503465,
      "loss": 3.1162,
      "step": 690
    },
    {
      "epoch": 0.40415704387990764,
      "grad_norm": 14.020576477050781,
      "learning_rate": 0.00017305619707467284,
      "loss": 3.0816,
      "step": 700
    },
    {
      "epoch": 0.40993071593533487,
      "grad_norm": 10.787164688110352,
      "learning_rate": 0.000172671285604311,
      "loss": 3.1705,
      "step": 710
    },
    {
      "epoch": 0.41570438799076215,
      "grad_norm": 9.375713348388672,
      "learning_rate": 0.0001722863741339492,
      "loss": 3.2293,
      "step": 720
    },
    {
      "epoch": 0.4214780600461894,
      "grad_norm": 10.990602493286133,
      "learning_rate": 0.00017190146266358737,
      "loss": 2.8857,
      "step": 730
    },
    {
      "epoch": 0.42725173210161665,
      "grad_norm": 8.547743797302246,
      "learning_rate": 0.00017151655119322556,
      "loss": 3.0561,
      "step": 740
    },
    {
      "epoch": 0.4330254041570439,
      "grad_norm": 10.058903694152832,
      "learning_rate": 0.00017113163972286375,
      "loss": 2.7991,
      "step": 750
    },
    {
      "epoch": 0.4387990762124711,
      "grad_norm": 9.026091575622559,
      "learning_rate": 0.00017074672825250194,
      "loss": 3.0996,
      "step": 760
    },
    {
      "epoch": 0.4445727482678984,
      "grad_norm": 8.321759223937988,
      "learning_rate": 0.0001703618167821401,
      "loss": 3.2741,
      "step": 770
    },
    {
      "epoch": 0.4503464203233256,
      "grad_norm": 9.777701377868652,
      "learning_rate": 0.0001699769053117783,
      "loss": 3.1041,
      "step": 780
    },
    {
      "epoch": 0.4561200923787529,
      "grad_norm": 10.58922004699707,
      "learning_rate": 0.0001695919938414165,
      "loss": 3.149,
      "step": 790
    },
    {
      "epoch": 0.4618937644341801,
      "grad_norm": 7.897780418395996,
      "learning_rate": 0.00016920708237105466,
      "loss": 2.866,
      "step": 800
    },
    {
      "epoch": 0.4676674364896074,
      "grad_norm": 9.255369186401367,
      "learning_rate": 0.00016882217090069285,
      "loss": 3.1137,
      "step": 810
    },
    {
      "epoch": 0.47344110854503463,
      "grad_norm": 13.70384407043457,
      "learning_rate": 0.00016843725943033105,
      "loss": 3.0324,
      "step": 820
    },
    {
      "epoch": 0.4792147806004619,
      "grad_norm": 8.303434371948242,
      "learning_rate": 0.00016805234795996924,
      "loss": 3.0504,
      "step": 830
    },
    {
      "epoch": 0.48498845265588914,
      "grad_norm": 9.037691116333008,
      "learning_rate": 0.00016766743648960738,
      "loss": 2.929,
      "step": 840
    },
    {
      "epoch": 0.4907621247113164,
      "grad_norm": 9.314058303833008,
      "learning_rate": 0.00016728252501924557,
      "loss": 3.1306,
      "step": 850
    },
    {
      "epoch": 0.49653579676674364,
      "grad_norm": 9.7821044921875,
      "learning_rate": 0.00016689761354888376,
      "loss": 3.0,
      "step": 860
    },
    {
      "epoch": 0.5023094688221709,
      "grad_norm": 9.12315845489502,
      "learning_rate": 0.00016651270207852195,
      "loss": 2.9745,
      "step": 870
    },
    {
      "epoch": 0.5080831408775982,
      "grad_norm": 9.299561500549316,
      "learning_rate": 0.00016612779060816012,
      "loss": 3.0482,
      "step": 880
    },
    {
      "epoch": 0.5138568129330254,
      "grad_norm": 8.13341999053955,
      "learning_rate": 0.0001657428791377983,
      "loss": 3.0078,
      "step": 890
    },
    {
      "epoch": 0.5196304849884527,
      "grad_norm": 8.452197074890137,
      "learning_rate": 0.0001653579676674365,
      "loss": 2.8831,
      "step": 900
    },
    {
      "epoch": 0.5254041570438799,
      "grad_norm": 8.27440071105957,
      "learning_rate": 0.00016497305619707467,
      "loss": 3.0739,
      "step": 910
    },
    {
      "epoch": 0.5311778290993071,
      "grad_norm": 8.096866607666016,
      "learning_rate": 0.00016458814472671286,
      "loss": 2.9157,
      "step": 920
    },
    {
      "epoch": 0.5369515011547344,
      "grad_norm": 8.405878067016602,
      "learning_rate": 0.00016420323325635106,
      "loss": 2.9484,
      "step": 930
    },
    {
      "epoch": 0.5427251732101617,
      "grad_norm": 9.843050003051758,
      "learning_rate": 0.00016381832178598925,
      "loss": 2.96,
      "step": 940
    },
    {
      "epoch": 0.5484988452655889,
      "grad_norm": 9.297737121582031,
      "learning_rate": 0.00016343341031562741,
      "loss": 3.112,
      "step": 950
    },
    {
      "epoch": 0.5542725173210161,
      "grad_norm": 7.888749599456787,
      "learning_rate": 0.0001630484988452656,
      "loss": 2.7901,
      "step": 960
    },
    {
      "epoch": 0.5600461893764435,
      "grad_norm": 9.360055923461914,
      "learning_rate": 0.00016266358737490377,
      "loss": 2.9091,
      "step": 970
    },
    {
      "epoch": 0.5658198614318707,
      "grad_norm": 8.994209289550781,
      "learning_rate": 0.00016227867590454196,
      "loss": 2.912,
      "step": 980
    },
    {
      "epoch": 0.5715935334872979,
      "grad_norm": 8.85750675201416,
      "learning_rate": 0.00016189376443418013,
      "loss": 2.9235,
      "step": 990
    },
    {
      "epoch": 0.5773672055427251,
      "grad_norm": 8.940287590026855,
      "learning_rate": 0.00016150885296381832,
      "loss": 2.911,
      "step": 1000
    },
    {
      "epoch": 0.5831408775981525,
      "grad_norm": 8.587447166442871,
      "learning_rate": 0.00016112394149345652,
      "loss": 2.9678,
      "step": 1010
    },
    {
      "epoch": 0.5889145496535797,
      "grad_norm": 8.103964805603027,
      "learning_rate": 0.00016073903002309468,
      "loss": 2.9688,
      "step": 1020
    },
    {
      "epoch": 0.5946882217090069,
      "grad_norm": 10.637654304504395,
      "learning_rate": 0.00016035411855273287,
      "loss": 2.8337,
      "step": 1030
    },
    {
      "epoch": 0.6004618937644342,
      "grad_norm": 10.733487129211426,
      "learning_rate": 0.00015996920708237107,
      "loss": 3.0565,
      "step": 1040
    },
    {
      "epoch": 0.6062355658198614,
      "grad_norm": 7.658048152923584,
      "learning_rate": 0.00015958429561200926,
      "loss": 2.8844,
      "step": 1050
    },
    {
      "epoch": 0.6120092378752887,
      "grad_norm": 7.902583122253418,
      "learning_rate": 0.00015919938414164742,
      "loss": 2.8983,
      "step": 1060
    },
    {
      "epoch": 0.6177829099307159,
      "grad_norm": 7.546911716461182,
      "learning_rate": 0.00015881447267128562,
      "loss": 3.0956,
      "step": 1070
    },
    {
      "epoch": 0.6235565819861432,
      "grad_norm": 7.930085182189941,
      "learning_rate": 0.0001584295612009238,
      "loss": 2.9494,
      "step": 1080
    },
    {
      "epoch": 0.6293302540415704,
      "grad_norm": 8.84947681427002,
      "learning_rate": 0.000158044649730562,
      "loss": 2.6959,
      "step": 1090
    },
    {
      "epoch": 0.6351039260969977,
      "grad_norm": 7.434804916381836,
      "learning_rate": 0.00015765973826020014,
      "loss": 2.9805,
      "step": 1100
    },
    {
      "epoch": 0.640877598152425,
      "grad_norm": 9.552919387817383,
      "learning_rate": 0.00015727482678983833,
      "loss": 3.0191,
      "step": 1110
    },
    {
      "epoch": 0.6466512702078522,
      "grad_norm": 9.131082534790039,
      "learning_rate": 0.00015688991531947653,
      "loss": 2.9583,
      "step": 1120
    },
    {
      "epoch": 0.6524249422632794,
      "grad_norm": 10.913092613220215,
      "learning_rate": 0.00015650500384911472,
      "loss": 2.7228,
      "step": 1130
    },
    {
      "epoch": 0.6581986143187067,
      "grad_norm": 10.575553894042969,
      "learning_rate": 0.00015612009237875288,
      "loss": 2.862,
      "step": 1140
    },
    {
      "epoch": 0.663972286374134,
      "grad_norm": 9.00633716583252,
      "learning_rate": 0.00015573518090839108,
      "loss": 2.8878,
      "step": 1150
    },
    {
      "epoch": 0.6697459584295612,
      "grad_norm": 8.720050811767578,
      "learning_rate": 0.00015535026943802927,
      "loss": 2.887,
      "step": 1160
    },
    {
      "epoch": 0.6755196304849884,
      "grad_norm": 8.383231163024902,
      "learning_rate": 0.00015496535796766743,
      "loss": 2.7932,
      "step": 1170
    },
    {
      "epoch": 0.6812933025404158,
      "grad_norm": 8.585256576538086,
      "learning_rate": 0.00015458044649730563,
      "loss": 2.8003,
      "step": 1180
    },
    {
      "epoch": 0.687066974595843,
      "grad_norm": 8.998056411743164,
      "learning_rate": 0.00015419553502694382,
      "loss": 2.9106,
      "step": 1190
    },
    {
      "epoch": 0.6928406466512702,
      "grad_norm": 8.479927062988281,
      "learning_rate": 0.000153810623556582,
      "loss": 2.7588,
      "step": 1200
    },
    {
      "epoch": 0.6986143187066974,
      "grad_norm": 9.079183578491211,
      "learning_rate": 0.00015342571208622018,
      "loss": 2.9337,
      "step": 1210
    },
    {
      "epoch": 0.7043879907621247,
      "grad_norm": 8.46640396118164,
      "learning_rate": 0.00015304080061585837,
      "loss": 2.7174,
      "step": 1220
    },
    {
      "epoch": 0.710161662817552,
      "grad_norm": 9.807114601135254,
      "learning_rate": 0.00015265588914549654,
      "loss": 2.853,
      "step": 1230
    },
    {
      "epoch": 0.7159353348729792,
      "grad_norm": 9.31185245513916,
      "learning_rate": 0.00015227097767513473,
      "loss": 2.8017,
      "step": 1240
    },
    {
      "epoch": 0.7217090069284064,
      "grad_norm": 9.316097259521484,
      "learning_rate": 0.0001518860662047729,
      "loss": 3.0512,
      "step": 1250
    },
    {
      "epoch": 0.7274826789838337,
      "grad_norm": 7.982548236846924,
      "learning_rate": 0.0001515011547344111,
      "loss": 2.6752,
      "step": 1260
    },
    {
      "epoch": 0.733256351039261,
      "grad_norm": 8.987900733947754,
      "learning_rate": 0.00015111624326404928,
      "loss": 2.7623,
      "step": 1270
    },
    {
      "epoch": 0.7390300230946882,
      "grad_norm": 10.237798690795898,
      "learning_rate": 0.00015073133179368745,
      "loss": 2.9314,
      "step": 1280
    },
    {
      "epoch": 0.7448036951501155,
      "grad_norm": 13.888945579528809,
      "learning_rate": 0.00015034642032332564,
      "loss": 2.9313,
      "step": 1290
    },
    {
      "epoch": 0.7505773672055427,
      "grad_norm": 9.215217590332031,
      "learning_rate": 0.00014996150885296383,
      "loss": 2.7886,
      "step": 1300
    },
    {
      "epoch": 0.75635103926097,
      "grad_norm": 11.31294059753418,
      "learning_rate": 0.00014957659738260202,
      "loss": 2.6314,
      "step": 1310
    },
    {
      "epoch": 0.7621247113163973,
      "grad_norm": 9.509126663208008,
      "learning_rate": 0.0001491916859122402,
      "loss": 2.874,
      "step": 1320
    },
    {
      "epoch": 0.7678983833718245,
      "grad_norm": 9.685349464416504,
      "learning_rate": 0.00014880677444187838,
      "loss": 2.7729,
      "step": 1330
    },
    {
      "epoch": 0.7736720554272517,
      "grad_norm": 9.850232124328613,
      "learning_rate": 0.00014842186297151657,
      "loss": 2.8315,
      "step": 1340
    },
    {
      "epoch": 0.7794457274826789,
      "grad_norm": 8.744932174682617,
      "learning_rate": 0.00014803695150115474,
      "loss": 2.7662,
      "step": 1350
    },
    {
      "epoch": 0.7852193995381063,
      "grad_norm": 7.939655303955078,
      "learning_rate": 0.0001476520400307929,
      "loss": 2.7593,
      "step": 1360
    },
    {
      "epoch": 0.7909930715935335,
      "grad_norm": 8.76732349395752,
      "learning_rate": 0.0001472671285604311,
      "loss": 2.8323,
      "step": 1370
    },
    {
      "epoch": 0.7967667436489607,
      "grad_norm": 7.93919563293457,
      "learning_rate": 0.0001468822170900693,
      "loss": 2.7808,
      "step": 1380
    },
    {
      "epoch": 0.8025404157043879,
      "grad_norm": 7.96236515045166,
      "learning_rate": 0.00014649730561970748,
      "loss": 2.6179,
      "step": 1390
    },
    {
      "epoch": 0.8083140877598153,
      "grad_norm": 10.287138938903809,
      "learning_rate": 0.00014611239414934565,
      "loss": 2.4996,
      "step": 1400
    },
    {
      "epoch": 0.8140877598152425,
      "grad_norm": 8.51200008392334,
      "learning_rate": 0.00014572748267898384,
      "loss": 2.8944,
      "step": 1410
    },
    {
      "epoch": 0.8198614318706697,
      "grad_norm": 8.609482765197754,
      "learning_rate": 0.00014534257120862203,
      "loss": 2.6742,
      "step": 1420
    },
    {
      "epoch": 0.825635103926097,
      "grad_norm": 9.596278190612793,
      "learning_rate": 0.0001449576597382602,
      "loss": 2.7158,
      "step": 1430
    },
    {
      "epoch": 0.8314087759815243,
      "grad_norm": 8.424407005310059,
      "learning_rate": 0.0001445727482678984,
      "loss": 2.7521,
      "step": 1440
    },
    {
      "epoch": 0.8371824480369515,
      "grad_norm": 8.032039642333984,
      "learning_rate": 0.00014418783679753658,
      "loss": 2.8017,
      "step": 1450
    },
    {
      "epoch": 0.8429561200923787,
      "grad_norm": 8.772943496704102,
      "learning_rate": 0.00014380292532717478,
      "loss": 2.7348,
      "step": 1460
    },
    {
      "epoch": 0.848729792147806,
      "grad_norm": 8.333553314208984,
      "learning_rate": 0.00014341801385681294,
      "loss": 2.7912,
      "step": 1470
    },
    {
      "epoch": 0.8545034642032333,
      "grad_norm": 9.284392356872559,
      "learning_rate": 0.0001430331023864511,
      "loss": 2.6487,
      "step": 1480
    },
    {
      "epoch": 0.8602771362586605,
      "grad_norm": 8.720118522644043,
      "learning_rate": 0.0001426481909160893,
      "loss": 2.882,
      "step": 1490
    },
    {
      "epoch": 0.8660508083140878,
      "grad_norm": 8.868083953857422,
      "learning_rate": 0.0001422632794457275,
      "loss": 2.7102,
      "step": 1500
    },
    {
      "epoch": 0.871824480369515,
      "grad_norm": 8.957069396972656,
      "learning_rate": 0.00014187836797536566,
      "loss": 2.6844,
      "step": 1510
    },
    {
      "epoch": 0.8775981524249422,
      "grad_norm": 8.738401412963867,
      "learning_rate": 0.00014149345650500385,
      "loss": 2.6647,
      "step": 1520
    },
    {
      "epoch": 0.8833718244803695,
      "grad_norm": 8.677921295166016,
      "learning_rate": 0.00014110854503464204,
      "loss": 2.7292,
      "step": 1530
    },
    {
      "epoch": 0.8891454965357968,
      "grad_norm": 7.464322566986084,
      "learning_rate": 0.0001407236335642802,
      "loss": 2.7269,
      "step": 1540
    },
    {
      "epoch": 0.894919168591224,
      "grad_norm": 9.271146774291992,
      "learning_rate": 0.0001403387220939184,
      "loss": 2.5503,
      "step": 1550
    },
    {
      "epoch": 0.9006928406466512,
      "grad_norm": 9.176734924316406,
      "learning_rate": 0.0001399538106235566,
      "loss": 2.9021,
      "step": 1560
    },
    {
      "epoch": 0.9064665127020786,
      "grad_norm": 8.316612243652344,
      "learning_rate": 0.0001395688991531948,
      "loss": 2.8925,
      "step": 1570
    },
    {
      "epoch": 0.9122401847575058,
      "grad_norm": 9.754244804382324,
      "learning_rate": 0.00013918398768283295,
      "loss": 2.7477,
      "step": 1580
    },
    {
      "epoch": 0.918013856812933,
      "grad_norm": 8.789291381835938,
      "learning_rate": 0.00013879907621247115,
      "loss": 2.7109,
      "step": 1590
    },
    {
      "epoch": 0.9237875288683602,
      "grad_norm": 7.795121669769287,
      "learning_rate": 0.00013841416474210934,
      "loss": 2.5629,
      "step": 1600
    },
    {
      "epoch": 0.9295612009237876,
      "grad_norm": 8.099385261535645,
      "learning_rate": 0.0001380292532717475,
      "loss": 2.7569,
      "step": 1610
    },
    {
      "epoch": 0.9353348729792148,
      "grad_norm": 9.178093910217285,
      "learning_rate": 0.00013764434180138567,
      "loss": 2.7288,
      "step": 1620
    },
    {
      "epoch": 0.941108545034642,
      "grad_norm": 10.113588333129883,
      "learning_rate": 0.00013725943033102386,
      "loss": 2.6031,
      "step": 1630
    },
    {
      "epoch": 0.9468822170900693,
      "grad_norm": 9.05881404876709,
      "learning_rate": 0.00013687451886066205,
      "loss": 2.4539,
      "step": 1640
    },
    {
      "epoch": 0.9526558891454965,
      "grad_norm": 9.167428970336914,
      "learning_rate": 0.00013648960739030025,
      "loss": 2.7978,
      "step": 1650
    },
    {
      "epoch": 0.9584295612009238,
      "grad_norm": 9.07043743133545,
      "learning_rate": 0.0001361046959199384,
      "loss": 2.5501,
      "step": 1660
    },
    {
      "epoch": 0.964203233256351,
      "grad_norm": 11.918726921081543,
      "learning_rate": 0.0001357197844495766,
      "loss": 2.88,
      "step": 1670
    },
    {
      "epoch": 0.9699769053117783,
      "grad_norm": 7.876969337463379,
      "learning_rate": 0.0001353348729792148,
      "loss": 2.6113,
      "step": 1680
    },
    {
      "epoch": 0.9757505773672055,
      "grad_norm": 10.895013809204102,
      "learning_rate": 0.00013494996150885296,
      "loss": 2.5815,
      "step": 1690
    },
    {
      "epoch": 0.9815242494226328,
      "grad_norm": 7.77251672744751,
      "learning_rate": 0.00013456505003849116,
      "loss": 2.753,
      "step": 1700
    },
    {
      "epoch": 0.9872979214780601,
      "grad_norm": 7.4927802085876465,
      "learning_rate": 0.00013418013856812935,
      "loss": 2.6976,
      "step": 1710
    },
    {
      "epoch": 0.9930715935334873,
      "grad_norm": 8.56277084350586,
      "learning_rate": 0.00013379522709776754,
      "loss": 2.6333,
      "step": 1720
    },
    {
      "epoch": 0.9988452655889145,
      "grad_norm": 8.24951171875,
      "learning_rate": 0.0001334103156274057,
      "loss": 2.7264,
      "step": 1730
    }
  ],
  "logging_steps": 10,
  "max_steps": 5196,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.03639629299712e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
